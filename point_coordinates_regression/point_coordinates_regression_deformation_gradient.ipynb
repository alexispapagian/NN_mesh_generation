{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point_coordinates_regression import *\n",
    "from Deformation_gradient import *\n",
    "from math import acos,sqrt,pi\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting information about deformation gradient and barycenter\n",
    "\n",
    "def extract_barycenter_deformation_gradients(points,nb_of_points):\n",
    "    \n",
    "    ref_triangle = np.array([[-0.5,0],[0.5,0],[0,0.5*sqrt(3)]])\n",
    "\n",
    "    dX=np.array([      \n",
    "       [ref_triangle[1][0]-ref_triangle[0][0] , ref_triangle[2][0]-ref_triangle[0][0]] ,   \n",
    "       [ref_triangle[1][1]-ref_triangle[0][1] , ref_triangle[2][1]-ref_triangle[0][1]]\n",
    "                ])\n",
    "    \n",
    "    \n",
    "    deformation_gradients=[]\n",
    "    barycenters=[]\n",
    "    \n",
    "    polygons=points.reshape(len(points),nb_of_points,2)\n",
    "    \n",
    "    for polygon in polygons:\n",
    "        barycenter=np.array([polygon[:,0].sum()/nb_of_points,polygon[:,1].sum()/nb_of_points])\n",
    "        barycenter_triangles=[]\n",
    "        barycenters.append(barycenter)\n",
    "        for i in range(nb_of_points):\n",
    "            barycenter_triangles.append(np.array([barycenter,polygon[i],polygon[(i+1)%nb_of_points]]))\n",
    "        for triangle in barycenter_triangles:\n",
    "            F=get_deformation_gradient(triangle)\n",
    "            deformation_gradients.append(F)\n",
    "    barycenters=np.array(barycenters).reshape(len(points),1,2)\n",
    "    deformation_gradients=np.array(deformation_gradients).reshape(len(points),1,4*nb_of_points)\n",
    "    \n",
    "    return np.dstack([deformation_gradients,barycenters])\n",
    "\n",
    "def extract_barycenter(points,nb_of_points):\n",
    "        \n",
    "    barycenters=[]\n",
    "    \n",
    "    polygons=points.reshape(len(points),nb_of_points,2)\n",
    "    for polygon in polygons:\n",
    "        barycenter=np.array([polygon[:,0].sum()/nb_of_points,polygon[:,1].sum()/nb_of_points])\n",
    "        barycenter_triangles=[]\n",
    "        barycenters.append(barycenter)\n",
    "        for i in range(nb_of_points):\n",
    "            barycenter_triangles.append(np.array([barycenter,polygon[i],polygon[(i+1)%nb_of_points]]))\n",
    "    barycenters=np.array(barycenters).reshape(len(points),1,2)\n",
    "    \n",
    "    return np.array(barycenters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_deformation_gradients(points):\n",
    "    \n",
    "    ref_triangle = np.array([[-0.5,0],[0.5,0],[0,0.5*sqrt(3)]])\n",
    "\n",
    "    dX=np.array([      \n",
    "       [ref_triangle[1][0]-ref_triangle[0][0] , ref_triangle[2][0]-ref_triangle[0][0]] ,   \n",
    "       [ref_triangle[1][1]-ref_triangle[0][1] , ref_triangle[2][1]-ref_triangle[0][1]]\n",
    "                ])\n",
    "    \n",
    "    \n",
    "    deformation_gradients=[]\n",
    "    barycenters=[]\n",
    "    \n",
    "    points=points.reshape(len(points),3,2)\n",
    "    for triangle in points:\n",
    "        barycenter=compute_triangle_barycenter(triangle)\n",
    "        barycenter_triangles=[]\n",
    "        barycenters.append(barycenter)\n",
    "        for i in range(3):\n",
    "            barycenter_triangles.append(np.array([barycenter,triangle[i],triangle[(i+1)%3]]))\n",
    "        for triangle in barycenter_triangles:\n",
    "            F=get_deformation_gradient(triangle)\n",
    "            deformation_gradients.append(F)\n",
    "    barycenters=np.array(barycenters).reshape(len(points),1,2)\n",
    "    deformation_gradients=np.array(deformation_gradients).reshape(len(points),1,12)\n",
    "    \n",
    "    return np.array(deformation_gradients)\n",
    "\n",
    "\n",
    "\n",
    "def extract_distances_from_barycenter(polygon_set,nb_of_points):   \n",
    "    contour_barycenter_distances=[]\n",
    "    for polygon  in polygon_set:\n",
    "        distances_from_barycenter=[]\n",
    "        polygon=np.delete(polygon,2*nb_of_points).reshape(nb_of_points,2)\n",
    "        polygon_barycenter=np.array([polygon[:,0].sum()/nb_of_points,polygon[:,1].sum()/nb_of_points])\n",
    "        for polygon_point in polygon:\n",
    "            distance_from_barycenter=np.linalg.norm(polygon_point-polygon_barycenter)\n",
    "            distances_from_barycenter.append(distance_from_barycenter)\n",
    "        contour_barycenter_distances.append(distances_from_barycenter)\n",
    "    contour_barycenter_distances=np.array(contour_barycenter_distances)\n",
    "    return contour_barycenter_distances\n",
    "\n",
    "\n",
    "\n",
    "def acquire_points_from_deformation(deformgrad_barycenters,nb_of_points):\n",
    "    \n",
    "    ref_triangle = np.array([[-0.5,0],[0.5,0],[0,0.5*sqrt(3)]])\n",
    "\n",
    "    dX=np.array([      \n",
    "       [ref_triangle[1][0]-ref_triangle[0][0] , ref_triangle[2][0]-ref_triangle[0][0]] ,   \n",
    "       [ref_triangle[1][1]-ref_triangle[0][1] , ref_triangle[2][1]-ref_triangle[0][1]]\n",
    "                ])\n",
    "    \n",
    "    deformgrad_barycenters=deformgrad_barycenters.reshape(1,nb_of_points*4+2)\n",
    "    for deformation_gradients_with_barycenter in deformgrad_barycenters:\n",
    "        points=[]\n",
    "        barycenter=np.array(deformation_gradients_with_barycenter[-2:])\n",
    "        print(\"barycenter is \", barycenter)\n",
    "        deformation_gradients=np.array(deformation_gradients_with_barycenter[0:4*nb_of_points]).reshape(nb_of_points,2,2)\n",
    "        for deformation_gradient in deformation_gradients:\n",
    "            X=(np.array([barycenter,barycenter]).T+np.matmul(deformation_gradient,dX)).T\n",
    "            points.append(X)\n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def length(v):\n",
    "    return sqrt(v[0]**2+v[1]**2)\n",
    "def dot_product(v,w):\n",
    "    return v[0]*w[0]+v[1]*w[1]\n",
    "def determinant(v,w):\n",
    "    return v[0]*w[1]-v[1]*w[0]\n",
    "def inner_angle(v,w):\n",
    "    cosx=dot_product(v,w)/(length(v)*length(w))\n",
    "    rad=acos(cosx) # in radians\n",
    "    return rad*180/pi # returns degrees\n",
    "def angle_counterclockwise(A, B):\n",
    "    inner=inner_angle(A,B)\n",
    "    det = determinant(A,B)\n",
    "    if det<0: \n",
    "        return 360-inner\n",
    "    else: \n",
    "        return inner\n",
    "\n",
    "    \n",
    "    \n",
    "def sort_points(point_coordinates,nb_of_points):\n",
    "    polygon=point_coordinates.reshape(len(point_coordinates),nb_of_points,2)\n",
    "    barycenters=extract_barycenter(point_coordinates,nb_of_points)\n",
    "    angles=[]\n",
    "    polygons=point_coordinates.reshape(len(point_coordinates),nb_of_points,2)\n",
    "    vectors=polygons-barycenters\n",
    "\n",
    "    for  barycenter_vectors in vectors:\n",
    "        for vector in barycenter_vectors:\n",
    "            angles.append(angle_counterclockwise(np.array([1,0]),vector))\n",
    "                      \n",
    "    angles=np.array(angles).reshape(len(vectors),nb_of_points,1)\n",
    "    point_coordinates_with_angles=np.dstack([polygons,angles])\n",
    "    point_coordinates_sorted=[]\n",
    "    for points in point_coordinates_with_angles:\n",
    "        points_sorted=np.array(sorted(points,key=lambda x: x[2]))\n",
    "        points_sorted=points_sorted[:,0:2]\n",
    "        point_coordinates_sorted.append(points_sorted.reshape(1,nb_of_points,2))\n",
    "    return np.array(point_coordinates_sorted)    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_initial=load_dataset('12_polygons.pkl')\n",
    "point_coordinates_initial=load_dataset('12_point_coordinates_del.pkl')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points_del.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_points=10\n",
    "polygons_reshaped,point_coordinates=reshape_data(polygons_initial,point_coordinates_initial,number_of_insertion_points,nb_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polygons_reshaped.shape,point_coordinates.shape\n",
    "#\n",
    "point_coordinates=sort_points(point_coordinates,nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "\n",
    "polygons_reshaped,point_coordinates=unison_shuffled_copies(polygons_reshaped,point_coordinates)\n",
    "_,lengths,polygon_angles=extract_lengths_angles(polygons_reshaped,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 training/test data ratio\n",
    "\n",
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n",
    "nb_of_test_data,nb_of_training_data\n",
    "\n",
    "\n",
    "barycenter_distances=extract_distances_from_barycenter(polygons_reshaped,12)\n",
    "polygons_reshaped_with_angles=np.hstack([polygons_reshaped,polygon_angles])\n",
    "polygons_reshaped_with_angles_and_lengths=np.hstack([polygons_reshaped_with_angles,lengths])\n",
    "polygons_reshaped_with_angles_and_lengths_bd=np.hstack([polygons_reshaped_with_angles_and_lengths,barycenter_distances])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lengths_and_angles=np.hstack([lengths,polygon_angles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the coordinates as input\n",
    "x_tensor=torch.from_numpy(polygons_reshaped_with_angles_and_lengths_bd[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped_with_angles_and_lengths_bd[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_gradients_with_barycenters=extract_barycenter_deformation_gradients(point_coordinates,nb_of_points)\n",
    "\n",
    "#deformation_gradients_with_barycenters=deformation_gradients_with_barycenters.reshape(len(deformation_gradients_with_barycenters),1,3,4)[:,0,1]\n",
    "#deformation_gradients_with_barycenters=deformation_gradients_with_barycenters.reshape(len(deformation_gradients_with_barycenters),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output are the deformation gradients of the triagles that are formed after connecting the barycenter with each edge \n",
    "# of the polygon and the barycenter of the polygon. The polygon is formed after connecting each interior point.\n",
    "\n",
    "\n",
    "y_tensor=torch.from_numpy(deformation_gradients_with_barycenters[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(deformation_gradients_with_barycenters[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 61 42\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(x_variable.size()[1],y_variable.size()[2],nb_of_hidden_layers=2, nb_of_hidden_nodes=70,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable.size()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0.7)\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if  torch.cuda.is_available():\n",
    "    #loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 35.02343916046759 27.956112498671484\n",
      "Epoch: 10 Training Loss: 34.19338801806589 34.00778509937294\n",
      "Epoch: 20 Training Loss: 33.393085812964934 33.32974412796259\n",
      "Epoch: 30 Training Loss: 32.62397383103082 32.559487591667555\n",
      "Epoch: 40 Training Loss: 31.884893065887354 31.819776942289298\n",
      "Epoch: 50 Training Loss: 31.17370151434644 31.108772186204696\n",
      "Epoch: 60 Training Loss: 30.48915382571732 30.42516141460304\n",
      "Epoch: 70 Training Loss: 29.829679197662063 29.767111276437454\n",
      "Epoch: 80 Training Loss: 29.19382970244421 29.133210224253375\n",
      "Epoch: 90 Training Loss: 28.580609059511158 28.52237551812095\n",
      "Epoch: 100 Training Loss: 27.989814691817216 27.934271707939207\n",
      "Epoch: 110 Training Loss: 27.42118922688629 27.368943631097885\n",
      "Epoch: 120 Training Loss: 26.874745948459086 26.826505872037412\n",
      "Epoch: 130 Training Loss: 26.3502424282678 26.30555518652354\n",
      "Epoch: 140 Training Loss: 25.8473731402763 25.80608991391221\n",
      "Epoch: 150 Training Loss: 25.36563164187035 25.327243198001913\n",
      "Epoch: 160 Training Loss: 24.90527862646121 24.86963445902859\n",
      "Epoch: 170 Training Loss: 24.46649010361318 24.432383555638218\n",
      "Epoch: 180 Training Loss: 24.049040249734325 24.016505141354024\n",
      "Epoch: 190 Training Loss: 23.65214864505845 23.62143625783824\n",
      "Epoch: 200 Training Loss: 23.275360321466525 23.24622535604209\n",
      "Epoch: 210 Training Loss: 22.918203706163656 22.89061337549155\n",
      "Epoch: 220 Training Loss: 22.57983030021254 22.55414862100117\n",
      "Epoch: 230 Training Loss: 22.259785135494155 22.235814778403657\n",
      "Epoch: 240 Training Loss: 21.957638150903293 21.93548065681794\n",
      "Epoch: 250 Training Loss: 21.67264379649309 21.652571673397812\n",
      "Epoch: 260 Training Loss: 21.404033275770455 21.38616284939951\n",
      "Epoch: 270 Training Loss: 21.15124036928799 21.13547036082474\n",
      "Epoch: 280 Training Loss: 20.913682252922424 20.900110598894674\n",
      "Epoch: 290 Training Loss: 20.69073957226355 20.679266526729727\n",
      "Epoch: 300 Training Loss: 20.48168006110521 20.471979952704856\n",
      "Epoch: 310 Training Loss: 20.28572994155154 20.277739398448293\n",
      "Epoch: 320 Training Loss: 20.102113775239108 20.095372449250718\n",
      "Epoch: 330 Training Loss: 19.93008103081828 19.924410803486023\n",
      "Epoch: 340 Training Loss: 19.768879516471838 19.76438449888405\n",
      "Epoch: 350 Training Loss: 19.617788589266738 19.61460935009034\n",
      "Epoch: 360 Training Loss: 19.476124136556855 19.474525720055265\n",
      "Epoch: 370 Training Loss: 19.34323691551541 19.343054190137103\n",
      "Epoch: 380 Training Loss: 19.218439492561107 19.21956869752365\n",
      "Epoch: 390 Training Loss: 19.101243690223168 19.103835091401848\n",
      "Epoch: 400 Training Loss: 18.991201182252922 18.994981533638008\n",
      "Epoch: 410 Training Loss: 18.88784205632306 18.89286188755447\n",
      "Epoch: 420 Training Loss: 18.790683116365567 18.79708424115209\n",
      "Epoch: 430 Training Loss: 18.699340794367693 18.70695444521203\n",
      "Epoch: 440 Training Loss: 18.613474694473965 18.62205733871825\n",
      "Epoch: 450 Training Loss: 18.53264977417641 18.54209234509512\n",
      "Epoch: 460 Training Loss: 18.456379516471838 18.466582859496228\n",
      "Epoch: 470 Training Loss: 18.384384962805527 18.39536779944734\n",
      "Epoch: 480 Training Loss: 18.316430326780022 18.328120018067807\n",
      "Epoch: 490 Training Loss: 18.252294766206163 18.264562187798916\n",
      "Epoch: 500 Training Loss: 18.191656150371944 18.204362179827825\n",
      "Epoch: 510 Training Loss: 18.134180725292243 18.147310753002444\n",
      "Epoch: 520 Training Loss: 18.07966757438895 18.093193684238496\n",
      "Epoch: 530 Training Loss: 18.027900836875663 18.041770180146667\n",
      "Epoch: 540 Training Loss: 17.978784205632305 17.993036919438836\n",
      "Epoch: 550 Training Loss: 17.93214997343252 17.94669000425125\n",
      "Epoch: 560 Training Loss: 17.887795563230604 17.902604886279093\n",
      "Epoch: 570 Training Loss: 17.845553267800213 17.86063376820066\n",
      "Epoch: 580 Training Loss: 17.80532179861849 17.820635495270487\n",
      "Epoch: 590 Training Loss: 17.76693842986185 17.78253699914975\n",
      "Epoch: 600 Training Loss: 17.7302802869288 17.746185500584545\n",
      "Epoch: 610 Training Loss: 17.69518464399575 17.71136511584653\n",
      "Epoch: 620 Training Loss: 17.661545231137087 17.677957939207143\n",
      "Epoch: 630 Training Loss: 17.629320536663123 17.645952346157934\n",
      "Epoch: 640 Training Loss: 17.598425876726886 17.615268625783823\n",
      "Epoch: 650 Training Loss: 17.568746679064823 17.58582706716973\n",
      "Epoch: 660 Training Loss: 17.540234790116898 17.5575097645871\n",
      "Epoch: 670 Training Loss: 17.512755712008502 17.530233685832712\n",
      "Epoch: 680 Training Loss: 17.486211477151965 17.50395233287278\n",
      "Epoch: 690 Training Loss: 17.46060374601488 17.478547799978745\n",
      "Epoch: 700 Training Loss: 17.43585115568544 17.454049978743758\n",
      "Epoch: 710 Training Loss: 17.411933780552605 17.43033764215113\n",
      "Epoch: 720 Training Loss: 17.38877689957492 17.407395844404295\n",
      "Epoch: 730 Training Loss: 17.3663655685441 17.385191372621957\n",
      "Epoch: 740 Training Loss: 17.34464831296493 17.363697656499095\n",
      "Epoch: 750 Training Loss: 17.323591923485655 17.3428781618663\n",
      "Epoch: 760 Training Loss: 17.303168172157278 17.32268805133383\n",
      "Epoch: 770 Training Loss: 17.28335381243358 17.30309577266447\n",
      "Epoch: 780 Training Loss: 17.264074123273115 17.284043203315974\n",
      "Epoch: 790 Training Loss: 17.2453125 17.265462256881708\n",
      "Epoch: 800 Training Loss: 17.227070603081827 17.24734296949729\n",
      "Epoch: 810 Training Loss: 17.20933846971307 17.22967039536614\n",
      "Epoch: 820 Training Loss: 17.19208787194474 17.212479408013603\n",
      "Epoch: 830 Training Loss: 17.175295563230605 17.195763364863428\n",
      "Epoch: 840 Training Loss: 17.158934976089267 17.17946580401743\n",
      "Epoch: 850 Training Loss: 17.14300112911796 17.16358008289935\n",
      "Epoch: 860 Training Loss: 17.12745417109458 17.14806966733978\n",
      "Epoch: 870 Training Loss: 17.11228081827843 17.132909647677756\n",
      "Epoch: 880 Training Loss: 17.097471107863974 17.118119951642043\n",
      "Epoch: 890 Training Loss: 17.08299017003188 17.103667366351367\n",
      "Epoch: 900 Training Loss: 17.068824721041445 17.089518678924435\n",
      "Epoch: 910 Training Loss: 17.054984723698194 17.07567056807312\n",
      "Epoch: 920 Training Loss: 17.041450252391073 17.0621429615262\n",
      "Epoch: 930 Training Loss: 17.02821134431456 17.048902646402382\n",
      "Epoch: 940 Training Loss: 17.0152480738576 17.03595128334573\n",
      "Epoch: 950 Training Loss: 17.002550478214665 17.02325565947497\n",
      "Epoch: 960 Training Loss: 16.990100292242296 17.010804150281647\n",
      "Epoch: 970 Training Loss: 16.977880911264613 16.99860173769795\n",
      "Epoch: 980 Training Loss: 16.96586908873539 16.986638457859495\n",
      "Epoch: 990 Training Loss: 16.954058182784273 16.97487943724094\n",
      "Epoch: 1000 Training Loss: 16.942448193411266 16.963326336486343\n",
      "Epoch: 1010 Training Loss: 16.931045762486715 16.951997422680414\n",
      "Epoch: 1020 Training Loss: 16.919837606269926 16.94085450100967\n",
      "Epoch: 1030 Training Loss: 16.90881210148778 16.929912517270697\n",
      "Epoch: 1040 Training Loss: 16.897942680658872 16.919189738548198\n",
      "Epoch: 1050 Training Loss: 16.887226022848033 16.90865295196089\n",
      "Epoch: 1060 Training Loss: 16.876680393198725 16.89829551493251\n",
      "Epoch: 1070 Training Loss: 16.866285866099894 16.888115766819002\n",
      "Epoch: 1080 Training Loss: 16.856065688097768 16.878078834095014\n",
      "Epoch: 1090 Training Loss: 16.84601155685441 16.86821294770964\n",
      "Epoch: 1100 Training Loss: 16.836106867693942 16.85852807152726\n",
      "Epoch: 1110 Training Loss: 16.826336676408076 16.848976046870018\n",
      "Epoch: 1120 Training Loss: 16.81670430393199 16.83954857051759\n",
      "Epoch: 1130 Training Loss: 16.807218052603613 16.830217411520884\n",
      "Epoch: 1140 Training Loss: 16.79784637353879 16.82102076469338\n",
      "Epoch: 1150 Training Loss: 16.78859922954304 16.811947005526623\n",
      "Epoch: 1160 Training Loss: 16.779476620616364 16.802984509512168\n",
      "Epoch: 1170 Training Loss: 16.770481867693942 16.794148222446594\n",
      "Epoch: 1180 Training Loss: 16.761609989373007 16.78546305399086\n",
      "Epoch: 1190 Training Loss: 16.752869287991498 16.776935646721224\n",
      "Epoch: 1200 Training Loss: 16.744238177470777 16.768544412264852\n",
      "Epoch: 1210 Training Loss: 16.735710015940487 16.760254477096396\n",
      "Epoch: 1220 Training Loss: 16.72729144527099 16.752075805080242\n",
      "Epoch: 1230 Training Loss: 16.71898578639745 16.743998432352004\n",
      "Epoch: 1240 Training Loss: 16.710784736981935 16.736019037623553\n",
      "Epoch: 1250 Training Loss: 16.70266837141339 16.72812931767457\n",
      "Epoch: 1260 Training Loss: 16.69462672688629 16.720359164098205\n",
      "Epoch: 1270 Training Loss: 16.686666445270987 16.712701934318208\n",
      "Epoch: 1280 Training Loss: 16.678797489373007 16.705149325114252\n",
      "Epoch: 1290 Training Loss: 16.671009896386824 16.697701336486343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1300 Training Loss: 16.66328540116897 16.69031147040068\n",
      "Epoch: 1310 Training Loss: 16.655624003719446 16.683002975874164\n",
      "Epoch: 1320 Training Loss: 16.648025704038258 16.67580740514401\n",
      "Epoch: 1330 Training Loss: 16.64048219978746 16.66871811563397\n",
      "Epoch: 1340 Training Loss: 16.633008435175345 16.661673663513657\n",
      "Epoch: 1350 Training Loss: 16.625596107863974 16.65464249654586\n",
      "Epoch: 1360 Training Loss: 16.618248538788524 16.647671112764375\n",
      "Epoch: 1370 Training Loss: 16.610954104675876 16.640772797321713\n",
      "Epoch: 1380 Training Loss: 16.60372442879915 16.633930943777234\n",
      "Epoch: 1390 Training Loss: 16.59655452975558 16.62714223084281\n",
      "Epoch: 1400 Training Loss: 16.589432784272052 16.62041828302689\n",
      "Epoch: 1410 Training Loss: 16.58236417375133 16.613765742905727\n",
      "Epoch: 1420 Training Loss: 16.57534205632306 16.607149736953982\n",
      "Epoch: 1430 Training Loss: 16.56839465993624 16.600566943883514\n",
      "Epoch: 1440 Training Loss: 16.561498738044634 16.593989132745243\n",
      "Epoch: 1450 Training Loss: 16.554642667375134 16.58744619513232\n",
      "Epoch: 1460 Training Loss: 16.54783807120085 16.580929827824423\n",
      "Epoch: 1470 Training Loss: 16.541083289054196 16.57446327983845\n",
      "Epoch: 1480 Training Loss: 16.534358395324123 16.568081424699756\n",
      "Epoch: 1490 Training Loss: 16.527665050478216 16.561752710171113\n",
      "Epoch: 1500 Training Loss: 16.521004914984058 16.555513670421938\n",
      "Epoch: 1510 Training Loss: 16.514364705100956 16.549282933893082\n",
      "Epoch: 1520 Training Loss: 16.507759365037195 16.543087070889573\n",
      "Epoch: 1530 Training Loss: 16.50120549946865 16.53692442076735\n",
      "Epoch: 1540 Training Loss: 16.49469148512221 16.53077837708577\n",
      "Epoch: 1550 Training Loss: 16.488215661530287 16.52468547401424\n",
      "Epoch: 1560 Training Loss: 16.481774707757705 16.518627444468063\n",
      "Epoch: 1570 Training Loss: 16.475368623804464 16.51265244712509\n",
      "Epoch: 1580 Training Loss: 16.46899907013815 16.50668409235838\n",
      "Epoch: 1590 Training Loss: 16.46267767003188 16.500798769794876\n",
      "Epoch: 1600 Training Loss: 16.456404423485655 16.49492839302795\n",
      "Epoch: 1610 Training Loss: 16.450182651434645 16.489122781379532\n",
      "Epoch: 1620 Training Loss: 16.443995749202976 16.483352043256456\n",
      "Epoch: 1630 Training Loss: 16.437850358661 16.477597911574026\n",
      "Epoch: 1640 Training Loss: 16.431738177470777 16.471872010840684\n",
      "Epoch: 1650 Training Loss: 16.425667507970246 16.466225821022427\n",
      "Epoch: 1660 Training Loss: 16.419640010626992 16.460563024763523\n",
      "Epoch: 1670 Training Loss: 16.413647383103083 16.45495835104687\n",
      "Epoch: 1680 Training Loss: 16.40768464399575 16.4493868902115\n",
      "Epoch: 1690 Training Loss: 16.40174847236982 16.443848642257414\n",
      "Epoch: 1700 Training Loss: 16.395855472901168 16.438325340099905\n",
      "Epoch: 1710 Training Loss: 16.3900189293305 16.432860160484644\n",
      "Epoch: 1720 Training Loss: 16.38421891604676 16.427479673716654\n",
      "Epoch: 1730 Training Loss: 16.37845875398512 16.422253626846636\n",
      "Epoch: 1740 Training Loss: 16.372743424548354 16.417100648315444\n",
      "Epoch: 1750 Training Loss: 16.367061304463338 16.411929402699542\n",
      "Epoch: 1760 Training Loss: 16.361417375132838 16.406741550643\n",
      "Epoch: 1770 Training Loss: 16.35580167375133 16.40157362631523\n",
      "Epoch: 1780 Training Loss: 16.35022250265675 16.396404041343395\n",
      "Epoch: 1790 Training Loss: 16.34467155951116 16.391304203422255\n",
      "Epoch: 1800 Training Loss: 16.339152165249736 16.386284076416196\n",
      "Epoch: 1810 Training Loss: 16.333652696599362 16.38124402168137\n",
      "Epoch: 1820 Training Loss: 16.328191418703508 16.376213930810927\n",
      "Epoch: 1830 Training Loss: 16.322761689691816 16.371215392177703\n",
      "Epoch: 1840 Training Loss: 16.31737347236982 16.366276636730788\n",
      "Epoch: 1850 Training Loss: 16.312001859723697 16.361372754809224\n",
      "Epoch: 1860 Training Loss: 16.306643530818278 16.35650042512488\n",
      "Epoch: 1870 Training Loss: 16.301291843783208 16.35159820384738\n",
      "Epoch: 1880 Training Loss: 16.295958421891605 16.346717570942715\n",
      "Epoch: 1890 Training Loss: 16.290636623273112 16.34190004251249\n",
      "Epoch: 1900 Training Loss: 16.28533641073326 16.337075871506006\n",
      "Epoch: 1910 Training Loss: 16.280062765674813 16.332351339143372\n",
      "Epoch: 1920 Training Loss: 16.274819009032942 16.327666662238283\n",
      "Epoch: 1930 Training Loss: 16.269611782678002 16.323023501434797\n",
      "Epoch: 1940 Training Loss: 16.264424481934114 16.318386983207567\n",
      "Epoch: 1950 Training Loss: 16.259277032412328 16.313773713997236\n",
      "Epoch: 1960 Training Loss: 16.254164452709883 16.30920030024445\n",
      "Epoch: 1970 Training Loss: 16.24908010095643 16.304626886491658\n",
      "Epoch: 1980 Training Loss: 16.244032279489904 16.30010993463705\n",
      "Epoch: 1990 Training Loss: 16.239009365037194 16.29562785630779\n",
      "Epoch: 2000 Training Loss: 16.23400471572795 16.291134153470082\n",
      "Epoch: 2010 Training Loss: 16.229039917640808 16.28662716547986\n",
      "Epoch: 2020 Training Loss: 16.22409338469713 16.28206869752365\n",
      "Epoch: 2030 Training Loss: 16.21917175876727 16.27752351471995\n",
      "Epoch: 2040 Training Loss: 16.214258435175346 16.27308461313636\n",
      "Epoch: 2050 Training Loss: 16.20935507438895 16.268629105112126\n",
      "Epoch: 2060 Training Loss: 16.204476620616365 16.2642151131895\n",
      "Epoch: 2070 Training Loss: 16.199639678533476 16.259840976724412\n",
      "Epoch: 2080 Training Loss: 16.19482100159405 16.255536587310022\n",
      "Epoch: 2090 Training Loss: 16.190028892136027 16.25123053725157\n",
      "Epoch: 2100 Training Loss: 16.185258368756642 16.24685640078648\n",
      "Epoch: 2110 Training Loss: 16.180502789585546 16.24257193910086\n",
      "Epoch: 2120 Training Loss: 16.17576879649309 16.23831902965246\n",
      "Epoch: 2130 Training Loss: 16.17106303134963 16.2341524736954\n",
      "Epoch: 2140 Training Loss: 16.166378852284804 16.229949383568922\n",
      "Epoch: 2150 Training Loss: 16.161702975557915 16.225782827611862\n",
      "Epoch: 2160 Training Loss: 16.157045363974497 16.221674394197045\n",
      "Epoch: 2170 Training Loss: 16.15239107332625 16.217592531087256\n",
      "Epoch: 2180 Training Loss: 16.14775338735388 16.21350734668934\n",
      "Epoch: 2190 Training Loss: 16.143112380446333 16.209493569986183\n",
      "Epoch: 2200 Training Loss: 16.13844812699256 16.205471490062706\n",
      "Epoch: 2210 Training Loss: 16.133785534006375 16.201447749495163\n",
      "Epoch: 2220 Training Loss: 16.12913124335813 16.19742234828356\n",
      "Epoch: 2230 Training Loss: 16.124493557385758 16.193551386969922\n",
      "Epoch: 2240 Training Loss: 16.119864173751328 16.18964223084281\n",
      "Epoch: 2250 Training Loss: 16.11525305526036 16.185739717291955\n",
      "Epoch: 2260 Training Loss: 16.110663522848036 16.181847167605483\n",
      "Epoch: 2270 Training Loss: 16.106073990435707 16.178045953342544\n",
      "Epoch: 2280 Training Loss: 16.101499402231667 16.17419159846955\n",
      "Epoch: 2290 Training Loss: 16.09694640010627 16.170340564884686\n",
      "Epoch: 2300 Training Loss: 16.092406681721574 16.16644967584228\n",
      "Epoch: 2310 Training Loss: 16.087873605207225 16.162600302901478\n",
      "Epoch: 2320 Training Loss: 16.083357133368757 16.15878414284196\n",
      "Epoch: 2330 Training Loss: 16.078862247608928 16.155036069189073\n",
      "Epoch: 2340 Training Loss: 16.074387287460148 16.15131124455309\n",
      "Epoch: 2350 Training Loss: 16.06992561105207 16.147596383781487\n",
      "Epoch: 2360 Training Loss: 16.065465595111583 16.143949609416516\n",
      "Epoch: 2370 Training Loss: 16.060992295430392 16.14022976671272\n",
      "Epoch: 2380 Training Loss: 16.0565057120085 16.136486674992028\n",
      "Epoch: 2390 Training Loss: 16.052012486716258 16.132775135508556\n",
      "Epoch: 2400 Training Loss: 16.04752424282678 16.129125039855456\n",
      "Epoch: 2410 Training Loss: 16.04303101753454 16.1255147996599\n",
      "Epoch: 2420 Training Loss: 16.03854111317747 16.121814884684877\n",
      "Epoch: 2430 Training Loss: 16.034041246014876 16.118194680624935\n",
      "Epoch: 2440 Training Loss: 16.029533076514348 16.114635920395365\n",
      "Epoch: 2450 Training Loss: 16.02502158607864 16.110872900945903\n",
      "Epoch: 2460 Training Loss: 16.02049681190223 16.107111542140505\n",
      "Epoch: 2470 Training Loss: 16.015947130712007 16.103361807843555\n",
      "Epoch: 2480 Training Loss: 16.01138748671626 16.099572218089065\n",
      "Epoch: 2490 Training Loss: 16.006844447396386 16.095779307046445\n",
      "Epoch: 2500 Training Loss: 16.00230472901169 16.092024590817303\n",
      "Epoch: 2510 Training Loss: 15.997776633900106 16.088263232011904\n",
      "Epoch: 2520 Training Loss: 15.993275106269925 16.084664616324794\n",
      "Epoch: 2530 Training Loss: 15.988790183315622 16.080992932298862\n",
      "Epoch: 2540 Training Loss: 15.984336809245484 16.077346157933892\n",
      "Epoch: 2550 Training Loss: 15.97991498405951 16.07372927516208\n",
      "Epoch: 2560 Training Loss: 15.975508103081827 16.07008250079711\n",
      "Epoch: 2570 Training Loss: 15.97111118490967 16.06641247741524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2580 Training Loss: 15.966722569075452 16.0627457753215\n",
      "Epoch: 2590 Training Loss: 15.962338934643995 16.059128892549687\n",
      "Epoch: 2600 Training Loss: 15.957946997874602 16.05557511425231\n",
      "Epoch: 2610 Training Loss: 15.953568344845909 16.052072815920926\n",
      "Epoch: 2620 Training Loss: 15.949183049946866 16.048543947284514\n",
      "Epoch: 2630 Training Loss: 15.944786131774707 16.045046630885324\n",
      "Epoch: 2640 Training Loss: 15.940394195005313 16.041609097672442\n",
      "Epoch: 2650 Training Loss: 15.936028825717322 16.038257917950897\n",
      "Epoch: 2660 Training Loss: 15.931676740170031 16.034890131788714\n",
      "Epoch: 2670 Training Loss: 15.927331296493092 16.03155555850781\n",
      "Epoch: 2680 Training Loss: 15.92300245749203 16.028249216176\n",
      "Epoch: 2690 Training Loss: 15.918680260361318 16.024944534488256\n",
      "Epoch: 2700 Training Loss: 15.914379649309245 16.02166310181741\n",
      "Epoch: 2710 Training Loss: 15.9100890010627 16.018205640875756\n",
      "Epoch: 2720 Training Loss: 15.905798352816152 16.014784714103516\n",
      "Epoch: 2730 Training Loss: 15.901494420828906 16.01136876926347\n",
      "Epoch: 2740 Training Loss: 15.897215395855472 16.00800596503348\n",
      "Epoch: 2750 Training Loss: 15.892954636025506 16.004659767244128\n",
      "Epoch: 2760 Training Loss: 15.888702178533475 16.001265410776917\n",
      "Epoch: 2770 Training Loss: 15.884476288522848 15.997917552343502\n",
      "Epoch: 2780 Training Loss: 15.88026534272051 15.99461121001169\n",
      "Epoch: 2790 Training Loss: 15.876075982996811 15.991258369646083\n",
      "Epoch: 2800 Training Loss: 15.871926474495218 15.987963651822723\n",
      "Epoch: 2810 Training Loss: 15.867790249734325 15.984732038473801\n",
      "Epoch: 2820 Training Loss: 15.863668969181722 15.98157515410777\n",
      "Epoch: 2830 Training Loss: 15.859554330499469 15.978381735572324\n",
      "Epoch: 2840 Training Loss: 15.85544301275239 15.975180013816558\n",
      "Epoch: 2850 Training Loss: 15.8513200717322 15.971955043043895\n",
      "Epoch: 2860 Training Loss: 15.847197130712008 15.968751660644065\n",
      "Epoch: 2870 Training Loss: 15.84307252922423 15.965551599532363\n",
      "Epoch: 2880 Training Loss: 15.8389412858661 15.962341574556277\n",
      "Epoch: 2890 Training Loss: 15.834800079702445 15.959085051546392\n",
      "Epoch: 2900 Training Loss: 15.83065057120085 15.955851777553406\n",
      "Epoch: 2910 Training Loss: 15.826496081296494 15.952694893187374\n",
      "Epoch: 2920 Training Loss: 15.822369819341127 15.949569561058562\n",
      "Epoch: 2930 Training Loss: 15.818280087672688 15.946582062387076\n",
      "Epoch: 2940 Training Loss: 15.814215263018065 15.943479979275162\n",
      "Epoch: 2950 Training Loss: 15.81013051275239 15.940427715485173\n",
      "Epoch: 2960 Training Loss: 15.806047422954304 15.937340578169838\n",
      "Epoch: 2970 Training Loss: 15.801944407545164 15.934168748007227\n",
      "Epoch: 2980 Training Loss: 15.79783807120085 15.930972008183653\n",
      "Epoch: 2990 Training Loss: 15.793720111583422 15.92771216388564\n",
      "Epoch: 3000 Training Loss: 15.78959717056323 15.924533691146774\n",
      "Epoch: 3010 Training Loss: 15.785467587672688 15.921345254543523\n",
      "Epoch: 3020 Training Loss: 15.781314758235919 15.917992414177915\n",
      "Epoch: 3030 Training Loss: 15.777145324123273 15.914724266659581\n",
      "Epoch: 3040 Training Loss: 15.772969248140276 15.911383050802423\n",
      "Epoch: 3050 Training Loss: 15.768808116365568 15.908138152300989\n",
      "Epoch: 3060 Training Loss: 15.764663589266737 15.905104155595707\n",
      "Epoch: 3070 Training Loss: 15.760515741232732 15.901973841534701\n",
      "Epoch: 3080 Training Loss: 15.756362911795962 15.898770459134871\n",
      "Epoch: 3090 Training Loss: 15.752221705632307 15.895542167074078\n",
      "Epoch: 3100 Training Loss: 15.7480738575983 15.892365354979276\n",
      "Epoch: 3110 Training Loss: 15.743945935175345 15.889103850037198\n",
      "Epoch: 3120 Training Loss: 15.73985454303932 15.885910431501754\n",
      "Epoch: 3130 Training Loss: 15.735807983528161 15.88277679615262\n",
      "Epoch: 3140 Training Loss: 15.731787991498406 15.87991384578595\n",
      "Epoch: 3150 Training Loss: 15.727772980871414 15.876803459453715\n",
      "Epoch: 3160 Training Loss: 15.72377291445271 15.873644914443618\n",
      "Epoch: 3170 Training Loss: 15.719801075982996 15.870610917738336\n",
      "Epoch: 3180 Training Loss: 15.715834218916047 15.867520459134871\n",
      "Epoch: 3190 Training Loss: 15.711888947927736 15.864612671378467\n",
      "Epoch: 3200 Training Loss: 15.707966923485653 15.861567050164735\n",
      "Epoch: 3210 Training Loss: 15.70404489904357 15.858538035391646\n",
      "Epoch: 3220 Training Loss: 15.70009298618491 15.855382811669678\n",
      "Epoch: 3230 Training Loss: 15.696129450053135 15.852086433202253\n",
      "Epoch: 3240 Training Loss: 15.692154290648247 15.84890962110745\n",
      "Epoch: 3250 Training Loss: 15.68819241498406 15.845661401317887\n",
      "Epoch: 3260 Training Loss: 15.684252125398512 15.842820039324051\n",
      "Epoch: 3270 Training Loss: 15.680321798618492 15.839852468381338\n",
      "Epoch: 3280 Training Loss: 15.676374867162593 15.836836738760761\n",
      "Epoch: 3290 Training Loss: 15.67242461477152 15.833825991072377\n",
      "Epoch: 3300 Training Loss: 15.66849262752391 15.830913221383781\n",
      "Epoch: 3310 Training Loss: 15.664575584484592 15.827841029865024\n",
      "Epoch: 3320 Training Loss: 15.66062367162593 15.824878440854501\n",
      "Epoch: 3330 Training Loss: 15.656704968119023 15.821914191199914\n",
      "Epoch: 3340 Training Loss: 15.652787925079702 15.818812108088\n",
      "Epoch: 3350 Training Loss: 15.648907412327311 15.81598070995855\n",
      "Epoch: 3360 Training Loss: 15.645040183315622 15.813038048676798\n",
      "Epoch: 3370 Training Loss: 15.641171293836344 15.810118636411946\n",
      "Epoch: 3380 Training Loss: 15.637315688097768 15.807260667977468\n",
      "Epoch: 3390 Training Loss: 15.633446798618492 15.804173530662132\n",
      "Epoch: 3400 Training Loss: 15.629533076514347 15.801380327346157\n",
      "Epoch: 3410 Training Loss: 15.625576182252923 15.798235067488575\n",
      "Epoch: 3420 Training Loss: 15.621624269394262 15.795272478478052\n",
      "Epoch: 3430 Training Loss: 15.617662393730074 15.792130539908598\n",
      "Epoch: 3440 Training Loss: 15.613690555260362 15.789172932830269\n",
      "Epoch: 3450 Training Loss: 15.609695470244422 15.7862867334467\n",
      "Epoch: 3460 Training Loss: 15.605733594580235 15.783520100435753\n",
      "Epoch: 3470 Training Loss: 15.601728546758768 15.780374840578169\n",
      "Epoch: 3480 Training Loss: 15.59774508501594 15.777674633329791\n",
      "Epoch: 3490 Training Loss: 15.59377158607864 15.774861502285047\n",
      "Epoch: 3500 Training Loss: 15.58980472901169 15.77216295568073\n",
      "Epoch: 3510 Training Loss: 15.58583787194474 15.769381376873206\n",
      "Epoch: 3520 Training Loss: 15.581894261424017 15.766561603252205\n",
      "Epoch: 3530 Training Loss: 15.577935706695005 15.763618941970455\n",
      "Epoch: 3540 Training Loss: 15.574020324123273 15.760614836858327\n",
      "Epoch: 3550 Training Loss: 15.570078374070139 15.757791741949198\n",
      "Epoch: 3560 Training Loss: 15.566176275239107 15.754947058667234\n",
      "Epoch: 3570 Training Loss: 15.562290781083954 15.752002736741417\n",
      "Epoch: 3580 Training Loss: 15.55838370085016 15.749103252205336\n",
      "Epoch: 3590 Training Loss: 15.554461676408076 15.746359868211288\n",
      "Epoch: 3600 Training Loss: 15.55055625664187 15.743568325539377\n",
      "Epoch: 3610 Training Loss: 15.546659139213602 15.74065555585078\n",
      "Epoch: 3620 Training Loss: 15.542757040382572 15.737983579551493\n",
      "Epoch: 3630 Training Loss: 15.538854941551541 15.735054203422255\n",
      "Epoch: 3640 Training Loss: 15.534951182252922 15.73229587363163\n",
      "Epoch: 3650 Training Loss: 15.531088934643996 15.729323320756722\n",
      "Epoch: 3660 Training Loss: 15.527306389479277 15.726654665745563\n",
      "Epoch: 3670 Training Loss: 15.523532146652498 15.72381496439579\n",
      "Epoch: 3680 Training Loss: 15.519801075982997 15.721053313317038\n",
      "Epoch: 3690 Training Loss: 15.516073326248671 15.718077439154001\n",
      "Epoch: 3700 Training Loss: 15.512310706695006 15.715330733871825\n",
      "Epoch: 3710 Training Loss: 15.508536463868225 15.712532548623658\n",
      "Epoch: 3720 Training Loss: 15.504767202444208 15.709569959613136\n",
      "Epoch: 3730 Training Loss: 15.500968052603612 15.706552569348496\n",
      "Epoch: 3740 Training Loss: 15.497148977151966 15.703601604846424\n",
      "Epoch: 3750 Training Loss: 15.493351487778959 15.700751939632267\n",
      "Epoch: 3760 Training Loss: 15.489515807651435 15.697825884791158\n",
      "Epoch: 3770 Training Loss: 15.485698392667375 15.695006111170157\n",
      "Epoch: 3780 Training Loss: 15.481925810308184 15.692060128600277\n",
      "Epoch: 3790 Training Loss: 15.478168172157279 15.689305120097778\n",
      "Epoch: 3800 Training Loss: 15.474458687566418 15.68660159156127\n",
      "Epoch: 3810 Training Loss: 15.47075584484591 15.683994380380486\n",
      "Epoch: 3820 Training Loss: 15.4670662858661 15.681300815708365\n",
      "Epoch: 3830 Training Loss: 15.463409936238044 15.679203422255288\n",
      "Epoch: 3840 Training Loss: 15.459800079702445 15.676792167074078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3850 Training Loss: 15.456218451115834 15.674090299181634\n",
      "Epoch: 3860 Training Loss: 15.452681655154091 15.671542871187162\n",
      "Epoch: 3870 Training Loss: 15.449171426673752 15.669236236581996\n",
      "Epoch: 3880 Training Loss: 15.445672821466525 15.666727003400998\n",
      "Epoch: 3890 Training Loss: 15.442197462805526 15.664496758422787\n",
      "Epoch: 3900 Training Loss: 15.438693876195536 15.661823121479435\n",
      "Epoch: 3910 Training Loss: 15.43520191285866 15.6593620469763\n",
      "Epoch: 3920 Training Loss: 15.431754782146653 15.656854474439367\n",
      "Epoch: 3930 Training Loss: 15.428324256110521 15.65441664895313\n",
      "Epoch: 3940 Training Loss: 15.424945204569607 15.651777885535125\n",
      "Epoch: 3950 Training Loss: 15.421566153028692 15.649615726963546\n",
      "Epoch: 3960 Training Loss: 15.41823691551541 15.647159634392603\n",
      "Epoch: 3970 Training Loss: 15.414864505844847 15.644710184397917\n",
      "Epoch: 3980 Training Loss: 15.411526965993623 15.642186005420342\n",
      "Epoch: 3990 Training Loss: 15.408215993623804 15.639934172069296\n",
      "Epoch: 4000 Training Loss: 15.404931588735387 15.637650786481029\n",
      "Epoch: 4010 Training Loss: 15.401650504782147 15.635232888723563\n",
      "Epoch: 4020 Training Loss: 15.398352816153029 15.632849864491444\n",
      "Epoch: 4030 Training Loss: 15.395075053134963 15.630026769582315\n",
      "Epoch: 4040 Training Loss: 15.391798950584485 15.627921072908917\n",
      "Epoch: 4050 Training Loss: 15.388537792242296 15.625587867998725\n",
      "Epoch: 4060 Training Loss: 15.385284936238044 15.623076974173664\n",
      "Epoch: 4070 Training Loss: 15.382043703506907 15.62044983526411\n",
      "Epoch: 4080 Training Loss: 15.378784205632305 15.618506881709003\n",
      "Epoch: 4090 Training Loss: 15.375559577577045 15.616133821341268\n",
      "Epoch: 4100 Training Loss: 15.372361517003188 15.61363953395685\n",
      "Epoch: 4110 Training Loss: 15.369118623804463 15.611113694335211\n",
      "Epoch: 4120 Training Loss: 15.365937167906482 15.609150813051334\n",
      "Epoch: 4130 Training Loss: 15.362754051540914 15.607046777022001\n",
      "Epoch: 4140 Training Loss: 15.35951780021254 15.60483313848443\n",
      "Epoch: 4150 Training Loss: 15.356311437300745 15.602460078116696\n",
      "Epoch: 4160 Training Loss: 15.353139944208289 15.600621745137634\n",
      "Epoch: 4170 Training Loss: 15.349966790648246 15.59842637368477\n",
      "Epoch: 4180 Training Loss: 15.34683182784272 15.595759379317675\n",
      "Epoch: 4190 Training Loss: 15.343698525504783 15.59384133542353\n",
      "Epoch: 4200 Training Loss: 15.340578506907546 15.59137361834414\n",
      "Epoch: 4210 Training Loss: 15.337476753453773 15.58915333723031\n",
      "Epoch: 4220 Training Loss: 15.33434677205101 15.586738760760973\n",
      "Epoch: 4230 Training Loss: 15.331253320935176 15.584413859071102\n",
      "Epoch: 4240 Training Loss: 15.328183116365569 15.582303180465512\n",
      "Epoch: 4250 Training Loss: 15.32507804197662 15.580273873419067\n",
      "Epoch: 4260 Training Loss: 15.3219962141339 15.57797388139016\n",
      "Epoch: 4270 Training Loss: 15.31892102816153 15.575873166648954\n",
      "Epoch: 4280 Training Loss: 15.315865767800213 15.573465232755872\n",
      "Epoch: 4290 Training Loss: 15.312798884165781 15.571450871506006\n",
      "Epoch: 4300 Training Loss: 15.30981668437832 15.569039616324796\n",
      "Epoch: 4310 Training Loss: 15.306821200850159 15.566711393346795\n",
      "Epoch: 4320 Training Loss: 15.303850624335812 15.564740208842597\n",
      "Epoch: 4330 Training Loss: 15.300923219978745 15.56279725528749\n",
      "Epoch: 4340 Training Loss: 15.297970908607864 15.56063841800404\n",
      "Epoch: 4350 Training Loss: 15.295046825185972 15.558331783398874\n",
      "Epoch: 4360 Training Loss: 15.292159272051009 15.556177928047614\n",
      "Epoch: 4370 Training Loss: 15.289228546758768 15.553824795408651\n",
      "Epoch: 4380 Training Loss: 15.28636590063762 15.551541409820384\n",
      "Epoch: 4390 Training Loss: 15.283451780021254 15.54948387182485\n",
      "Epoch: 4400 Training Loss: 15.28055094314559 15.547482795727495\n",
      "Epoch: 4410 Training Loss: 15.27766172954304 15.545272478478052\n",
      "Epoch: 4420 Training Loss: 15.274779157810839 15.543057179296419\n",
      "Epoch: 4430 Training Loss: 15.27194640010627 15.541150759910725\n",
      "Epoch: 4440 Training Loss: 15.269105340063762 15.53916130832182\n",
      "Epoch: 4450 Training Loss: 15.266267600956429 15.537025720055267\n",
      "Epoch: 4460 Training Loss: 15.263423219978746 15.53449323785737\n",
      "Epoch: 4470 Training Loss: 15.260635294899043 15.532276278031672\n",
      "Epoch: 4480 Training Loss: 15.257800876726886 15.530072603358487\n",
      "Epoch: 4490 Training Loss: 15.255027895855473 15.527691239770432\n",
      "Epoch: 4500 Training Loss: 15.252228347502657 15.526116949197577\n",
      "Epoch: 4510 Training Loss: 15.249486915515408 15.523748870762036\n",
      "Epoch: 4520 Training Loss: 15.24668238575983 15.522003533850569\n",
      "Epoch: 4530 Training Loss: 15.243904423485654 15.519653722499735\n",
      "Epoch: 4540 Training Loss: 15.241134763549416 15.517851923690085\n",
      "Epoch: 4550 Training Loss: 15.238343517534538 15.515671498033797\n",
      "Epoch: 4560 Training Loss: 15.235568876195536 15.513819879902222\n",
      "Epoch: 4570 Training Loss: 15.232799216259298 15.51168927356786\n",
      "Epoch: 4580 Training Loss: 15.230019593517534 15.509583576894462\n",
      "Epoch: 4590 Training Loss: 15.227230007970244 15.507853185779572\n",
      "Epoch: 4600 Training Loss: 15.224452045696069 15.506072975342757\n",
      "Epoch: 4610 Training Loss: 15.22172555791711 15.504005473482836\n",
      "Epoch: 4620 Training Loss: 15.218979144527099 15.502130606334362\n",
      "Epoch: 4630 Training Loss: 15.216247675345377 15.499807365288554\n",
      "Epoch: 4640 Training Loss: 15.21354941551541 15.497759791157403\n",
      "Epoch: 4650 Training Loss: 15.210804662592986 15.495697271229673\n",
      "Epoch: 4660 Training Loss: 15.208078174814027 15.493749335742374\n",
      "Epoch: 4670 Training Loss: 15.20535832890542 15.491809703475395\n",
      "Epoch: 4680 Training Loss: 15.202653427205101 15.489662490700393\n",
      "Epoch: 4690 Training Loss: 15.199918637088205 15.487932099585503\n",
      "Epoch: 4700 Training Loss: 15.19718882837407 15.485525826336486\n",
      "Epoch: 4710 Training Loss: 15.194487247608926 15.483390238069934\n",
      "Epoch: 4720 Training Loss: 15.191800611052072 15.481360931023488\n",
      "Epoch: 4730 Training Loss: 15.189050876726887 15.4793382665533\n",
      "Epoch: 4740 Training Loss: 15.186357598299681 15.477395312998194\n",
      "Epoch: 4750 Training Loss: 15.1836477151966 15.475209905409715\n",
      "Epoch: 4760 Training Loss: 15.180944473963867 15.472997927516207\n",
      "Epoch: 4770 Training Loss: 15.178153227948991 15.471000172706983\n",
      "Epoch: 4780 Training Loss: 15.175448326248672 15.468645379423956\n",
      "Epoch: 4790 Training Loss: 15.17273844314559 15.466084666276968\n",
      "Epoch: 4800 Training Loss: 15.169973764612115 15.46419983526411\n",
      "Epoch: 4810 Training Loss: 15.167280486184909 15.462062586353492\n",
      "Epoch: 4820 Training Loss: 15.164575584484592 15.46018937984908\n",
      "Epoch: 4830 Training Loss: 15.161840794367693 15.457874442023595\n",
      "Epoch: 4840 Training Loss: 15.15913921360255 15.455625929960675\n",
      "Epoch: 4850 Training Loss: 15.156467521253985 15.45362817515145\n",
      "Epoch: 4860 Training Loss: 15.153819075451647 15.45164038420661\n",
      "Epoch: 4870 Training Loss: 15.151129117959618 15.449378586991179\n",
      "Epoch: 4880 Training Loss: 15.148522183846971 15.446816213200128\n",
      "Epoch: 4890 Training Loss: 15.14588370085016 15.444997807949836\n",
      "Epoch: 4900 Training Loss: 15.14328672954304 15.44289709320863\n",
      "Epoch: 4910 Training Loss: 15.140686437300744 15.441110240195558\n",
      "Epoch: 4920 Training Loss: 15.138036331030818 15.439255300775853\n",
      "Epoch: 4930 Training Loss: 15.135419434112647 15.437106427356786\n",
      "Epoch: 4940 Training Loss: 15.132764346439957 15.434680226379\n",
      "Epoch: 4950 Training Loss: 15.130100956429331 15.43294983526411\n",
      "Epoch: 4960 Training Loss: 15.127500664187036 15.431046737166543\n",
      "Epoch: 4970 Training Loss: 15.124883767268862 15.429130353916463\n",
      "Epoch: 4980 Training Loss: 15.122288456429331 15.4269349824636\n",
      "Epoch: 4990 Training Loss: 15.119668238575983 15.425226179721543\n",
      "Epoch: 5000 Training Loss: 15.117147648777896 15.423231746200447\n",
      "Epoch: 5010 Training Loss: 15.114542375132837 15.421026410883197\n",
      "Epoch: 5020 Training Loss: 15.111960348034007 15.419031977362101\n",
      "Epoch: 5030 Training Loss: 15.10944640010627 15.417306568179402\n",
      "Epoch: 5040 Training Loss: 15.106882638150903 15.414998272930173\n",
      "Epoch: 5050 Training Loss: 15.104355406482465 15.413007160697205\n",
      "Epoch: 5060 Training Loss: 15.101873007438895 15.410722114464875\n",
      "Epoch: 5070 Training Loss: 15.09935075717322 15.408644648740568\n",
      "Epoch: 5080 Training Loss: 15.096765409139213 15.406681767456691\n",
      "Epoch: 5090 Training Loss: 15.094191684378321 15.404821846104793\n",
      "Epoch: 5100 Training Loss: 15.091657810839532 15.402807484854927\n",
      "Epoch: 5110 Training Loss: 15.089203639744952 15.400671896588372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5120 Training Loss: 15.08677935706695 15.39855623605059\n",
      "Epoch: 5130 Training Loss: 15.084260427736451 15.39673450951217\n",
      "Epoch: 5140 Training Loss: 15.081723233262487 15.394542459347432\n",
      "Epoch: 5150 Training Loss: 15.079247476089266 15.39283033531725\n",
      "Epoch: 5160 Training Loss: 15.076756774707757 15.390781100542034\n",
      "Epoch: 5170 Training Loss: 15.074247808182784 15.388642190987353\n",
      "Epoch: 5180 Training Loss: 15.071757106801275 15.386654400042513\n",
      "Epoch: 5190 Training Loss: 15.069191684378321 15.384625092996068\n",
      "Epoch: 5200 Training Loss: 15.066584750265674 15.382580840153045\n",
      "Epoch: 5210 Training Loss: 15.064020988310308 15.380305757785099\n",
      "Epoch: 5220 Training Loss: 15.061427337938364 15.378107065044107\n",
      "Epoch: 5230 Training Loss: 15.058923352816153 15.37593826389627\n",
      "Epoch: 5240 Training Loss: 15.05639944208289 15.37411321606972\n",
      "Epoch: 5250 Training Loss: 15.053918703506907 15.372191850887448\n",
      "Epoch: 5260 Training Loss: 15.051467853347503 15.370343554044\n",
      "Epoch: 5270 Training Loss: 15.048990435706695 15.368400600488894\n",
      "Epoch: 5280 Training Loss: 15.046546227417641 15.366806382187267\n",
      "Epoch: 5290 Training Loss: 15.044100358661 15.36500956530981\n",
      "Epoch: 5300 Training Loss: 15.041656150371944 15.363138019449464\n",
      "Epoch: 5310 Training Loss: 15.039221904888416 15.361186762674036\n",
      "Epoch: 5320 Training Loss: 15.036898910733262 15.359927994473377\n",
      "Epoch: 5330 Training Loss: 15.034469646652497 15.358176014985652\n",
      "Epoch: 5340 Training Loss: 15.03211344314559 15.356181581464556\n",
      "Epoch: 5350 Training Loss: 15.029704104675877 15.354494367095334\n",
      "Epoch: 5360 Training Loss: 15.027329636025504 15.352704192794134\n",
      "Epoch: 5370 Training Loss: 15.025016604675876 15.351078422255288\n",
      "Epoch: 5380 Training Loss: 15.022753387353879 15.349215179615262\n",
      "Epoch: 5390 Training Loss: 15.020430393198724 15.34749973429695\n",
      "Epoch: 5400 Training Loss: 15.018170496811901 15.345819162503986\n",
      "Epoch: 5410 Training Loss: 15.015862446865038 15.344010721118078\n",
      "Epoch: 5420 Training Loss: 15.013572662061637 15.34217404878308\n",
      "Epoch: 5430 Training Loss: 15.011284537725823 15.340481852481666\n",
      "Epoch: 5440 Training Loss: 15.009026301806589 15.33897066638325\n",
      "Epoch: 5450 Training Loss: 15.00676972635494 15.337258542353066\n",
      "Epoch: 5460 Training Loss: 15.004464997343252 15.335566346051653\n",
      "Epoch: 5470 Training Loss: 15.002215063761955 15.33373963758104\n",
      "Epoch: 5480 Training Loss: 15.000081362911796 15.33228657402487\n",
      "Epoch: 5490 Training Loss: 14.997917773645058 15.33052463067276\n",
      "Epoch: 5500 Training Loss: 14.995752523910733 15.328566731321075\n",
      "Epoch: 5510 Training Loss: 14.993575650903294 15.326808109257094\n",
      "Epoch: 5520 Training Loss: 14.9914602151966 15.325320172175577\n",
      "Epoch: 5530 Training Loss: 14.989387951647183 15.323495124349028\n",
      "Epoch: 5540 Training Loss: 14.987244287991498 15.321826177064512\n",
      "Epoch: 5550 Training Loss: 14.985085680127524 15.32031665161016\n",
      "Epoch: 5560 Training Loss: 14.982993490967056 15.31873239717292\n",
      "Epoch: 5570 Training Loss: 14.980897980871413 15.317430452226592\n",
      "Epoch: 5580 Training Loss: 14.978765940488842 15.31574323785737\n",
      "Epoch: 5590 Training Loss: 14.976756774707757 15.314233712403018\n",
      "Epoch: 5600 Training Loss: 14.974599827311371 15.312343899457966\n",
      "Epoch: 5610 Training Loss: 14.972505977683316 15.310694879902222\n",
      "Epoch: 5620 Training Loss: 14.970500132837406 15.309251780210436\n",
      "Epoch: 5630 Training Loss: 14.968469380977684 15.30734868211287\n",
      "Epoch: 5640 Training Loss: 14.966407080233793 15.305925510149857\n",
      "Epoch: 5650 Training Loss: 14.964407877258235 15.304479089169943\n",
      "Epoch: 5660 Training Loss: 14.96232897183847 15.30303598947816\n",
      "Epoch: 5670 Training Loss: 14.960274973432519 15.301584586566054\n",
      "Epoch: 5680 Training Loss: 14.958172821466524 15.300040187586353\n",
      "Epoch: 5690 Training Loss: 14.95611384165781 15.298520698267616\n",
      "Epoch: 5700 Training Loss: 14.954089731668438 15.296826841322138\n",
      "Epoch: 5710 Training Loss: 14.952087207757705 15.295262514613668\n",
      "Epoch: 5720 Training Loss: 14.950054795430393 15.293962230311404\n",
      "Epoch: 5730 Training Loss: 14.94799581562168 15.292447722924859\n",
      "Epoch: 5740 Training Loss: 14.945975026567481 15.291164045063237\n",
      "Epoch: 5750 Training Loss: 14.943962539851222 15.289380513338292\n",
      "Epoch: 5760 Training Loss: 14.941921825185972 15.288116763205442\n",
      "Epoch: 5770 Training Loss: 14.939907678002125 15.286471064937826\n",
      "Epoch: 5780 Training Loss: 14.937881907545165 15.285107676161122\n",
      "Epoch: 5790 Training Loss: 14.935854476620616 15.283654612604952\n",
      "Epoch: 5800 Training Loss: 14.933795496811902 15.28224804708258\n",
      "Epoch: 5810 Training Loss: 14.931746479808714 15.280574117865873\n",
      "Epoch: 5820 Training Loss: 14.929733993092455 15.279494699224147\n",
      "Epoch: 5830 Training Loss: 14.927635162061637 15.277586619194388\n",
      "Epoch: 5840 Training Loss: 14.925594447396387 15.275826336486343\n",
      "Epoch: 5850 Training Loss: 14.923532146652498 15.274562586353492\n",
      "Epoch: 5860 Training Loss: 14.921542906482465 15.273079631204165\n",
      "Epoch: 5870 Training Loss: 14.919532080233793 15.271355882665533\n",
      "Epoch: 5880 Training Loss: 14.917461477151965 15.269783252736742\n",
      "Epoch: 5890 Training Loss: 14.915462274176408 15.268453076841322\n",
      "Epoch: 5900 Training Loss: 14.913398312964931 15.266928605590392\n",
      "Epoch: 5910 Training Loss: 14.911392468119022 15.265651570305026\n",
      "Epoch: 5920 Training Loss: 14.90943145589798 15.264361249867148\n",
      "Epoch: 5930 Training Loss: 14.90745549946865 15.263049341056435\n",
      "Epoch: 5940 Training Loss: 14.90552935706695 15.261244220958657\n",
      "Epoch: 5950 Training Loss: 14.903641405419766 15.260123286215325\n",
      "Epoch: 5960 Training Loss: 14.90164054197662 15.258472606015516\n",
      "Epoch: 5970 Training Loss: 14.899737646121148 15.257114199171006\n",
      "Epoch: 5980 Training Loss: 14.897861317747077 15.25567442076735\n",
      "Epoch: 5990 Training Loss: 14.895858793836345 15.254242945584016\n",
      "Epoch: 6000 Training Loss: 14.893954237513285 15.252537464130087\n",
      "Epoch: 6010 Training Loss: 14.891998206695005 15.251389959081731\n",
      "Epoch: 6020 Training Loss: 14.889957492029756 15.249750903390371\n",
      "Epoch: 6030 Training Loss: 14.888057917109458 15.248375890105219\n",
      "Epoch: 6040 Training Loss: 14.886088602550478 15.247196832819641\n",
      "Epoch: 6050 Training Loss: 14.884185706695005 15.24616723349984\n",
      "Epoch: 6060 Training Loss: 14.882239638682252 15.244548105537252\n",
      "Epoch: 6070 Training Loss: 14.88028692879915 15.243246160590923\n",
      "Epoch: 6080 Training Loss: 14.878428865568544 15.24161374747582\n",
      "Epoch: 6090 Training Loss: 14.876506044102019 15.240071009140184\n",
      "Epoch: 6100 Training Loss: 14.874606469181721 15.23871094165161\n",
      "Epoch: 6110 Training Loss: 14.872718517534537 15.237530223721969\n",
      "Epoch: 6120 Training Loss: 14.870848831030818 15.23617679880965\n",
      "Epoch: 6130 Training Loss: 14.868974163124335 15.234770233287279\n",
      "Epoch: 6140 Training Loss: 14.86708787194474 15.233202585290678\n",
      "Epoch: 6150 Training Loss: 14.865186636556855 15.232103238920184\n",
      "Epoch: 6160 Training Loss: 14.863300345377258 15.230592052821766\n",
      "Epoch: 6170 Training Loss: 14.86141571466525 15.228913141672866\n",
      "Epoch: 6180 Training Loss: 14.859542707226355 15.227707514082262\n",
      "Epoch: 6190 Training Loss: 14.857596639213602 15.226407229779998\n",
      "Epoch: 6200 Training Loss: 14.855708687566418 15.225198280901266\n",
      "Epoch: 6210 Training Loss: 14.853804131243358 15.223678791582527\n",
      "Epoch: 6220 Training Loss: 14.851871346971308 15.222272226060156\n",
      "Epoch: 6230 Training Loss: 14.849920297555792 15.220691292911043\n",
      "Epoch: 6240 Training Loss: 14.8479659272051 15.219543787862685\n",
      "Epoch: 6250 Training Loss: 14.846004914984059 15.21803758369646\n",
      "Epoch: 6260 Training Loss: 14.844012353878853 15.216823652885536\n",
      "Epoch: 6270 Training Loss: 14.842048020722636 15.215616364650867\n",
      "Epoch: 6280 Training Loss: 14.840148445802338 15.213965684451058\n",
      "Epoch: 6290 Training Loss: 14.838147582359193 15.212280130725901\n",
      "Epoch: 6300 Training Loss: 14.836257970244422 15.210895153576363\n",
      "Epoch: 6310 Training Loss: 14.834301939426142 15.209322523647572\n",
      "Epoch: 6320 Training Loss: 14.83241232731137 15.207937546498034\n",
      "Epoch: 6330 Training Loss: 14.830484524442083 15.20669704538208\n",
      "Epoch: 6340 Training Loss: 14.828621479808714 15.205290479859709\n",
      "Epoch: 6350 Training Loss: 14.826682053666312 15.20360824742268\n",
      "Epoch: 6360 Training Loss: 14.824765874070138 15.202399298543947\n",
      "Epoch: 6370 Training Loss: 14.822907810839533 15.200712084174727\n",
      "Epoch: 6380 Training Loss: 14.820921891604677 15.199262341906685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6390 Training Loss: 14.818949256110521 15.197742852587949\n",
      "Epoch: 6400 Training Loss: 14.817046360255048 15.196181847167605\n",
      "Epoch: 6410 Training Loss: 14.815055459617428 15.194848349984058\n",
      "Epoch: 6420 Training Loss: 14.813119354410201 15.193410232224466\n",
      "Epoch: 6430 Training Loss: 14.811228081827842 15.1918874216176\n",
      "Epoch: 6440 Training Loss: 14.809282013815091 15.19054894250186\n",
      "Epoch: 6450 Training Loss: 14.80738409936238 15.188924832607078\n",
      "Epoch: 6460 Training Loss: 14.805443012752391 15.18770591986396\n",
      "Epoch: 6470 Training Loss: 14.80350358660999 15.186641447018811\n",
      "Epoch: 6480 Training Loss: 14.801625597768332 15.185424194919758\n",
      "Epoch: 6490 Training Loss: 14.799765874070138 15.184035896482092\n",
      "Epoch: 6500 Training Loss: 14.797831429330499 15.183095971941757\n",
      "Epoch: 6510 Training Loss: 14.79603314293305 15.181832221808907\n",
      "Epoch: 6520 Training Loss: 14.794165116896918 15.180438941439048\n",
      "Epoch: 6530 Training Loss: 14.79228712805526 15.179592012966308\n",
      "Epoch: 6540 Training Loss: 14.790417441551542 15.178298371240302\n",
      "Epoch: 6550 Training Loss: 14.788449787460149 15.176925018599214\n",
      "Epoch: 6560 Training Loss: 14.786560175345377 15.175422135721117\n",
      "Epoch: 6570 Training Loss: 14.78466060042508 15.174289576469338\n",
      "Epoch: 6580 Training Loss: 14.782777630180659 15.172937812201084\n",
      "Epoch: 6590 Training Loss: 14.780823259829969 15.17164915240727\n",
      "Epoch: 6600 Training Loss: 14.778908740701382 15.170380420342225\n",
      "Epoch: 6610 Training Loss: 14.776970975026568 15.168887501328514\n",
      "Epoch: 6620 Training Loss: 14.77521420031881 15.167502524178978\n",
      "Epoch: 6630 Training Loss: 14.773243225292243 15.16631516367308\n",
      "Epoch: 6640 Training Loss: 14.771416710945802 15.164674447337655\n",
      "Epoch: 6650 Training Loss: 14.76957691285866 15.16365481188224\n",
      "Epoch: 6660 Training Loss: 14.7677935706695 15.162362830800298\n",
      "Epoch: 6670 Training Loss: 14.765915581827842 15.161178791582527\n",
      "Epoch: 6680 Training Loss: 14.764122276833156 15.15999973429695\n",
      "Epoch: 6690 Training Loss: 14.762353878852284 15.158568259113615\n",
      "Epoch: 6700 Training Loss: 14.760645257704569 15.157409129556807\n",
      "Epoch: 6710 Training Loss: 14.75892501328374 15.156132094271442\n",
      "Epoch: 6720 Training Loss: 14.75711344314559 15.154660763630567\n",
      "Epoch: 6730 Training Loss: 14.75538655685441 15.153428565734934\n",
      "Epoch: 6740 Training Loss: 14.75363808448459 15.152231241364651\n",
      "Epoch: 6750 Training Loss: 14.751896253985123 15.150778177808482\n",
      "Epoch: 6760 Training Loss: 14.750116232731138 15.149398182591137\n",
      "Epoch: 6770 Training Loss: 14.74838768597237 15.148151038898927\n",
      "Epoch: 6780 Training Loss: 14.746702311370882 15.14706331703688\n",
      "Epoch: 6790 Training Loss: 14.744980406482465 15.145824476564991\n",
      "Epoch: 6800 Training Loss: 14.74337141339001 15.144937692634711\n",
      "Epoch: 6810 Training Loss: 14.741671094580234 15.143844988840472\n",
      "Epoch: 6820 Training Loss: 14.740030552603614 15.14256131097885\n",
      "Epoch: 6830 Training Loss: 14.738418238575983 15.141608101286003\n",
      "Epoch: 6840 Training Loss: 14.736782678002125 15.140414098203847\n",
      "Epoch: 6850 Training Loss: 14.735190289585548 15.139387820172175\n",
      "Epoch: 6860 Training Loss: 14.733597901168968 15.137971290785417\n",
      "Epoch: 6870 Training Loss: 14.732066950053134 15.13729872993942\n",
      "Epoch: 6880 Training Loss: 14.73049448724761 15.135764294824105\n",
      "Epoch: 6890 Training Loss: 14.72898180127524 15.134357729301732\n",
      "Epoch: 6900 Training Loss: 14.727450850159405 15.133399537676693\n",
      "Epoch: 6910 Training Loss: 14.725989638682252 15.133306541609098\n",
      "Epoch: 6920 Training Loss: 14.724425478214664 15.131944813476458\n",
      "Epoch: 6930 Training Loss: 14.72294434112646 15.13090358964821\n",
      "Epoch: 6940 Training Loss: 14.72148312964931 15.129681355616963\n",
      "Epoch: 6950 Training Loss: 14.719993690223166 15.129120057923265\n",
      "Epoch: 6960 Training Loss: 14.718446134431456 15.127869592942927\n",
      "Epoch: 6970 Training Loss: 14.716986583421892 15.126969523860135\n",
      "Epoch: 6980 Training Loss: 14.715477218384697 15.126012992879158\n",
      "Epoch: 6990 Training Loss: 14.713971174282678 15.12523747210118\n",
      "Epoch: 7000 Training Loss: 14.712526567481403 15.124172999256032\n",
      "Epoch: 7010 Training Loss: 14.711093583953241 15.123787729833138\n",
      "Epoch: 7020 Training Loss: 14.709647316684379 15.122849465936868\n",
      "Epoch: 7030 Training Loss: 14.708285733262487 15.12207394515889\n",
      "Epoch: 7040 Training Loss: 14.706817879914984 15.121092504516952\n",
      "Epoch: 7050 Training Loss: 14.70542308714134 15.120056262620896\n",
      "Epoch: 7060 Training Loss: 14.704000066418704 15.119536481028803\n",
      "Epoch: 7070 Training Loss: 14.702520589798088 15.118518506217452\n",
      "Epoch: 7080 Training Loss: 14.701059378320934 15.117618437134658\n",
      "Epoch: 7090 Training Loss: 14.69964299946865 15.116874468593899\n",
      "Epoch: 7100 Training Loss: 14.69825152763018 15.11585317249442\n",
      "Epoch: 7110 Training Loss: 14.696855074388948 15.114680757785099\n",
      "Epoch: 7120 Training Loss: 14.695417109458024 15.11381390158359\n",
      "Epoch: 7130 Training Loss: 14.694004051540913 15.112608273992985\n",
      "Epoch: 7140 Training Loss: 14.692609258767268 15.111910803486024\n",
      "Epoch: 7150 Training Loss: 14.691221107863974 15.111100409182697\n",
      "Epoch: 7160 Training Loss: 14.689822994155154 15.110255141354022\n",
      "Epoch: 7170 Training Loss: 14.688453108395324 15.109205614305452\n",
      "Epoch: 7180 Training Loss: 14.687126394792774 15.108363667764905\n",
      "Epoch: 7190 Training Loss: 14.685734922954303 15.107055080242322\n",
      "Epoch: 7200 Training Loss: 14.684378320935176 15.106113495057924\n",
      "Epoch: 7210 Training Loss: 14.682988509564293 15.105147000212563\n",
      "Epoch: 7220 Training Loss: 14.681681721572795 15.104584041874801\n",
      "Epoch: 7230 Training Loss: 14.680333421891605 15.103164191199914\n",
      "Epoch: 7240 Training Loss: 14.678910401168968 15.102589608353703\n",
      "Epoch: 7250 Training Loss: 14.677543836344315 15.101613149643958\n",
      "Epoch: 7260 Training Loss: 14.676250332093517 15.100226511850357\n",
      "Epoch: 7270 Training Loss: 14.67485719978746 15.09959048517377\n",
      "Epoch: 7280 Training Loss: 14.67348731402763 15.098619008396216\n",
      "Epoch: 7290 Training Loss: 14.672178865568544 15.097625943245829\n",
      "Epoch: 7300 Training Loss: 14.670865435706695 15.0968387979594\n",
      "Epoch: 7310 Training Loss: 14.66954204303932 15.09550031884366\n",
      "Epoch: 7320 Training Loss: 14.668233594580235 15.094953966946541\n",
      "Epoch: 7330 Training Loss: 14.66688861583422 15.093957580508023\n",
      "Epoch: 7340 Training Loss: 14.665641604675876 15.093065814645552\n",
      "Epoch: 7350 Training Loss: 14.664309909670564 15.092476286002764\n",
      "Epoch: 7360 Training Loss: 14.66309942879915 15.091763869699225\n",
      "Epoch: 7370 Training Loss: 14.661754450053134 15.090999973429694\n",
      "Epoch: 7380 Training Loss: 14.660460945802338 15.090159687533212\n",
      "Epoch: 7390 Training Loss: 14.659180725292241 15.08921976299288\n",
      "Epoch: 7400 Training Loss: 14.657897183846972 15.088319693910085\n",
      "Epoch: 7410 Training Loss: 14.656655154091393 15.087690309809757\n",
      "Epoch: 7420 Training Loss: 14.655366631243359 15.08653948347327\n",
      "Epoch: 7430 Training Loss: 14.65405154091392 15.086081145711553\n",
      "Epoch: 7440 Training Loss: 14.652759697130712 15.085181076628759\n",
      "Epoch: 7450 Training Loss: 14.651504383634432 15.084257758529068\n",
      "Epoch: 7460 Training Loss: 14.650217521253985 15.083473934530769\n",
      "Epoch: 7470 Training Loss: 14.648945603081827 15.08258382931236\n",
      "Epoch: 7480 Training Loss: 14.647677005844846 15.08195776650016\n",
      "Epoch: 7490 Training Loss: 14.646371878320934 15.081046072908917\n",
      "Epoch: 7500 Training Loss: 14.645081695005313 15.08005300775853\n",
      "Epoch: 7510 Training Loss: 14.64380645589798 15.079147956743544\n",
      "Epoch: 7520 Training Loss: 14.642499667906483 15.078329259219895\n",
      "Epoch: 7530 Training Loss: 14.64130247077577 15.077769622170262\n",
      "Epoch: 7540 Training Loss: 14.639960812964931 15.077113667764905\n",
      "Epoch: 7550 Training Loss: 14.638692215727948 15.076700167392922\n",
      "Epoch: 7560 Training Loss: 14.637380446333687 15.07580840153045\n",
      "Epoch: 7570 Training Loss: 14.636168304994687 15.074959812413647\n",
      "Epoch: 7580 Training Loss: 14.634836609989373 15.074184291635667\n",
      "Epoch: 7590 Training Loss: 14.633526501062699 15.073445305027102\n",
      "Epoch: 7600 Training Loss: 14.63226288522848 15.072619964927197\n",
      "Epoch: 7610 Training Loss: 14.631093916046758 15.072404081198853\n",
      "Epoch: 7620 Training Loss: 14.629740634962806 15.07184444414922\n",
      "Epoch: 7630 Training Loss: 14.628508568012752 15.070361488999893\n",
      "Epoch: 7640 Training Loss: 14.627238310308183 15.069808494526518\n",
      "Epoch: 7650 Training Loss: 14.62603447130712 15.069301998086939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7660 Training Loss: 14.624765874070139 15.06800005314061\n",
      "Epoch: 7670 Training Loss: 14.623517202444209 15.067257745243916\n",
      "Epoch: 7680 Training Loss: 14.62231668437832 15.066696447550218\n",
      "Epoch: 7690 Training Loss: 14.621013217321998 15.06559211924753\n",
      "Epoch: 7700 Training Loss: 14.619696466524973 15.064746851418855\n",
      "Epoch: 7710 Training Loss: 14.618603878852285 15.063762089488787\n",
      "Epoch: 7720 Training Loss: 14.617214067481402 15.06321241630354\n",
      "Epoch: 7730 Training Loss: 14.616058382040382 15.06207653576363\n",
      "Epoch: 7740 Training Loss: 14.61483295696068 15.061257838239984\n",
      "Epoch: 7750 Training Loss: 14.613574322529225 15.060356108513126\n",
      "Epoch: 7760 Training Loss: 14.612392069606802 15.060007373259644\n",
      "Epoch: 7770 Training Loss: 14.611171625929861 15.059309902752684\n",
      "Epoch: 7780 Training Loss: 14.609994354410201 15.05873033797428\n",
      "Epoch: 7790 Training Loss: 14.608785534006376 15.057609403230948\n",
      "Epoch: 7800 Training Loss: 14.607601620616366 15.057152726113296\n",
      "Epoch: 7810 Training Loss: 14.606611981934112 15.05639049048783\n",
      "Epoch: 7820 Training Loss: 14.605355007970244 15.055543562015092\n",
      "Epoch: 7830 Training Loss: 14.60425909936238 15.054603637474758\n",
      "Epoch: 7840 Training Loss: 14.603184776833157 15.05395930757785\n",
      "Epoch: 7850 Training Loss: 14.602092189160468 15.053263497714953\n",
      "Epoch: 7860 Training Loss: 14.600973034006376 15.052371731852482\n",
      "Epoch: 7870 Training Loss: 14.59992195802338 15.051956570836433\n",
      "Epoch: 7880 Training Loss: 14.59890077045696 15.05108805399086\n",
      "Epoch: 7890 Training Loss: 14.597784936238044 15.050342424806036\n",
      "Epoch: 7900 Training Loss: 14.596811902231668 15.049784448400468\n",
      "Epoch: 7910 Training Loss: 14.595611384165782 15.048852827080454\n",
      "Epoch: 7920 Training Loss: 14.59454204303932 15.048004237963651\n",
      "Epoch: 7930 Training Loss: 14.593449455366631 15.047368211287065\n",
      "Epoch: 7940 Training Loss: 14.592383435175345 15.0463602003401\n",
      "Epoch: 7950 Training Loss: 14.591280884697131 15.045933414815602\n",
      "Epoch: 7960 Training Loss: 14.590170031880978 15.045335582952493\n",
      "Epoch: 7970 Training Loss: 14.589072462805525 15.044614863428633\n",
      "Epoch: 7980 Training Loss: 14.58801308448459 15.043618476990115\n",
      "Epoch: 7990 Training Loss: 14.586915515409139 15.042999056754171\n",
      "Epoch: 8000 Training Loss: 14.585819606801275 15.042349744925072\n",
      "Epoch: 8010 Training Loss: 14.584713735387885 15.04154101126581\n",
      "Epoch: 8020 Training Loss: 14.583624468650372 15.040599426081412\n",
      "Epoch: 8030 Training Loss: 14.582621546227418 15.039554880965033\n",
      "Epoch: 8040 Training Loss: 14.581502391073327 15.03853192422149\n",
      "Epoch: 8050 Training Loss: 14.580378254516472 15.037837775002657\n",
      "Epoch: 8060 Training Loss: 14.579504848565357 15.037118716122862\n",
      "Epoch: 8070 Training Loss: 14.578385693411265 15.03652420554788\n",
      "Epoch: 8080 Training Loss: 14.577244952178534 15.035504570092465\n",
      "Epoch: 8090 Training Loss: 14.576247011158342 15.034534753958976\n",
      "Epoch: 8100 Training Loss: 14.575280619022317 15.033739305452226\n",
      "Epoch: 8110 Training Loss: 14.57417308714134 15.032884073759167\n",
      "Epoch: 8120 Training Loss: 14.573135294899043 15.032068697523648\n",
      "Epoch: 8130 Training Loss: 14.572172223698193 15.031437652779253\n",
      "Epoch: 8140 Training Loss: 14.571122808182784 15.03067541715379\n",
      "Epoch: 8150 Training Loss: 14.570106602019129 15.029768705494739\n",
      "Epoch: 8160 Training Loss: 14.569070470244421 15.028855351259432\n",
      "Epoch: 8170 Training Loss: 14.568055924548354 15.02818112976937\n",
      "Epoch: 8180 Training Loss: 14.566999867162593 15.027598243702839\n",
      "Epoch: 8190 Training Loss: 14.56611815887354 15.026766261026676\n",
      "Epoch: 8200 Training Loss: 14.565020589798086 15.025673557232437\n",
      "Epoch: 8210 Training Loss: 14.564100690754517 15.024991032522053\n",
      "Epoch: 8220 Training Loss: 14.56304961477152 15.023918256456584\n",
      "Epoch: 8230 Training Loss: 14.56203672954304 15.023185912424275\n",
      "Epoch: 8240 Training Loss: 14.5610719978746 15.022299128493994\n",
      "Epoch: 8250 Training Loss: 14.560105605738576 15.021893931342332\n",
      "Epoch: 8260 Training Loss: 14.559029622741765 15.020852707514083\n",
      "Epoch: 8270 Training Loss: 14.558099760892667 15.020273142735679\n",
      "Epoch: 8280 Training Loss: 14.557143331562168 15.019637116059092\n",
      "Epoch: 8290 Training Loss: 14.556248339532413 15.018546072908917\n",
      "Epoch: 8300 Training Loss: 14.555178998405951 15.018172427994473\n",
      "Epoch: 8310 Training Loss: 14.554332159936237 15.017396907216495\n",
      "Epoch: 8320 Training Loss: 14.55323957226355 15.016784129556807\n",
      "Epoch: 8330 Training Loss: 14.552251594048885 15.01632579179509\n",
      "Epoch: 8340 Training Loss: 14.551281880977683 15.01507034488256\n",
      "Epoch: 8350 Training Loss: 14.550352019128587 15.014665147730897\n",
      "Epoch: 8360 Training Loss: 14.549310905951115 15.013896269529175\n",
      "Epoch: 8370 Training Loss: 14.548517202444208 15.013148979700286\n",
      "Epoch: 8380 Training Loss: 14.547504317215727 15.01218414549899\n",
      "Epoch: 8390 Training Loss: 14.546620948459086 15.011807179296419\n",
      "Epoch: 8400 Training Loss: 14.5456761424017 15.010910431501754\n",
      "Epoch: 8410 Training Loss: 14.544829303931987 15.010571660112658\n",
      "Epoch: 8420 Training Loss: 14.543995749202976 15.009993755978318\n",
      "Epoch: 8430 Training Loss: 14.543009431455898 15.00937267509831\n",
      "Epoch: 8440 Training Loss: 14.542106137088204 15.008819680624933\n",
      "Epoch: 8450 Training Loss: 14.541133103081828 15.007766832288235\n",
      "Epoch: 8460 Training Loss: 14.540226487778959 15.007481201509194\n",
      "Epoch: 8470 Training Loss: 14.539379649309245 15.006400122223402\n",
      "Epoch: 8480 Training Loss: 14.53843484325186 15.00586041290254\n",
      "Epoch: 8490 Training Loss: 14.537453506907545 15.005149657243065\n",
      "Epoch: 8500 Training Loss: 14.536533607863975 15.004970307684133\n",
      "Epoch: 8510 Training Loss: 14.535690090329437 15.003676665958125\n",
      "Epoch: 8520 Training Loss: 14.534627391073327 15.003238255925178\n",
      "Epoch: 8530 Training Loss: 14.533760626992562 15.00220533531725\n",
      "Epoch: 8540 Training Loss: 14.532787592986185 15.001539417047507\n",
      "Epoch: 8550 Training Loss: 14.531917507970244 15.000913354235307\n",
      "Epoch: 8560 Training Loss: 14.530919566950054 15.000647651185036\n",
      "Epoch: 8570 Training Loss: 14.53009431455898 14.999828953661389\n",
      "Epoch: 8580 Training Loss: 14.529119620085016 14.99911653735785\n",
      "Epoch: 8590 Training Loss: 14.52825285600425 14.998272930173238\n",
      "Epoch: 8600 Training Loss: 14.527309710414453 14.997768094377724\n",
      "Epoch: 8610 Training Loss: 14.526459551009564 14.997107158040174\n",
      "Epoch: 8620 Training Loss: 14.525586145058448 14.996510986821129\n",
      "Epoch: 8630 Training Loss: 14.524749269394261 14.995833444042937\n",
      "Epoch: 8640 Training Loss: 14.523957226354941 14.995315323094909\n",
      "Epoch: 8650 Training Loss: 14.523126992561105 14.99478889892656\n",
      "Epoch: 8660 Training Loss: 14.522278493623805 14.993927024657243\n",
      "Epoch: 8670 Training Loss: 14.521509697130712 14.993634751301945\n",
      "Epoch: 8680 Training Loss: 14.52068278427205 14.993214608353703\n",
      "Epoch: 8690 Training Loss: 14.519895722635495 14.993239518014667\n",
      "Epoch: 8700 Training Loss: 14.519067149309246 14.992547029439898\n",
      "Epoch: 8710 Training Loss: 14.518256841126462 14.991788115102562\n",
      "Epoch: 8720 Training Loss: 14.517414984059512 14.991590498458923\n",
      "Epoch: 8730 Training Loss: 14.516579768862911 14.991175337442874\n",
      "Epoch: 8740 Training Loss: 14.515776102550479 14.991088983951535\n",
      "Epoch: 8750 Training Loss: 14.514989040913921 14.990326748326071\n",
      "Epoch: 8760 Training Loss: 14.51415548618491 14.990125810394304\n",
      "Epoch: 8770 Training Loss: 14.513277098831031 14.989265596769052\n",
      "Epoch: 8780 Training Loss: 14.512490037194475 14.989195849718355\n",
      "Epoch: 8790 Training Loss: 14.511746147715197 14.98871094165161\n",
      "Epoch: 8800 Training Loss: 14.510957425611052 14.988232676161122\n",
      "Epoch: 8810 Training Loss: 14.510085680127524 14.987719537145287\n",
      "Epoch: 8820 Training Loss: 14.509272051009564 14.98685766287597\n",
      "Epoch: 8830 Training Loss: 14.508612845377257 14.986596941757892\n",
      "Epoch: 8840 Training Loss: 14.507706230074389 14.985985824742269\n",
      "Epoch: 8850 Training Loss: 14.506902563761955 14.985339834201296\n",
      "Epoch: 8860 Training Loss: 14.506095576514346 14.985211964608354\n",
      "Epoch: 8870 Training Loss: 14.505434710414452 14.98483001647359\n",
      "Epoch: 8880 Training Loss: 14.50455964399575 14.984340126474653\n",
      "Epoch: 8890 Training Loss: 14.50384564293305 14.983594497289829\n",
      "Epoch: 8900 Training Loss: 14.503071865037194 14.9837256881709\n",
      "Epoch: 8910 Training Loss: 14.502271519659937 14.982769157189924\n",
      "Epoch: 8920 Training Loss: 14.50151268597237 14.982463598682113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8930 Training Loss: 14.500702377789585 14.98202684929323\n",
      "Epoch: 8940 Training Loss: 14.49990037194474 14.981869088107132\n",
      "Epoch: 8950 Training Loss: 14.499141538257174 14.98113176214263\n",
      "Epoch: 8960 Training Loss: 14.49837938363443 14.98053393027952\n",
      "Epoch: 8970 Training Loss: 14.497597303400637 14.979949383568924\n",
      "Epoch: 8980 Training Loss: 14.496861716259298 14.979589023806993\n",
      "Epoch: 8990 Training Loss: 14.495903626461212 14.978698918588586\n",
      "Epoch: 9000 Training Loss: 14.495025239107333 14.97808281964077\n",
      "Epoch: 9010 Training Loss: 14.494236517003188 14.977551413540228\n",
      "Epoch: 9020 Training Loss: 14.493434511158343 14.977347154320332\n",
      "Epoch: 9030 Training Loss: 14.492697263549415 14.976742679880966\n",
      "Epoch: 9040 Training Loss: 14.491938429861849 14.97618636411946\n",
      "Epoch: 9050 Training Loss: 14.491176275239107 14.975468965883728\n",
      "Epoch: 9060 Training Loss: 14.49042574388948 14.97507373259645\n",
      "Epoch: 9070 Training Loss: 14.489658607863975 14.974472579445212\n",
      "Epoch: 9080 Training Loss: 14.488951248671626 14.973904639175258\n",
      "Epoch: 9090 Training Loss: 14.488240568544102 14.97342637368477\n",
      "Epoch: 9100 Training Loss: 14.487486716259298 14.972702332872782\n",
      "Epoch: 9110 Training Loss: 14.486827510626993 14.972433308534383\n",
      "Epoch: 9120 Training Loss: 14.486118490967057 14.972023129450527\n",
      "Epoch: 9130 Training Loss: 14.485348034006376 14.971612950366671\n",
      "Epoch: 9140 Training Loss: 14.48461078639745 14.971038367520459\n",
      "Epoch: 9150 Training Loss: 14.483978148246546 14.970463784674248\n",
      "Epoch: 9160 Training Loss: 14.483131309776834 14.969669996811563\n",
      "Epoch: 9170 Training Loss: 14.482433913390011 14.96886458444043\n",
      "Epoch: 9180 Training Loss: 14.481841126461212 14.968477654373473\n",
      "Epoch: 9190 Training Loss: 14.481070669500532 14.968067475289617\n",
      "Epoch: 9200 Training Loss: 14.480313496280553 14.96733679190137\n",
      "Epoch: 9210 Training Loss: 14.479592853347503 14.96665592783505\n",
      "Epoch: 9220 Training Loss: 14.47886058714134 14.966043150175365\n",
      "Epoch: 9230 Training Loss: 14.478282744420829 14.965521707939207\n",
      "Epoch: 9240 Training Loss: 14.477578706163655 14.965378892549687\n",
      "Epoch: 9250 Training Loss: 14.476851421360255 14.96477441811032\n",
      "Epoch: 9260 Training Loss: 14.476114173751329 14.96425961845042\n",
      "Epoch: 9270 Training Loss: 14.475509763549416 14.963620270485706\n",
      "Epoch: 9280 Training Loss: 14.4747575717322 14.962736807843553\n",
      "Epoch: 9290 Training Loss: 14.474045231137088 14.962574064725263\n",
      "Epoch: 9300 Training Loss: 14.47335945802338 14.961893200658944\n",
      "Epoch: 9310 Training Loss: 14.47285301540914 14.96166237113402\n",
      "Epoch: 9320 Training Loss: 14.471994553666313 14.960931687745775\n",
      "Epoch: 9330 Training Loss: 14.47132538522848 14.960340498458923\n",
      "Epoch: 9340 Training Loss: 14.470699388947928 14.959953568391965\n",
      "Epoch: 9350 Training Loss: 14.469942215727949 14.959337469444149\n",
      "Epoch: 9360 Training Loss: 14.469392600956429 14.95898541290254\n",
      "Epoch: 9370 Training Loss: 14.468569009032944 14.958359350090339\n",
      "Epoch: 9380 Training Loss: 14.467851687035068 14.957734947922201\n",
      "Epoch: 9390 Training Loss: 14.467192481402764 14.957253361143586\n",
      "Epoch: 9400 Training Loss: 14.466611317747077 14.956660511212668\n",
      "Epoch: 9410 Training Loss: 14.465917242295431 14.956341667552344\n",
      "Epoch: 9420 Training Loss: 14.465241431987248 14.95565914284196\n",
      "Epoch: 9430 Training Loss: 14.464505844845908 14.955175895419279\n",
      "Epoch: 9440 Training Loss: 14.463864904357067 14.955114451588905\n",
      "Epoch: 9450 Training Loss: 14.463346838469713 14.954531565522371\n",
      "Epoch: 9460 Training Loss: 14.462586344314559 14.954119725794452\n",
      "Epoch: 9470 Training Loss: 14.461985255047821 14.953643120948028\n",
      "Epoch: 9480 Training Loss: 14.461234723698194 14.953262833457329\n",
      "Epoch: 9490 Training Loss: 14.460734922954304 14.952676626102667\n",
      "Epoch: 9500 Training Loss: 14.46009564293305 14.95264175257732\n",
      "Epoch: 9510 Training Loss: 14.459534404888416 14.951502550749282\n",
      "Epoch: 9520 Training Loss: 14.458745682784272 14.951451070783293\n",
      "Epoch: 9530 Training Loss: 14.458181123804463 14.950876487937082\n",
      "Epoch: 9540 Training Loss: 14.4576912858661 14.950305226379\n",
      "Epoch: 9550 Training Loss: 14.45692747077577 14.949886744074822\n",
      "Epoch: 9560 Training Loss: 14.45643597236982 14.949370283770858\n",
      "Epoch: 9570 Training Loss: 14.455768464399576 14.94864624295887\n",
      "Epoch: 9580 Training Loss: 14.45510925876727 14.948116497502392\n",
      "Epoch: 9590 Training Loss: 14.4545662858661 14.947855776384312\n",
      "Epoch: 9600 Training Loss: 14.453925345377257 14.94762494685939\n",
      "Epoch: 9610 Training Loss: 14.453407279489904 14.94712177170794\n",
      "Epoch: 9620 Training Loss: 14.452779622741764 14.946590365607397\n",
      "Epoch: 9630 Training Loss: 14.452180193942615 14.946161919438834\n",
      "Epoch: 9640 Training Loss: 14.451585746546227 14.94581982676161\n",
      "Epoch: 9650 Training Loss: 14.451072662061637 14.945270153576363\n",
      "Epoch: 9660 Training Loss: 14.450403493623805 14.945004450526092\n",
      "Epoch: 9670 Training Loss: 14.449827311370882 14.944687267509831\n",
      "Epoch: 9680 Training Loss: 14.449188031349628 14.944189074290573\n",
      "Epoch: 9690 Training Loss: 14.448656681721573 14.943541423105538\n",
      "Epoch: 9700 Training Loss: 14.448048950584484 14.943486621851418\n",
      "Epoch: 9710 Training Loss: 14.447544168437831 14.942420488362206\n",
      "Epoch: 9720 Training Loss: 14.446836809245484 14.94199038154958\n",
      "Epoch: 9730 Training Loss: 14.446257306057385 14.941603451482623\n",
      "Epoch: 9740 Training Loss: 14.445624667906483 14.940949157721331\n",
      "Epoch: 9750 Training Loss: 14.445066750797025 14.940751541077692\n",
      "Epoch: 9760 Training Loss: 14.444359391604676 14.939828222977999\n",
      "Epoch: 9770 Training Loss: 14.44379981402763 14.939627285046232\n",
      "Epoch: 9780 Training Loss: 14.443168836344315 14.939100860877883\n",
      "Epoch: 9790 Training Loss: 14.44249468650372 14.939027792539058\n",
      "Epoch: 9800 Training Loss: 14.442021453241233 14.93867573599745\n",
      "Epoch: 9810 Training Loss: 14.44133069872476 14.937830468168775\n",
      "Epoch: 9820 Training Loss: 14.440822595642933 14.93799487193113\n",
      "Epoch: 9830 Training Loss: 14.44018331562168 14.937317329152938\n",
      "Epoch: 9840 Training Loss: 14.439620417109458 14.93748007227123\n",
      "Epoch: 9850 Training Loss: 14.439030951115834 14.936301014985652\n",
      "Epoch: 9860 Training Loss: 14.43837008501594 14.936440509087044\n",
      "Epoch: 9870 Training Loss: 14.43790681455898 14.935822749495165\n",
      "Epoch: 9880 Training Loss: 14.437312367162592 14.935694879902222\n",
      "Epoch: 9890 Training Loss: 14.436643198724761 14.934879503666702\n",
      "Epoch: 9900 Training Loss: 14.436136756110521 14.934304920820491\n",
      "Epoch: 9910 Training Loss: 14.435542308714133 14.933868171431609\n",
      "Epoch: 9920 Training Loss: 14.434989373007438 14.93364730577107\n",
      "Epoch: 9930 Training Loss: 14.434413190754517 14.933273660856626\n",
      "Epoch: 9940 Training Loss: 14.43393331562168 14.93289503400999\n",
      "Epoch: 9950 Training Loss: 14.433343849628056 14.932444999468593\n",
      "Epoch: 9960 Training Loss: 14.432800876726887 14.93192023594431\n",
      "Epoch: 9970 Training Loss: 14.432287792242295 14.931661175470294\n",
      "Epoch: 9980 Training Loss: 14.431804596174283 14.931249335742374\n",
      "Epoch: 9990 Training Loss: 14.431243358129649 14.930777712828144\n",
      "Epoch: 10000 Training Loss: 14.430723631774708 14.930711287065575\n",
      "Epoch: 10010 Training Loss: 14.430185640276301 14.930234682219153\n",
      "Epoch: 10020 Training Loss: 14.429682518597238 14.930066957168668\n",
      "Epoch: 10030 Training Loss: 14.429169434112646 14.929704936762674\n",
      "Epoch: 10040 Training Loss: 14.428667972901168 14.929198440323095\n",
      "Epoch: 10050 Training Loss: 14.428287725823592 14.928743423849506\n",
      "Epoch: 10060 Training Loss: 14.42775305526036 14.92874508449357\n",
      "Epoch: 10070 Training Loss: 14.427186835812964 14.92792970825805\n",
      "Epoch: 10080 Training Loss: 14.426685374601488 14.927934690190243\n",
      "Epoch: 10090 Training Loss: 14.4263333554729 14.927547760123286\n",
      "Epoch: 10100 Training Loss: 14.42574222901169 14.927120974598788\n",
      "Epoch: 10110 Training Loss: 14.42529888416578 14.927237219683281\n",
      "Epoch: 10120 Training Loss: 14.42477251594049 14.926504875650972\n",
      "Epoch: 10130 Training Loss: 14.42423618490967 14.92643678924434\n",
      "Epoch: 10140 Training Loss: 14.423925677470775 14.925960184397917\n",
      "Epoch: 10150 Training Loss: 14.423324588204038 14.926024949516421\n",
      "Epoch: 10160 Training Loss: 14.42281648512221 14.925178021043681\n",
      "Epoch: 10170 Training Loss: 14.422422954303933 14.924784448400468\n",
      "Epoch: 10180 Training Loss: 14.422029423485654 14.924540333723032\n",
      "Epoch: 10190 Training Loss: 14.421556190223168 14.924327771282814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10200 Training Loss: 14.420958421891605 14.923562214369221\n",
      "Epoch: 10210 Training Loss: 14.420461942082891 14.923271601657987\n",
      "Epoch: 10220 Training Loss: 14.420093318278427 14.922984310234881\n",
      "Epoch: 10230 Training Loss: 14.419600159404888 14.922374853863323\n",
      "Epoch: 10240 Training Loss: 14.419115302869288 14.921770379423956\n",
      "Epoch: 10250 Training Loss: 14.418612181190223 14.921692329152938\n",
      "Epoch: 10260 Training Loss: 14.41806588735388 14.920984894781592\n",
      "Epoch: 10270 Training Loss: 14.417640807651434 14.920486701562334\n",
      "Epoch: 10280 Training Loss: 14.417214067481403 14.920123020512275\n",
      "Epoch: 10290 Training Loss: 14.416672755047822 14.919878905834839\n",
      "Epoch: 10300 Training Loss: 14.416282545164718 14.919687931767458\n",
      "Epoch: 10310 Training Loss: 14.415819274707758 14.919305983632691\n",
      "Epoch: 10320 Training Loss: 14.415302869287991 14.919065190243384\n",
      "Epoch: 10330 Training Loss: 14.414801408076514 14.918043894143905\n",
      "Epoch: 10340 Training Loss: 14.414271718916046 14.917799779466469\n",
      "Epoch: 10350 Training Loss: 14.413795164718385 14.917298264959081\n",
      "Epoch: 10360 Training Loss: 14.413282080233794 14.916730324689128\n",
      "Epoch: 10370 Training Loss: 14.412842056323061 14.91649783452014\n",
      "Epoch: 10380 Training Loss: 14.412312367162594 14.916346715910299\n",
      "Epoch: 10390 Training Loss: 14.411937101487778 14.915664191199914\n",
      "Epoch: 10400 Training Loss: 14.411447263549416 14.915220799234776\n",
      "Epoch: 10410 Training Loss: 14.411040448990436 14.91501321872675\n",
      "Epoch: 10420 Training Loss: 14.410640276301807 14.914868742693166\n",
      "Epoch: 10430 Training Loss: 14.410090661530287 14.91425264374535\n",
      "Epoch: 10440 Training Loss: 14.409640674814028 14.914064990966097\n",
      "Epoch: 10450 Training Loss: 14.40921891604676 14.913523621001168\n",
      "Epoch: 10460 Training Loss: 14.408749003719448 14.913507014560528\n",
      "Epoch: 10470 Training Loss: 14.408328905419767 14.913236329578064\n",
      "Epoch: 10480 Training Loss: 14.407950318809776 14.912643479647146\n",
      "Epoch: 10490 Training Loss: 14.4074488575983 14.912738136358806\n",
      "Epoch: 10500 Training Loss: 14.406972303400638 14.912085503241578\n",
      "Epoch: 10510 Training Loss: 14.406532279489904 14.911999149750239\n",
      "Epoch: 10520 Training Loss: 14.406050743889478 14.911354819853331\n",
      "Epoch: 10530 Training Loss: 14.405607399043571 14.910836698905303\n",
      "Epoch: 10540 Training Loss: 14.405371612646121 14.910996120735467\n",
      "Epoch: 10550 Training Loss: 14.404803732731137 14.910064499415453\n",
      "Epoch: 10560 Training Loss: 14.404360387885228 14.909923344669997\n",
      "Epoch: 10570 Training Loss: 14.403968517534537 14.909629410670634\n",
      "Epoch: 10580 Training Loss: 14.40356668437832 14.909405223721969\n",
      "Epoch: 10590 Training Loss: 14.40313164187035 14.909094683281964\n",
      "Epoch: 10600 Training Loss: 14.402769659936238 14.908285949622702\n",
      "Epoch: 10610 Training Loss: 14.402445868756642 14.90827764640238\n",
      "Epoch: 10620 Training Loss: 14.4020473565356 14.907917286640451\n",
      "Epoch: 10630 Training Loss: 14.401539253453773 14.907448985014348\n",
      "Epoch: 10640 Training Loss: 14.401288522848034 14.907638298437666\n",
      "Epoch: 10650 Training Loss: 14.400883368756642 14.907085303964289\n",
      "Epoch: 10660 Training Loss: 14.40054297290117 14.907075340099905\n",
      "Epoch: 10670 Training Loss: 14.400152763018065 14.906509060474015\n",
      "Epoch: 10680 Training Loss: 14.399752590329436 14.906279891593156\n",
      "Epoch: 10690 Training Loss: 14.399319208289054 14.906010867254755\n",
      "Epoch: 10700 Training Loss: 14.39898711477152 14.905612312679349\n",
      "Epoch: 10710 Training Loss: 14.398655021253985 14.905688702306302\n",
      "Epoch: 10720 Training Loss: 14.398186769394261 14.905001195663726\n",
      "Epoch: 10730 Training Loss: 14.397839731668437 14.904860040918269\n",
      "Epoch: 10740 Training Loss: 14.397447861317747 14.904639175257731\n",
      "Epoch: 10750 Training Loss: 14.397059311902233 14.904449861834413\n",
      "Epoch: 10760 Training Loss: 14.39662426939426 14.903857011903497\n",
      "Epoch: 10770 Training Loss: 14.396235719978746 14.903387049633329\n",
      "Epoch: 10780 Training Loss: 14.395910268331562 14.903466760548412\n",
      "Epoch: 10790 Training Loss: 14.39566950053135 14.902968567329152\n",
      "Epoch: 10800 Training Loss: 14.39505512752391 14.902169797534276\n",
      "Epoch: 10810 Training Loss: 14.394673219978745 14.902100050483579\n",
      "Epoch: 10820 Training Loss: 14.39429297290117 14.901869220958657\n",
      "Epoch: 10830 Training Loss: 14.393974163124335 14.90168821075566\n",
      "Epoch: 10840 Training Loss: 14.393552404357067 14.901834347433308\n",
      "Epoch: 10850 Training Loss: 14.393270124867163 14.901178393027951\n",
      "Epoch: 10860 Training Loss: 14.392748738044633 14.900628719842704\n",
      "Epoch: 10870 Training Loss: 14.392493026036131 14.900313197470506\n",
      "Epoch: 10880 Training Loss: 14.391996546227418 14.90017204272505\n",
      "Epoch: 10890 Training Loss: 14.391621280552604 14.899775148793708\n",
      "Epoch: 10900 Training Loss: 14.39130247077577 14.89950280316718\n",
      "Epoch: 10910 Training Loss: 14.390968716790649 14.899009591880114\n",
      "Epoch: 10920 Training Loss: 14.390458953241232 14.898863455202466\n",
      "Epoch: 10930 Training Loss: 14.390223166843784 14.898511398660856\n",
      "Epoch: 10940 Training Loss: 14.38975491498406 14.898174287915825\n",
      "Epoch: 10950 Training Loss: 14.389293304994686 14.89770100435753\n",
      "Epoch: 10960 Training Loss: 14.389017667375132 14.898087934424487\n",
      "Epoch: 10970 Training Loss: 14.388501261955367 14.8975864199171\n",
      "Epoch: 10980 Training Loss: 14.38818411264612 14.89723436337549\n",
      "Epoch: 10990 Training Loss: 14.387830433049947 14.89699190934212\n",
      "Epoch: 11000 Training Loss: 14.387433581296493 14.896389095546818\n",
      "Epoch: 11010 Training Loss: 14.387035069075452 14.896352561377405\n",
      "Epoch: 11020 Training Loss: 14.386669766206163 14.896118410564354\n",
      "Epoch: 11030 Training Loss: 14.386419035600426 14.895864332022532\n",
      "Epoch: 11040 Training Loss: 14.385992295430393 14.895482383887767\n",
      "Epoch: 11050 Training Loss: 14.385698392667376 14.895786281751514\n",
      "Epoch: 11060 Training Loss: 14.385291578108395 14.89496426293974\n",
      "Epoch: 11070 Training Loss: 14.384876461211476 14.894799859177382\n",
      "Epoch: 11080 Training Loss: 14.384472967587673 14.894304987246253\n",
      "Epoch: 11090 Training Loss: 14.384202311370881 14.89419704538208\n",
      "Epoch: 11100 Training Loss: 14.383778892136025 14.893512860027633\n",
      "Epoch: 11110 Training Loss: 14.383435175345378 14.893343474333085\n",
      "Epoch: 11120 Training Loss: 14.383003453772583 14.893097699011586\n",
      "Epoch: 11130 Training Loss: 14.382686304463338 14.892808746944414\n",
      "Epoch: 11140 Training Loss: 14.382274508501594 14.892493224572219\n",
      "Epoch: 11150 Training Loss: 14.381983926673751 14.892895100435753\n",
      "Epoch: 11160 Training Loss: 14.381623605207226 14.89211791901371\n",
      "Epoch: 11170 Training Loss: 14.38122177205101 14.891893732065045\n",
      "Epoch: 11180 Training Loss: 14.38097934378321 14.891564924540333\n",
      "Epoch: 11190 Training Loss: 14.380404821997875 14.891633010946965\n",
      "Epoch: 11200 Training Loss: 14.380061105207226 14.891332434371346\n",
      "Epoch: 11210 Training Loss: 14.379647648777896 14.890772797321713\n",
      "Epoch: 11220 Training Loss: 14.379255778427206 14.890540307152726\n",
      "Epoch: 11230 Training Loss: 14.378829038257173 14.890631642576256\n",
      "Epoch: 11240 Training Loss: 14.378513549415516 14.89027626474652\n",
      "Epoch: 11250 Training Loss: 14.378178134962806 14.889784714103518\n",
      "Epoch: 11260 Training Loss: 14.377779622741764 14.889600382612393\n",
      "Epoch: 11270 Training Loss: 14.377401036131774 14.889374535019662\n",
      "Epoch: 11280 Training Loss: 14.376917840063761 14.888765078648103\n",
      "Epoch: 11290 Training Loss: 14.376685374601488 14.888733526410883\n",
      "Epoch: 11300 Training Loss: 14.376258634431457 14.888170568073122\n",
      "Epoch: 11310 Training Loss: 14.375836875664188 14.887796923158678\n",
      "Epoch: 11320 Training Loss: 14.375454968119023 14.887446527261133\n",
      "Epoch: 11330 Training Loss: 14.375079702444209 14.887160896482092\n",
      "Epoch: 11340 Training Loss: 14.374740967056322 14.886682630991604\n",
      "Epoch: 11350 Training Loss: 14.37442215727949 14.886576349771495\n",
      "Epoch: 11360 Training Loss: 14.374021984590861 14.886825446381124\n",
      "Epoch: 11370 Training Loss: 14.37360188629118 14.886056568179402\n",
      "Epoch: 11380 Training Loss: 14.373185108926673 14.885658013603996\n",
      "Epoch: 11390 Training Loss: 14.372846373538788 14.88549360984164\n",
      "Epoch: 11400 Training Loss: 14.3725890010627 14.885913752789882\n",
      "Epoch: 11410 Training Loss: 14.372183846971307 14.885188051333829\n",
      "Epoch: 11420 Training Loss: 14.371826846439957 14.884928990859816\n",
      "Epoch: 11430 Training Loss: 14.371378520191286 14.884537078860665\n",
      "Epoch: 11440 Training Loss: 14.371036463868226 14.884314552556063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11450 Training Loss: 14.37074754250797 14.883854554150282\n",
      "Epoch: 11460 Training Loss: 14.370315820935176 14.883494194388351\n",
      "Epoch: 11470 Training Loss: 14.36998372741764 14.883595493676268\n",
      "Epoch: 11480 Training Loss: 14.369596838469713 14.883127192050164\n",
      "Epoch: 11490 Training Loss: 14.369317879914984 14.882550948559889\n",
      "Epoch: 11500 Training Loss: 14.368804795430393 14.882426400255074\n",
      "Epoch: 11510 Training Loss: 14.368502590329436 14.881929867679881\n",
      "Epoch: 11520 Training Loss: 14.368135626992562 14.881635933680519\n",
      "Epoch: 11530 Training Loss: 14.367753719447396 14.881679110426187\n",
      "Epoch: 11540 Training Loss: 14.367419965462274 14.881506403443511\n",
      "Epoch: 11550 Training Loss: 14.36696665781084 14.88099492507174\n",
      "Epoch: 11560 Training Loss: 14.36668105738576 14.880657814326709\n",
      "Epoch: 11570 Training Loss: 14.366319075451647 14.880103159209268\n",
      "Epoch: 11580 Training Loss: 14.365963735387885 14.88021110107344\n",
      "Epoch: 11590 Training Loss: 14.365558581296494 14.879769369752365\n",
      "Epoch: 11600 Training Loss: 14.365196599362381 14.879227999787437\n",
      "Epoch: 11610 Training Loss: 14.364822994155155 14.879033704431928\n",
      "Epoch: 11620 Training Loss: 14.364495882040382 14.878434211924754\n",
      "Epoch: 11630 Training Loss: 14.364213602550478 14.878518904772028\n",
      "Epoch: 11640 Training Loss: 14.363810108926673 14.878030675417154\n",
      "Epoch: 11650 Training Loss: 14.363466392136026 14.8779775348071\n",
      "Epoch: 11660 Training Loss: 14.363061238044633 14.877703528536507\n",
      "Epoch: 11670 Training Loss: 14.3627291445271 14.877158837283453\n",
      "Epoch: 11680 Training Loss: 14.362380446333688 14.877082447656498\n",
      "Epoch: 11690 Training Loss: 14.362011822529224 14.876778549792752\n",
      "Epoch: 11700 Training Loss: 14.361719580233794 14.876808441385908\n",
      "Epoch: 11710 Training Loss: 14.361414054197661 14.876223894675311\n",
      "Epoch: 11720 Training Loss: 14.361138416578108 14.876514507386545\n",
      "Epoch: 11730 Training Loss: 14.360779755579172 14.876149165692421\n",
      "Epoch: 11740 Training Loss: 14.360450982996811 14.875582886066532\n",
      "Epoch: 11750 Training Loss: 14.360069075451648 14.875327146880647\n",
      "Epoch: 11760 Training Loss: 14.359793437832094 14.875242454033373\n",
      "Epoch: 11770 Training Loss: 14.359438097768331 14.874604766712721\n",
      "Epoch: 11780 Training Loss: 14.359006376195536 14.874493503560421\n",
      "Epoch: 11790 Training Loss: 14.358700850159405 14.874123179934106\n",
      "Epoch: 11800 Training Loss: 14.358395324123274 14.87415307152726\n",
      "Epoch: 11810 Training Loss: 14.358096439957492 14.873903974917631\n",
      "Epoch: 11820 Training Loss: 14.357767667375132 14.873583470613243\n",
      "Epoch: 11830 Training Loss: 14.357415648246546 14.873194879902222\n",
      "Epoch: 11840 Training Loss: 14.357056987247608 14.872982317462004\n",
      "Epoch: 11850 Training Loss: 14.356796293836345 14.872645206716973\n",
      "Epoch: 11860 Training Loss: 14.35638615834219 14.872044053565736\n",
      "Epoch: 11870 Training Loss: 14.356077311370882 14.872132067701138\n",
      "Epoch: 11880 Training Loss: 14.355783408607865 14.871833151769582\n",
      "Epoch: 11890 Training Loss: 14.355248738044633 14.871163912211712\n",
      "Epoch: 11900 Training Loss: 14.355006309776833 14.870944707195239\n",
      "Epoch: 11910 Training Loss: 14.354561304463337 14.8707470905516\n",
      "Epoch: 11920 Training Loss: 14.354200982996812 14.870685646721224\n",
      "Epoch: 11930 Training Loss: 14.35390209883103 14.870461459772558\n",
      "Epoch: 11940 Training Loss: 14.353491963336875 14.870109403230948\n",
      "Epoch: 11950 Training Loss: 14.353120018597236 14.869855324689128\n",
      "Epoch: 11960 Training Loss: 14.352741431987248 14.86942023594431\n",
      "Epoch: 11970 Training Loss: 14.35233295696068 14.869496625571262\n",
      "Epoch: 11980 Training Loss: 14.352092189160468 14.869332221808907\n",
      "Epoch: 11990 Training Loss: 14.351592388416577 14.869013378148582\n",
      "Epoch: 12000 Training Loss: 14.351301806588735 14.868631430013817\n",
      "Epoch: 12010 Training Loss: 14.350802005844846 14.868176413540228\n",
      "Epoch: 12020 Training Loss: 14.350526368225292 14.868403921777022\n",
      "Epoch: 12030 Training Loss: 14.350048153560042 14.86778948347327\n",
      "Epoch: 12040 Training Loss: 14.349662925079702 14.867606812626208\n",
      "Epoch: 12050 Training Loss: 14.349417175876727 14.867684862897226\n",
      "Epoch: 12060 Training Loss: 14.348953905419766 14.866915984695504\n",
      "Epoch: 12070 Training Loss: 14.348560374601488 14.866500823679456\n",
      "Epoch: 12080 Training Loss: 14.348214997343252 14.86668515517058\n",
      "Epoch: 12090 Training Loss: 14.347876261955367 14.866102269104049\n",
      "Epoch: 12100 Training Loss: 14.34757405685441 14.865929562121373\n",
      "Epoch: 12110 Training Loss: 14.347070935175346 14.86572696354554\n",
      "Epoch: 12120 Training Loss: 14.346765409139213 14.865119167818047\n",
      "Epoch: 12130 Training Loss: 14.346441617959618 14.865479527579977\n",
      "Epoch: 12140 Training Loss: 14.346063031349628 14.864438303751728\n",
      "Epoch: 12150 Training Loss: 14.345686105207227 14.864175921989585\n",
      "Epoch: 12160 Training Loss: 14.345428732731136 14.864114478159209\n",
      "Epoch: 12170 Training Loss: 14.345073392667375 14.864089568498246\n",
      "Epoch: 12180 Training Loss: 14.34479277364506 14.863664443617813\n",
      "Epoch: 12190 Training Loss: 14.344452377789585 14.863211087788288\n",
      "Epoch: 12200 Training Loss: 14.344095377258236 14.862593328196407\n",
      "Epoch: 12210 Training Loss: 14.34367859989373 14.862837442873845\n",
      "Epoch: 12220 Training Loss: 14.343421227417641 14.862545169518546\n",
      "Epoch: 12230 Training Loss: 14.343115701381508 14.862033691146774\n",
      "Epoch: 12240 Training Loss: 14.342745417109459 14.862037012434902\n",
      "Epoch: 12250 Training Loss: 14.342529556323061 14.861522212775002\n",
      "Epoch: 12260 Training Loss: 14.342190820935175 14.86148899989372\n",
      "Epoch: 12270 Training Loss: 14.341812234325186 14.861186762674036\n",
      "Epoch: 12280 Training Loss: 14.34146685706695 14.860984164098204\n",
      "Epoch: 12290 Training Loss: 14.341118158873538 14.86073672813264\n",
      "Epoch: 12300 Training Loss: 14.340862446865037 14.86047932830269\n",
      "Epoch: 12310 Training Loss: 14.340536995217853 14.860040918269743\n",
      "Epoch: 12320 Training Loss: 14.340208222635495 14.859866550643002\n",
      "Epoch: 12330 Training Loss: 14.339852882571732 14.85948128122011\n",
      "Epoch: 12340 Training Loss: 14.33948259829968 14.858851897119779\n",
      "Epoch: 12350 Training Loss: 14.339187035069076 14.858782150069082\n",
      "Epoch: 12360 Training Loss: 14.338881509032944 14.858564605696673\n",
      "Epoch: 12370 Training Loss: 14.338526168969182 14.858353703900521\n",
      "Epoch: 12380 Training Loss: 14.338215661530286 14.857797388139016\n",
      "Epoch: 12390 Training Loss: 14.337850358660999 14.857456956105857\n",
      "Epoch: 12400 Training Loss: 14.337588004782146 14.857368941970455\n",
      "Epoch: 12410 Training Loss: 14.337280818278428 14.857121506004889\n",
      "Epoch: 12420 Training Loss: 14.336966989904358 14.856560208311192\n",
      "Epoch: 12430 Training Loss: 14.336714598831032 14.856447284514827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-7558cd910239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m        \u001b[1;31m# loss= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#loss=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*4+2)).sum().cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_torch_loss_function3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msum_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0] )\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "       # loss= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*4+2)).sum().cuda()\n",
    "        loss=my_torch_loss_function3(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*4+2),x_variable.narrow(0,b,batch_size)[:,24],.4).sum().cuda()\n",
    "\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net.eval()\n",
    "        out_test=my_net(x_variable_test)   \n",
    "        test_loss=my_torch_loss_function3(out_test, y_variable_test.resize(y_variable_test.size()[0],nb_of_points*4+2),x_variable_test[:,24],.4).sum().cuda()\n",
    "\n",
    "        #test_loss=loss_func(out_test, y_variable_test)\n",
    "       # test_loss=my_torch_loss_function(out_test, y_variable_test.resize(len(y_variable_test),nb_of_points*4+2)).sum()\n",
    "\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),test_loss.data[0]/(x_variable_test.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising outputs on the test data \n",
    "my_net.eval()\n",
    "plt.clf()\n",
    "sample_index=8\n",
    "sample_tests=out_test.cpu()\n",
    "sample_tests=sample_tests.data.numpy()\n",
    "\n",
    "sample_coordinates=acquire_points_from_deformation(sample_tests[sample_index],nb_of_points)\n",
    "sample_coordinates=np.array(sample_coordinates).reshape(2*nb_of_points,2)\n",
    "\n",
    "real_coordinates=point_coordinates[nb_of_training_data+sample_index].reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "sample_contour=np.delete(polygons_reshaped[nb_of_training_data+sample_index],24).reshape(12,2)\n",
    "plot_contour(sample_contour)\n",
    "plt.scatter(sample_coordinates[:,0],sample_coordinates[:,1])\n",
    "plt.scatter(real_coordinates[:,0],real_coordinates[:,1],marker='x')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.2117492449085283 -0.2172432296902659 0\n",
      "14 -0.638198886730653 -0.2617212038515113 0\n",
      "15 -0.2473971754530878 0.1936550456069948 -0\n",
      "16 -0.2307674195237404 -0.6989718699045068 0\n",
      "17 0.231041415365946 -0.02815582919811441 0\n",
      "18 -0.7063818241783092 0.007937554617718463 -0\n",
      "19 0.144202745188038 -0.472718999473605 0\n",
      "20 0.6373473864642478 -0.1259208313570763 0\n",
      "21 -0.5802597784760987 -0.5157365262388072 0\n",
      "22 -0.3944963989730145 0.3612105992382272 -0\n",
      "23 -0.1398466660218595 0.4531664373820274 -0\n",
      "24 -0.5089314194438258 0.1722965644613412 -0\n",
      "25 0.0497377137986519 0.2545479108400921 0\n",
      "26 -0.4009117118727608 -0.0606611890113806 0\n",
      "27 0.6183329983525365 -0.3827890866514956 0\n",
      "28 -0.08963547658971638 -0.01858264055326609 0\n",
      "29 0.04207276911079789 -0.2535295312034795 0\n",
      "30 -0.4027816247210394 -0.3007589976066427 0\n",
      "31 0.08381072597209621 -0.6599483039809222 0\n",
      "32 0.1659902077555062 0.5502312232444658 0\n",
      "33 -0.1650674039083535 -0.445558208299747 0\n",
      "34 0.3543016825971604 -0.3370056797081165 0\n",
      "13 -0.2792570949366011 -0.06717509236909948 0\n",
      "14 0.1441518129810692 0.5585677846258492 0\n",
      "15 -0.5803453390989046 0.3026077598722483 -0\n",
      "16 0.1281262294649456 -0.2271801633628059 0\n",
      "17 0.5151742989895626 -0.2822633984176287 0\n",
      "18 0.04580678322413775 0.1323081517217835 0\n",
      "13 0.2742210812231831 -0.06652235127714626 0\n",
      "14 0.5772657063339057 -0.1523480233760682 0\n",
      "15 0.3551101671135274 -0.539563033391405 0\n",
      "16 0.6078013549978187 -0.4654928952456915 0\n",
      "17 -0.1170369529684146 -0.1969315495766566 0\n",
      "18 0.7058718867860674 -0.03124170234535162 0\n",
      "19 -0.6803991548960046 -0.03936363345051321 0\n",
      "20 0.33387641258213 0.3712142204391692 0\n",
      "21 0.6552308586155689 -0.2902703200836259 0\n",
      "22 0.7654759213650959 -0.2146485157233289 0\n",
      "23 -0.03828313050635591 0.1885635007016091 -0\n",
      "24 0.5185636550433552 0.1056342958664824 0\n",
      "25 0.09754497819250604 -0.3899132562962843 0\n",
      "26 -0.7572241968355071 -0.3425846566066387 0\n",
      "27 -0.4513180801684316 -0.1912856332168771 0\n",
      "28 0.4336151214200836 -0.3204975624687315 0\n",
      "13 0.1911843939239121 0.008628706741375992 0\n",
      "14 -0.2702721990244786 -0.4951356260470111 0\n",
      "15 0.5808343383478389 -0.09593303665153904 0\n",
      "16 0.3159615604887429 0.4818354588333155 0\n",
      "17 -0.3681939944041789 0.3897074876945182 -0\n",
      "18 0.2348120647713784 -0.2978242842070607 0\n",
      "19 0.02885848117954433 0.34634850561271 0\n",
      "20 -0.1425862403792633 -0.233510543888259 0\n",
      "21 -0.1644984485555826 0.09247625595938296 -0\n",
      "22 0.4762627513449144 -0.3698334763253634 0\n",
      "23 0.40421434898773 0.7224812775903529 0\n",
      "24 0.4410165971251253 0.2132717791113883 0\n",
      "25 -0.4983659845822273 -0.439399061191521 0\n",
      "26 -0.08236811898220196 -0.6460367315015186 0\n",
      "27 0.04079137349848239 -0.4545269221317263 0\n",
      "13 0.1134356083854852 -0.07460781670151157 0\n",
      "14 -0.01708929871609583 0.5799153859671604 0\n",
      "15 0.1600415106997422 0.2676248608425854 0\n",
      "13 0.4140353768764594 -0.05123114684398513 0\n",
      "14 -0.05367157132532067 -0.2391071449577064 0\n",
      "15 -0.5852970833568791 -0.2244023203721118 0\n",
      "16 0.5136870416809403 -0.3521627372097241 0\n",
      "17 0.08324005617474255 0.1917651805853116 0\n",
      "18 0.7549396159253392 -0.05000809422731006 0\n",
      "19 0.4688443067112018 0.2212269909444836 0\n",
      "20 -0.04630213578425112 -0.5644028595464417 0\n",
      "21 -0.1668084073052723 0.6144763076603093 0\n",
      "22 0.2588960610251411 -0.3207122112999669 0\n",
      "barycenter is  [ 0.13686639 -0.01783803]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032455243871699"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3Xd4VFX+x/H3mcmkN0IKqRAIPYQQQlGQIoKUCGKhiErRRXftrquwu7rKuorr/kRdXRUVxBaEtQE2uqwIJAEhdAgQkhAISYD0NpPz+2MmYRISSJlkZpLzep48ydy5d+43lPnMOefec4SUEkVRFEWporF2AYqiKIptUcGgKIqi1KCCQVEURalBBYOiKIpSgwoGRVEUpQYVDIqiKEoNKhgURVGUGlQwKIqiKDWoYFAURVFqcLB2AU3h6+sru3TpYu0yFEVR7Mru3btzpJR+19rPLoOhS5cuJCUlWbsMRVEUuyKEON2Q/VRXkqIoilKDCoZ2wmAwMGDAAOLi4qxdiqIoNk4FQzvxxhtv0Lt3b2uXoSiKHbDLMQalcTIyMvjuu+/4y1/+wmuvvWbtchQ7UVFRQUZGBqWlpdYuRWkkZ2dnQkJC0Ol0TTpeBUM78Pjjj/PPf/6TgoICa5ei2JGMjAw8PDzo0qULQghrl6M0kJSS3NxcMjIyCA8Pb9JrqK6kNm7dunX4+/szcOBAa5ei2JnS0lI6duyoQsHOCCHo2LFjs1p6KhjauO3bt7NmzRq6dOnCjBkz2Lx5M3fffbe1y1LshAoF+9TcvzcVDG3Y7tMXiJg0n0827mFr0kE+++xzbrzxRj799FNrl6Yoig1TYwxt2DtbT7LxcFb144r0/ZSdzOW+jxIJ93Uj3M+NcF83uvq6E+DppD4dKjZHq9XSr18/9Ho9vXv3ZsWKFbi6uta7//XXX8+vv/561dd8/fXXmT9/fp2vM2rUKP71r38RGxvb7NoB9u7dS2ZmJhMnTgRgzZo1HDp0iAULFljk9VuKCoY2bGQPXzYezuKlqf3QaQWncrpxasJYTuUU8UtKDmX6yup9XR21dOloCouObtXB0dXXDW9XRyv+Foq9kFLW+HBR+3FTuLi4sHfvXgBmzZrFu+++y5NPPlnv/tcKBTAGw913333VgLGUvXv3kpSUVB0MkydPZvLkyS1+3uZSwdCGjY8M5G9rDpJ5qYSnbu5Z47nKSsnZ/FJSc4o4mVPEqewiTuUUcvBMHj8eOIehUlbv28FVZwwKX3e6mloZXTq60cXXFVdH9U9IgSUbjpFfWsFzcX0QQiClZNG6Q3g663hibA+LnOOGG24gOTkZgNdee41ly5YBcP/99/P4448D4O7uTmFhIVu3buX555/H19eXAwcOMHDgQD799FP+/e9/k5mZyejRo/H19WXLli31ni8+Pp6XXnoJKSWTJk3ilVdeAeDHH3/kz3/+MwaDAV9fXzZt2kRCQgKPP/44JSUluLi4sHz5csLDw3nuuecoKSnhl19+YeHChZSUlJCUlMRbb73F6dOnmTdvHtnZ2fj5+bF8+XLCwsKYM2cOnp6eJCUlce7cOf75z39yxx13WOTPsKHU/+o2zM/Dieu6dWRdciZ/HNejxqc3jUYQ7O1CsLcLwyJ8axxXrq8k/WKxKSyKOJVrDI7tKTl8uSejxr6BXs6m0DB1S/kZAySkgws6rW0PYZWWljJixAjKysrQ6/XccccdvPDCC9Yuy+5IKckvrWD59lQAnovrw6J1h1i+PZW5w7pYpOWg1+v54YcfGD9+PLt372b58uXs2rULKSVDhgxh5MiRDBgwoMYxv/32GwcPHiQoKIhhw4axfft2Hn30UV577TW2bNmCr69vPWeDzMxMnnnmGXbv3k2HDh0YN24c33zzDcOGDeN3v/sd27ZtIzw8nAsXLgDQq1cvtm3bhoODAxs3buTPf/4zX375JYsWLaoOAoCPPvqo+hwPP/ww9957L7Nnz2bZsmU8+uijfPPNNwCcPXuWX375hSNHjjB58mQVDIplxUUFsfCr/RzMzCcy2KtBxzg6aOjm5043P/crnisq05OaawoMU3CczCliXfJZ8koqqvdz0AhCfVxrhoapeyrAwxmNxvrjGU5OTmzevBl3d3cqKioYPnw4EyZMYOjQodYuza4IIXgurg8Ay7enVgfE3GFdqlsQTVVSUkJ0dDRgbDHcd999vPPOO0ydOhU3NzcAbrvtNv73v/9dEQyDBw8mJCQEgOjoaFJTUxk+fHiDzpuYmMioUaPw8zNORDpr1iy2bduGVqtlxIgR1fcH+Pj4AJCXl8fs2bM5fvw4QggqKirqfe0qO3bs4KuvvgLgnnvu4emnn65+7tZbb0Wj0dCnTx+ysrLqe4kWo4KhjRvftxPPfnOAtcmZDQ6Gq3FzcqBvkBd9g658rYtF5cZuqRxjt9SpnCJOZhfx64kcSisuj2e46LR08XUj3Ne1uouqKjg6uLXeeIYQAnd3Y/hVVFRQUVGhBuCbqCocqkIBaHYoQM0xhipSynr2rsnJyan6Z61Wi16vb/B56ztHfa2fZ599ltGjR/P111+TmprKqFGjGnyuKuava157Q39fS1LB0MZ1cHNkWIQv3yWfZcH4Xi36xtfBzZGBbo4M7NyhxvbKSsm5/FJTYFz+Ony2gJ8OZtUYz/CuHs8wtTB83eliCpCWGM8wGAwMHDiQlJQUHnroIYYMGWLxc7QHVWMK5hatO2SRcKhtxIgRzJkzhwULFiCl5Ouvv+aTTz5p8PEeHh4UFBRctStpyJAhPPbYY+Tk5NChQwfi4+N55JFHuO6663jooYc4depUdVeSj48PeXl5BAcHAzW7i6rOVZfrr7+elStXcs899/DZZ581uDXTGizyP00IsQyIA85LKSPreF4AbwATgWJgjpRyj+m52cBfTbu+KKVcYYmalMviogL503+T2Zt+iQFhHa59gIVpNIIgbxeC6hjPqDBUkn6huDosqgbCd5zI5as9Z2rs28nTucbVUlUBEurj2uTxDK1Wy969e7l06RJTp07lwIEDREZe8U9YuYqqUKgaUzAfYwDLtBzMxcTEMGfOHAYPHgwYB59rdyNdzfz585kwYQKBgYH1Dj4HBgby8ssvM3r0aKSUTJw4kSlTpgCwdOlSbrvtNiorK/H392fDhg08/fTTzJ49m9dee40bb7yx+nVGjx7N4sWLiY6OZuHChTXO8eabbzJv3jxeffXV6sFnWyEs0UwRQowACoGP6wmGicAjGINhCPCGlHKIEMIHSAJiAQnsBgZKKS9e7XyxsbFSLdTTcHklFQx6cSP3XNeZZ019wfaguFxPak5xddfUSbPWxqXiy324Wo0gtIPL5W4ps+Do5Nnw8YwXXngBNzc3nnrqqZb6lezK4cOHGzwjb2tclaQ0Tl1/f0KI3VLKa96kYZEWg5RymxCiy1V2mYIxNCSwUwjhLYQIBEYBG6SUFwCEEBuA8UC8JepSjLxcdIzoYexO+svE3jYx8NsQro4O9AnypE+Q5xXPXSwqr75ayry1seNkbo3xDGedxnh/Rq0rp/oEelGYdwGdToe3tzclJSVs3LiRZ555pjV/xTbjibE9avS/V405qDEb+9RaYwzBQLrZ4wzTtvq2X0EIMR+YDxAWFtYyVbZhcVFBbDx8nt1pFxnUxcfa5TRbBzdHOrg5EhN25XhGVkEpp7KNQZFqCo2j5wrYcCgLvWk8I6SDC38b5s7TjzyAwWCgsrKSadOmqYWMmqF2CKhQsF+tFQx1/QuRV9l+5UYplwJLwdiVZLnS2oeb+gTg5KBh3b7MNhEM9dFoBIFeLgR6uXB9HeMZGRdLOJiZx8Kv9vPijiJ+/HkHAZ7OVqpWUWxTa92BlAGEmj0OATKvsl2xMHcnB0b39Of7Wnc1tyc6rYZwXzfiooL4eN5gcgrKuOv9nWQXlFm7NEWxKa0VDGuAe4XRUCBPSnkW+AkYJ4ToIIToAIwzbVNaQFz/QLILyth1KtfapVjdgLAOLJ87mMxLpdz9wS4uFJVbuyRFsRkWCQYhRDywA+gphMgQQtwnhHhQCPGgaZfvgZNACvA+8AcA06Dz34FE09eiqoFoxfJu7OWPi07LuuSz1i7FJgwO9+HD2bGk5hZxz4e7yCu+9t2qitIeWCQYpJQzpZSBUkqdlDJESvmhlPJdKeW7puellPIhKWU3KWU/KWWS2bHLpJQRpi/buZC3DXJ1dGBMb39+PHAOvaHy2ge0A9dH+PLePQM5nlXIvcsTKChV4WBLtFot0dHRREZGcuedd1JcXNzk19q6dWv1xQVr1qxh8eLF9e576dIl/vOf/zT6HM8//zz/+te/mlxjlaSkJB599NGr7tPUGhvCtmc5UywuLiqIC0Xl/HpCdSdVGdXTn7fuGsDBM3nMXZ5IUVnDp05QWlbVlBgHDhzA0dGRd999t8bzUkoqKxv/IWfy5MlXXROhJd90GyI2NpY333zzqvuoYFAsZlRPP9ydHFiXrMb4zY3r24k3ZgxgT9pF7l+RRGmFwdol2Z/kVbAkEp73Nn5PXmXRl7/hhhtISUkhNTWV3r1784c//IGYmBjS09NZv3491113HTExMdx5550UFhYCximye/XqxfDhw6snrAPjtBUPP/wwAFlZWUydOpX+/fvTv39/fv31VxYsWMCJEyeIjo7mT3/6EwCvvvoqgwYNIioqir/97W/Vr/WPf/yDnj17ctNNN3H06NE6a58zZw4PPvggN9xwAz169GDdunWAcYbfuXPn0q9fPwYMGFB9J7Z56+b5559n3rx5jBo1iq5du1YHRl01WoqaK6mdcdZpGdsngJ8OZvHirZU4OqjPBlUmRQVSYYjmiVV7mf/Jbt6/dyBODlprl2UfklfB2kehosT4OC/d+BggalqzX9582m2Ao0ePsnz5cv7zn/+Qk5PDiy++yMaNG3Fzc+OVV17htdde4+mnn+Z3v/sdmzdvJiIigunTp9f52o8++igjR47k66+/xmAwUFhYyOLFizlw4ED1BH7r16/n+PHjJCQkIKVk8uTJbNu2DTc3N1auXMlvv/2GXq8nJiaGgQMH1nme1NRUfv75Z06cOMHo0aNJSUnh7bffBmD//v0cOXKEcePGcezYsSuOPXLkCFu2bKGgoICePXvy+9///ooaLUm9K7RDcVGB5JVUsD0lx9ql2JxbBwTzym1RbDuWzUOf7aFcr8ZiGmTTosuhUKWixLi9Gaqm3Y6NjSUsLIz77rsPgM6dO1dPj75z504OHTrEsGHDiI6OZsWKFZw+fZojR44QHh5O9+7dEUJw991313mOzZs38/vf/x4wjml4eV05c/D69etZv349AwYMICYmhiNHjnD8+HH+97//MXXqVFxdXfH09Lzq6mzTpk1Do9HQvXt3unbtypEjR/jll1+45557AOOaDp07d64zGCZNmoSTkxO+vr74+/u3+FTcqsXQDt3Q3Q9PZwfWJmcyupe/tcuxOdMGhVJmqOTZbw7w+Be/8eaMATjY+KJDVpeX0bjtDVTXtNtA9VoMYBxnGDt2LPHxNWfS2bt3r8XuvpZSsnDhQh544IEa219//fUGn6OuO8NbYwrxplD/2tshRwcNN/ftxIaDWaovvR73DO3MXyf15vv95/jj6n3t9qbABvMKadx2Cxo6dCjbt28nJSUFgOLiYo4dO0avXr04deoUJ06cALgiOKqMGTOGd955BzBOw56fn3/FdNk333wzy5Ytqx67OHPmDOfPn2fEiBF8/fXXlJSUUFBQwNq1a+utc/Xq1VRWVnLixAlOnjxJz549GTFiBJ999hkAx44dIy0tjZ49e9b7GuauNqV3c6lgaKfi+gdRUKZn27Fsa5dis+6/oStPj+/Jt3szWfhVMpUqHOo35jnQudTcpnMxbm9hfn5+fPTRR8ycOZOoqCiGDh3KkSNHcHZ2ZunSpUyaNInhw4fTuXPnOo9/44032LJlC/369WPgwIEcPHiQjh07MmzYMCIjI/nTn/7EuHHjuOuuu7juuuvo168fd9xxBwUFBcTExDB9+nSio6O5/fbbueGGG+qts2fPnowcOZIJEybw7rvv4uzszB/+8AcMBgP9+vVj+vTpfPTRRzVaB1dTu0ZLssi0261NTbvdfBWGSgb/YyM3dPfjzZkNn8u+PVqy4RhvbDrO3UPD+PuUyHYzOVxjpt0GjAPQmxYZu4+8QoyhYIGB57Zgzpw5xMXFterazVafdluxPzqthvGRgXy79wwl5QZcHNXVN/V5/KbulOkreffnEzhqtTwb17vdhEOjRE1TQdBGqGBox+KiAolPSGPL0fNM7Bdo7XJslhCCZ8b3pExvYNn2UzjpNDx9c08VDkqDmS/3aQ/UGEM7NiTcB193R3WzWwNULTwza0gY72w9wZubUqxdUquwx65mpfl/b6rF0I45aDVMiAxk9e50isr0uDmpfw5XI4Tg71MiKdNXsmTjMRwdNPx+VDdrl9VinJ2dyc3NpWPHjqp1ZEeklOTm5uLs3PR1RtQ7QTsXFxXIJztPs/FwFlOi61w8TzGj0QheuT2Kcn0lr/x4BEcHDfcND7d2WS0iJCSEjIwMsrPVlWv2xtnZmZCQpl8qrIKhnRvUxYcATyfWJZ9VwdBAWo3gtWn9qTBU8vd1h3B00HDP0LovhbRnOp2O8PC2GXrK1akxhnZOoxFM7BfIz0ezyVdTTjeYg1bDGzMGMKaXP89+c4BVienXPkhR7IQKBoW4qCDKDZVsONiy86+0NY4OGt6eFcMN3X155qtkvt17xtolKYpFWGoFt/FCiKNCiBQhxBWTnAshlggh9pq+jgkhLpk9ZzB7bo0l6lEaJybMm2BvF3V1UhM467QsvSeWoeEdeXLVPr7fr1bHU+xfs4NBCKEF3gYmAH2AmUKIPub7SCmfkFJGSymjgX8DX5k9XVL1nJSy/qkJlRYjhGBSVCD/O57DpWK19nFjuThq+WB2LANCvXk0/jc2HlItL8W+WaLFMBhIkVKelFKWAyuBKVfZfyZQ92xWitXERQWir5T8dPCctUuxS25ODiyfO4i+wV784bM9/KzmoFLsmCWCIRgwH3nLMG27ghCiMxAObDbb7CyESBJC7BRC3FrfSYQQ8037JanL5yyvX7AXYT6urEtWXSFN5eGs4+O5g4nwd2f+x0n8ekKtd6HYJ0sEQ113vtR3290M4L9SSvO5nsNMkzrdBbwuhKjzjiEp5VIpZayUMtbPz695FStXEEIQFxXIrydyyS0ss3Y5dsvLVcen9w+hc0dX7vsoicTUC9YuSVEazRLBkAGEmj0OAeobxZxBrW4kKWWm6ftJYCugpvq0krioIAyVkh8OqO6k5vBxc+Sz+4cS6O3M3OWJ7E2/dO2DFMWGWCIYEoHuQohwIYQjxjf/K64uEkL0BDoAO8y2dRBCOJl+9gWGAYcsUJPSBL0DPejq56auTrIAPw8nPr9/KD5ujtz74S4OnMmzdkmK0mDNDgYppR54GPgJOAysklIeFEIsEkKYX2U0E1gpa87u1BtIEkLsA7YAi6WUKhisxNidFMSuUxc4n19q7XLsXicvZz7/3RA8nHXc8+Eujp5rmdW2FMXS1EI9Sg3HswoYu2Qbz9/ShznD1HQIlnA6t4hp7+3AUClZOf86IvzdrV2S0k41dKEedeezUkP3AA96Bnioq5MsqHNHNz7/3VBAMOuDnaTmFFm7JEW5KhUMyhXiogJJOn2Rs3kl1i6lzejm585n9w+hXF/JrA92kXGx2NolKUq9VDAoV4jrHwTAd6rVYFE9O3nwyX1DKCit4K73d6ngVWyWCgblCuG+bvQN8lTdSS0gMtiLj+8bwoWicma9v4vzBXUP8qenpzN69Gh69+5N3759eeONN1q5UqU9U8Gg1CkuKoi96ZdIv2A/XR61L6Sw1QsrokO9+WjuIM7llzLr/V113lDo4ODA//3f/3H48GF27tzJ22+/zaFD6oI9pXWoYFDqFBcVCMB3djJb6JINx1i07lB1GEgpWbTuEEs2HLNyZXWL7eLDB7NjSbtQzD0fJlwxeWFgYCAxMTEAeHh40Lt3b86cUdN6K61DBYNSp1AfV/qHetvFzW5SSvJLK1i+PbU6HBatO8Ty7ankl1bYbMvh+m6+vH9vLCnnC5m9LKHehZJSU1P57bffGDJkSCtXqLRXKhiUet0SFciBM/k2f3mlEILn4vowd1gXlm9PJXzh9yzfnsrcYV14Lq6PTS9kP6KHH+/cHcPBzHzmLk+kqExf4/nCwkJuv/12Xn/9dTw9Pa1UpdLeqGBQ6jWxn7E7yR5aDVXhYM7WQ6HKmN4B/HvmAPamX+K+FYmUlBvnmKyoqOD2229n1qxZ3HbbbVauUmlPVDAo9QrydiG2cwe7uDqpqvvInPmYg62b0C+Q16b1Z9epC8z/JIm9aReZPWcuvXv35sknn7R2eUo7o4JBuaq4qECOnCsg5bztzvNjPqYwd1gXTr08sbpbyZ7CYUp0MP+8PYpfUnIY/+f3if/8M97/Yg0+YT0I7NqLZ5Z8xO7TF8grqXssQlEsRc2VpFzV+fxShry8iUdv7M4TY3tYu5x6LdlwjPzSiuruo6qw8HTW2XTddcm4WMzBzHxSzhdyPKuA4+cLSTlfSJm+snoffw8nuge4093fgwh/d7r7u9M9wAMfN0crVq7YuobOlaSCQbmm6e/tIKewjI1PjrTpPnspZY36aj+2Z4ZKyZmLJRw/bwyK41mFpGQXkpJVQFH55XWvOro5GoPCFBrd/d2JCHDHz92pzfxZKE3X0GBwaI1iFPsW1z+IZ785wJFzBfQOtN0rY2q/8bWlN0KtRhDW0ZWwjq6M6R1QvV1Kydm8UlNYFBhbGecL+XZvJgWll69w8nLRmVoV7nTzM7Yuuvu7E+jl3Kb+nBTLUMGgXNOEyE787dsDrEvOtOlgaI+EEAR5uxDk7cLIHpeXvJVSkl1QVh0Yx02B8eOBc1wsvjxG4e7kQLeqriizlkawtwsajQqM9soiwSCEGA+8AWiBD6SUi2s9Pwd4Fai6dfMtKeUHpudmA381bX9RSrnCEjUpluPr7sT13XxZl3yWp8b1VJ8w7YAQAn9PZ/w9nRkW4VvjudzCsuqgSMkqICW7kG3Hsvnv7ozqfZx1GtPYRc0xjDAfV7QqMNq8ZgeDEEILvA2Mxbj+c6IQYk0dK7F9IaV8uNaxPsDfgFhAArtNx15sbl2KZcVFBbLgq/0cOJNPvxAva5ejNENHdyc6ujsxtGvHGtvziitIyS7geFZhdXDsOpnL179dnorD0UFDV1+36q6oqlZG545u6LTqIse2whIthsFAipTyJIAQYiUwhYat3XwzsEFKecF07AZgPBBvgboUCxof2Ym/fmPsTlLB0DZ5ueoY2NmHgZ19amwvKK3gRHZRjTGMvekXWbvv8o2PDhpBF1+36rCIMAVHuK8bzjpta/8qSjNZIhiCgXSzxxlAXZO63C6EGAEcA56QUqbXc2ywBWpSLMzb1ZHh3Y3dSQsm9FLdSe2Ih7OO6FBvokO9a2wvLtdzMrvIeKVUlvGS2qPnCvjp4DkqTRc7aoRxBbuIWmMY3fzccXG03cCYN28e69atw9/fnwMHDli7nFZniWCo6x2i9jWwa4F4KWWZEOJBYAVwYwOPNZ5EiPnAfICwsLCmV6s0WVxUEE+t3sdv6ZeICetg7XIUK3N1dCAy2IvI4JotyNIKA6m5RdVdUimm4Nhy5Dx6U2IIASEdXC5fUmsaw4jwd8fdyfrXxMyZM4eHH36Ye++919qlWIUl/gYygFCzxyFAjcl1pJS5Zg/fB14xO3ZUrWO31nUSKeVSYCkY72NoTsFK04zrG4DjVxrW7TurgkGpl7NOS69OnvTqVPMKtgpDJafNAqPqiqlfUnIoN7t5L9DLuXrg29jCMP7s5aprtd9hxIgRpKamttr5bI0lgiER6C6ECMd41dEM4C7zHYQQgVLKqgl3JgOHTT//BLwkhKh6lxkHLLRATUoL8HTWMaKHH9/vP8tfJ/VWlzMqjaLTaojw9yDC34MJZtv1hkrSL5YYxzCyC0kxBUd8QholFZdv3vPzcLpiDKO7vzsd3Z1a/5dp45odDFJKvRDiYYxv8lpgmZTyoBBiEZAkpVwDPCqEmAzogQvAHNOxF4QQf8cYLgCLqgaiFdt0S/9ANh7OIun0RQaH+1z7AEW5BgethnBfN8J93Rhntr2yUnLmUolpwPvy1VJf7jlDodn05D5Vd3ubXVbb3d8dPw91t3dTWaQzT0r5PfB9rW3Pmf28kHpaAlLKZcAyS9ShtLwxvQNwctCwLjlTBUNtyatg0yLIywCvEBjzHERNs3ZVdkujEYT6uBLq48roXv7V26WUnMsvvWIMY13y2RoTDHo4O1R3Q3UPuDyOEaTu9r4m64/yKHbF3cmBG3v58/3+c/ztlr7qZqcqyatg7aNQUWJ8nJdufAwqHCxMCEGglwuBXi6MqH23d2EZKaZ5pIzBUcCmI1l8kXT54kc3Ry0R/u5E1BrDCOmg7vauooJBabS4qCB+OHCOXadyub6b77UPaA82LbocClUqSozbVTC0CiEE/h7O+Hs4c32tu70vFJXX6JJKOV/ILynZfLmn5t3evTp58o+pkbz4xwfZunUrOTk5hISE8MILL3Dfffe19q9kNSoYlEa7sZc/ro5a1iWfVcFQJS+jcduVVuXj5sjgcJ8ruj/zSipIqdUdNXPpTj7+5zvE17pvoz1R97ArjebiqGVM7wB+PHAOvaHy2ge0B14hjduu2AQvFx0DO3dg+qAw/hrXh9UPXoe3qyN3f7CLhFPt9zoYFQxKk8RFBXKhqJxfT+Ree+f2YMxzoHOpuU3nYtyu2I1QH1dWPXAdAZ5OzF6WwC/Hc6xdklWoYFCaZGQPPzycHFiXnHntnduDqGlwy5vgFQoI4/db3lTjC3aok5czXzxwHZ07ujJvRSKbj2RZu6RWp4JBaRJnnZaxfYzdSeZ3rbZrUdPgiQPw/CXjdxUKdsvX3YmV84fSq5MH8z/ezff7z177oDZEBYPSZHH9A8kv1fNLSra1S1EUi/N2deTT+4cQHerNw5/v4evf2s+FBCoYlCYbHuGHl4uOdfva16cppf3wdNaxYt4b4Q21AAAgAElEQVRghnbtyJOr9hGfkGbtklqFCgalyRwdNNzcN4D1h7IoNZvTRlHaEjcnB5bNGcTIHn4s/Go/y7efsnZJLU4Fg9IscVFBFJbp+fmY6k5S2i5nnZb37hnIzX0DeGHtId7ZesLaJbUoFQxKs1zfrSM+bo6sS7bD7qTkVbAkEp73Nn5PXmXtihQb5uSg5a27YpjcP4hXfjzCaxuOIWXbXAFA3fmsNIuDVsP4yE5889sZSsoNNr0qVw1qbiOlCXRaDUumR+Os0/DmpuOUVhhY2AZXNFQtBqXZ4qICKS43sPnIeWuX0nBXm9tIUa5CqxEsvi2Ke6/rzNJtJ3nu24NUVratloMKBqXZhoR3xM/Dyb5udlNzGynNoNEIXpjclwdGdOWTnad55stkDG0oHFRXktJsWo1gYmQnViamU1imt4k1e6/JK8TYfVTXdkVpACEECyb0wlmn5Y1NxynVV/LatP7otPb/edsiv4EQYrwQ4qgQIkUIsaCO558UQhwSQiQLITYJITqbPWcQQuw1fa2xRD1K65sUFUSZvpJNh+1k+gA1t5FiAUIInhjbg2fG92Ltvkwe/nwPZXr7v3S72cEghNACbwMTgD7ATCFEn1q7/QbESimjgP8C/zR7rkRKGW36mtzcehTriO3cgU6ezqy1l5vd1NxGigX9flQ3nr+lDz8dzGL+x7vt/r4eS7T5BwMpUsqTAEKIlcAU4FDVDlLKLWb77wTutsB5FRui0Qgm9gvk052nySupwMtFB0CXLl3w8PBAq9Xi4OBAUlKSlSs1EzVNBYFiMXOGheOs07Lw6/3MXZ7IB7NjcbOHbtU6WKIrKRgw76zNMG2rz33AD2aPnYUQSUKInUKIWy1Qj2Ilcf0DKTdUsuFQze6kLVu2sHfvXtsKBaXBal+r31av3beEGYPDWDItmoTUC9y7LIH80oprH2SDLBEMdV3AW+e/HCHE3UAs8KrZ5jApZSxwF/C6EKJbPcfONwVIUna2usvWFg0I9SbY28W+rk5SrmrJhmMsWneoOgyklCxad4glG45ZuTLbdeuAYN6aOYB96ZeY9f4uLhaVW7ukRrNEMGQAoWaPQ4Ar3hmEEDcBfwEmSynLqrZLKTNN308CW4EBdZ1ESrlUShkrpYz18/OraxfFyoQQxEUF8svxnOr/DEIIxo0bx8CBA1m6dKmVK1QaQ0pJfmkFy7enVofDonWHWL49lfzSCtVyuIoJ/QJZeu9AjmYVMPP9nWQXlF37IBtiiWBIBLoLIcKFEI7ADKDG1UVCiAHAexhD4bzZ9g5CCCfTz77AMMzGJhT7ExcVhL5S8tPBcwBs376dPXv28MMPP/D222+zbds2K1fY8tpK14sQgufi+jB3WBeWb08lfOH3LN+eytxhXXgurk+bu9vX0m7sFcDyOYM4nVvM9KU7OJdXau2SGqzZwSCl1AMPAz8Bh4FVUsqDQohFQoiqq4xeBdyB1bUuS+0NJAkh9gFbgMVSShUMdiwy2JPOHV2r504KCgoCwN/fn6lTp5KQkGDN8lpcW+t6qQoHcyoUGm5YhC8r5g3mfH4Z097bQfqFYmuX1CAWuY9BSvm9lLKHlLKblPIfpm3PSSnXmH6+SUoZUPuyVCnlr1LKflLK/qbvH1qiHsV6hBCM7OHHrydyOJ11gYKCAgCKiopYv349kZGRVq6w5dhF10sjJw6s+h3MmQefcm2Dw3349P4hXCouZ/p7OziVU2Ttkq7JPq+lUmxOSbmBdcmZxCeksSftEs46DafTM5l83ywA9Ho9d911F+PHj7dypS3H/NP18u2pLN+eCmA7XS+NnDjQPNiqfoeqx6BaDo0RHerNyvnXcfeHu5j23g4+u38IPQI8rF1WvYQ9Jn9sbKxUlz7ahkOZ+axMTOPr385QUKqnm58bMweHcVtMCD5ujtYuzyqklIQv/L768amXJ9rGG+iSyHqmAQk1rlFd1yEbjpFfWlEdAlVh4ems44mxPVq44LbneFYBsz7Yhb5S8vG8wUQGe7Xq+YUQu01XgV59PxUMSmMVl+tZt+8snyeksTf9Eo4OGib1C2Tm4DAGdelgG2+CVmL+KbuKzbQYnvem7ivJBTx/qd7DpJQ1aq/9WGmc1JwiZn2wi4LSClbMG8yAsA6tdu6GBoPqSlIa7MCZPOIT0vh2byaFZXq6+7vzXFwfbosJxtu1fbYOzNl810sTJw6sXbMKhebp4uvGFw8MZdYHu7j7g10smzOIIV07WrusGlQwKFdVWKZn7T7j2EFyRh5ODhomRQVy1+AwBnZu362D2oQQeDrrarQQqsYcPJ111v+zGvNczTEGUBMHWklIB1dWPXAdd72/k9nLE3j/3lhu6G4792epriSlTvsz8vg8IY01e89QVG6gZ4AHMweHMnVACF6uOmuXZ9NsuusleZVxMaK8DGNLYcxzar4oK8opLOPuD3ZxMruI/8yK4aY+AS16PjXGoDRaQWkFa0ytgwNn8nHWabglKoiZQ8IYEOptO29uitKGXCouZ/ayBA5m5vPGjAFMigpssXOpMQalQaSUJGcYxw7W7MukuNxAr04e/H1KXyZHB1fPkqooSsvwdnXk0/uHMO+jRB6J30OZvj+3xVh3wSgVDO1UfmkF3+7NJH5XGofO5uOi0zK5v7F10D/ES7UOFMtT3Vj18nDWsWLeYO5fkcQfV++jtKKSu4aE1bv/jz/+yGOPPYbBYOD+++9nwYIr1kdrFhUM7YiUkr3pl4hPSGPtvrOUVBjoE+jJi7dGMiU6CA9n1TpQWkgjb65rj1wdHVg2ZxC//3Q3f/56P6UVBuYND79iP4PBwEMPPcSGDRsICQlh0KBBTJ48mT59aq+P1nQqGNqBvJIKvvntDPEJaRw5V4Cro5ZbBwQxc3AY/YJV60BpBZsW1bwaCoyPNy1SwWDGWaflvXtieWzlbyxad4iSCgMPjY6osU9CQgIRERF07doVgBkzZvDtt9+qYFCuTUrJnrSLfL4rne/2Z1JaUUlUiBcv39aPW/oH4W6nK0spdiovo3Hb2zFHBw3/njmAP67ex6s/HaW0wsCTY3tUf4A7c+YMoaGXVzoICQlh165dFq1BvTu0MXnFFXz1WwbxCWkcyyrE3cmB22NCmDk4rNVvv1eUak28ua69ctBqeG1aNC46Lf/enEJJuYG/TOpdPS1JbZZu9atgaAOklCSdvkj8rjS+23+WMn0l/UO9eeX2fsRFBdnturNKG6Jurms0rUbw0tR+OOu0fPDLKUr1BhZNjiQkJIT09Mshm5GRUT29vaWodww7drGonK9MYwcp5wvxcHJgWmwoMwaH0jdItQ4UG1I1jqCuSmoUjUbwt1v64KTT8N7PJ/l0ZxqJC0dz/PhxTp06RXBwMCtXruTzzz+36HlVMNgZKSW7Tl0gPiGNHw6co1xfyYAwb/55RxRxUYG4Oqq/UsVGRU1TQdAEQggWjO/FR9tTKdNXsv3kRd566y1uvvlmDAYD8+bNo2/fvhY9p0XeRYQQ44E3AC3wgZRyca3nnYCPgYFALjBdSplqem4hcB9gAB6VUv5kiZramgtF5Xy5O4P4xDROZhfh4ezAzEGhzBgcRu9AT2uXpyhKC5ISAjydKa0wcOuAYBgQzMSJE1vsfM0OBiGEFngbGAtkAIlCiDW1lui8D7gopYwQQswAXgGmCyH6YFwjui8QBGwUQvSQUhqaW1dbIKVkx8lc4hPS+enAOcoNlQzs3IF/3RnBpH6BuDhqrV2ioiitYMfJXNIuFPP69OhWOZ8lWgyDgRQp5UkAIcRKYApgHgxTgOdNP/8XeEsYh9GnACullGXAKSFEiun1dligLruVU1jGl7szWJmYzqmcIjydHZg1NIyZg8NsetUnRVFaRnxCGl4uOsZHdmqV81kiGIIB8+vQMoAh9e0jpdQLIfKAjqbtO2sdG2yBmuxOZaWxdfB5QhrrD56jwiAZ3MWHR8dEMCEyEGedah0oTaCmobB7F4rKWX8wi7uGhLXa+4AlgqGuC2hrX2hb3z4NOdb4AkLMB+YDhIXVP4eIvckuKOO/uzNYmZjG6dxivF113HtdF2YODiXCX7UOlGZo49NQ2PT05hb01Z4Myg2VzBzceu97lgiGDCDU7HEIkFnPPhlCCAfAC7jQwGMBkFIuBZaCcdptC9RtNZWVku0ncohPSGP9wSz0lZIh4T48ObYHN/ftpFoHimU0cBoKe3yDbS9rUUspiU9IY0CYNz07td4HRUsEQyLQXQgRDpzBOJh8V6191gCzMY4d3AFsllJKIcQa4HMhxGsYB5+7AwkWqMkmnc8vZbWpdZB+oYQOrsbVvmYMDqObn7u1y7M81Y1hXQ2YhsIe32CllOSXVtRYMtV8SVV7CLaGSjp9kRPZRfzz9qhWPW+zg8E0ZvAw8BPGy1WXSSkPCiEWAUlSyjXAh8AnpsHlCxjDA9N+qzAOVOuBh9raFUmGSsn/jmcTn5DGxsPnMVRKru/WkT/d3Iub+wbg5NBGWwdtvBvDLlxjGgp7fYM1XzJ1+fbU6vrNl1RtK+IT0nB3ciCuf8st3lMXtYJbC8nKL2VVYjorE9M5c6mEjm6O3BEbwoxBYYT7ulm7vJa3JLKeN6VQeOJA69fTHtUOZzBOQ3HLm9XhXNVCqHpzBft5g5VSEr7w++rHp16eaPM1N0ZeSQVDXtrI7TEh/GNqP4u8plrBzQoMlZJtx7L5PCGNzUeMrYPhEb78eWJvxvYJwNFBY+0SW4+aTdP6GjANRdWnb/NgsJdQWLTuUI1ti9YdsovaG+rbvWcorWjdQecqKhgs4GxeCasSM/giMY3MvFJ83R2ZP6IrMwaF0rljO2gd1EXNpmkbrjENhT2+wZq3cqpaN+atHluuvaGMg87pRAZ7WmVWZBUMTaQ3VLL1qHHsYMvR81RKuKG7L8/G9WFM73bWOqiLmk3T5tnrG6wQAk9nXY0ur6oxB09nnU3W3FjJGXkcPpvPi7dGWuX8Khga6cylEr5ITGdVYjrn8kvx83Di96O6MWNQGKE+rtYuz3ao2TRtnj2/wT4xtkeNwfGq2m255sZYmZiGi07LlGjLTqfdUO128PnSpUvcf//9HDhwACEEy5Yt47rrrqtzX72hks1HzrMyMZ2tR88jgZE9/Jg5OIwbe/mj07bz1oFi1+zxPoa2rLBMz5B/bGRiv0BevbO/RV9bDT5fw2OPPcb48eP573//S3l5OcXFxVfsk36hmFVJ6axKSicrv4wATyceHh3BtEGhhHRQrQOlbagdAioUrGvtvkyKyg3MsMKgc5V2GQz5+fls27aNjz76CABHR0ccHR0BqDBUsunweeIT0th2PBsBjOrpz4u3hjG6px8OqnWgKEoLWpmQRo8Ad2LCvK1WQ7sMhpMnT+Ln58fcuXPZt28fAwcO5E9/e5k1h3JZlZRBdkEZgV7OPHpjd6YNCiXY28XaJSuK0g4cysxnX0ae1cdL2mUw6PV69uzZw5LX3yDfowtPPPE4/53+CD4j7+HGXv7MHBzGyB6qdaAoSutamZiGo4OG22KsO8l0uwwG6eKDR8cAHttcRE7hHjy6XUfw3m/YuOBGAr1U60BRbEo7mXOrpNzA17+dYUJkJ7xdHa1aS7sKhr3pl3j1pyNsT8ml1LEDXXWXeHXuCLZ8nkDJiEEqFBTF1rSjObe+33+WglI9MwZZf1mBdtVX8kViGttTcnlgZFfWxX/IydWLeezOm0jet48///nP1i7PPiWvMs6L9Ly38XvyKmtXpLQlV5s6vI1ZmZhGuK8bQ7v6WLuU9tViuGtwZ+IT0vF01jFm+BBsfSI+m9eOPs0pVtJO5txKOV9AYupFFkzoZROXC7erFkO/EC9G9vBj2S+nKClvU7N7N0jtmxmbfXNjO/o0p1hJfXNrtbE5t1YmpOOgEdweYxu/V7sKBoCHb4wgt6ic+IQ0a5fSqpZsOMaidYeqw6BqnpwlG441/UXb0Ke5Boem6jprXWOeM86xZa6NzblVpjfw5Z4MxvYJwM/DydrlAO0wGAZ18WFwuA9Lt52kTN8+Wg3mC7JUhUPVZGn5pRVNbzm0kU9zDQ7Nqq6zvHRAXu46U+HQcqKmGdeP8AoFhPG72XoSbcH6g1lcLK6wyvTa9WlWMAghfIQQG4QQx03fO9SxT7QQYocQ4qAQIlkIMd3suY+EEKeEEHtNX9HNqaehHh4dwbn8Ur7ac6Y1Tmd1VROMzR3WheXbUwlf+H2NGTWb3KfZBj7NNSo0VdeZdURNMy7u9Pwl4/c2FApgHHQO6eDC8Ahfa5dSrbkthgXAJilld2CT6XFtxcC9Usq+wHjgdSGE+b3ef5JSRpu+9jaznga5obsvUSFevLP1BHpDZWuc0urMZ86s0uy7K9vAp7lGhWYb6jpTbMPp3CK2p+QyPTYUjcb6g85VmhsMU4AVpp9XALfW3kFKeUxKedz0cyZwHvBr5nmbRQjBQ6MjSLtQzLrks9YspdXUtyBLsweg28CnuQaHZhvpOlNsxxeJ6WgE3Bkbau1SamhuMARIKc8CmL77X21nIcRgwBE4Ybb5H6YupiVCiHpHXoQQ84UQSUKIpOzs7GaWDWN7B9AjwJ23t6RQWWl/U483Ru0FWU69PLH6E7JFwsHONTg020DXmWI7KgyVrN6dwY29/Onk5Wztcmq4ZjAIITYKIQ7U8TWlMScSQgQCnwBzpZRV/TcLgV7AIMAHeKa+46WUS6WUsVLKWD+/5jc4NBpjq+H4+ULWH8pq9uvZsvoWZJk7rIvNL8jS0hoVmm2g60yxHZuPnCe7oMwm7nSu7Zo3uEkpb6rvOSFElhAiUEp51vTGf76e/TyB74C/Sil3mr12VT9OmRBiOfBUo6pvpkn9AnltwzHe3pLCzX0D2vQbZFtf8aqpGr2K2TXWUFaUhlqZkEaApxOjelq1Z71Oze1KWgPMNv08G/i29g5CCEfga+BjKeXqWs8Fmr4LjOMTB5pZT6M4aDU8OLIb+8/kse14Tmue2irUgix1e2JsjxohWRUOT4ztYeXKrmTxmxQVqzhzqYStx7KZFhtqk7M4N7eixcBYIcRxYKzpMUKIWCHEB6Z9pgEjgDl1XJb6mRBiP7Af8AVebGY9jXZbTDCdPJ15e3NKa59asSH2EJotcpOiYhWrEtMBmGZjg85VmjVXkpQyFxhTx/Yk4H7Tz58Cn9Zz/I3NOb8lODlomT+iK4vWHSLh1AUGh1t/AitFqc38fgswXjVlPjai1mm2H4ZKyeqkdIZH+BLqY5tLBNteG8YKZg4Oo6ObI29tUa0GxTa12E2KzaC6tZpm27FsMvNKbepO59pUMAAujlrmDQ9n27FskjMuWbscRalTi9yk2ESqW6vp4hPS6OjmyE29A6xdSr1UMJjcc11nPJwdeFu1GhQb1WI3KTahjhaZe6sdOJ9fyqYj57ljYAiODrb79tuu1mO4Gk9nHXOu78K/N6dwLKuAHgEe1i5JUarVvt/CfIwBWrflYN5yWb49tboGa3Zr2YvVuzMwVEqmD7LNQecqthtZVjB3WDguOi3vbD1x7Z0VpRXZ2k2KttStZS8qKyVfJKYzJNyHrn7u1i7nqlQwmPFxc2TWkDDW7MskLbfY2uUoSg22dL+FrXRr2ZMdJ3NJu1Bs04POVVQw1PK7EV3RCsE7P6tWg2J7bOF+CzX3VtPEJ6Th5aJjfGQna5dyTSoYagnwdObO2BC+3J3BubxSa5ejKDbH1rq17MGFonLWH8zitphgnHVaa5dzTSoY6vDgyG4YpGTptpM1th89epTo6OjqL09PT15//XUrVako1mNL3Vr24Ks9GZQbKu2iGwnUVUl1CvVxZUp0EJ8nnOah0d3o6G6cDbxnz57s3WtcS8hgMBAcHMzUqVOtWaqiWI0tdGvZAykl8QlpxIR5283VjqrFUI8/jIqgTF/Jsu2n6nx+06ZNdOvWjc6dO7dyZfZtyZIl9O3bl8jISGbOnElpqequU9q2pNMXOZFdxAw7aS2ACoZ6Rfi7MyGyEx//epq8koornl+5ciUzZ860QmX268yZM7z55pskJSVx4MABDAYDK1eutHZZitKi4hPS8HByIC4q0NqlNJgKhqv4w6gICsr0fLIjtcb28vJy1qxZw5133mmVuuyZXq+npKQEvV5PcXExQUFB1i5JUVpMXkkF3+8/y+ToIFwd7afnXgXDVUQGezG6px8f/nKK4nJ99fYffviBmJgYAgJsd64TWxQcHMxTTz1FWFgYgYGBeHl5MW7cOGuXpSgt5tu9ZyitsJ9B5yoqGK7h4RsjuFhcwee70qq3xcfHq26kJrh48SLffvstp06dIjMzk6KiIj79tM4Z2RXF7kkp+XxXGpHBnkQGe1m7nEZpVttGCOEDfAF0AVKBaVLKi3XsZ8C4GA9AmpRysml7OLAS43rPe4B7pJTlzanJ0gZ29mFoVx8W/3CE/2w9gdZQzt6133M28m6+eH0bjg4aHLUadFoNOtPPjg6ijm3m+4k6tpkfq0WnFVce66BBpxU4abXVr6HVCLu5GmTjxo2Eh4dTtWb3bbfdxq+//srdd99t5coUxfL2ZeRx5FwBL94aae1SGq25nV4LgE1SysVCiAWmx8/UsV+JlDK6ju2vAEuklCuFEO8C9wHvNLMmi3vx1kjiE9Ip0xuo0EuGL9tGuaGScn0lFQbjV7m+kuISwxXbKgyVlFVvkxgqLXtXqBCg02pwMgsXnYNAp70cKlU/m4fPlduMoeNoFjqNO/bya1QFl06rwcEsuMLCwti5cyfFxcW4uLiwadMmYmNjLfrnoSi2YmVCGi46LVOi7W8crbnBMAUYZfp5BbCVuoPhCqZ1nm8E7jI7/nlsMBgi/D14ttaEYU1lqJS1wqJ2gMjqbXWFT7lBXt5WtY/5fnp55TYbCK7BXXx4aWo/7rjjDmJiYnBwcGDAgAHMnz/foudTFFtQWKZnzb5M4qIC8XDWWbucRmtuMARIKc8CSCnPCiH869nPWQiRBOiBxVLKb4COwCUpZdWobgYQ3Mx6bJ5WI9BqtDZ7W3xLBFdRmYHVSemMf2MbCyfO4/Dzz9tN95eiNMXafZkUlxvs6t4Fc9cMBiHERqCuWZ/+0ojzhEkpM4UQXYHNQoj9QH4d+9X7cVUIMR+YD8YuCaVltFRw3XdDOAu+TObZbw7w04FzvHJHFMHeLhY9R5uTvAo2LYK8DPAKgTHPQdQ0a1elNMDKhDR6BLgTE+Zt7VKa5JpXJUkpb5JSRtbx9S2QJYQIBDB9P1/Pa2Savp/E2N00AMgBvIUQVeEUAmRepY6lUspYKWVs1eClYj+CvV34eN5gXrw1kj1pFxm/ZBurEtPVTJz1SV4Fax+FvHRAGr+vfdS4XbFphzLz2ZeRx4xBYXbbMm7u5aprgNmmn2cD39beQQjRQQjhZPrZFxgGHJLGd4QtwB1XO15pO4QQ3D20Mz8+NoI+QZ48/WUy8z5KJCvfNqbFsKnF7TctgoqSmtsqSozbFZu2MjENRwcNt8XYb894c4NhMTBWCHEcGGt6jBAiVgjxgWmf3kCSEGIfxiBYLKWsWuHjGeBJIUQKxjGHD5tZj2IHwjq6Ev+7oTwX14cdJ3MZ+9rPfP1bhlXfiG1ucfu8jMZtV2xCSbmBr387w4TITni7Olq7nCZr1uCzlDIXGFPH9iTgftPPvwL96jn+JDC4OTUo9kmjEcwbHs6onn48tXofT3yxjx/2n+MfU/vh5+HUqrWYL24P1FhPee6wLkgpW79LwCvE1I1Ux3bFZn2//ywFpXpmDLLvcVB157NiVV393Fn94PUsnNCLrceyGbfkZ75LPtuqNZgvNLN8eyrhC7+vDgWrrWM85jnQ1Rqc17kYtys2a2ViGuG+bgzt6mPtUppFBYNidVqN4IGR3fjukeGE+rjy0Od7ePjzPVwoar2b4K29uP28efPw9/cnMtJ0l2zUNC6MeImx8Qa6/7uQsfEGLo58SV2VZMNSzheQmHqRGYNC7XbQuYoKBsVmdA/w4KvfX88fx/bgp4PnGLdkG+sPnmuVc1t7cfs5c+bw448/1ti2eO0Rxsz9K8dzDYyZ+1cWrz3aKrUoTbMyIR2dVnD7QPvv7lPBoNgUB62GR8Z059uHhuPn4cT8T3bz5Bd7ySu+ck0MS7GFxe1HjBiBj0/N7odvv/2W2bONF/3Nnj2bb775psXrUJqmTG/gyz0ZjO0TgK97646RtQT7mSBcaVf6BHny7UPDeGvzcd7eeoLtJ3JYfHsUo3vWd3N909W3uD1g1cXts7KyCAw0Lu4SGBjI+fN13iak2ID1B7O4WFxh94POVVQwKDbL0UHDk+N6clOfAP64ah9zlycyPTaUv8b1tvj8M0+M7VHj6qOqcLD3vmKldaxMTCOkgwvDI3ytXYpFqK4kxeZFhXiz9pHhPDiyG6t3pzP+9f+xPSXH4uextcXtAwICOHvWeIXW2bNn8fe3fGtJab7TuUVsT8llemwoGk3b+CChgkGxC846LQsm9GL1g9fj5KBh1ge7ePabAxSV6a99sJ2aPHkyK1asAGDFihVMmTLFyhUpdbl95r2k/3sWbz9ya/W21atX07dvXzQaDUlJSVasrmlUMCh2ZWDnDnz36A3MGxbOp7tOM+GN/5Fw6oK1y2qyX47nMO+jRLpfP57IAYM4fOQo3n6duPeZl4kYezeff72OkM5d+XLN90y59w+czi0ip7CM0gqDmmfKBlQYKikIG8YtT/8bB7PWQmRkJF999RUjRoywYnVNp8YYFLvj4qjluVv6cHPfAJ767z6mL93B3OvDeXp8T5udzrwulZWS59ceJLugjNDbF+BTpqewVE9RuYFtwLbNGTB6IQ5ANjDr04M1jnfQCNycHHCv+nJ2MD3W4u5k/NnD9N3NyQEPZwfcHI37VT1fdayzTmP1rjN7tOnwecp8e3LH9f4sXn15e+/eva1XlAWoYFDs1pCuHfnxsRG8/MNhluasEw4AAAvxSURBVG0/xdaj5/nXtP7EhHWwdmkNsv5QFinnC3ljRjRToi9PuFZZKSmuMFBYqqewzPhVZPpuDA49BaXGbUVlegrMns8rqSDzUolxvzI9heV6GtKw0GoEbo7aWgFzZYDUDB8dbk5aPEzfq5530WnbTcisTEwjwNOJoV07WrsUi1LBoNg1NycHXry1H+P7BvLMl8nc8c6vzB/RjSfGdsfJwXZbD1JK/rM1hc4dXZnUL7DGcxqNqH4jbq7KSklJhaFmgJgCp6i86mcDhWUVFJUZrgifc3mlNY5tyAJ/GkF1mNQIFSezUHGuo0VT9bxZi8ZVp7XZAd0zl0r4+Vg2D4+OwEHbtnrlVTAobcLw7r78+PgNvLjuMO/+fILNR7L4vzuj6RfiZe3S6rQ9JZfkjDxemtqvRd9UNKbuJjcnB5p7TZOUxpAxtmCubNFUhUdR2eUWjfnz5wtKKSozUFBaQVG5oUHLyAqBsfvLyaFGq6Rq27VaN+5O2ssh5ehg0ZBZlWic5HBabCiG/LZ1j4kKBqXN8HDW8codUYyP7MSCr5K59T/beWh0BA+PjsDRwbY+0b29JQV/DyduH2g/c/YLIXB1dMDV0QE8mvdaUkrK9JV1BkihebdZWc0WTVX45BYWG481tXr0DVyr3M3xclC4XzHmYuweMx+jMQ8f89aNi07L6qR0hkf4EurjSmpd61HaMRUMSpszupc/6x8fyfNrD/LmpuNsPJTF/03rT+9AT2uXBsCetIvsOJnLXyb2tunurpYkhMBZZ1xCtrnTrFeFzBWhUm4KldI6WjRm3WrpF4rNutX0VBgafrXXX+P6MHPmTLZu3UpOTg4hISG88MIL+Pj48Mgjj5Cdnc2kSZOIjo7mp59+atbv2ZqEPV7yFhsbK+3x2mCl9f108Bx/+Xo/eSUVPDamOw+O7Gb1/uD7VySRmHqBXxfciJsFxhEUyyrTGygqM1zuEjMLDfPw0Wk1PDCiq9X/PTWGEGK3lDL2Wvs161+lEMIH+ALoAqQC06SUF2vtMxpYYrapFzBDSvmNEOIjYCSQZ3pujpRyb3NqUhRzN/ftxKAuPjz7zQH+tf4YG0ythwj/ZvaFNNHRcwVsPJzFY2O6q1CwUU4OWpwctPi42e8KbM3V3KhbAGySUnYHNpke1yCl3CKljJZSRgM3AsXAerNd/lT1vAoFpSX4uDny9qwY3rprAGkXipn45i8s3XaiQYOflvbO1hRcHbXMub5Lq59bURqqucEwBVhh+nkFcOtV9gW4A/hBSlnczPMqSqPFRQWx/omRjOzhx0vfH2Haezs4lVPUaudPyy1mbfJZZg0Jo0M7/jSq2L7mBkOAlPIsgOn7ta6ImwHE19r2DyFEshBiiRDC/icyV2yan4cTS+8ZyJLp/TmeVcCEN7axfPspKluh9fDethNoheD+G7q2+LkUpTmuGQxCiI1CiAN1fDVqRi8hRCDQDzAfml+IccxhEOADPHOV4+cLIZKEEEnZ2dmNObWi1CCEYOqAENY/MZKhXTvywtpD3PXBTtL/v727j62qvuM4/v4WCs3YBi0UsONJoMpwisrDDGBQBKlGYZM5x+KEDGOAwNjmHkzcksUtUZdtJUvIFoY4GAYUQqWSbkZkenWBMWRgCwyKjFKg43mwQceT3/3RA+3BPlzae89tbz+v5OSce+/vnvvt95ye7z0Pv3tOJm9H9uiZ/7Hqg4NMHd6HXp/NStrniCRCk4XB3Se4+xfqGdYCR4IN/pUNf2O9PL4KFLn71VtxuXuV1zgPvAyMaiSORe4+wt1H5Obmxvv3iTSod9csXp4xkhen3krZoTNMWhBj+aaKpPw43Uvv/5NLlz9m1jjtLUjr19JDScXA9GB6OrC2kbbTuOYwUp2iYtScnyhrYTwi18XMeGxkP/707bu5o183fvR6GU8s2czhf1cn7DNOn7vI8k0VPHRbHv27d0nYfEWSpaWF4QVgopmVAxODx5jZCDNbfKWRmQ0A+gLvXvP+V8ysFCgFegA/a2E8Is3SJ/tT/OGbX+SnU25hy/5TTCqM8dqWyoTsPSzduJ+zFy4z+55BLQ9UJALq4CZyjYoTZ/n+qg/ZvP8k9w3pyfOP3ErPZp4XOHfhEmNe2MCd/bJ5acbIBEcqcn3i7eDWdrrsiUSkf/curHzqLn780FDe33uciYUx1m471Ky9hxWbKzl17iJz7tXegrQdKgwi9cjIMGaOvZGS+XczMLcL81duY/byrRz/7/m45/HLXxUyb+p4Ti2bx3tFy5IYrUhiqTCINGJQ7qdZPWs0PywYwoZ/HOX+whglpVVNvq+srIwFC39Lj8d/wZr1f2HdunWUl5dHELFIy6kwiDShQ4Yx+55BvDFvLHndspjzylbmrfg7p85eaPA9O3bs5FKPwdw2oCfjP9+bcePGUVRUFGHUIs2nwiASp5t7f4aiOWP4zoSb+GNpFfcviLF+55F6257s3Ivj5dt4/PYcqqurKSkpobKyMuKIRZpHhUHkOmR2yGD+hHzWzh1D9y6deHLZFp5+bTunq6/228TdKa7IYNCEr/Pz+d+goKCAYcOG0bGjfk1V2gYVBpFmuCWvK8VzxzL33sG8vu0QkwpjvLun5qda3tlzjF1VZ3juB99i69atxGIxcnJyyM/PT3HUIvHRVxiRZurUMYPvTbqZiUN78fSq7Uxfsplpo/qy+1//Ia9rFqPzMgE4cOAAa9asYePGjSmOWCQ+KgwiLTSsbzfWzRtL4Vt7WPTePtzhJw8PZdpjj3LixAkyMzNZuHAh2dnZqQ5VJC7q+SySQB9UnOTNHUf47sSbyMpsn/dzltYrklt7ikjY8P45DO+fk+owRFpEJ59FRCREhUFEREJUGEREJESFQUREQlQYREQkRIVBRERCVBhERCREhUFERELaZM9nMzsGVCTxI3oAx5M4/7ZEuailXIQpH7XaSi76u3tuU43aZGFINjPbEk+38fZAuailXIQpH7XSLRc6lCQiIiEqDCIiEqLCUL9FqQ6gFVEuaikXYcpHrbTKhc4xiIhIiPYYREQkRIUBMLNHzWyHmX1sZg1eWWBmBWa228z2mtkzUcYYFTPLMbO3zKw8GNd72zEzu2xm24KhOOo4k6mp5Wxmnc3s1eD1v5rZgOijjE4c+ZhhZsfqrA9PpiLOZDOzJWZ21MzKGnjdzOzXQZ4+NLM7o44xUVQYapQBjwCxhhqYWQdgIfAAMBSYZmZDowkvUs8Ab7t7PvB28Lg+1e5+ezBMji685IpzOc8ETrn7YKAQeDHaKKNzHev9q3XWh8WRBhmd3wMFjbz+AJAfDE8Bv4kgpqRQYQDcfZe7726i2Shgr7vvc/cLwEpgSvKji9wUYGkwvRT4UgpjSYV4lnPdHK0G7jMzizDGKLWX9b5J7h4DTjbSZAqwzGtsArqZ2Q3RRJdYKgzx+xxQWefxweC5dNPL3asAgnHPBtplmdkWM9tkZulUPOJZzlfbuPsl4DTQPZLoohfvej81OHyy2sz6RhNaq5M224h2c89nM1sP9K7npWfdfW08s6jnuTZ5SVdjubiO2fRz98NmNhDYYGal7v5RYiJMqXiWc9qsC3GI5299A1jh7ufNbBY1e1Pjkx5Z65M260W7KQzuPqGFszgI1P0m1Ac43MJ5pkRjuTCzI2Z2g7tXBbvBRxuYx+FgvM/M3gHuANKhMMSznK+0OWhmHYGuNH6IoS1rMh/ufqLOw9+RxudcmpA22wgdSorf34B8M7vRzDoBXwPS6mqcQDEwPZieDnxib8rMss2sczDdAxgD7IwswuSKZznXzdFXgA2evh2CmszHNcfRJwO7IoyvNSkGngiuTroLOH3lsGyb4+7tfgC+TE21Pw8cAd4Mns8DSuq0exDYQ80342dTHXeSctGdmquRyoNxTvD8CGBxMD0aKAW2B+OZqY47wTn4xHIGngMmB9NZwCpgL7AZGJjqmFOcj+eBHcH68GdgSKpjTlIeVgBVwMVgezETmAXMCl43aq7g+ij4vxiR6pibO6jns4iIhOhQkoiIhKgwiIhIiAqDiIiEqDCIiEiICoOIiISoMIiISIgKg4iIhKgwiIhIyP8BepDN/mcFOqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.clf()\n",
    "for i in range(1000):\n",
    "    random_contour=apply_procrustes(generate_contour(12))\n",
    "    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length(random_contour,0.8,algorithm='del2d')\n",
    "    if random_nb_of_points==nb_of_points:\n",
    "            break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*12)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[.8]]])\n",
    "_,contour_length,contour_angles=extract_lengths_angles(random_contour_with_target,12)\n",
    "distance_from_barycenter=extract_distances_from_barycenter(random_contour_with_target,12)\n",
    "random_contour_with_target=np.hstack([random_contour_with_target,contour_angles])\n",
    "random_contour_with_target=np.hstack([random_contour_with_target,contour_length])\n",
    "random_contour_with_target=np.hstack([random_contour_with_target,distance_from_barycenter])\n",
    "#random_contour_with_target=np.hstack([contour_length,contour_angles])\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "random_point_coordinated_delaunay.reshape(nb_of_points,2)\n",
    "mid_point_delaunay=(random_point_coordinated_delaunay[0]+random_point_coordinated_delaunay[1])/2\n",
    "plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location',marker='x')\n",
    "\n",
    "\n",
    "\n",
    "random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "random_x_variable=random_x_variable.expand(1000,2*12+3*12+1).type(torch.FloatTensor)\n",
    "my_net=my_net.cpu()\n",
    "my_net.eval()\n",
    "random_prediction=my_net(random_x_variable)\n",
    "\n",
    "random_prediction=random_prediction.data[0].numpy()\n",
    "predicted_coordinates=acquire_points_from_deformation(random_prediction,nb_of_points)\n",
    "predicted_coordinates=np.array(predicted_coordinates).reshape(2*nb_of_points,2)\n",
    "plt.scatter(predicted_coordinates[:,0][::2],predicted_coordinates[:,1][::2],label='Predicted point')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "regression_error(real_points=random_point_coordinated_delaunay,prediction_points=predicted_coordinates[::2],nb_of_interior_points=nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_,_,contour_angles=extract_lengths_angles(random_contour_with_target,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error between predicted points and real point in a multiple point framework\n",
    "\n",
    "contour_points=sort_points(random_point_coordinated_delaunay.reshape(1,1,3,2),3).reshape(3,2)\n",
    "#plot_contour(apply_procrustes(contour_points))\n",
    "predicted_contour_points=sort_points(predicted_coordinates[0::2].reshape(1,1,3,2),3).reshape(3,2)\n",
    "procrustes_predicted_contour_points=apply_procrustes(predicted_contour_points,contour_points)\n",
    "plot_contour(apply_procrustes(predicted_contour_points,contour_points))\n",
    "plot_contour(contour_points)\n",
    "#plot_contour(predicted_contour_points)\n",
    "\n",
    "error_in_transformation=np.array([np.linalg.norm(predicted_contour_points[indices]-procrustes_predicted_contour_points[indices]) for indices in range(len(predicted_contour_points))])\n",
    "contour_points,predicted_contour_points\n",
    "maximum_index,maximum_distance=np.argmax([np.linalg.norm(contour_points[index]-procrustes_predicted_contour_points[index]) for index in range(len(contour_points))]),np.max([np.linalg.norm(contour_points[index]-procrustes_predicted_contour_points[index]) for index in range(len(contour_points))])\n",
    "maximum_distance\n",
    "maximum_distance,maximum_index,error_in_transformation\n",
    "total_error=maximum_distance+error_in_transformation[maximum_index]\n",
    "print(total_error,maximum_index)\n",
    "\n",
    "\n",
    "\n",
    "print(regression_error(random_point_coordinated_delaunay[::2],predicted_coordinates[0::2],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking as input the lengths and angles of the polygon\n",
    "\n",
    "data,_,_=extract_lengths_angles(polygons_reshaped,12)\n",
    "#data=extract_lengths_angles_in_triangle_form(polygons_reshaped,12)\n",
    "\n",
    "x_tensor_conv=torch.from_numpy(data[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test_conv=torch.from_numpy(data[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable_conv,x_variable_test_conv=Variable(x_tensor_conv),Variable(x_tensor_test_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape variables for convolutional neural net\n",
    "\n",
    "x_variable_conv,x_variable_test_conv=x_variable_conv.resize(x_variable_conv.size()[0],1,x_variable_conv.size()[1]),Variable(x_tensor_test_conv.view(x_variable_test_conv.size()[0],1,x_variable_test_conv.size()[1]),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#my_conv_net=triangle_convoluting_net(nb_of_filters=20,nb_of_hidden_nodes=10,out_dimension=4*12+2)\n",
    "\n",
    "my_conv_net=alt_conv_net(nb_of_filters=5,nb_of_hidden_nodes=100,out_dimension=4*12+2)\n",
    "print(my_conv_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-4,weight_decay=0.7)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable_conv , y_variable,x_variable_test_conv,y_variable_test= x_variable_conv.cuda(), y_variable.cuda(),x_variable_test_conv.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=int(x_variable_conv.size()[0])\n",
    "nb_of_epochs=33000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable_conv.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable_conv.narrow(0,b,batch_size))                 # input x and predict based on x        \n",
    "        #loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted        \n",
    "        loss=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*4+2)).sum().cuda()\n",
    "\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        out_test_conv=my_conv_net(x_variable_test_conv)\n",
    "        #test_loss=loss_func(out_test_conv,y_variable_test).data[0]\n",
    "        test_loss=my_torch_loss_function(out_test_conv, y_variable_test.resize(len(y_variable_test),nb_of_points*4+2)).sum().data[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable_conv.size(0),\"Test Loss:\",test_loss/x_variable_test_conv.size(0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_gradients_with_barycenters.reshape(len(deformation_gradients_with_barycenters),1,3,4)[:,0,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_gradients_with_barycenters.reshape(len(deformation_gradients_with_barycenters),1,3,4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_gradients_with_barycenters.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "triangles=point_coordinates.reshape(len(point_coordinates),3,2)\n",
    "barycenters=extract_barycenter(point_coordinates)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "barycenters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "triangles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors=triangles-barycenters\n",
    "vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "angles=[]\n",
    "for  barycenter_vectors in vectors:\n",
    "    for vector in barycenter_vectors:\n",
    "        angles.append(angle_counterclockwise(np.array([1,0]),vector))\n",
    "                      \n",
    "angles=np.array(angles).reshape(len(vectors),3,1)\n",
    "point_coordinates_with_angles=np.dstack([triangles,angles])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#point_coordinates_with_angles_sorted=np.sort(point_coordinates_with_angles,point_coordinates_with_angles[0][:,2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "point_coordinates_sorted=[]\n",
    "for points in point_coordinates_with_angles:\n",
    "    points_sorted=np.array(sorted(points,key=lambda x: x[2]))\n",
    "    points_sorted=points_sorted[:,0:2]\n",
    "    point_coordinates_sorted.append(points_sorted.reshape(1,3,2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.2191585767492475 -0.2900295028119402 0\n",
      "14 0.003009820581806462 -0.5156914314898119 0\n",
      "15 -0.1831242652554345 -0.0299518490960665 0\n",
      "16 0.5138857857122678 -0.480016527331137 0\n",
      "17 0.3783820877747181 0.1739080499460676 0\n",
      "18 -0.2995311555002407 -0.2427133569979735 0\n",
      "19 0.4554059003903581 -0.06765084028766526 0\n",
      "20 0.8046923311921836 0.2437843896766339 0\n",
      "21 0.2369716504252916 -0.4998587195304494 0\n",
      "22 0.06954910224193017 -0.6534395907742776 0\n",
      "23 -0.1567944759473648 -0.5571159266265009 0\n",
      "24 0.1014543219595113 0.08225916364797013 0\n",
      "25 -0.5675720704935882 0.2648920043023894 0\n",
      "26 -0.08975396118687962 -0.2449928671407565 0\n",
      "27 -0.1707286935976337 0.3568024339586203 0\n",
      "28 0.6124297636992232 0.05672752289679316 0\n",
      "29 -0.2927312442343393 -0.4293505462261763 0\n",
      "30 0.4215362580511499 -0.2916207791178258 0\n",
      "31 0.6381042992312168 -0.3099574509362348 0\n",
      "32 -0.1193828581483766 -0.4111650647828054 0\n",
      "33 -0.489229178432317 -0.05215786961445613 0\n",
      "34 -0.7729455125057568 0.03560616279041743 0\n",
      "35 0.05707757082210788 -0.3552507497452238 0\n",
      "36 0.5807258417622246 -0.1674957243379641 0\n",
      "37 0.6543475841435551 0.4528477585601725 0\n",
      "38 -0.07020220134549403 -0.6130292188818545 0\n",
      "39 0.2780665569060904 -0.09381680355649552 0\n",
      "40 0.3510864094587863 0.5091979969906606 0\n",
      "41 0.09246219651155793 -0.1697669127155797 0\n",
      "42 0.1408945636857805 0.3699893057497117 0\n",
      "13 -0.331708331766053 0.08915443547200803 0\n",
      "14 0.4412117108838971 0.4851368722100245 0\n",
      "15 0.09562895365160132 0.2902903721640007 0\n",
      "16 -0.06176605319898282 -0.4206577094824907 0\n",
      "17 -0.5805351736224851 -0.04711764252140412 0\n",
      "18 0.1883421086577047 -0.6824976373023152 0\n",
      "19 -0.41607415464195 0.3325950335537531 0\n",
      "20 0.05998149791439909 -0.005279156046531413 0\n",
      "21 -0.6219652069267037 0.3797160647535616 0\n",
      "22 -0.1408620865021642 0.3529330226656763 0\n",
      "23 -0.3609518081326598 -0.3041097413284297 0\n",
      "24 0.2743039390795468 0.2706082221921914 0\n",
      "25 0.1550464101510562 -0.2646198500063878 0\n",
      "26 -0.5518070927367264 0.2059065530199646 0\n",
      "27 0.1890924641146236 0.5428993998446666 0\n",
      "13 0.3733109993606895 -0.2524029999906727 0\n",
      "14 -0.2473725073130538 0.1104346373645647 0\n",
      "15 -0.2407326126025602 0.801356283895765 0\n",
      "16 0.6056502599988634 -0.3826788128014115 0\n",
      "17 0.5271982478507542 0.05541979526277768 0\n",
      "18 -0.01911857260331712 -0.2727137674950997 0\n",
      "19 0.1720564523163796 0.05600370039715531 0\n",
      "20 -0.01870772570750259 0.5071331378108496 0\n",
      "21 0.6974026298055912 -0.1870001177881954 0\n",
      "22 -0.3957620290629488 0.5055441549418146 0\n",
      "13 0.1343040647991914 0.150830685258877 0\n",
      "14 0.2712564013168647 -0.3530041824836901 0\n",
      "15 -0.227025353508092 0.1486446661415214 -0\n",
      "16 -0.2926286112374392 -0.3253798238141677 0\n",
      "17 -0.05616544947411654 0.5446808105113163 -0\n",
      "18 0.4779473057827798 -0.03785541675129997 0\n",
      "19 -0.5609216472389109 -0.394687281518042 0\n",
      "20 -0.3050841405627803 0.3953180670014912 -0\n",
      "21 0.6256513873511369 -0.3157406988249965 0\n",
      "22 -0.3236366050542048 -0.1197351543117576 0\n",
      "23 -0.01071591937833401 -0.1796475074312266 0\n",
      "24 0.4557612968489725 -0.53053010597633 0\n",
      "barycenter is  [-0.02113278  0.03213254]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.31710768,  0.02947322,  0.15350033,  0.36708528,  0.22880685,\n",
       "       -0.10605225,  0.40237805,  0.35835713,  0.07111871, -0.13329557,\n",
       "        0.54247904,  0.19693363, -0.0883254 , -0.31581506,  0.45388404,\n",
       "        0.07455856, -0.34137675, -0.1703098 ,  0.3430122 , -0.17395464,\n",
       "       -0.33550903, -0.2921546 ,  0.04991145, -0.2207306 , -0.38830137,\n",
       "       -0.11073624, -0.14381233, -0.33140713, -0.3349758 ,  0.16160469,\n",
       "       -0.3361093 , -0.36372793, -0.06014021,  0.21365911, -0.5013236 ,\n",
       "       -0.28903657,  0.16250135,  0.2939509 , -0.50826854, -0.07061396,\n",
       "        0.28746492,  0.41270238, -0.3249956 , -0.02397906,  0.49616912,\n",
       "        0.12523206, -0.14116508,  0.29953206, -0.02113278,  0.03213254],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing convolutional network \n",
    "\n",
    "\n",
    "plt.clf()\n",
    "for i in range(1000):\n",
    "    random_contour_conv=apply_procrustes(generate_contour(12))\n",
    "    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length(random_contour_conv,0.8,algorithm='del2d')\n",
    "    if random_nb_of_points==nb_of_points:\n",
    "            break\n",
    "random_contour_reshaped_conv=random_contour_conv.reshape(1,2*12)\n",
    "random_contour_with_target_conv=np.hstack([random_contour_reshaped_conv,[[.8]]])\n",
    "\n",
    "plot_contour(random_contour_conv)\n",
    "real_points=np.array(random_point_coordinated_delaunay)\n",
    "plt.scatter(real_points[:,0],real_points[:,1])\n",
    "\n",
    "data,_,_=extract_lengths_angles(random_contour_with_target_conv.reshape(1,25),12)\n",
    "#data=extract_lengths_angles_in_triangle_form(random_contour_with_target_conv.reshape(1,25),12)\n",
    "\n",
    "random_x_variable_conv=Variable(torch.from_numpy(data).type(torch.FloatTensor)).expand(1000,1,25).cuda()\n",
    "\n",
    "my_conv_net.eval()\n",
    "predicted_df_and_br=my_conv_net(random_x_variable_conv).cpu()\n",
    "predicted_df_and_br=predicted_df_and_br[0].data.numpy()\n",
    "\n",
    "predicted_points=acquire_points_from_deformation(predicted_df_and_br,12)\n",
    "predicted_points=np.array(predicted_points)\n",
    "\n",
    "real_points=np.array(random_point_coordinated_delaunay)\n",
    "\n",
    "plt.scatter(predicted_points[:,0],predicted_points[:,1])\n",
    "plt.scatter(real_points[:,0],real_points[:,1])\n",
    "\n",
    "predicted_df_and_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52683707, 0.81007919, 0.53668459, 0.41284485, 0.58972531,\n",
       "        0.64828769, 0.81285994, 1.0013437 , 0.2380856 , 0.61617677,\n",
       "        0.68318367, 0.61139103, 2.04634931, 2.19383182, 2.95129436,\n",
       "        1.82416443, 3.54040685, 1.58576578, 4.88186463, 0.71275544,\n",
       "        2.16451307, 4.47740056, 1.2643371 , 3.77324317, 0.8       ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apply_procrustes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bf1555c046f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrandom_element\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_procrustes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'apply_procrustes' is not defined"
     ]
    }
   ],
   "source": [
    "random_element=apply_procrustes(generate_contour(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(random_element)\n",
    "barycenter=get_barycenter(random_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_element"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# barycenter_with_deformation_gradients=extract_barycenter_deformation_gradients(random_element.reshape(1,3,2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "thetas=[]\n",
    "deformation_gradients_with_barycent=[]\n",
    "for i in np.linspace(0,2*pi,1000):\n",
    "    rotated_triangle=np.dot(rot(i),random_element.T).T+barycenter\n",
    "    \n",
    "    thetas.append(i)\n",
    "    deformation_gradients_with_barycent.append(extract_barycenter_deformation_gradients(rotated_triangle.reshape(1,3,2)))\n",
    "    \n",
    "    \n",
    "    plot_contour(rotated_triangle)\n",
    "    \n",
    "plt.scatter(barycenter[0],barycenter[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_gradients_with_barycent=np.array(deformation_gradients_with_barycent)\n",
    "deformation_gradients_with_barycent=deformation_gradients_with_barycent.reshape(len(deformation_gradients_with_barycent),1,14)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "thetas_variable=Variable(torch.from_numpy(np.array(thetas).reshape(len(thetas),1)).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_variable=Variable(torch.from_numpy(deformation_gradients_with_barycent).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_net=Net(thetas_variable.size()[1],deformation_variable.size()[2],nb_of_hidden_layers=1, nb_of_hidden_nodes=23,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training data length:\",thetas_variable.size()[1],deformation_variable.size()[2])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(deformation_net.parameters(), lr=1e-4,weight_decay=.8)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    deformation_net.cuda()\n",
    "    thetas_variable=thetas_variable.cuda()\n",
    "    deformation_variable=deformation_variable.cuda()\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size=int(thetas_variable.size()[0] )\n",
    "nb_of_epochs=13000\n",
    "deformation_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "deformation_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,thetas_variable.size(0),batch_size):\n",
    "        out = deformation_net(thetas_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss= loss_func(out, deformation_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deformation_net.eval()\n",
    "\n",
    "deformation_net=deformation_net.cpu()\n",
    "\n",
    "theta=1.6\n",
    "random_theta=np.array([[theta]])\n",
    "random_theta_variable=Variable(torch.from_numpy(random_theta))\n",
    "random_theta_variable=random_theta_variable.expand(1000,1).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "prediction=deformation_net(random_theta_variable)\n",
    "prediction=prediction.data[0].numpy()\n",
    "\n",
    "predicted_points=np.array(acquire_points_from_deformation(prediction)).reshape(6,2)\n",
    "plt.scatter(predicted_points[:,0],predicted_points[:,1])\n",
    "\n",
    "\n",
    "rotated_triangle=np.dot(rot(theta),random_element.T).T+barycenter\n",
    "plot_contour(rotated_triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f338d70b86b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_tensor_barycenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_barycenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_variable_barycenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tensor_barycenter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnb_of_training_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_tensor_barycenter_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tensor_barycenter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnb_of_training_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Trying seperate network for the barycenter \n",
    "\n",
    "\n",
    "y_tensor_barycenter=torch.from_numpy(extract_barycenter(point_coordinates.reshape(len(point_coordinates),2*nb_of_points),nb_of_points)).type(torch.FloatTensor)\n",
    "y_variable_barycenter=Variable(y_tensor_barycenter[:nb_of_training_data])\n",
    "y_tensor_barycenter_test=y_tensor_barycenter[nb_of_training_data:]\n",
    "y_variable_barycenter_test=Variable(y_tensor_barycenter[nb_of_training_data:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_barycenter=Net(x_variable.size()[1],y_variable_barycenter.size()[2],nb_of_hidden_layers=2, nb_of_hidden_nodes=45,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable_barycenter.size()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net_barycenter.parameters(), lr=1e-4,weight_decay=0.3)\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if  torch.cuda.is_available():\n",
    "    #loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable_barycenter=x_variable.cuda(), y_variable_barycenter.cuda()\n",
    "    x_variable_test,y_variable_barycenter_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_barycenter_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=int(x_variable.size()[0] )\n",
    "nb_of_epochs=23000\n",
    "my_net_barycenter.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net_barycenter.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net_barycenter(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss= loss_func(out, y_variable_barycenter.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net_barycenter.eval()\n",
    "        out_test=my_net_barycenter(x_variable_test)   \n",
    "        \n",
    "        test_loss=loss_func(out_test, y_variable_barycenter_test)\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),test_loss.data[0]/(x_variable_test.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.05568314219353468 0.002519668884081854 -0\n",
      "14 -0.4109898595469855 0.2489642068483436 -0\n",
      "15 -0.4900887901485589 -0.1801269125775185 0\n",
      "16 0.2702173035276295 0.2081952882348311 0\n",
      "17 0.2696795222527284 -0.7176838424424394 0\n",
      "18 0.08808105550607472 0.4720116802330942 0\n",
      "13 -0.08675257535496986 0.02253086088514891 0\n",
      "14 -0.2027575095732738 -0.4536663705683257 0\n",
      "15 0.1772160785910867 0.4995345067693739 0\n",
      "16 -0.5068347880749873 0.0200993556006923 0\n",
      "17 0.3231277168443751 -0.2048432602270779 0\n",
      "18 -0.3012797974193605 -0.1772456902838346 0\n",
      "19 -0.736160392484604 -0.1079278571107537 0\n",
      "20 0.1327079256998553 0.8411332017920136 0\n",
      "21 0.2265992290362324 0.1049034505567851 0\n",
      "22 -0.2804707773917259 0.2408825971793528 0\n",
      "23 0.4607855482730201 0.3760019895816634 0\n",
      "24 -0.5248660521285465 -0.16978112690237 0\n",
      "25 0.1784932136795091 -0.5129122172218632 0\n",
      "26 0.3964240279883187 -0.7526013688183613 0\n",
      "27 0.04309493147068798 -0.229675423076602 0\n",
      "28 -0.2886980093623265 0.0236427304668028 0\n",
      "29 -0.0239400619554018 0.2684237969094293 0\n",
      "13 0.05760356458342415 -0.06534741135369834 0\n",
      "14 -0.1056579626975366 -0.5326780715127474 0\n",
      "15 0.4639139430782215 0.09076960052703403 0\n",
      "16 -0.609860674640078 0.214273958314434 0\n",
      "17 -0.1883187957062745 0.2659317576585804 0\n",
      "18 -0.007448896935533036 0.6291093624493075 0\n",
      "13 -0.2609866705031738 -0.03299524511141511 0\n",
      "14 0.4053259856026058 -0.05773351821538792 0\n",
      "15 -0.2588674882778322 0.5977693027889032 0\n",
      "16 0.0404804148423026 -0.2257197591455182 0\n",
      "17 -0.3417059119720849 -0.4631338697089256 0\n",
      "18 0.3947234899006242 -0.3710990714453097 0\n",
      "19 0.1214660186012095 0.1186925629716719 0\n",
      "20 0.4671112475379722 0.2242961950783896 0\n",
      "21 0.6664132000930845 -0.1741791868572924 0\n",
      "22 -0.6058446285706525 -0.1686055011525579 0\n",
      "23 -0.1429056328211091 0.2839255451213692 0\n",
      "24 -0.5359110627705175 0.2157791235300574 0\n",
      "25 0.7417620864067708 0.08160779277800064 0\n",
      "13 -0.01253755530584604 0.05455467261204934 -0\n",
      "14 0.1835360278801875 0.3546004244048284 0\n",
      "15 0.3512354648173933 0.01321428140544058 0\n",
      "16 -0.2367322215005848 0.244796316182613 -0\n",
      "17 0.07922894914003067 -0.2411380159225102 0\n",
      "13 -0.08502231630926194 0.179606439678706 -0\n",
      "14 -0.4718846600831873 -0.2571157630460322 0\n",
      "15 0.1131553258053266 0.516470086034422 0\n",
      "16 -0.2578498651222536 0.4344106912264861 -0\n",
      "17 -0.7575726377508983 -0.1155830148037349 0\n",
      "18 -0.4779140872508708 0.1305207548052333 -0\n",
      "19 0.3787914008959432 0.02527255984426188 0\n",
      "20 -0.4444644340098896 0.3951265927106059 -0\n",
      "21 -0.06862917034232044 -0.3073693669370449 0\n",
      "22 0.4049081982003062 0.547501573959541 0\n",
      "23 -0.4223306760080274 -0.638225016565773 0\n",
      "24 0.7693273390011168 0.03120909768057166 0\n",
      "25 0.1945983336999527 0.2967583908985366 0\n",
      "26 -0.0904435416161512 0.5979049114672327 -0\n",
      "13 -0.07589094582033913 0.225568540680772 0\n",
      "14 -0.5834400576018338 0.3151597947998389 0\n",
      "15 -0.01210315822274683 -0.3372803609544047 0\n",
      "16 0.2899369606374964 0.4254714107188343 0\n",
      "17 -0.1936759030104905 -0.6426243631670211 0\n",
      "18 0.1882126379650345 -0.01372980059346193 0\n",
      "19 -0.4132643376871608 0.1021209922633579 0\n",
      "20 -0.1608114586564753 -0.04894744298845213 0\n",
      "21 -0.07783574887272268 0.5420472568918661 0\n",
      "22 0.6097995353679484 0.07690634122010885 0\n",
      "23 0.412465916512694 0.7598236415397447 0\n",
      "24 0.106389850754277 -0.669948954253681 0\n",
      "25 -0.298613647830872 0.3998531144794111 0\n",
      "13 0.1534431948578107 0.09206074899776795 0\n",
      "14 -0.401931487266579 -0.1346779942289381 0\n",
      "15 0.2420707433419162 0.4109600816168214 0\n",
      "16 0.4174961601510437 -0.4854670411923026 0\n",
      "17 0.5097888336252072 0.2241893039217215 0\n",
      "18 -0.6082682628091888 0.2557085098944609 0\n",
      "19 0.6241312520854567 -0.134908141424984 0\n",
      "20 -0.0541044916879416 0.299170502560693 0\n",
      "21 -0.001107234982160506 -0.2753246631847759 0\n",
      "22 -0.4568694966370877 -0.5363361503009865 0\n",
      "23 -0.297795387895078 0.1952673471763556 0\n",
      "24 -0.1167440233275053 0.009248787402097482 0\n",
      "25 0.002326943711173233 0.5405985768679027 0\n",
      "26 0.3407504411474716 -0.1158899585765146 0\n",
      "27 0.4711319386907087 0.4327011923782704 0\n",
      "28 0.4049048076278919 0.6026289350048396 0\n",
      "29 0.2070003218913045 0.6853635882963387 0\n",
      "13 -0.1247379188658078 -0.2390592250270932 0\n",
      "14 0.3575866652722604 -0.1502881278537166 0\n",
      "15 0.05902649929483494 0.2047785436796738 0\n",
      "16 0.1817280132728238 -0.6407033780671729 0\n",
      "17 -0.2922180346713453 0.06023644066641752 -0\n",
      "18 0.3636484225972262 0.5342780759800914 0\n",
      "19 -0.145795147118215 -0.5626338099360256 0\n",
      "20 0.4382729640689934 -0.3961814941831636 0\n",
      "21 -0.6768391227760929 -0.05174877288941896 0\n",
      "22 -0.412932744892872 -0.2869925176061542 0\n",
      "23 0.5447832842019746 -0.005227471992630229 0\n",
      "24 0.147216164738828 0.7437896708476411 0\n",
      "25 0.5924569705851064 -0.2112861470405448 0\n",
      "26 0.339970973437858 0.1176941194143631 0\n",
      "27 -0.4156249886961855 -0.5278984844905899 0\n",
      "28 0.0820791121768327 -0.05932594863111328 0\n",
      "29 0.145060673012807 -0.3465355912755594 0\n",
      "30 -0.1667560181050932 0.3013834201979505 -0\n",
      "31 0.287020641042956 0.3397539384215304 0\n",
      "32 0.08256731664451197 0.4456960470813894 0\n",
      "13 0.1208138091471264 0.2110860924209753 0\n",
      "14 -0.04652989272063438 -0.2747345130400735 0\n",
      "15 -0.1550821189663044 -0.7797588221262526 0\n",
      "16 -0.3045863837348788 -0.2393646514224987 0\n",
      "17 -0.3065222261210492 0.2854199939883747 -0\n",
      "18 0.4017817256677181 -0.05793721434402928 0\n",
      "19 -0.2157995528200188 -0.4966411203961197 0\n",
      "20 0.6537803983747658 0.03163439659166705 0\n",
      "21 0.3530547243959844 0.6216078565700487 0\n",
      "22 -0.1699034807887227 -0.02248809645658835 0\n",
      "23 -0.3500543557529858 0.6060183785454613 -0\n",
      "24 -0.4807343918916649 -0.4121286027162951 0\n",
      "25 0.4620587747897377 0.2358819209711624 0\n",
      "26 -0.02191857712960443 0.5417453465538634 -0\n",
      "27 0.1273386047751336 -0.126520999607017 0\n",
      "13 0.3716330849312071 -0.2089745246731272 0\n",
      "14 0.1334112441691168 0.3256354919867985 0\n",
      "15 -0.3916183158773035 -0.433218968067656 0\n",
      "16 -0.06512321534019484 -0.2084116138868803 0\n",
      "17 0.4552220754246349 -0.0128659723555704 0\n",
      "18 0.6880204316679474 -0.3220571656732912 0\n",
      "19 0.7748609912938052 -0.1151954727561683 0\n",
      "20 0.3854619362244209 0.1959138029332674 0\n",
      "21 0.3904577689223702 0.485332195699161 0\n",
      "22 0.28889585366077 -0.4331937687984587 0\n",
      "23 -0.7231684719314836 -0.3919753622633373 0\n",
      "24 -0.4593304839445982 -0.1302215604663943 0\n",
      "25 -0.5548274231515936 -0.6212102079204748 0\n",
      "26 0.1859417093316291 -0.01122332247404905 0\n",
      "27 -0.1536470460353664 0.508293373115195 -0\n",
      "28 -0.2098594011573781 -0.3478842729401224 0\n",
      "29 0.5605132191778168 -0.17080920637519 0\n",
      "30 0.4557669508546512 -0.3662302372400262 0\n",
      "31 0.1706726504518605 -0.2749626115651861 0\n",
      "32 0.551851548517569 0.1260257138715657 0\n",
      "33 0.6135273250715423 0.001049490141084911 0\n",
      "34 -0.1547977952296555 0.1335074124904542 -0\n",
      "13 0.09807197202906196 -0.1170730103017568 0\n",
      "14 -0.4096733782686167 -0.6315559105069059 0\n",
      "15 -0.6259365392801872 -0.001624491997301211 0\n",
      "16 0.4642069541776485 -0.07579172323054996 0\n",
      "17 0.0646854989743826 -0.4798897900529875 0\n",
      "18 0.1882225827228944 0.2283473767350593 0\n",
      "19 -0.1537235799944176 -0.7700898202704292 0\n",
      "20 -0.3358944629594255 -0.2591289262802057 0\n",
      "21 0.6901431023079878 -0.2357533758412402 0\n",
      "22 -0.2218958958197324 0.1365563685102068 -0\n",
      "23 -0.5783426687466795 0.3603593878876408 -0\n",
      "24 0.3008947720045917 -0.2952209516488591 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_net_barycenter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-427-497d4d854f5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmy_net_barycenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net_barycenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mmy_net_barycenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mrandom_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_x_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_net_barycenter' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.clf()\n",
    "for i in range(1000):\n",
    "    random_contour=apply_procrustes(generate_contour(12))\n",
    "    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length(random_contour,0.8,algorithm='del2d')\n",
    "    if random_nb_of_points==nb_of_points:\n",
    "            break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*12)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[.8]]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "random_point_coordinated_delaunay.reshape(nb_of_points,2)\n",
    "mid_point_delaunay=(random_point_coordinated_delaunay[0]+random_point_coordinated_delaunay[1])/2\n",
    "plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location',marker='x')\n",
    "\n",
    "\n",
    "\n",
    "random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "random_x_variable=random_x_variable.expand(1000,2*12+1).type(torch.FloatTensor)\n",
    "my_net=my_net.cpu()\n",
    "my_net.eval()\n",
    "my_net_barycenter=my_net_barycenter.cpu()\n",
    "my_net_barycenter.eval()\n",
    "random_prediction=my_net(random_x_variable)\n",
    "random_prediction_barycenter=my_net_barycenter(random_x_variable)\n",
    "\n",
    "\n",
    "random_prediction=random_prediction.data[0].numpy()\n",
    "random_prediction_barycenter=random_prediction_barycenter.data[0].numpy()\n",
    "\n",
    "print(random_prediction)\n",
    "predicted_coordinates=acquire_points_from_deformation(random_prediction,nb_of_points)\n",
    "predicted_coordinates=np.array(predicted_coordinates).reshape(2*nb_of_points,2)\n",
    "\n",
    "random_prediction[-2:]=random_prediction_barycenter\n",
    "print(random_prediction)\n",
    "\n",
    "predicted_coordinates2=acquire_points_from_deformation(random_prediction,nb_of_points)\n",
    "predicted_coordinates2=np.array(predicted_coordinates2).reshape(2*nb_of_points,2)\n",
    "\n",
    "plt.scatter(predicted_coordinates[:,0],predicted_coordinates[:,1],label='Predicted point location')\n",
    "plt.scatter(predicted_coordinates2[:,0],predicted_coordinates2[:,1],label='Predicted point location with different NN for barycenter',marker='d')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_gradients=deformation_gradients_with_barycenters.reshape(len(deformation_gradients_with_barycenters),4*nb_of_points+2)[:,:4*nb_of_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformations=[]\n",
    "for i in range(nb_of_points):\n",
    "    deformations.append( deformation_gradients.reshape(len(deformation_gradients),nb_of_points,4)[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformations=np.array(deformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_gradients_with_barycenters.shape,deformation_gradients.shape,deformations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformations.reshape(nb_of_points*len(polygons_reshaped),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_index=np.empty([len(polygons_reshaped),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "polygons_and_deformation_index=[]\n",
    "for i in range(1,nb_of_points+1):\n",
    "    deformation_index=np.empty([len(polygons_reshaped),1])\n",
    "    deformation_index.fill(i)\n",
    "    polygons_and_deformation_index.append(np.hstack([polygons_reshaped,deformation_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_and_deformation_index=np.array(polygons_and_deformation_index)\n",
    "polygons_and_deformation_index=polygons_and_deformation_index.reshape(nb_of_points*len(polygons_reshaped),2*nb_of_points+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformations_indexed=deformations[0].reshape(len(polygons_reshaped),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 training/test data ratio\n",
    "\n",
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n",
    "nb_of_test_data,nb_of_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the coordinates, the target edge lentgh and the deformation index as an input\n",
    "x_tensor=torch.from_numpy(polygons_reshaped[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is now the deformation gradient of each traingle according to an assigned index\n",
    "\n",
    "\n",
    "\n",
    "y_multiple_tensor=torch.from_numpy(deformations[0][:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_multiple_tensor_test=torch.from_numpy(deformations[0][nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_multiple_variable,y_multiple_variable_test=Variable(y_multiple_tensor),Variable(y_multiple_tensor_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_index=Net(x_variable.size()[1],y_multiple_variable.size()[1],nb_of_hidden_layers=3, nb_of_hidden_nodes=10,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable.size()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net_index.parameters(), lr=1e-4,weight_decay=0.3)\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_multiple_variable=x_variable.cuda(), y_multiple_variable.cuda()\n",
    "    x_variable_test,y_multiple_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_multiple_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size=int(x_variable.size()[0] )\n",
    "nb_of_epochs=23000\n",
    "my_net_index.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net_index.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net_index(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss= loss_func(out, y_multiple_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net_index.eval()\n",
    "        out_test=my_net_index(x_variable_test)   \n",
    "        \n",
    "        test_loss=loss_func(out_test, y_multiple_variable_test)\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),test_loss.data[0]/(x_variable_test.size(0)))\n",
    "        my_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Visualising outputs on the test data \n",
    "my_net.eval()\n",
    "plt.clf()\n",
    "sample_index=144\n",
    "sample_tests=out_test.cpu()\n",
    "sample_tests=sample_tests.data.numpy()\n",
    "\n",
    "sample_coordinates=acquire_points_from_deformation(sample_tests[sample_index],nb_of_points)\n",
    "sample_coordinates=np.array(sample_coordinates).reshape(2,2)\n",
    "\n",
    "real_coordinates=point_coordinates[nb_of_training_data+sample_index].reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "sample_contour=np.delete(polygons_reshaped[nb_of_training_data+sample_index],24).reshape(12,2)\n",
    "plot_contour(sample_contour)\n",
    "plt.scatter(sample_coordinates[:,0],sample_coordinates[:,1])\n",
    "plt.scatter(real_coordinates[:,0],real_coordinates[:,1],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_multiple_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a list of networks to be trained. Each network is trained to regress on the points of each triangle\n",
    "# that is formed conneting the barycenter of the interior point polygon with its edges.\n",
    "\n",
    "\n",
    "network_list=[]\n",
    "for i in range(nb_of_points):  \n",
    "    \n",
    "    deformations_indexed=deformations[i].reshape(len(polygons_reshaped),1,4)\n",
    "    \n",
    "    \n",
    "    # output is now the deformation gradient of each traingle according to an assigned index\n",
    "\n",
    "\n",
    "\n",
    "    y_multiple_tensor=torch.from_numpy(deformations_indexed[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "    y_multiple_tensor_test=torch.from_numpy(deformations_indexed[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "    y_multiple_variable,y_multiple_variable_test=Variable(y_multiple_tensor),Variable(y_multiple_tensor_test)\n",
    "\n",
    "    net=Net(x_variable.size()[1],y_multiple_variable.size()[2],nb_of_hidden_layers=3, nb_of_hidden_nodes=15,batch_normalization=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4,weight_decay=0.3)\n",
    "    loss_func =torch.nn.MSELoss(size_average=False) \n",
    "\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if  torch.cuda.is_available():\n",
    "        loss_func.cuda()\n",
    "        \n",
    "        x_variable , y_multiple_variable=x_variable.cuda(), y_multiple_variable.cuda()\n",
    "        x_variable_test,y_multiple_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_multiple_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "        print(\"cuda activated\")\n",
    "        \n",
    "    \n",
    "    batch_size=int(x_variable.size()[0] )\n",
    "    nb_of_epochs=4000\n",
    "    net.cuda()\n",
    "\n",
    "    # Train the network #\n",
    "    net.train()\n",
    "    for t in range(nb_of_epochs):\n",
    "        sum_loss=0\n",
    "        for b in range(0,x_variable.size(0),batch_size):\n",
    "            out = net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "            loss= loss_func(out, y_multiple_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "            sum_loss+=loss.data[0]\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "            optimizer.step()        # apply gradients\n",
    "        if t%10==0: \n",
    "            net.eval()\n",
    "            out_test=net(x_variable_test)   \n",
    "        \n",
    "            test_loss=loss_func(out_test, y_multiple_variable_test)\n",
    "            print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),\" Test Loss :\",test_loss.data[0]/(x_variable_test.size(0)))\n",
    "            net.train()\n",
    "    net.eval()\n",
    "\n",
    "    network_list.append(net)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_gradients=[]\n",
    "for i in range(nb_of_points):\n",
    "    net=network_list[i]\n",
    "    prediction=net(x_variable_test).cpu().data.numpy()\n",
    "    deformation_gradients.append(prediction)\n",
    "    \n",
    "deformation_gradients=np.array(deformation_gradients) \n",
    "deformation_gradients_resized=[]\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(x_variable_test.size()[0]):\n",
    "    polygon_deformations=[]\n",
    "    for i in range(nb_of_points):\n",
    "        polygon_deformations.append(deformation_gradients[i][j])\n",
    "    polygon_deformations=np.array(polygon_deformations).reshape(1,4*nb_of_points)\n",
    "    deformation_gradients_resized.append(polygon_deformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_gradients_resized=np.array(deformation_gradients_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results on test data\n",
    "\n",
    "plt.clf()\n",
    "sample_index=659\n",
    "sample_deformation=deformation_gradients_resized[sample_index]\n",
    "x_variable=x_variable.cpu()\n",
    "my_net_barycenter=my_net_barycenter.cpu()\n",
    "random_x_variable=x_variable_test[sample_index]\n",
    "random_x_variable=random_x_variable.expand(1000,2*12+1).type(torch.FloatTensor)\n",
    "\n",
    "prediction_barycenter=my_net_barycenter(random_x_variable).data[0].numpy().reshape(1,2)\n",
    "\n",
    "\n",
    "def_with_bar=np.hstack([sample_deformation,prediction_barycenter])\n",
    "\n",
    "predicted_points=np.array(acquire_points_from_deformation(def_with_bar,12))\n",
    "predicted_points=np.array(predicted_points).reshape(2*nb_of_points,2)\n",
    "real_coordinates=point_coordinates[nb_of_training_data+sample_index].reshape(nb_of_points,2)\n",
    "sample_contour=np.delete(polygons_reshaped[nb_of_training_data+sample_index],24).reshape(12,2)\n",
    "plot_contour(sample_contour)\n",
    "plt.scatter(real_coordinates[:,0],real_coordinates[:,1])\n",
    "plt.scatter(predicted_points[:,0],predicted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising result on random data\n",
    "\n",
    "\n",
    "#plt.clf()\n",
    "#for i in range(1000):\n",
    "#    random_contour=apply_procrustes(generate_contour(12))\n",
    "#    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length(random_contour,0.8,algorithm='del2d')\n",
    "#    if random_nb_of_points==nb_of_points:\n",
    "#            break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*12)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[.8]]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_variable=Variable(torch.from_numpy(random_contour_with_target).type(torch.FloatTensor))\n",
    "random_variable=random_variable.expand_as(x_variable_test)\n",
    "\n",
    "random_deformation_gradients=[]\n",
    "for i in range(nb_of_points):\n",
    "    net=network_list[i]\n",
    "    net=net.cpu()\n",
    "    prediction=net(random_variable).cpu().data.numpy()\n",
    "    random_deformation_gradients.append(prediction)\n",
    "    \n",
    "random_deformation_gradients=np.array(random_deformation_gradients) \n",
    "random_deformation_gradients_resized=[]\n",
    "\n",
    "for j in range(random_variable.size()[0]):\n",
    "    polygon_deformations=[]\n",
    "    for i in range(nb_of_points):\n",
    "        polygon_deformations.append(random_deformation_gradients[i][j])\n",
    "    polygon_deformations=np.array(polygon_deformations).reshape(1,4*nb_of_points)\n",
    "    random_deformation_gradients_resized.append(polygon_deformations)\n",
    "\n",
    "\n",
    "random_prediction_barycenter=my_net_barycenter(random_variable).data[0].numpy().reshape(1,2)\n",
    "\n",
    "\n",
    "random_sample_deformation=random_deformation_gradients_resized[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def_with_bar=np.hstack([random_sample_deformation,random_prediction_barycenter])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_predicted_points=np.array(acquire_points_from_deformation(def_with_bar,12))\n",
    "random_predicted_points=np.array(random_predicted_points).reshape(2*nb_of_points,2)\n",
    "\n",
    "\n",
    "plt.scatter(random_predicted_points[:,0],random_predicted_points[:,1],label='Prediction using multiple NNs')\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "random_point_coordinated_delaunay.reshape(nb_of_points,2,)\n",
    "#plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location',marker='x')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_deformation_gradients_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #contour_barycenter_distances.append(distances_from_barycenter)\n",
    "#contour_barycenter_distances=np.array(contour_barycenter_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(polygon)\n",
    "plt.scatter(polygon_barycenter[0],polygon_barycenter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44973, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7439891867039772,\n",
       " 0.6329082815113287,\n",
       " 0.72343688300655,\n",
       " 1.3455405775169296,\n",
       " 0.9716423448782179,\n",
       " 0.5362334949191034,\n",
       " 0.7322431783003066,\n",
       " 1.1644061228987055,\n",
       " 1.1825842413776904,\n",
       " 0.8176587868812325,\n",
       " 1.0447014757385589,\n",
       " 1.1656516636572016]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_from_barycenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (12,2) (24,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-332-fa5c3a0a8de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpolygon_barycenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (12,2) (24,) "
     ]
    }
   ],
   "source": [
    "np.linalg.norm(polygon-polygon_barycenter.repeat(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_barycenter.du"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
