{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Triangulation/')\n",
    "sys.path.insert(0, '../network_datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from math import atan2,pow,acos\n",
    "from  Neural_network import *\n",
    "\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    angle=np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))*180/pi\n",
    "    if angle>180:\n",
    "        angle=angle-360\n",
    "    return angle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A fucntion to acquire the 2 points from the braycenter and chosen direction\n",
    "def get_points(barycenter,direction):\n",
    "    rotation=np.array([[cos(pi),-sin(pi)],\n",
    "                         [sin(pi),cos(pi)] ])\n",
    "    point1=np.array(direction+barycenter)\n",
    "    direction=np.dot(rotation,direction)\n",
    "    point2=np.array(direction+barycenter)\n",
    "    return point1, point2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def get_edge_lengths(polygon):\n",
    "    polygon_edge_lengths=np.empty([polygon.shape[0]])\n",
    "    for index,_ in enumerate(polygon):\n",
    "        polygon_edge_lengths[index]=np.linalg.norm(polygon[(index+1)%(polygon.shape[0])]-polygon[index])\n",
    "    return polygon_edge_lengths\n",
    "    \n",
    "\n",
    "def extract_lengths_angles(polygon_set,nb_of_points):\n",
    "    lengths=[]\n",
    "    angles=[]\n",
    "    target_edge_length=[]\n",
    "    for polygons in polygon_set:\n",
    "        polygon=np.delete(polygons,2*nb_of_points).reshape(nb_of_points,2)\n",
    "        lengths.append(get_edge_lengths(polygon))\n",
    "        angles.append(np.array(np.multiply(pi/180,get_polygon_angles(polygon))))\n",
    "        target_edge_length.append([polygons[2*nb_of_points]])\n",
    "    data=np.hstack([lengths,angles,target_edge_length])\n",
    "    return data,lengths,angles\n",
    "    \n",
    "def extract_lengths_angles_in_triangle_form(polygon_set,nb_of_points):\n",
    "    data,lengths,angles=extract_lengths_angles(polygon_set,nb_of_points)\n",
    "    data_reformed=np.empty([data.shape[0],3*nb_of_points+1])\n",
    "    for polygon_index,polygon_lengths in enumerate(lengths.copy()):\n",
    "        data_reformed[polygon_index][0:3*nb_of_points:3]=polygon_lengths[0:nb_of_points]\n",
    "        data_reformed[polygon_index][1:3*(nb_of_points-1):3]=polygon_lengths[1:nb_of_points]\n",
    "        data_reformed[polygon_index][3*nb_of_points-2]=polygon_lengths[0]\n",
    "        data_reformed[polygon_index][2:3*nb_of_points:3]=angles[polygon_index]\n",
    "        #including target edge length\n",
    "        data_reformed[polygon_index][-1]=polygon_set[polygon_index][2*nb_of_points]\n",
    "        \n",
    "    return data_reformed\n",
    "\n",
    "def extract_midpoint_directions(points):\n",
    "    mid_points=[]\n",
    "    directions=[]\n",
    "    for points in points:\n",
    "        for coordinates in points:\n",
    "            coordinates_reshape=coordinates.reshape(2,2)\n",
    "            point_A=np.array(coordinates_reshape[0])\n",
    "            point_B=np.array(coordinates_reshape[1])\n",
    "            mid_point=(point_A+point_B)/2\n",
    "            direction=point_B-mid_point\n",
    "            if direction[0]<0:\n",
    "                direction=point_A-mid_point\n",
    "            mid_points.append(mid_point)\n",
    "            directions.append(direction)\n",
    "    mid_points=np.array(mid_points)\n",
    "    directions=np.array(directions)\n",
    "        \n",
    "    mid_points_with_directions=np.hstack([mid_points,directions])\n",
    "    return mid_points_with_directions,mid_points,directions\n",
    "\n",
    "\n",
    "def extract_barycenter(points,nb_of_points):\n",
    "        \n",
    "    barycenters=[]\n",
    "    \n",
    "    polygons=points.reshape(len(points),nb_of_points,2)\n",
    "    for polygon in polygons:\n",
    "        barycenter=np.array([polygon[:,0].sum()/nb_of_points,polygon[:,1].sum()/nb_of_points])\n",
    "        barycenter_triangles=[]\n",
    "        barycenters.append(barycenter)\n",
    "        for i in range(nb_of_points):\n",
    "            barycenter_triangles.append(np.array([barycenter,polygon[i],polygon[(i+1)%nb_of_points]]))\n",
    "    barycenters=np.array(barycenters).reshape(len(points),1,2)\n",
    "    \n",
    "    return np.array(barycenter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def length(v):\n",
    "    return sqrt(v[0]**2+v[1]**2)\n",
    "def dot_product(v,w):\n",
    "    return v[0]*w[0]+v[1]*w[1]\n",
    "def determinant(v,w):\n",
    "    return v[0]*w[1]-v[1]*w[0]\n",
    "def inner_angle(v,w):\n",
    "    cosx=dot_product(v,w)/(length(v)*length(w))\n",
    "    rad=acos(cosx) # in radians\n",
    "    return rad*180/pi # returns degrees\n",
    "def angle_counterclockwise(A, B):\n",
    "    inner=inner_angle(A,B)\n",
    "    det = determinant(A,B)\n",
    "    if det<0: \n",
    "        return 360-inner\n",
    "    else: \n",
    "        return inner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpolate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9a9c25c9b9df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mquadratic_bspline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterp2d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;34m''' Iveride interp2d to include quadratic spline'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'interpolate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    \n",
    "    \n",
    "class myOtherLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target)->Variable:\n",
    "        \n",
    "        torch_sum=0\n",
    "        for i in range(0,output.size()[1],2):\n",
    "            euclidean_distance=(output[:,i]-target[:,i]).pow(2)+(output[:,i+1]-target[:,i+1]).pow(2)\n",
    "            torch_sum+=euclidean_distance\n",
    "        return  torch_sum\n",
    "\n",
    "def my_torch_loss_function(a,b)->Variable:\n",
    "    torch_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "    return  torch_sum   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def my_torch_loss_function2(a,b,target_edge_length)->Variable:\n",
    "    torch_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "    \n",
    "    torch_sum=torch.div(torch_sum,target_edge_length)\n",
    "    return  torch_sum   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def my_torch_loss_function3(a,b,target_edge_length,l_param)->Variable:\n",
    "    torch_sum=0\n",
    "    torch_point_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "\n",
    "    for i in range(0,a.size()[1]-3,2):\n",
    "        distance_between_points=torch.sqrt((a[:,i]-a[:,i+2]).pow(2)+(a[:,i+1]-a[:,i+3]).pow(2))\n",
    "        target_edge_length_distance=torch.abs((distance_between_points-target_edge_length))\n",
    "        torch_point_sum+=target_edge_length_distance\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    #torch_sum=torch.div(torch_sum,target_edge_length)+l_param*torch_point_sum\n",
    "    torch_sum=torch.div(torch_sum,target_edge_length)*(1-l_param)+(l_param)*target_edge_length_distance\n",
    "    #torch_sum=(1-l_param)*torch_sum+(l_param)*torch_point_sum\n",
    "\n",
    "    \n",
    "\n",
    "    return  torch_sum   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort_points(point_coordinates,nb_of_points):\n",
    "    polygon=point_coordinates.reshape(len(point_coordinates),nb_of_points,2)\n",
    "    barycenters=extract_barycenter(point_coordinates,nb_of_points)\n",
    "    angles=[]\n",
    "    polygons=point_coordinates.reshape(len(point_coordinates),nb_of_points,2)\n",
    "    vectors=polygons-barycenters\n",
    "\n",
    "    for  barycenter_vectors in vectors:\n",
    "        for vector in barycenter_vectors:\n",
    "            angles.append(angle_counterclockwise(np.array([1,0]),vector))\n",
    "                      \n",
    "    angles=np.array(angles).reshape(len(vectors),nb_of_points,1)\n",
    "    point_coordinates_with_angles=np.dstack([polygons,angles])\n",
    "    point_coordinates_sorted=[]\n",
    "    for points in point_coordinates_with_angles:\n",
    "        points_sorted=np.array(sorted(points,key=lambda x: x[2]))\n",
    "        points_sorted=points_sorted[:,0:2]\n",
    "        point_coordinates_sorted.append(points_sorted.reshape(1,nb_of_points,2))\n",
    "    return np.array(point_coordinates_sorted)    \n",
    "    \n",
    "\n",
    "class quadratic_bspline(interpolate.interp2d):\n",
    "    ''' Iveride interp2d to include quadratic spline'''\n",
    "    def __init__(self,*args,**kws):\n",
    "        try:\n",
    "            super(quadratic_bspline,self).__init__(*args,**kws)\n",
    "        \n",
    "        except ValueError:      \n",
    "            kx=ky=2\n",
    "            x=args[0]\n",
    "            y=args[1]\n",
    "            z=args[2]\n",
    "            rectangular_grid = (z.size == len(x) * len(y))\n",
    "            if rectangular_grid:\n",
    "                self.tck = scipy.interpolate.fitpack.bisplrep(x, y, z, kx=kx, ky=ky, s=0.0)\n",
    "            else:\n",
    "                nx, tx, ny, ty, c, fp, ier = scipy.interpolate.dfitpack.regrid_smth(\n",
    "                x, y, z, None, None, None, None,\n",
    "                kx=kx, ky=ky, s=0.0)\n",
    "                self.tck = (tx[:nx], ty[:ny], c[:(nx - kx - 1) * (ny - ky - 1)],\n",
    "                        kx, ky)\n",
    "            self.bounds_error = False\n",
    "            self.fill_value = None\n",
    "            self.x, self.y, self.z = [np.array(a, copy=copy) for a in (x, y, z)]\n",
    "\n",
    "            self.x_min, self.x_max = np.amin(x), np.amax(x)\n",
    "            self.y_min, self.y_max = np.amin(y), np.amax(y)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "# Included batch normalization with dropout layers\n",
    "\n",
    "nb_of_points_output=1\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=14,stride=1)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*9)\n",
    "        self.fc1=nn.Linear(28*9,12)\n",
    "        self.dp_1=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        self.dp_2=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        self.dp_3=nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,nb_of_points_output*2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),kernel_size=2,stride=1))\n",
    "        \n",
    "        #x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        #x=self.conv2(x)\n",
    "\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*9))))\n",
    "        x=F.relu(self.fc2( self.dp_1(self.bn2(x))))\n",
    "        x=F.relu(self.fc3(self.dp_2(self.bn3(x))))\n",
    "\n",
    "        x=self.fc4(self.bn4(self.dp_3(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolution network involving the pairing of lengths and angles through convolution and then \n",
    "# fully connectiong in a  linear fashion\n",
    "# Input : a matrix X where lengths take X/2 and angles X/2 and the target edge length\n",
    "\n",
    "class alt_conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_of_filters,nb_of_hidden_nodes,out_dimension,nb_of_edges):\n",
    "        super(alt_conv_net,self).__init__()\n",
    "        \n",
    "        self.nb_of_edges=nb_of_edges\n",
    "        self.nb_of_filters=nb_of_filters\n",
    "        \n",
    "        self.conv1=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                 nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "        self.conv2=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                 nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "        # The linear connections are fixed for 12 polygon example\n",
    "   \n",
    "        self.fc=nn.Sequential(  nn.BatchNorm1d(num_features=nb_of_filters*(nb_of_edges-2)+nb_of_filters*(nb_of_edges-2)+1),\n",
    "\n",
    "                                nn.Linear(nb_of_filters*(nb_of_edges-2)+nb_of_filters*(nb_of_edges-2)+1,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                               nn.Linear(nb_of_hidden_nodes,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "\n",
    "                               nn.Linear(nb_of_hidden_nodes,out_dimension) )                                                                        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        lenghts=x.narrow(1,0,1).narrow(2,0,self.nb_of_edges)\n",
    "        angles=x.narrow(1,0,1).narrow(2,self.nb_of_edges,self.nb_of_edges)\n",
    "        target_edge_length=x.narrow(1,0,1).narrow(2,2*self.nb_of_edges,1).resize(x.size()[0],1)\n",
    "\n",
    "       \n",
    "        conv_result1=self.conv1(lenghts)\n",
    "        conv_result2=self.conv2(angles)        \n",
    "        \n",
    "        # reshape the convolution results\n",
    "        \n",
    "        conv_result1=conv_result1.view(-1,self.nb_of_filters*(self.nb_of_edges-2))\n",
    "        conv_result2=conv_result2.view(-1,self.nb_of_filters*(self.nb_of_edges-2))\n",
    "        \n",
    "        concat_tensor=torch.cat([conv_result1,conv_result2,target_edge_length],1)\n",
    "        output=self.fc(concat_tensor)\n",
    "        return output\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolutional network. The input of the network is : Length[index], Length[index+1], angle[index] e.g the triangles\n",
    "# formed by connecting the neighbouring edges of the contour.\n",
    "# The input is convoluted by a kernel of size 3 with a stride of 3.\n",
    "\n",
    "\n",
    "class triangle_convoluting_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_of_filters,nb_of_hidden_nodes,out_dimension,nb_of_edges):\n",
    "        super(triangle_convoluting_net,self).__init__()\n",
    "        self.nb_of_filters=nb_of_filters\n",
    "        self.nb_of_edges=nb_of_edges\n",
    "        self.conv=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=3,kernel_size=3),\n",
    "                                #nn.Conv1d(in_channels=nb_of_filters,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                #nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True),\n",
    "                                nn.Conv1d(in_channels=nb_of_filters,out_channels=nb_of_filters,kernel_size=2,stride=1),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "       \n",
    "        # The linear connections are fixed for 12 polygon example\n",
    "   \n",
    "        self.fc=nn.Sequential(  nn.BatchNorm1d(num_features= nb_of_filters*(nb_of_edges-2)+1),\n",
    "                              \n",
    "                                # 1rst layer\n",
    "                                nn.Linear(nb_of_filters*(nb_of_edges-2)+1,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                             \n",
    "                              # 2nd layer\n",
    "                            #  nn.Linear(nb_of_hidden_nodes,nb_of_hidden_nodes),\n",
    "                             #     nn.ReLU(inplace=True),\n",
    "                             #  nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                                \n",
    "                              # 3rd layer\n",
    "                              #nn.Linear(nb_of_hidden_nodes,nb_of_hidden_nodes),\n",
    "                               #   nn.ReLU(inplace=True),\n",
    "                              #nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                              \n",
    "                            \n",
    "                              nn.Linear(nb_of_hidden_nodes,out_dimension)\n",
    "                             \n",
    "                             \n",
    "                             \n",
    "                             )      \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        triangles=x.narrow(1,0,1).narrow(2,0,3*self.nb_of_edges)\n",
    "        target_edge_length=x.narrow(1,0,1).narrow(2,3*self.nb_of_edges,1).resize(x.size()[0],1)\n",
    "        \n",
    "        conv_result=self.conv(triangles)\n",
    "        \n",
    "        \n",
    "        \n",
    "        conv_result=conv_result.view(-1,self.nb_of_filters*(self.nb_of_edges-2))\n",
    "        #print(conv_result,target_edge_length)\n",
    "\n",
    "        concat_tensor=torch.cat([conv_result,target_edge_length],1)\n",
    "        output=self.fc(concat_tensor)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of polygons with the number of points inserted\n",
    "\n",
    "def plot_population_nb_of_points(set_of_points,point_coordinates):\n",
    "    number_of_polygons=[]\n",
    "    for numbers in set_of_points:\n",
    "        indixes=get_indices_nb_of_points(set_of_points,numbers,point_coordinates)\n",
    "        number_of_polygons.append(len(indixes))\n",
    "\n",
    "    \n",
    "    #plt.xticks(np.array(list(set_of_points)))\n",
    "    #plt.yticks(np.array(number_of_polygons))\n",
    "    plt.figure(figsize=(24, 24))  \n",
    "    plt.xticks(list(set_of_points))\n",
    "    plt.yticks(number_of_polygons)\n",
    "    plt.xlabel('number of insertion points')\n",
    "    plt.ylabel('Polygon counts')\n",
    "    plt.title('Plot bar for number of polygons with number of insertion points (20000 total)')\n",
    "    plt.bar(list(set_of_points), number_of_polygons, color='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_point_regression_network(filename,net):\n",
    "    path=os.path.join('../network_datasets/point_coord_NN',filename)\n",
    "\n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(net,output)\n",
    "        \n",
    "def load_point_regression_network(filename):\n",
    "    path=os.path.join('../network_datasets/point_coord_NN',filename)\n",
    "        \n",
    "    with open(path,'rb') as input:\n",
    "        net=pickle.load(input)\n",
    "        \n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset of polygons with a specific target edge length \n",
    "def get_polygons_target_edge_length(polygons_reshaped,point_coordinates,target_edge_length):\n",
    "    polygons_reshaped_target_edge_length=polygons_reshaped[polygons_reshaped[:,int(len(polygons_reshaped))]==target_edge_length]\n",
    "\n",
    "    target_lentgh_indices=np.where(polygons_reshaped[:,int(len(polygons_reshaped))]==target_edge_length)\n",
    "    point_coordinates_target_edge_length=point_coordinates[target_lentgh_indices]\n",
    "\n",
    "    point_coordinates=point_coordinates_target_edge_length\n",
    "    polygons_reshaped=polygons_reshaped_target_edge_length\n",
    "    \n",
    "    \n",
    "# Given a neural network and a test set for this generate more polygons pertubing those that had an error more than tol\n",
    "def generate_data(net,x_test,tol):\n",
    "    net=net.cpu()\n",
    "    x_test=x_test.cpu()\n",
    "    net.eval()\n",
    "    predictions=net(x_test).data.numpy()\n",
    "    target_edge_lengths=x_test[:,16]\n",
    "    contours=x_test.data.numpy()\n",
    "    nb_sampling=10\n",
    "    \n",
    "    pertubed_contours=[]\n",
    "    edge_lengths=[]\n",
    "    inserted_points_delaunay=[]\n",
    "    set_of_pertubed_contours=set()\n",
    "    \n",
    "    \n",
    "    for index,prediction in enumerate(predictions):\n",
    "            contour=contours[index]\n",
    "            contour=np.delete(contour,16)\n",
    "            contour=contour.reshape(8,2)\n",
    "            _,real_value=get_extrapoints_target_length_jupyter(contour,target_edge_lengths[index],algorithm='del2d')\n",
    "            error=np.linalg.norm(prediction-np.array(real_value))\n",
    "\n",
    "            if error > tol and hash(tuple(contour.reshape(16))) not in set_of_pertubed_contours:\n",
    "                print(\"Pertubing contour, \" ,contour)\n",
    "                set_of_pertubed_contours.add(hash(tuple(contour.reshape(16))))\n",
    "                print(set_of_pertubed_contours)\n",
    "                for j in range(8):\n",
    "                    for i in range(0,nb_of_sampling):\n",
    "                        pertubed_contour=contour.copy()\n",
    "                        pertubed_contour[j]=np.random.normal(contour[j],0.1)\n",
    "                        print(contour[j],\"changed to \",pertubed_contour[j])\n",
    "                        print(pertubed_contour,contour)\n",
    "                        \n",
    "                        for edge_length in np.linspace(0.1,1,10):\n",
    "                            print(\"Examining target edge length \",edge_length,\"for \",pertubed_contour)\n",
    "                            nb_of_points_delaunay,point_coords_delaunay=get_extrapoints_target_length_jupyter(pertubed_contour,edge_length,algorithm='del2d')\n",
    "                            if nb_of_points_delaunay==1:\n",
    "                                \n",
    "                                inserted_points_delaunay.append(point_coords_delaunay)\n",
    "                                print(\"Inserting pertubed_contour\",pertubed_contour)\n",
    "                                pertubed_contours.append(pertubed_contour)\n",
    "                                edge_lengths.append(edge_length)  \n",
    "                                #print(\"List includes\",pertubed_contours)\n",
    "                                #plot_contour(pertubed_contour)\n",
    "                                #plt.scatter(point_coords_delaunay[:,0],point_coords_delaunay[:,1])\n",
    "\n",
    "    return    pertubed_contours,edge_lengths,inserted_points_delaunay\n",
    "\n",
    "\n",
    "# Given a neural network and a test set for this generate more polygons pertubing those that had an error more than tol\n",
    "# and for a specific target edge length\n",
    "def generate_data_for_target_edge_length(net,x_test,tol,target_edge_length):\n",
    "    net=net.cpu()\n",
    "    x_test=x_test.cpu()\n",
    "    net.eval()\n",
    "    predictions=net(x_test).data.numpy()\n",
    "    target_edge_lengths=x_test[:,16]\n",
    "    contours=x_test.data.numpy()\n",
    "    nb_sampling=10\n",
    "    \n",
    "    pertubed_contours=[]\n",
    "    edge_lengths=[]\n",
    "    inserted_points_delaunay=[]\n",
    "    set_of_pertubed_contours=set()\n",
    "    \n",
    "    \n",
    "    for index,prediction in enumerate(predictions):\n",
    "            contour=contours[index]\n",
    "            contour=np.delete(contour,16)\n",
    "            contour=contour.reshape(8,2)\n",
    "            _,real_value=get_extrapoints_target_length_jupyter(contour,target_edge_lengths[index],algorithm='del2d')\n",
    "            error=np.linalg.norm(prediction-np.array(real_value))\n",
    "\n",
    "            if error > tol and hash(tuple(contour.reshape(16))) not in set_of_pertubed_contours:\n",
    "                print(\"Pertubing contour, \" ,contour)\n",
    "                set_of_pertubed_contours.add(hash(tuple(contour.reshape(16))))\n",
    "                print(set_of_pertubed_contours)\n",
    "                for j in range(8):\n",
    "                    for i in range(0,nb_of_sampling):\n",
    "                        pertubed_contour=contour.copy()\n",
    "                        pertubed_contour[j]=np.random.normal(contour[j],0.1)\n",
    "                        print(contour[j],\"changed to \",pertubed_contour[j])\n",
    "                        print(pertubed_contour,contour)\n",
    "                        \n",
    "                        print(\"Examining target edge length \",edge_length,\"for \",pertubed_contour)\n",
    "                        nb_of_points_delaunay,point_coords_delaunay=get_extrapoints_target_length_jupyter(pertubed_contour,target_edge_length,algorithm='del2d')\n",
    "                        if nb_of_points_delaunay==1:\n",
    "                                \n",
    "                            inserted_points_delaunay.append(point_coords_delaunay)\n",
    "                            print(\"Inserting pertubed_contour\",pertubed_contour)\n",
    "                            pertubed_contours.append(pertubed_contour)\n",
    "                            edge_lengths.append(target_edge_length)  \n",
    "                                #print(\"List includes\",pertubed_contours)\n",
    "                                #plot_contour(pertubed_contour)\n",
    "                                #plt.scatter(point_coords_delaunay[:,0],point_coords_delaunay[:,1])\n",
    "\n",
    "    return    pertubed_contours,edge_lengths,inserted_points_delaunay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the data of polygons where a specific number of points was inserted\n",
    "\n",
    "def reshape_data(polygons,coordinates,numb_of_ins_points,nb_of_points):\n",
    "    polygons=np.array([i for i in polygons for j in range(10)])\n",
    "    polygons_reshaped=[]\n",
    "    for polygon in polygons:\n",
    "        polygons_reshaped.append(polygon.reshape(2,polygons.shape[1]))\n",
    "\n",
    "    polygons_reshaped=np.array(polygons_reshaped)\n",
    "    set_of_points=get_set_nb_of_points(coordinates)        \n",
    "    indices=get_indices_nb_of_points(set_of_points,nb_of_points,coordinates)\n",
    "    indices=np.asarray(indices)\n",
    "    number_of_insertion_points=np.array(numb_of_ins_points)\n",
    "    polygons_reshaped.resize(len(coordinates),2*polygons.shape[1])\n",
    "\n",
    "    polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "    coordinates=[ coordinates[i][0]for i in indices]\n",
    "    coordinates=np.array(coordinates)\n",
    "    coordinates=coordinates.reshape(polygons_reshaped.shape[0],1,2*nb_of_points)\n",
    "    return polygons_reshaped,coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a prediction of a random contour based on a trained network (FFN)\n",
    "\n",
    "def plot_random_prediction_ffn(polygon_edges,net,number_of_points):\n",
    "    plt.clf()\n",
    "    for i in range(1000):\n",
    "        random_contour=apply_procrustes(generate_contour(polygon_edges))\n",
    "        random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,0.8,algorithm='del2d')\n",
    "        if random_nb_of_points==number_of_points:\n",
    "            break\n",
    "    random_contour_reshaped=random_contour.reshape(1,2*polygon_edges)\n",
    "    random_contour_with_target=np.hstack([random_contour_reshaped,[[1]]])\n",
    "\n",
    "\n",
    "\n",
    "    plot_contour(random_contour)\n",
    "    random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "    random_point_coordinated_delaunay.reshape(number_of_points,2)\n",
    "    plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "    random_x_variable=random_x_variable.expand(1000,2*polygon_edges+1).type(torch.FloatTensor)\n",
    "    net=net.cpu()\n",
    "    random_prediction=net(random_x_variable)\n",
    "    random_prediction=random_prediction.data[0].numpy()\n",
    "    random_prediction=random_prediction.reshape(number_of_points,2)\n",
    "    plt.scatter(random_prediction[:,0],random_prediction[:,1],label=' fnn prediction')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "# Plot a prediction of a random contour based on a trained network (CNN)\n",
    "\n",
    "def plot_random_prediction_cnn(polygon_edges,net,number_of_points):\n",
    "    plt.clf()\n",
    "    for i in range(1000):\n",
    "        random_contour=apply_procrustes(generate_contour(polygon_edges))\n",
    "        random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,0.8,algorithm='del2d')\n",
    "        if random_nb_of_points==number_of_points:\n",
    "            break\n",
    "    random_contour_reshaped=random_contour.reshape(1,2*polygon_edges)\n",
    "    random_contour_with_target=np.hstack([random_contour_reshaped,[[1]]])\n",
    "    triangles_with_edges=extract_lengths_angles_in_triangle_form(random_contour_with_target,polygon_edges)\n",
    "\n",
    "\n",
    "\n",
    "    plot_contour(random_contour)\n",
    "    random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "    random_point_coordinated_delaunay.reshape(number_of_points,2)\n",
    "    plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random_x_variable=Variable(torch.from_numpy(triangles_with_edges))\n",
    "    print(random_x_variable)\n",
    "    random_x_variable=random_x_variable.expand(1000,1,3*polygon_edges+1).type(torch.FloatTensor)\n",
    "    net=net.cpu()\n",
    "    random_prediction=net(random_x_variable)\n",
    "    random_prediction=random_prediction.data[0].numpy()\n",
    "    random_prediction=random_prediction.reshape(number_of_points,2)\n",
    "    plt.scatter(random_prediction[:,0],random_prediction[:,1],label='cnn prediction')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net,epochs,optimization,loss_function,variable_x,variable_x_test,variable_y,variable_y_test):\n",
    "    batch_size=int(variable_x.size()[0]/149 )\n",
    "    net=net.cuda()\n",
    "\n",
    "    # Train the network #\n",
    "    \n",
    "    net.train()\n",
    "    for t in range(nb_of_epochs):\n",
    "        sum_loss=0\n",
    "        for b in range(0,variable_x.size(0),batch_size):\n",
    "            out = net(variable_x.narrow(0,b,batch_size)) # input x and predict based on x\n",
    "            loss=loss_function(out, variable_y.narrow(0,b,batch_size).resize(batch_size,nb_of_points*1)).sum()\n",
    "            sum_loss+=loss.data[0]       \n",
    "            optimization.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimization.step()        # apply gradients\n",
    "            \n",
    "        if t%10==0: \n",
    "            net.eval()\n",
    "            out_test=net(variable_x_test)  \n",
    "            test_loss=loss_function(out_test, variable_y_test.resize(len(variable_y_test),2)).sum()\n",
    "            print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(variable_x.size(0)),\" Test loss: \",test_loss.data[0]/(variable_x_test.size(0)))\n",
    "            net.train()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "\n",
    "# Precision error for two points\n",
    "def precision_error(real_points,predictions):\n",
    "    distances=np.empty([2,2])\n",
    "    for point_index,point in enumerate(real_points):\n",
    "        for prediction_index,prediction in enumerate(predictions):\n",
    "               distances[point_index][prediction_index]=np.linalg.norm(point-prediction)\n",
    "    \n",
    "    max_distance,prediction_index,point_index=max_element(distances)\n",
    "    distance1=distances[prediction_index][(point_index+1)%2]\n",
    "    distance2=distances[(prediction_index+1)%2][point_index]\n",
    "    return max(distance2,distance1)\n",
    "\n",
    "\n",
    "\n",
    "# Precision error for multiple points\n",
    "\n",
    "def regression_error(real_points,prediction_points,nb_of_interior_points):\n",
    "    \n",
    "    real_point_polygon=sort_points(real_points.reshape(1,1,nb_of_interior_points,2),nb_of_interior_points).reshape(nb_of_interior_points,2)\n",
    "    prediction_points_polygon=sort_points(prediction_points.reshape(1,1,nb_of_interior_points,2),nb_of_interior_points).reshape(nb_of_interior_points,2)\n",
    "    procrusted_prediction_points_polygon=apply_procrustes(prediction_points_polygon,real_point_polygon)\n",
    "    \n",
    "    error_in_transformation=np.array([np.linalg.norm(prediction_points_polygon[indices]-procrusted_prediction_points_polygon[indices]) for indices in range(len(prediction_points_polygon))])\n",
    "\n",
    "    maximum_index,maximum_distance=np.argmax([np.linalg.norm(real_point_polygon[index]-procrusted_prediction_points_polygon[index]) for index in range(len(real_point_polygon))]),np.max([np.linalg.norm(real_point_polygon[index]-procrusted_prediction_points_polygon[index]) for index in range(len(real_point_polygon))])\n",
    "    regression_error=maximum_distance+error_in_transformation[maximum_index]\n",
    "    \n",
    "    return regression_error    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_initial=load_dataset('6_polygons.pkl')\n",
    "point_coordinates_initial=load_dataset('6_point_coordinates_del.pkl')\n",
    "number_of_insertion_points=load_dataset('6_nb_of_points_del.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_points=3\n",
    "polygons_reshaped,point_coordinates=reshape_data(polygons_initial,point_coordinates_initial,number_of_insertion_points,nb_of_points)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The data of 8 polygons that includes that pertubations ont he polygons and the set of new data\n",
    "\n",
    "polygons_reshaped,point_coordinates=load_dataset('8_additional_polygons_target_edge_length_additional_one_point.pkl'),load_dataset('8_additional_point_coordinates.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# adding additional data to the two point dataset\n",
    "\n",
    "additional_polygons_with_target_edge_length=load_dataset('8_polygons_with_target_edgelengths_2_points.pkl')\n",
    "polygons_reshaped=np.vstack([polygons_reshaped,additional_polygons_with_target_edge_length])\n",
    "additional_point_coordinates=load_dataset('8_polygons_other_coordinates_2_points.pkl')\n",
    "point_coordinates=np.vstack([point_coordinates,additional_point_coordinates.reshape(len(additional_point_coordinates),1,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27765, 13), (27765, 1, 6))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped.shape,point_coordinates.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unison_shuffled_copies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-261ba4c0a7c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Shuffle the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munison_shuffled_copies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unison_shuffled_copies' is not defined"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "\n",
    "polygons_reshaped,point_coordinates=unison_shuffled_copies(polygons_reshaped,point_coordinates)\n",
    "point_coordinates=sort_points(point_coordinates,nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 training/test data ratio\n",
    "\n",
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n",
    "nb_of_test_data,nb_of_training_data\n",
    "point_coordinates=point_coordinates.reshape(len(point_coordinates),1,2*nb_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mid_points_with_directions,midpoints,directions=extract_midpoint_directions(point_coordinates)\n",
    "#mid_points_with_directions=mid_points_with_directions.reshape(len(mid_points_with_directions),1,4)\n",
    "#midpoints=midpoints.reshape(len(midpoints),1,2)\n",
    "#directions=directions.reshape(len(directions),1,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_tensor=torch.from_numpy(polygons_reshaped[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#mixed_tensor=torch.cat((lengths_tensor[:nb_of_training_data],angles_tensor[:nb_of_training_data]),1)\n",
    "#double_mixed_tensor=angles_tensor[:nb_of_training_data]\n",
    "\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "#mixed_tensor_test=torch.cat((lengths_tensor[nb_of_training_data:],angles_tensor[nb_of_training_data:]),1)\n",
    "#double_mixed_tensor_test=angles_tensor[nb_of_training_data:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(mixed_tensor),Variable(mixed_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(double_mixed_tensor),Variable(double_mixed_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing with different networks\n",
    "\n",
    "#y_tensor_midpoints=torch.from_numpy(midpoints[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#y_tensor_midpoints_test=torch.from_numpy(midpoints[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "#y_tensor_directions=torch.from_numpy(directions[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#y_tensor_directions_test=torch.from_numpy(directions[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "#y_variable_midpoints,y_variable_midpoints_test=Variable(y_tensor_midpoints),Variable(y_tensor_midpoints_test)\n",
    "#y_variable_directions,y_variable_directions_test=Variable(y_tensor_directions),Variable(y_tensor_directions_test)\n",
    "\n",
    "\n",
    "#shuffle=torch.randperm(x_variable.shape[0])\n",
    "#x_variable = x_variable[shuffle]\n",
    "#y_variable=y_variable[shuffle]\n",
    "\n",
    "#print(y_variable_midpoints.size(),x_variable.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-478c02e4cee9>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-478c02e4cee9>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    lengths,counts=np.unique(x_numpy[:,2*nb_of_edges+],return_counts=True)\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Plotting the number of polygons in reference with the target edge length\n",
    "\n",
    "x_variable=x_variable.cpu()\n",
    "x_numpy=x_variable.data.numpy()\n",
    "lengths,counts=np.unique(x_numpy[:,2*nb_of_edges+],return_counts=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,23))\n",
    "plt.xticks(lengths)\n",
    "plt.yticks(counts,rotation=23,fontsize=6)\n",
    "plt.xlabel('Target Edge Lengths')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Number of polygon and the correspoding target edge length for one point insertion')\n",
    "\n",
    "plt.bar(list(lengths), list(counts),  0.05,color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 13 6\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(x_variable.size()[1],y_variable.size()[2],nb_of_hidden_layers=2, nb_of_hidden_nodes=45,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable.size()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0.5)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-4,weight_decay=.5,momentum=0.9)\n",
    "#max_distance=0.6108970818704328\n",
    "#loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myOtherLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if  torch.cuda.is_available():\n",
    "    #loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "    #x_variable_test,y_variable_test= Variable(double_mixed_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of range for dimension 1 (of size 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6d1b2d19ed54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#target_edge_lengths=x_variable.narrow(0,b,batch_size).narrow(1,24,1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_torch_loss_function3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#loss1=my_torch_loss_function2(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2),x_variable.narrow(0,b,batch_size)[:,16]).sum()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mIndexSelect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m# else fall through and raise an error in Index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\_functions\\tensor.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, i, index)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_shared_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 24 is out of range for dimension 1 (of size 9)"
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0])\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss1=0\n",
    "    sum_loss2=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        #loss2= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "        #target_edge_lengths=x_variable.narrow(0,b,batch_size).narrow(1,24,1)\n",
    "        loss1=my_torch_loss_function3(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2),x_variable.narrow(0,b,batch_size)[:,24],.3).sum().cuda()\n",
    "        \n",
    "        #loss1=my_torch_loss_function2(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2),x_variable.narrow(0,b,batch_size)[:,16]).sum()\n",
    "        sum_loss1+=loss1.data[0]\n",
    "        #sum_loss2+=loss2.data[0]\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss1.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss1.data[0],loss2.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "        del loss1\n",
    "        del out\n",
    "    if t%10==0: \n",
    "        my_net.eval()\n",
    "        out_test=my_net(x_variable_test)   \n",
    "        \n",
    "        #test_loss=my_torch_loss_function3(out_test, y_variable_test.resize(len(y_variable_test),nb_of_points*2)).sum()\n",
    "\n",
    "        test_loss=my_torch_loss_function3(out_test, y_variable_test.resize(y_variable_test.size()[0],nb_of_points*2),x_variable_test[:,24],.3).sum().cuda()\n",
    "    \n",
    "        #test_loss=loss_func(out_test,y_variable_test)\n",
    "        #my_net.train()\n",
    "       # print(\"Epoch:\",t,\"Training Loss:\",sum_loss2/(x_variable.size(0)),\"Test Loss:\",test_loss.data[0]/x_variable_test.size(0))\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss1/(x_variable.size(0)),test_loss.data[0]/(x_variable_test.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_midpoint=Net(x_variable.size()[1],y_variable_midpoints.size()[2],nb_of_hidden_layers=3, nb_of_hidden_nodes=25,batch_normalization=True)\n",
    "my_net_directions=Net(x_variable.size()[1],y_variable_directions.size()[2],nb_of_hidden_layers=3, nb_of_hidden_nodes=25,batch_normalization=True)\n",
    "optimizer_midpoints = torch.optim.Adam(my_net_midpoint.parameters(), lr=1e-4,weight_decay=0.5)\n",
    "optimizer_directions = torch.optim.Adam(my_net_directions.parameters(), lr=1e-4,weight_decay=0.5)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    #loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable_midpoints,y_variable_directions=x_variable.cuda(), y_variable_midpoints.cuda(),y_variable_directions.cuda()\n",
    "    x_variable_test,y_variable_midpoints_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_midpoints_test.cuda(),volatile=True)\n",
    "    y_variable_directions_test=Variable(y_tensor_directions_test.cuda(),volatile=True)\n",
    "    #x_variable_test,y_variable_test= Variable(double_mixed_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(net=my_net_midpoint,loss_function=my_torch_loss_function,optimization=optimizer_midpoints,epochs=13000,variable_x=x_variable,variable_x_test=x_variable_test,\n",
    "              variable_y=y_variable_midpoints,variable_y_test=y_variable_midpoints_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_network(net=my_net_directions,loss_function=my_torch_loss_function,optimization=optimizer_directions,epochs=13000,variable_x=x_variable,variable_x_test=x_variable_test,\n",
    "                  variable_y=y_variable_directions,variable_y_test=y_variable_directions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of error on the test data (not including random contours)\n",
    "plt.clf()\n",
    "sample_index=77\n",
    "sample_contour=np.delete(polygons_reshaped[nb_of_training_data+sample_index],24)\n",
    "#sample_contour=polygons_reshaped[sample_index]\n",
    "sample_contour=sample_contour.reshape(12,2)\n",
    "real_values=y_variable_test.cpu()\n",
    "real_values=real_values.data.numpy()\n",
    "inserted_point_coordinates=real_values[sample_index].reshape(nb_of_points,2)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "\n",
    "my_net.eval()\n",
    "x_variable_test=x_variable_test.cuda()\n",
    "predictions=my_net(x_variable_test).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='FNN predicted point')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Point location')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "\n",
    "plot_contour(sample_contour)\n",
    "plt.legend()\n",
    "plt.axis('scaled')\n",
    "print('distance between points:',np.linalg.norm(predicted_inserted_points-inserted_point_coordinates))\n",
    "#original_points=point_coordinates.reshape(218722,2)\n",
    "#plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n",
    "#plt.scatter(prediction[0],prediction[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the random contour using the FNN network  by predictiong both the midpoint and the direction#\n",
    "plt.clf()\n",
    "for i in range(1000):\n",
    "    random_contour=apply_procrustes(generate_contour(12))\n",
    "    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,1,algorithm='del2d')\n",
    "    if random_nb_of_points==nb_of_points:\n",
    "            break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*12)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[1]]])\n",
    "\n",
    "\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "random_point_coordinated_delaunay.reshape(nb_of_points,2)\n",
    "#mid_point_delaunay=(random_point_coordinated_delaunay[0]+random_point_coordinated_delaunay[1])/2\n",
    "#plt.scatter(mid_point_delaunay[0],mid_point_delaunay[1])\n",
    "plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location',marker='x')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "random_x_variable=random_x_variable.expand(1000,2*12+1).type(torch.FloatTensor)\n",
    "my_net=my_net.cpu()\n",
    "my_net.eval()\n",
    "random_prediction=my_net(random_x_variable)\n",
    "random_prediction=random_prediction.data[0].numpy()\n",
    "#middle_point,direction=np.array(random_prediction[:2]),np.array(random_prediction[2:])\n",
    "#random_prediction=random_prediction.reshape(2,2)\n",
    "points=random_prediction.reshape(nb_of_points,2)\n",
    "#plt.scatter(middle_point[0],middle_point[1])\n",
    "plt.scatter(points[:,0],points[:,1],label=' Prediction')\n",
    "#plt.scatter(points[0,0]-0.05,points[0,1]+0.03,c='r',label=' CNN prediction')\n",
    "#plt.scatter(points[1,0]-0.03,points[1,1]+0.01,c='r')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "#print(\" Precision error: \",precision_error(random_point_coordinated_delaunay,points))\n",
    "regression_error(random_point_coordinated_delaunay,points,nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a statistic on the regression loss\n",
    "\n",
    "regression_errors=[]\n",
    "for k in range(100):\n",
    "    for i in range(1000):\n",
    "        random_contour=apply_procrustes(generate_contour(12))\n",
    "        random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,1,algorithm='del2d')\n",
    "        if random_nb_of_points==nb_of_points:\n",
    "                break\n",
    "    random_contour_reshaped=random_contour.reshape(1,2*12)\n",
    "    random_contour_with_target=np.hstack([random_contour_reshaped,[[1]]])\n",
    "    random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "    random_point_coordinated_delaunay.reshape(nb_of_points,2)\n",
    "    \n",
    "    random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "    random_x_variable=random_x_variable.expand(1000,2*12+1).type(torch.FloatTensor)\n",
    "    my_net=my_net.cpu()\n",
    "    my_net.eval()\n",
    "    random_prediction=my_net(random_x_variable)\n",
    "    random_prediction=random_prediction.data[0].numpy()\n",
    "    points=random_prediction.reshape(nb_of_points,2)\n",
    "    error=regression_error(random_point_coordinated_delaunay,points,nb_of_points)\n",
    "    regression_errors.append(error)\n",
    "\n",
    "\n",
    "regression_errors=np.array(regression_errors)\n",
    "regression_errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting random contour using 2 seperate jnn\\s for the midpoint location and the direction  seperately#\n",
    "\n",
    "#plt.clf()\n",
    "#for i in range(1000):\n",
    "#    random_contour=apply_procrustes(generate_contour(8))\n",
    "#    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,0.8,algorithm='del2d')\n",
    "#    if random_nb_of_points==2:\n",
    "#           break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*8)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[1]]])\n",
    "\n",
    "\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "#random_point_coordinated_delaunay.reshape(2,2)\n",
    "#mid_point_delaunay=(random_point_coordinated_delaunay[0]+random_point_coordinated_delaunay[1])/2\n",
    "plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Real point location',marker='x')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_x_variable=Variable(torch.from_numpy(random_contour_with_target))\n",
    "random_x_variable=random_x_variable.expand(1000,2*8+1).type(torch.FloatTensor)\n",
    "my_net_midpoint,my_net_directions=my_net_midpoint.cpu(),my_net_directions.cpu()\n",
    "my_net_midpoint.eval(),my_net_directions.eval()\n",
    "\n",
    "random_midpoint_prediction=my_net_midpoint(random_x_variable)\n",
    "random_midpoint_prediction=random_midpoint_prediction.data[0].numpy()\n",
    "\n",
    "\n",
    "random_direction_prediction=my_net_directions(random_x_variable)\n",
    "random_direction_prediction=random_direction_prediction.data[0].numpy()\n",
    "\n",
    "\n",
    "points=np.array(get_points(random_midpoint_prediction,random_direction_prediction))\n",
    "print(points[:,0]) \n",
    "plt.scatter(points[:,0],points[:,1],label='2 NN s prediction')\n",
    "#plt.scatter(points[:,0],points[:,1],label='Prediction')\n",
    "plt.legend()\n",
    "print(\" Precision error: \",precision_error(random_point_coordinated_delaunay,points))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_random_prediction_ffn(polygon_edges=8,net=my_net,number_of_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_contours=[]\n",
    "target_edge_length=[]\n",
    "random_point_coordinates=[]\n",
    "for i in np.linspace(0.1,1,10):\n",
    "    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,i,algorithm='del2d')\n",
    "    if random_nb_of_points==2:\n",
    "            random_point_coordinates.append(random_point_coordinated_delaunay)\n",
    "            random_contours.append(random_contour.reshape(1*16))\n",
    "            target_edge_length.append(i)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_point_coordinates=np.array(random_point_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_point_coordinates=random_point_coordinates.reshape(len(random_point_coordinates),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_target_edge_length=[]\n",
    "for index,contour in enumerate(random_contours):\n",
    "    contour_with_target_edge_length.append(np.append(contour,target_edge_length[index]))\n",
    "contour_with_target_edge_length=np.array(contour_with_target_edge_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_target_edge_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=np.vstack([polygons_reshaped,contour_with_target_edge_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=np.vstack([point_coordinates,random_point_coordinates])\n",
    "point_coordinates.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                # Plotting the convex hull of inserted points #\n",
    "\n",
    "#plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points')\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "original_points=point_coordinates.reshape(len(polygons_reshaped),2)\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for index,simplex in hull.simplices:\n",
    "    if index==0:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'r--',label='Convex hull centers of mass')\n",
    "    else:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'r--')\n",
    "    \n",
    "for index,simplex in enumerate(hull2.simplices):\n",
    "    if index==0:\n",
    "        pass\n",
    " #       plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'b--',label='Convex hull insertion points')\n",
    "    else:\n",
    "        print(gaga)\n",
    "  #      plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'b--')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "#for i,point_i in enumerate(convex_points):\n",
    "#    for j,point_j in enumerate(convex_points):\n",
    "#        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1],'--',label='maximum distance of convex hull of center of masses')\n",
    "#plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1],'--',label='maximum distance of convex hull of insertion points')\n",
    "sqrt( 0.045761923559121974)*100/ distances2[3][11]\n",
    "sqrt(0.05281884602592863)*100/ distances2[3][11]\n",
    "plt.legend(handles=[masses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_triangle=extract_lengths_angles_in_triangle_form(polygons_reshaped,12)\n",
    "#data,lengths,angles=extract_lengths_angles(polygons_reshaped,12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_tensor_conv=torch.from_numpy(data_triangle[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#mixed_tensor=torch.cat((lengths_tensor[:nb_of_training_data],angles_tensor[:nb_of_training_data]),1)\n",
    "#double_mixed_tensor=angles_tensor[:nb_of_training_data]\n",
    "\n",
    "x_tensor_test_conv=torch.from_numpy(data_triangle[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "#mixed_tensor_test=torch.cat((lengths_tensor[nb_of_training_data:],angles_tensor[nb_of_training_data:]),1)\n",
    "#double_mixed_tensor_test=angles_tensor[nb_of_training_data:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_variable_conv,x_variable_test_conv=Variable(x_tensor_conv),Variable(x_tensor_test_conv)\n",
    "#x_variable,x_variable_test=Variable(mixed_tensor),Variable(mixed_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(double_mixed_tensor),Variable(double_mixed_tensor_test)\n",
    "\n",
    "#y_tensor=torch.from_numpy(mid_points_with_directions[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#y_tensor_test=torch.from_numpy(mid_points_with_directions[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "#y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "#shuffle=torch.randperm(x_variable.shape[0])\n",
    "#x_variable_conv = x_variable_conv[shuffle]\n",
    "#y_variable=y_variable[shuffle]\n",
    "#print(y_variable.size(),x_variable.size())\n",
    "#distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "#distances=np.array(distances)\n",
    "#100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape variables for convolutional neural net\n",
    "\n",
    "\n",
    "x_variable_conv,x_variable_test_conv=x_variable_conv.resize(x_variable_conv.size()[0],1,x_variable_conv.size()[1]),Variable(x_tensor_test_conv.view(x_variable_test_conv.size()[0],1,x_variable_test_conv.size()[1]),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#my_conv_net=my(nb_of_filters=10,out_dimension=2*nb_of_points,nb_of_hidden_nodes=15)\n",
    "my_conv_net=triangle_convoluting_net(nb_of_filters=20,nb_of_hidden_nodes=63,out_dimension=2*nb_of_points,nb_of_edges=12)\n",
    "print(my_conv_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-4,weight_decay=0.5)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable_conv , y_variable,x_variable_test_conv,y_variable_test= x_variable_conv.cuda(), y_variable.cuda(),x_variable_test_conv.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=int(x_variable_conv.size()[0])\n",
    "nb_of_epochs=33000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable_conv.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable_conv.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        \n",
    "        \n",
    "       # target_edge_lengths=x_variable_conv.narrow(0,b,batch_size).resize(len(x_variable_conv.narrow(0,b,batch_size)),25)[:,24]\n",
    "        \n",
    "        \n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        \n",
    "        #loss1=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2)).sum()\n",
    "       # loss1=my_torch_loss_function3(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2),x_variable_conv.narrow(0,b,batch_size)[:,24],0.6).sum().cuda()\n",
    "\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        out_test_conv=my_conv_net(x_variable_test_conv)\n",
    "        #target_edge_lengths_test=x_variable_test_conv.resize(len(x_variable_test_conv),25)[:,24]\n",
    "        test_loss=loss_func(out_test_conv,y_variable_test).data[0]\n",
    "        #test_loss=my_torch_loss_function(my_conv_net(x_variable_test_conv),y_variable_test.resize(len(y_variable_test),2*nb_of_points)).sum().data[0]\n",
    "        #test_loss=my_torch_loss_function3(out_test_conv, y_variable_test.resize(y_variable_test.size()[0],nb_of_points*2),x_variable_test[:,24],0.6).sum().cuda()\n",
    "\n",
    "        #test_loss=my_torch_loss_function2(my_conv_net(x_variable_test_conv),y_variable_test.resize(len(y_variable_test),2*nb_of_points),target_edge_lengths_test).sum().data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable_conv.size(0),\"Test Loss:\",test_loss/x_variable_test_conv.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sample_index=85\n",
    "\n",
    "contour1=np.delete(polygons_reshaped[nb_of_training_data+sample_index],2*12)\n",
    "contour1=contour1.reshape(12,2)\n",
    "#nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length_jupyter(contour1,.3,algorithm='del2d'\n",
    "#                                                                              )\n",
    "real_values=y_variable_test.cpu()\n",
    "real_values=real_values.data.numpy()\n",
    "inserted_point_coordinates=real_values[sample_index].reshape(12,2)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable_test_conv).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(12,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='CNN predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Point location')\n",
    "plot_contour(contour1)\n",
    "plt.legend()\n",
    "plt.axis('scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable_conv[1].narrow(0,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_prediction_cnn(8,my_conv_net,1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_variable_conv=x_variable_conv.cpu()\n",
    "my_conv_net=my_conv_net.cpu()\n",
    "x_variable_test_conv=x_variable_test_conv.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a prediction of a random contour based on a trained network (CNN)\n",
    "#plt.clf()\n",
    "#for i in range(1000):\n",
    "#    random_contour=apply_procrustes(generate_contour(8))\n",
    "#    random_nb_of_points,random_point_coordinated_delaunay=get_extrapoints_target_length_jupyter(random_contour,0.8,algorithm='del2d')\n",
    "#    if random_nb_of_points==2:\n",
    "#            break\n",
    "random_contour_reshaped=random_contour.reshape(1,2*8)\n",
    "random_contour_with_target=np.hstack([random_contour_reshaped,[[.8]]])\n",
    "#triangles_with_edges=extract_lengths_angles_in_triangle_form(random_contour_with_target,8)\n",
    "lengths_angles,lengths,angles=extract_lengths_angles(random_contour_with_target,8)\n",
    "\n",
    "\n",
    "plot_contour(random_contour)\n",
    "random_point_coordinated_delaunay=np.array(random_point_coordinated_delaunay)\n",
    "random_point_coordinated_delaunay.reshape(2,2)\n",
    "plt.scatter(random_point_coordinated_delaunay[:,0],random_point_coordinated_delaunay[:,1],label='Point location')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_x_variable=Variable(torch.from_numpy(lengths_angles))\n",
    "print(random_x_variable)\n",
    "random_x_variable=random_x_variable.expand(1000,1,2*8+1).type(torch.FloatTensor)\n",
    "my_conv_net=my_conv_net.cpu()\n",
    "my_conv_net.eval()\n",
    "random_prediction=my_conv_net(random_x_variable).data[0].numpy()\n",
    "middle_point,direction=np.array(random_prediction[:2]),np.array(random_prediction[2:])\n",
    "#random_prediction=random_prediction.reshape(2,2)\n",
    "points=np.array(get_points(middle_point,direction))\n",
    "plt.scatter(points[:,0],points[:,1],label='cnn prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_index=150\n",
    "\n",
    "contour1=np.delete(polygons_reshaped[nb_of_training_data+sample_index],2*8)\n",
    "contour1=contour1.reshape(8,2)\n",
    "#nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length_jupyter(contour1,.3,algorithm='del2d'\n",
    "#                                                                              )\n",
    "real_values=y_variable_test.cpu()\n",
    "real_values=real_values.data.numpy()\n",
    "inserted_point_coordinates=real_values[sample_index].reshape(2,2)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable_test_conv).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(2,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='CNN predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Point location')\n",
    "plot_contour(contour1)\n",
    "plt.legend()\n",
    "plt.axis('scaled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_prediction_cnn(polygon_edges=8,net=my_conv_net,number_of_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubed_contours=np.array(pertubed_contours)\n",
    "pertubed_contours=pertubed_contours.reshape(len(pertubed_contours),8*2)\n",
    "edge_lengths=np.array(edge_lengths)\n",
    "edge_lengths=edge_lengths.reshape(len(edge_lengths),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubed_contours_with_edge_lengths=np.hstack([pertubed_contours,edge_lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubed_contours_with_edge_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a1,d1,g1=generate_data(my_net,x_variable_test,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_pertubed=np.array(a)\n",
    "contours_pertubed=contours_pertubed.reshape(len(contours_pertubed),16)\n",
    "edge_lengths=np.array(d)\n",
    "edge_lengths=edge_lengths.reshape(len(edge_lengths),1)\n",
    "\n",
    "contour_pertubed_with_edge_lengths=np.hstack([contours_pertubed,edge_lengths])\n",
    "point_coordinates_of_pertubed_contours=np.array(d)\n",
    "point_coordinates_of_pertubed_contouurs=point_coordinates_of_pertubed_contours.reshape(len(point_coordinates_of_pertubed_contours),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_pertubed1=np.array(a1)\n",
    "contours_pertubed1=contours_pertubed1.reshape(len(contours_pertubed1),16)\n",
    "edge_lengths1=np.array(d1)\n",
    "edge_lengths1=edge_lengths1.reshape(len(edge_lengths1),1)\n",
    "\n",
    "contour_pertubed_with_edge_lengths1=np.hstack([contours_pertubed1,edge_lengths1])\n",
    "point_coordinates_of_pertubed_contours1=np.array(g1)\n",
    "point_coordinates_of_pertubed_contours1=point_coordinates_of_pertubed_contours1.reshape(len(point_coordinates_of_pertubed_contours1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_con1,edgm1,sdf1=generate_data(my_net,x_variable_test,tol=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_pertubed3=np.array(pre_con1)\n",
    "contours_pertubed3=contours_pertubed3.reshape(len(contours_pertubed3),16)\n",
    "edge_lengths3=np.array(edgm1)\n",
    "edge_lengths3=edge_lengths3.reshape(len(edge_lengths3),1)\n",
    "\n",
    "contour_pertubed_with_edge_lengths3=np.hstack([contours_pertubed3,edge_lengths3])\n",
    "point_coordinates_of_pertubed_contours3=np.array(sdf1)\n",
    "point_coordinates_of_pertubed_contours3=point_coordinates_of_pertubed_contours3.reshape(len(point_coordinates_of_pertubed_contours3),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_pertubed_with_edge_lengths3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.unique(pre_con_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_contour_1,edge_lengths_1,points_coordinates_del_1=generate_data_for_target_edge_length(my_net,x_variable_test,0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_pertubed_1=np.array(perturbed_contour_1)\n",
    "contours_pertubed_1=contours_pertubed_1.reshape(len(contours_pertubed_1),16)\n",
    "edge_lengths_1=np.array(edge_lengths_1)\n",
    "edge_lengths_1=edge_lengths_1.reshape(len(edge_lengths_1),1)\n",
    "\n",
    "contour_pertubed_with_edge_lengths_1=np.hstack([contours_pertubed_1,edge_lengths_1])\n",
    "point_coordinates_of_pertubed_contours_1=np.array(points_coordinates_del_1)\n",
    "point_coordinates_of_pertubed_contours_1=point_coordinates_of_pertubed_contours_1.reshape(len(point_coordinates_of_pertubed_contours_1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_con1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for polygon in pre_con1[1:400]:\n",
    "    plot_contour(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_lengths_angles_in_triangle_form(random_contour.reshape(1,16),len(random_contour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3])\n",
    "# now create a subplot which represents the top plot of a grid\n",
    "# with 2 rows and 1 column. Since this subplot will overlap the\n",
    "# first, the plot (and its axes) previously created, will be removed\n",
    "plt.subplot(222)\n",
    "plt.plot(range(12))\n",
    "plt.subplot(222, facecolor='y') # creates 2nd subplot with yellow background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_variable_test=y_variable_test.cpu().resize(y_variable_test.size()[0],y_variable_test.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_contour(contour1)\n",
    "plt.scatter(predictions.data.numpy()[:,0],predictions.data.numpy()[:,1])\n",
    "plt.scatter(y_variable_test.data.numpy()[:,0],y_variable_test.data.numpy()[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_lengths_angles(random_contour_with_target,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_contour_with_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points=point_coordinates.reshape(len(point_coordinates),2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_points(mid_points[3],directions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points[13][0][0],points[13][0][1])\n",
    "plt.scatter(points[13][1][0],points[13][1][1])\n",
    "plt.scatter(mid_points[13][0],mid_points[13][1])\n",
    "plt.quiver(mid_points[13][0],mid_points[13][1],points[13][0][0],points[13][0][1],scale_units='xy')\n",
    "plt.axis('scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Idea for positioning index: For  a set of n interior point calculate the braycenter of the contour that is \n",
    "# formed by connnectiong the points.  For each edges of the interior point polygon connect it with the barycenter\n",
    "# to form a tricngle. Xw=( c-x_1,c-x_2) ->  F= XwXm*(-1) -> X=C+ FXm. F contains information about the rotation and trans-\n",
    "# ation. We have to keep in mind have fixed axis of the orthogonal eigenvectors.\n",
    "# If u_x <0 then u=-u and v=-v. If w_z<0 (3D)  tjem v=-v and w=-w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mid_points[13][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_midpoint_directions(point_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_point_with_direction=extract_midpoint_directions([point_coordinates[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_point=np.array(mid_point_with_direction[0])\n",
    "direction=np.array(mid_point_with_direction[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_points(mid_point,direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_point=mid_point_with_direction[0][:2]\n",
    "direction=mid_point_with_direction[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_points(mid_point,direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                # Project set of polygons in 2d and 3d space #\n",
    "                        # Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.7)\n",
    "pca.fit(point_coordinates_reshaped)\n",
    "point_coordinates_projected=pca.transform(point_coordinates_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(Polygons_projected[:,0],Polygons_projected[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates_reshaped= point_coordinates.reshape(len(point_coordinates),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_error(points,random_point_coordinated_delaunay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(points[0]- random_point_coordinated_delaunay[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0]\n",
    "random_point_coordinated_delaunay[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_element(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_edge_lengths=x_variable_test.data.numpy()[:,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_midpoints,real_directions=y_tensor_test.numpy().reshape(len(y_tensor_test),4)[:,0:2],y_tensor_test.numpy().reshape(len(y_tensor_test),4)[:,2:4]\n",
    "\n",
    "real_points=[]\n",
    "for index,_ in enumerate(real_directions):\n",
    "        real_points.append(get_points(real_midpoints[index],real_directions[index]))\n",
    "    \n",
    "\n",
    "x_variable_test=x_variable_test.cpu()\n",
    "my_net=my_net.cpu()\n",
    "predicted_midpoints,predicted_directions=my_net(x_variable_test).data.numpy()[:,0:2],my_net(x_variable_test).data.numpy()[:,2:4]\n",
    "\n",
    "predicted_points=[]\n",
    "for index,_ in enumerate(real_directions):\n",
    "            predicted_points.append(get_points(predicted_midpoints[index],predicted_directions[index]))\n",
    "\n",
    "precision_errors=np.empty(len(predicted_midpoints))\n",
    "real_points=np.array(real_points)\n",
    "predicted_points=np.array(predicted_points)\n",
    "for index,_ in enumerate(predicted_midpoints):\n",
    "    error=np.divide(precision_error(real_points[index],predicted_points[index]),test_target_edge_lengths[index])\n",
    "    #error=precision_error(real_points[index],predicted_points[index])\n",
    "    precision_errors[index]=error\n",
    "\n",
    "precision_errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_midpoint=my_net_midpoint.cpu()\n",
    "my_net_directions=my_net_directions.cpu()\n",
    "predicted_midpoints2=my_net_midpoint(x_variable_test).data.numpy()\n",
    "predicted_directions2=my_net_directions(x_variable_test).data.numpy()\n",
    "\n",
    "predicted_points2=[]\n",
    "for index,_ in enumerate(real_directions):\n",
    "            predicted_points2.append(get_points(predicted_midpoints2[index],predicted_directions2[index]))\n",
    "\n",
    "precision_errors=np.empty(len(predicted_midpoints))\n",
    "real_points=np.array(real_points)\n",
    "predicted_points=np.array(predicted_points)\n",
    "for index,_ in enumerate(predicted_midpoints):\n",
    "    error=np.divide(precision_error(real_points[index],predicted_points2[index]),test_target_edge_lengths[index])\n",
    "    #error=precision_error(real_points[index],predicted_points2[index])\n",
    "    precision_errors[index]=error\n",
    "\n",
    "precision_errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
