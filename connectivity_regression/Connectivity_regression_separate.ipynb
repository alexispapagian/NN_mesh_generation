{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Triangulation/')\n",
    "sys.path.insert(0, '../network_datasets')\n",
    "sys.path.insert(0, '../point_coordinates_regression/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "from Neural_network import *\n",
    "from functools import reduce\n",
    "import Triangulation_with_points\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from point_coordinates_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def triangulate_old_vers(polygon,ordered_quality_matrix,recursive=True):\n",
    "    set_edges=set(tuple(i) for i in get_contour_edges(polygon))\n",
    "    interior_edges=set()\n",
    "    set_elements=set()\n",
    "    set_locked_vertices=set()\n",
    "    set_forbidden_intersections=set()\n",
    "    \n",
    "    print(\"initial set edges:\", set_edges)\n",
    "    \n",
    "\n",
    "\n",
    "    for edge in ordered_quality_matrix.keys():\n",
    "        \n",
    "        found_in_interior_set,found_in_set,index=check_edge_validity(edge,polygon,set_edges,interior_edges)\n",
    "        \n",
    "        for qualities_with_edges in ordered_quality_matrix[edge][0]:\n",
    "            \n",
    "            element_created=False\n",
    "           \n",
    "            target_vtx=qualities_with_edges[1]\n",
    "            \n",
    "            if target_vtx==edge[0] or target_vtx==edge[1]:\n",
    "                continue\n",
    "           \n",
    "            print(\"Edge:\",edge,\"targeting:\",target_vtx)\n",
    "        \n",
    "            if found_in_interior_set:\n",
    "                element=(edge[0],edge[1],index)  \n",
    "                set_elements.add(element)\n",
    "                print(\"Element inserted:\",element)\n",
    "                continue\n",
    "        \n",
    "            if found_in_set and not found_in_interior_set:    \n",
    "                if(index != target_vtx):\n",
    "                    print('found',(edge[0],index),(edge[1],index),\"Canceling creation\")\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        \n",
    "            # Passed edges checking \n",
    "            # Proceed to check vertices\n",
    "            temp_element=(edge[0],edge[1],target_vtx)\n",
    "            print(temp_element)\n",
    "            existing_element=False\n",
    "            for element in set_elements:\n",
    "                if set(temp_element)== set(element):\n",
    "                    print(\"Element {} already in set\".format(element))\n",
    "                    existing_element=True\n",
    "                    break\n",
    "            if existing_element:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "            if target_vtx in set_locked_vertices:\n",
    "                print(\" Target vertex {} is locked\".format(target_vtx))\n",
    "                continue\n",
    "            set_elements.add(temp_element)\n",
    "\n",
    "            \n",
    "    \n",
    "            # Check if a locked vertex was created after the creation of the element\n",
    "            # If so, add it to the list\n",
    "            #Tracer()()\n",
    "            Found_locked_vertex=False\n",
    "            for vertex in temp_element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                    print(\"Vertex locked:\",vertex)\n",
    "                    Found_locked_vertex=True\n",
    "                    set_locked_vertices.add(vertex)\n",
    "            set_elements.remove(temp_element)\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Locking the vertices and checking if the connection is with a locked vertex has been checked/\n",
    "            # Proceeding to check if both internal edges intersect with other internal edges\n",
    "            internal_edge1=(edge[0],target_vtx)\n",
    "            internal_edge2=(edge[1],target_vtx)\n",
    "            set_a,set_b=get_intermediate_indices(target_vtx,polygon,edge[0],edge[1])\n",
    "        \n",
    "            internal_condition1= internal_edge1 in set_forbidden_intersections or tuple(reversed(internal_edge1)) in set_forbidden_intersections\n",
    "                                                                        \n",
    "            internal_condition2=internal_edge2 in set_forbidden_intersections or tuple(reversed(internal_edge2)) in set_forbidden_intersections\n",
    "                                                                            \n",
    "    \n",
    "                                                                                   \n",
    "            internal_intersection=False\n",
    "            if internal_condition1 or  internal_condition2:\n",
    "                print(\"edges :\",internal_edge1, \"and\",internal_edge2,\"intersecting\")\n",
    "                print(\"Abandoning creation of element\",temp_element)\n",
    "                internal_intersection=True\n",
    "        \n",
    "     \n",
    "            if internal_intersection:\n",
    "                for vtx in temp_element:\n",
    "                    if Found_locked_vertex and vtx in set_locked_vertices:\n",
    "                        print(\"Unlocking vertex\",vtx)\n",
    "                        set_locked_vertices.remove(vtx)                    \n",
    "                continue\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Create the element\n",
    "            element=temp_element\n",
    "        \n",
    "            triangle=polygon[np.asarray(element)]\n",
    "            \n",
    "            if compute_mean_quality_triangle(triangle,polygon)==0:\n",
    "                continue\n",
    "            \n",
    "            # Add to set of edges all the forbidden intersections after the creation of the element\n",
    "            \n",
    "            for i in set_a:\n",
    "                for j in set_b:\n",
    "                    set_forbidden_intersections.add((i,j))\n",
    "            #print(\"set of forbidden inter section edges updated:\",set_forbidden_intersections)\n",
    "    \n",
    "                \n",
    "        \n",
    "        \n",
    "        # New edges after creation of the element\n",
    "   \n",
    "            new_edge1=(edge[0],target_vtx)\n",
    "            new_edge2=(edge[1],target_vtx)\n",
    "        \n",
    "            if new_edge1 not in set_edges and tuple(reversed(new_edge1)) not in set_edges:\n",
    "                set_edges.add(new_edge1)\n",
    "                interior_edges.add(new_edge1)\n",
    "                print(\"edges inserted:\",new_edge1)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            if new_edge2 not in set_edges and tuple(reversed(new_edge2)) not in set_edges:    \n",
    "                set_edges.add(new_edge2)\n",
    "                interior_edges.add(new_edge2)\n",
    "                print(\"edges inserted:\",new_edge2)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "            # Checking list of elements to see whether the were created or were already there\n",
    "            \n",
    "            \n",
    "            set_elements.add(element)\n",
    "            element_created=True\n",
    "                \n",
    "            if element_created:\n",
    "                print(\"element inserted:\",element)\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    triangulated={'segment_markers': np.ones([polygon.shape[0]]), 'segments':np.array(get_contour_edges(polygon)), 'triangles': np.array(list( list(i) for i in set_elements)),\n",
    "                  'vertex_markers': np.ones([polygon.shape[0]]), 'vertices': polygon}\n",
    "    plot.plot(plt.axes(), **triangulated)\n",
    "    print(\"Final edges:\",set_edges)\n",
    "    print(\"Elements created:\",set_elements)\n",
    "    print(\"Set of locked vertices:\", set_locked_vertices)\n",
    "    \n",
    "    \n",
    "    # find open vertices\n",
    "    for element in set_elements:\n",
    "        for vertex in  element:\n",
    "                    _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                    if isclosed and vertex not in set_locked_vertices:\n",
    "                        print(\"Vertex locked:\",vertex)\n",
    "                        Found_locked_vertex=True\n",
    "                        set_locked_vertices.add(vertex)\n",
    "    set_open_vertices=set(range(len(polygon)))-set_locked_vertices\n",
    "    print(\"Set of open vertices:\", set_open_vertices)\n",
    "    set_edges.clear(),set_locked_vertices.clear(),set_forbidden_intersections.clear\n",
    "    if recursive:\n",
    "        sub_polygon_list=check_for_sub_polygon(set_open_vertices,interior_edges,set_elements,polygon)\n",
    "        for sub_polygon_indices in sub_polygon_list:\n",
    "            if len(sub_polygon_indices)>=3:\n",
    "                print(\"remeshing subpolygon\",sub_polygon_indices)\n",
    "                polygon_copy=polygon\n",
    "                sub_polygon=np.array(polygon_copy[sub_polygon_indices])\n",
    "                if not is_counterclockwise(sub_polygon):\n",
    "                    sub_polygon=np.array(polygon_copy[sub_polygon_indices[::-1]])\n",
    "\n",
    "                sub_quality,_=quality_matrix(sub_polygon,compute_minimum=True,normalize=False)\n",
    "                sub_order_matrix=order_quality_matrix(sub_quality,sub_polygon,check_for_equal=True)\n",
    "                print(sub_quality,sub_order_matrix)\n",
    "                triangulate(sub_polygon,sub_order_matrix,recursive=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size_factor(n,minimum,maximum):    \n",
    "    factor_set=set(reduce(list.__add__, \n",
    "                ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))\n",
    "    for factor in factor_set:\n",
    "        if factor>minimum and factor<maximum:\n",
    "            batch_size_factor=factor\n",
    "    return batch_size_factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "\n",
    "Polygons,quality_matrices=load_dataset('6_polygons.pkl'),load_dataset('6_polygons_qualities_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data are usually 20 percent of the whole\n",
    "\n",
    "nb_of_points=6\n",
    "nb_of_contours=int(Polygons.shape[0])\n",
    "\n",
    "nb_test_data=int(0.2*Polygons.shape[0])\n",
    "nb_training_data=int(Polygons.shape[0])-nb_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the polygon data with respect to NN\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nb_of_contours):\n",
    "    Polygons_reshaped.append(Polygons[i].reshape(2*nb_of_points))\n",
    "\n",
    "Polygons_reshaped=np.array(Polygons_reshaped) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_edges=6\n",
    "nb_of_points=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "filedirectory='..//polygon_datasets//'+str(int(nb_of_edges))+'_polygons_with_points'\n",
    "del_points=load_dataset(str(nb_of_edges)+'_point_coordinates_del.pkl')\n",
    "number_of_insertion_points=load_dataset(str(nb_of_edges)+'_nb_of_points_del.pkl')\n",
    "\n",
    "\n",
    "set_points=get_set_nb_of_points(del_points)\n",
    "\n",
    "for nb_of_inner_points in [4]:\n",
    "    if nb_of_inner_points==0:\n",
    "        continue\n",
    "\n",
    "    print(nb_of_inner_points)\n",
    "    polygons_filename=os.path.join(filedirectory, str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_polygons_with_points')\n",
    "    qualities_filename=os.path.join(filedirectory,str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_polygons_qualities_min.pkl')\n",
    "\n",
    "    with open (polygons_filename,'rb') as file:\n",
    "        polygons_with_points=pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "    with open (qualities_filename,'rb') as file:\n",
    "        quality_matrices=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (nb_of_edges==8 and nb_of_inner_points==1) or (nb_of_edges==10 and nb_of_inner_points==1) or (nb_of_edges==12 and nb_of_inner_points==4):\n",
    "        filename_polygons='../polygon_datasets/additional_polygon_datasets/'+str(nb_of_edges)+'_polygons/'+str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_polygons_with_points.pkl'\n",
    "        filename_min_qualities='../polygon_datasets/additional_polygon_datasets/'+str(nb_of_edges)+'_polygons/'+str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_min_qualities.pkl'\n",
    "     #   filename_polygons2='../polygon_datasets/additional_polygon_datasets/'+str(nb_of_edges)+'_polygons/'+str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_polygons_with_points_part_2.pkl'\n",
    "      #  filename_min_qualities2='../polygon_datasets/additional_polygon_datasets/'+str(nb_of_edges)+'_polygons/'+str(nb_of_edges)+'_'+str(nb_of_inner_points)+'_min_qualities_part_2.pkl'\n",
    "        with open(filename_polygons,'rb') as f:\n",
    "            additional_polygons_with_points=pickle.load(f)\n",
    "        with open(filename_min_qualities,'rb') as f:\n",
    "            additional_min_qualities=pickle.load(f)  \n",
    "      #  with open(filename_polygons2,'rb') as f:\n",
    "       #     additional_polygons_with_points2=pickle.load(f)\n",
    "        #with open(filename_min_qualities2,'rb') as f:\n",
    "         #   additional_min_qualities2=pickle.load(f)  \n",
    "        print(\"adding datasets\")    \n",
    "        polygons_with_points=np.vstack([polygons_with_points,additional_polygons_with_points])\n",
    "       # polygons_with_points=np.vstack([polygons_with_points,additional_polygons_with_points2])\n",
    "        quality_matrices=np.vstack([quality_matrices,additional_min_qualities])\n",
    "        #quality_matrices=np.vstack([quality_matrices,additional_min_qualities2])\n",
    "\n",
    "\n",
    "polygons=polygons_with_points[:,:2*nb_of_edges]\n",
    "inner_points=polygons_with_points[:,2*nb_of_edges:]\n",
    "\n",
    "# Testing data are usually 20 percent of the whole\n",
    "\n",
    "nb_test_data=int(0.1*polygons_with_points.shape[0])\n",
    "nb_training_data=int(polygons_with_points.shape[0])-nb_test_data\n",
    "\n",
    "quality_matrices=np.array(quality_matrices)\n",
    "#polygons_with_points,quality_matrices=unison_shuffled_copies(polygons_with_points,quality_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including edge label to polygon data\n",
    "\n",
    "Polygons_reshaped_with_edges_label=[]\n",
    "for polygon  in polygons_with_points:\n",
    "    for i in  range(int(nb_of_edges)):\n",
    "        Polygons_reshaped_with_edges_label.append(np.hstack([polygon,i]))\n",
    "Polygons_reshaped_with_edges_label=np.array(Polygons_reshaped_with_edges_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_matrices_reshaped=quality_matrices.reshape(len(quality_matrices)*quality_matrices.shape[1],quality_matrices.shape[2])\n",
    "quality_matrices_reshaped=quality_matrices.reshape(len(quality_matrices_reshaped),1,quality_matrices.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polygons_reshaped_with_edges_label,quality_matrices_reshaped=unison_shuffled_copies(Polygons_reshaped_with_edges_label,quality_matrices_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing data \n",
    "\n",
    "training_data = Polygons_reshaped_with_edges_label[:nb_training_data]\n",
    "testing_data  = Polygons_reshaped_with_edges_label[nb_training_data:]\n",
    "\n",
    "training_labels=quality_matrices_reshaped[:nb_training_data]\n",
    "testing_labels=quality_matrices_reshaped[nb_training_data:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tennsors\n",
    "\n",
    "x_tensor=torch.from_numpy(training_data).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(testing_data).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "y_tensor=torch.from_numpy(training_labels).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(testing_labels).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_variable=Variable(x_tensor)\n",
    "x_variable_test=Variable(x_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "y_variable=Variable(y_tensor)\n",
    "\n",
    "\n",
    "y_variable_test=Variable(y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net=Net(Polygons_reshaped_with_edges_label.shape[1],quality_matrices_reshaped.shape[2],nb_of_hidden_layers=2,\n",
    "           nb_of_hidden_nodes=30,batch_normalization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=.2)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    my_net.cuda()\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable = x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test , y_variable_test = x_variable_test.cuda(), y_variable_test.cuda()\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  3324\n",
      "Epoch: 0 Training Loss: 0.21761283048044186 Test Loss: 0.21283632399765717\n",
      "Epoch: 1 Training Loss: 0.2175788773418764 Test Loss: 0.2128426627873792\n",
      "Epoch: 2 Training Loss: 0.21758469219278057 Test Loss: 0.2130095972128199\n",
      "Epoch: 3 Training Loss: 0.21759710581939573 Test Loss: 0.21298297170343528\n",
      "Epoch: 4 Training Loss: 0.2176119539747743 Test Loss: 0.21303888008803892\n",
      "Epoch: 5 Training Loss: 0.2176309607286459 Test Loss: 0.21311788811126156\n",
      "Epoch: 6 Training Loss: 0.21764744867522506 Test Loss: 0.21319561282122756\n",
      "Epoch: 7 Training Loss: 0.21766453774289535 Test Loss: 0.21323255668770574\n",
      "Epoch: 8 Training Loss: 0.21768245549265083 Test Loss: 0.21349619330399808\n",
      "Epoch: 9 Training Loss: 0.2176993603220772 Test Loss: 0.21338693019854385\n",
      "Epoch: 10 Training Loss: 0.21771421409004718 Test Loss: 0.21352296140374455\n",
      "Epoch: 11 Training Loss: 0.21773343493827987 Test Loss: 0.2137443134963696\n",
      "Epoch: 12 Training Loss: 0.2177494344997492 Test Loss: 0.21351426339167198\n",
      "Epoch: 13 Training Loss: 0.21776628966760406 Test Loss: 0.21354420736765956\n",
      "Epoch: 14 Training Loss: 0.217785239918998 Test Loss: 0.21363606148620845\n",
      "Epoch: 15 Training Loss: 0.21780057758021154 Test Loss: 0.21371049365509184\n",
      "Epoch: 16 Training Loss: 0.2178148964484963 Test Loss: 0.21376076323902252\n",
      "Epoch: 17 Training Loss: 0.2178279038263142 Test Loss: 0.2137181935346315\n",
      "Epoch: 18 Training Loss: 0.21783720905456588 Test Loss: 0.21384447674418605\n",
      "Epoch: 19 Training Loss: 0.21784444953058313 Test Loss: 0.2139124534585059\n",
      "Epoch: 20 Training Loss: 0.21784649611810483 Test Loss: 0.21442838427594377\n",
      "Epoch: 21 Training Loss: 0.21783773364358 Test Loss: 0.21398103942255361\n",
      "Epoch: 22 Training Loss: 0.21781822077024426 Test Loss: 0.2140220017446838\n",
      "Epoch: 23 Training Loss: 0.21779945585988775 Test Loss: 0.21405828451039344\n",
      "Epoch: 24 Training Loss: 0.21778526147278351 Test Loss: 0.21410309677229347\n",
      "Epoch: 25 Training Loss: 0.2177763441140471 Test Loss: 0.21407695088503503\n",
      "Epoch: 26 Training Loss: 0.21777010997746107 Test Loss: 0.2141763493300016\n",
      "Epoch: 27 Training Loss: 0.21776220312725336 Test Loss: 0.2141483108797587\n",
      "Epoch: 28 Training Loss: 0.21775691874692873 Test Loss: 0.2141478960714333\n",
      "Epoch: 29 Training Loss: 0.21775129911737465 Test Loss: 0.2143734999493934\n",
      "Epoch: 30 Training Loss: 0.2177401814434933 Test Loss: 0.21431033241909578\n",
      "Epoch: 31 Training Loss: 0.21773630646914782 Test Loss: 0.21432466923184135\n",
      "Epoch: 32 Training Loss: 0.21773141495228962 Test Loss: 0.21462989038275193\n",
      "Epoch: 33 Training Loss: 0.21772855545352657 Test Loss: 0.2144309120141765\n",
      "Epoch: 34 Training Loss: 0.21771967204827838 Test Loss: 0.21441202527261202\n",
      "Epoch: 35 Training Loss: 0.2177104595443402 Test Loss: 0.21456423400250213\n",
      "Epoch: 36 Training Loss: 0.21769224828115868 Test Loss: 0.21449982204722842\n",
      "Epoch: 37 Training Loss: 0.21767156529010562 Test Loss: 0.21456393585901826\n",
      "Epoch: 38 Training Loss: 0.21764070854469614 Test Loss: 0.21454768055776788\n",
      "Epoch: 39 Training Loss: 0.21761031378011267 Test Loss: 0.21709128520893065\n",
      "Epoch: 40 Training Loss: 0.2175860539993224 Test Loss: 0.21470780953412047\n",
      "Epoch: 41 Training Loss: 0.2175637908127477 Test Loss: 0.21470452995579803\n",
      "Epoch: 42 Training Loss: 0.21754396911239796 Test Loss: 0.21475791060216895\n",
      "Epoch: 43 Training Loss: 0.21752379522642074 Test Loss: 0.21463559399722576\n",
      "Epoch: 44 Training Loss: 0.21750962369314314 Test Loss: 0.2148106431105315\n",
      "Epoch: 45 Training Loss: 0.21749458578555306 Test Loss: 0.21472974252432436\n",
      "Epoch: 46 Training Loss: 0.21748775727015826 Test Loss: 0.21460781480218621\n",
      "Epoch: 47 Training Loss: 0.21747861108624977 Test Loss: 0.21459000396971567\n",
      "Epoch: 48 Training Loss: 0.21747081461401765 Test Loss: 0.214622942343302\n",
      "Epoch: 49 Training Loss: 0.2174635394403507 Test Loss: 0.2145761467790963\n",
      "Epoch: 50 Training Loss: 0.21745570605591602 Test Loss: 0.21445310425958372\n",
      "Epoch: 51 Training Loss: 0.21744395607788783 Test Loss: 0.21565842058826457\n",
      "Epoch: 52 Training Loss: 0.21743599060927343 Test Loss: 0.2146864339426038\n",
      "Epoch: 53 Training Loss: 0.21742944344071274 Test Loss: 0.21463354588111924\n",
      "Epoch: 54 Training Loss: 0.2174234343636624 Test Loss: 0.21467341933139536\n",
      "Epoch: 55 Training Loss: 0.21742164256627236 Test Loss: 0.2146395994901176\n",
      "Epoch: 56 Training Loss: 0.21741955404569957 Test Loss: 0.21460425004314007\n",
      "Epoch: 57 Training Loss: 0.21741347489194893 Test Loss: 0.2145436491393557\n",
      "Epoch: 58 Training Loss: 0.21741175120774875 Test Loss: 0.21457735231579192\n",
      "Epoch: 59 Training Loss: 0.21741167187834187 Test Loss: 0.21458747623148294\n",
      "Epoch: 60 Training Loss: 0.21740925380182324 Test Loss: 0.2145191754481589\n",
      "Epoch: 61 Training Loss: 0.21740652411889227 Test Loss: 0.21520845725732055\n",
      "Epoch: 62 Training Loss: 0.21741028489583092 Test Loss: 0.21474309416729717\n",
      "Epoch: 63 Training Loss: 0.2174064626045319 Test Loss: 0.21455553599042954\n",
      "Epoch: 64 Training Loss: 0.21740840037412734 Test Loss: 0.21461697947362482\n",
      "Epoch: 65 Training Loss: 0.21740689428991647 Test Loss: 0.21458454664768503\n",
      "Epoch: 66 Training Loss: 0.21740775253224745 Test Loss: 0.2144956480384544\n",
      "Epoch: 67 Training Loss: 0.21740718330251038 Test Loss: 0.21446942437463498\n",
      "Epoch: 68 Training Loss: 0.21740522639989565 Test Loss: 0.21450995892567962\n",
      "Epoch: 69 Training Loss: 0.21740548333285376 Test Loss: 0.21441291970306361\n",
      "Epoch: 70 Training Loss: 0.21740289282605105 Test Loss: 0.21434568186607333\n",
      "Epoch: 71 Training Loss: 0.2173941021092532 Test Loss: 0.2191655342399384\n",
      "Epoch: 72 Training Loss: 0.21738390074022984 Test Loss: 0.2146746378308511\n",
      "Epoch: 73 Training Loss: 0.2173746888190687 Test Loss: 0.21442456026169426\n",
      "Epoch: 74 Training Loss: 0.21736915836337098 Test Loss: 0.21446158190473347\n",
      "Epoch: 75 Training Loss: 0.21736489996654748 Test Loss: 0.21438471273693852\n",
      "Epoch: 76 Training Loss: 0.2173632555041646 Test Loss: 0.21440311985637678\n",
      "Epoch: 77 Training Loss: 0.21736244418883582 Test Loss: 0.21446070043704205\n",
      "Epoch: 78 Training Loss: 0.21735955565880927 Test Loss: 0.214421526975815\n",
      "Epoch: 79 Training Loss: 0.21735759146699837 Test Loss: 0.21434153378281964\n",
      "Epoch: 80 Training Loss: 0.21735933612220698 Test Loss: 0.2143916737391486\n",
      "Epoch: 81 Training Loss: 0.21736052630538688 Test Loss: 0.21744550559327547\n",
      "Epoch: 82 Training Loss: 0.21735984533487243 Test Loss: 0.21440480501519857\n",
      "Epoch: 83 Training Loss: 0.21735839735801757 Test Loss: 0.21430547138403286\n",
      "Epoch: 84 Training Loss: 0.21735993795159636 Test Loss: 0.2143790868990257\n",
      "Epoch: 85 Training Loss: 0.2173604350245673 Test Loss: 0.21439645699765053\n",
      "Epoch: 86 Training Loss: 0.21735666570322035 Test Loss: 0.2144338415979744\n",
      "Epoch: 87 Training Loss: 0.2173578823982737 Test Loss: 0.2142876735143225\n",
      "Epoch: 88 Training Loss: 0.21736153362209043 Test Loss: 0.21430258068851546\n",
      "Epoch: 89 Training Loss: 0.217360074814548 Test Loss: 0.21442283621459196\n",
      "Epoch: 90 Training Loss: 0.2173605013625286 Test Loss: 0.21462233309357412\n",
      "Epoch: 91 Training Loss: 0.2173589070548363 Test Loss: 0.2144777335039025\n",
      "Epoch: 92 Training Loss: 0.21736057730286273 Test Loss: 0.21424964077599024\n",
      "Epoch: 93 Training Loss: 0.21736184710235515 Test Loss: 0.21454429727736407\n",
      "Epoch: 94 Training Loss: 0.21736037899727711 Test Loss: 0.21421964494896198\n",
      "Epoch: 95 Training Loss: 0.2173576469473747 Test Loss: 0.21419807491604279\n",
      "Epoch: 96 Training Loss: 0.21734943343091098 Test Loss: 0.21417117718869466\n",
      "Epoch: 97 Training Loss: 0.21734303294285395 Test Loss: 0.21436869076537113\n",
      "Epoch: 98 Training Loss: 0.21734164669553868 Test Loss: 0.2141598866245885\n",
      "Epoch: 99 Training Loss: 0.2173396617837618 Test Loss: 0.21412037613159712\n",
      "Epoch: 100 Training Loss: 0.21733930701598364 Test Loss: 0.2142189060716324\n",
      "Epoch: 101 Training Loss: 0.21733848759557078 Test Loss: 0.21422037086353138\n",
      "Epoch: 102 Training Loss: 0.2173379559146081 Test Loss: 0.21801490779640545\n",
      "Epoch: 103 Training Loss: 0.2173372020790293 Test Loss: 0.21435746501506583\n",
      "Epoch: 104 Training Loss: 0.2173358970260075 Test Loss: 0.21423683356894446\n",
      "Epoch: 105 Training Loss: 0.21733565515559503 Test Loss: 0.2141807307429383\n",
      "Epoch: 106 Training Loss: 0.21733741474782803 Test Loss: 0.21415328857966312\n",
      "Epoch: 107 Training Loss: 0.2173378706319094 Test Loss: 0.21417098274729213\n",
      "Epoch: 108 Training Loss: 0.21733837584582763 Test Loss: 0.21416421618648454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 Training Loss: 0.21733965497871885 Test Loss: 0.21406577698577042\n",
      "Epoch: 110 Training Loss: 0.21733850489060083 Test Loss: 0.2141364758797255\n",
      "Epoch: 111 Training Loss: 0.2173391336891195 Test Loss: 0.21416463099480992\n",
      "Epoch: 112 Training Loss: 0.21733972345054078 Test Loss: 0.21427875513532707\n",
      "Epoch: 113 Training Loss: 0.21733899221774616 Test Loss: 0.21449829244152863\n",
      "Epoch: 114 Training Loss: 0.2173392647511979 Test Loss: 0.21486323302853216\n",
      "Epoch: 115 Training Loss: 0.21733585071764555 Test Loss: 0.2144635392815188\n",
      "Epoch: 116 Training Loss: 0.21733958775314327 Test Loss: 0.21423508359632182\n",
      "Epoch: 117 Training Loss: 0.21733966989781164 Test Loss: 0.21417821596746575\n",
      "Epoch: 118 Training Loss: 0.21733937244839904 Test Loss: 0.21416380137815919\n",
      "Epoch: 119 Training Loss: 0.21733786830080115 Test Loss: 0.215320520318971\n",
      "Epoch: 120 Training Loss: 0.21734270925950774 Test Loss: 0.21434717258349262\n",
      "Epoch: 121 Training Loss: 0.21733994655553185 Test Loss: 0.21416504580313528\n",
      "Epoch: 122 Training Loss: 0.2173413737855233 Test Loss: 0.21423378732030504\n",
      "Epoch: 123 Training Loss: 0.21734106298603306 Test Loss: 0.21428858090753425\n",
      "Epoch: 124 Training Loss: 0.2173404940880306 Test Loss: 0.21412948895199507\n",
      "Epoch: 125 Training Loss: 0.21734241379153715 Test Loss: 0.21415843479544971\n",
      "Epoch: 126 Training Loss: 0.21734242501671994 Test Loss: 0.21419366757758576\n",
      "Epoch: 127 Training Loss: 0.21734104327920256 Test Loss: 0.21425702954928585\n",
      "Epoch: 128 Training Loss: 0.21733728607065267 Test Loss: 0.21443027683892826\n",
      "Epoch: 129 Training Loss: 0.21732988422814975 Test Loss: 0.21436640931958162\n",
      "Epoch: 130 Training Loss: 0.21732569112028885 Test Loss: 0.21415654223246522\n",
      "Epoch: 131 Training Loss: 0.21732246220362889 Test Loss: 0.21421216543634516\n",
      "Epoch: 132 Training Loss: 0.21731831919666375 Test Loss: 0.21418687509125783\n",
      "Epoch: 133 Training Loss: 0.21731665238253906 Test Loss: 0.2141025134480859\n",
      "Epoch: 134 Training Loss: 0.21731687911867953 Test Loss: 0.21423864835536796\n",
      "Epoch: 135 Training Loss: 0.21731462174517727 Test Loss: 0.2141689346311856\n",
      "Epoch: 136 Training Loss: 0.2173155785933872 Test Loss: 0.2142146931745779\n",
      "Epoch: 137 Training Loss: 0.21731343863601432 Test Loss: 0.21418366032673622\n",
      "Epoch: 138 Training Loss: 0.21731350596021373 Test Loss: 0.21424468900160615\n",
      "Epoch: 139 Training Loss: 0.21731475515629553 Test Loss: 0.2141336240724886\n",
      "Epoch: 140 Training Loss: 0.21731560584045634 Test Loss: 0.21406229000328528\n",
      "Epoch: 141 Training Loss: 0.21731375719988819 Test Loss: 0.2141076726266327\n",
      "Epoch: 142 Training Loss: 0.21731241620297037 Test Loss: 0.21413230187095147\n",
      "Epoch: 143 Training Loss: 0.21731390453489535 Test Loss: 0.2141429054087687\n",
      "Epoch: 144 Training Loss: 0.2173149034327118 Test Loss: 0.21409911720492195\n",
      "Epoch: 145 Training Loss: 0.21731279175347512 Test Loss: 0.2141050930373593\n",
      "Epoch: 146 Training Loss: 0.2173128755926799 Test Loss: 0.21419527495984655\n",
      "Epoch: 147 Training Loss: 0.21731172342449606 Test Loss: 0.2142390242754128\n",
      "Epoch: 148 Training Loss: 0.2173146540758555 Test Loss: 0.21426303130724356\n",
      "Epoch: 149 Training Loss: 0.21731468330436665 Test Loss: 0.21418360847569556\n",
      "Epoch: 150 Training Loss: 0.21731422066903716 Test Loss: 0.21414610721053015\n",
      "Epoch: 151 Training Loss: 0.21731530054596787 Test Loss: 0.21429506228761813\n",
      "Epoch: 152 Training Loss: 0.21731235919840786 Test Loss: 0.21418485290067166\n",
      "Epoch: 153 Training Loss: 0.2173139863298975 Test Loss: 0.21420943029394976\n",
      "Epoch: 154 Training Loss: 0.21731361015178666 Test Loss: 0.21443767857498408\n",
      "Epoch: 155 Training Loss: 0.21731401879506276 Test Loss: 0.21425189629625943\n",
      "Epoch: 156 Training Loss: 0.21731215800583434 Test Loss: 0.21414233504732133\n",
      "Epoch: 157 Training Loss: 0.21731417347306092 Test Loss: 0.21404364955416402\n",
      "Epoch: 158 Training Loss: 0.2173120753770127 Test Loss: 0.21580167205087872\n",
      "Epoch: 159 Training Loss: 0.21731646893328566 Test Loss: 0.21405237349175693\n",
      "Epoch: 160 Training Loss: 0.21731487888434878 Test Loss: 0.21406258814676915\n",
      "Epoch: 161 Training Loss: 0.2173124305303203 Test Loss: 0.21405206238551291\n",
      "Epoch: 162 Training Loss: 0.2173154980984262 Test Loss: 0.21411153552916268\n",
      "Epoch: 163 Training Loss: 0.21731465489174343 Test Loss: 0.21410093199134544\n",
      "Epoch: 164 Training Loss: 0.21731159443351766 Test Loss: 0.2149490724263632\n",
      "Epoch: 165 Training Loss: 0.21731489209993937 Test Loss: 0.21423779281319688\n",
      "Epoch: 166 Training Loss: 0.217311896894813 Test Loss: 0.21435710205778113\n",
      "Epoch: 167 Training Loss: 0.2173140611584723 Test Loss: 0.2142911734595678\n",
      "Epoch: 168 Training Loss: 0.2173119798553693 Test Loss: 0.21422005975728736\n",
      "Epoch: 169 Training Loss: 0.21731540525755727 Test Loss: 0.21412611863435144\n",
      "Epoch: 170 Training Loss: 0.21731565593238628 Test Loss: 0.2141758697078754\n",
      "Epoch: 171 Training Loss: 0.21731298053738013 Test Loss: 0.2141298389465196\n",
      "Epoch: 172 Training Loss: 0.21731454768766134 Test Loss: 0.21413472590710284\n",
      "Epoch: 173 Training Loss: 0.21731551190575968 Test Loss: 0.21692884886126818\n",
      "Epoch: 174 Training Loss: 0.21731437404699416 Test Loss: 0.21430866022303413\n",
      "Epoch: 175 Training Loss: 0.21731544539744815 Test Loss: 0.21411239107133376\n",
      "Epoch: 176 Training Loss: 0.21731639494749613 Test Loss: 0.2143382930927777\n",
      "Epoch: 177 Training Loss: 0.21731477740941352 Test Loss: 0.2142315836510765\n",
      "Epoch: 178 Training Loss: 0.2173141844830645 Test Loss: 0.2141593681141818\n",
      "Epoch: 179 Training Loss: 0.2173150145548489 Test Loss: 0.21417997890284857\n",
      "Epoch: 180 Training Loss: 0.21731203715580322 Test Loss: 0.2142153413125863\n",
      "Epoch: 181 Training Loss: 0.21731579097528034 Test Loss: 0.21424620564454577\n",
      "Epoch: 182 Training Loss: 0.21731301638265235 Test Loss: 0.2177046311690294\n",
      "Epoch: 183 Training Loss: 0.2173144807938204 Test Loss: 0.2141349851623062\n",
      "Epoch: 184 Training Loss: 0.2173152502478245 Test Loss: 0.2141159558303799\n",
      "Epoch: 185 Training Loss: 0.21731305111616528 Test Loss: 0.21404179587946\n",
      "Epoch: 186 Training Loss: 0.21731633875882153 Test Loss: 0.21403518487177445\n",
      "Epoch: 187 Training Loss: 0.21731496946583587 Test Loss: 0.21399991320135792\n",
      "Epoch: 188 Training Loss: 0.21731403463763307 Test Loss: 0.21404127736905332\n",
      "Epoch: 189 Training Loss: 0.2173146182395491 Test Loss: 0.2140224554412897\n",
      "Epoch: 190 Training Loss: 0.21731583339248467 Test Loss: 0.2139965039954338\n",
      "Epoch: 191 Training Loss: 0.2173148875004835 Test Loss: 0.21404258660783027\n",
      "Epoch: 192 Training Loss: 0.21731701692304026 Test Loss: 0.21411411511843606\n",
      "Epoch: 193 Training Loss: 0.2173152071761167 Test Loss: 0.21410349861785866\n",
      "Epoch: 194 Training Loss: 0.2173135300602867 Test Loss: 0.21407533054001407\n",
      "Epoch: 195 Training Loss: 0.2173153745138258 Test Loss: 0.21398067646526892\n",
      "Epoch: 196 Training Loss: 0.21731621496800768 Test Loss: 0.21403347378743232\n",
      "Epoch: 197 Training Loss: 0.2173160441963968 Test Loss: 0.21412210017869943\n",
      "Epoch: 198 Training Loss: 0.21731468416508354 Test Loss: 0.21431409161954443\n",
      "Epoch: 199 Training Loss: 0.21731414680876887 Test Loss: 0.21468754873997822\n",
      "Epoch: 200 Training Loss: 0.21731542730446182 Test Loss: 0.214084132254168\n",
      "Epoch: 201 Training Loss: 0.2173150801845119 Test Loss: 0.21414216653143917\n",
      "Epoch: 202 Training Loss: 0.21731310233778614 Test Loss: 0.2140543179057821\n",
      "Epoch: 203 Training Loss: 0.21731379406726173 Test Loss: 0.214039708875073\n",
      "Epoch: 204 Training Loss: 0.21731613803246966 Test Loss: 0.21405823265935275\n",
      "Epoch: 205 Training Loss: 0.2173142014911889 Test Loss: 0.21395796570945497\n",
      "Epoch: 206 Training Loss: 0.21731679821129166 Test Loss: 0.21954720975031858\n",
      "Epoch: 207 Training Loss: 0.21731754384316262 Test Loss: 0.21427572184944782\n",
      "Epoch: 208 Training Loss: 0.21731698604482175 Test Loss: 0.21409557837139614\n",
      "Epoch: 209 Training Loss: 0.21731699796933704 Test Loss: 0.21408178599457764\n",
      "Epoch: 210 Training Loss: 0.21731418089674412 Test Loss: 0.21407114356847987\n",
      "Epoch: 211 Training Loss: 0.21731695776668553 Test Loss: 0.21408538964190427\n",
      "Epoch: 212 Training Loss: 0.2173168209126997 Test Loss: 0.21419810084156313\n",
      "Epoch: 213 Training Loss: 0.2173136299662068 Test Loss: 0.21450575899138527\n",
      "Epoch: 214 Training Loss: 0.21731715996342876 Test Loss: 0.21414652201885553\n",
      "Epoch: 215 Training Loss: 0.2173159591019799 Test Loss: 0.21404804392986088\n",
      "Epoch: 216 Training Loss: 0.21731605298288176 Test Loss: 0.21529607255329458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217 Training Loss: 0.2173178345502929 Test Loss: 0.2140712213450409\n",
      "Epoch: 218 Training Loss: 0.21731772442335973 Test Loss: 0.214078415676934\n",
      "Epoch: 219 Training Loss: 0.21731567888483674 Test Loss: 0.2140099074894473\n",
      "Epoch: 220 Training Loss: 0.21731640243393993 Test Loss: 0.21407429351920065\n",
      "Epoch: 221 Training Loss: 0.21731519539505423 Test Loss: 0.21398639304250291\n",
      "Epoch: 222 Training Loss: 0.21731653452708546 Test Loss: 0.2140378940886495\n",
      "Epoch: 223 Training Loss: 0.21731430953805628 Test Loss: 0.21399033372159393\n",
      "Epoch: 224 Training Loss: 0.21731618564087274 Test Loss: 0.21416578468046485\n",
      "Epoch: 225 Training Loss: 0.2173193068603316 Test Loss: 0.21407505832205054\n",
      "Epoch: 226 Training Loss: 0.21731664641131562 Test Loss: 0.214022170260566\n",
      "Epoch: 227 Training Loss: 0.2173188252623331 Test Loss: 0.21409815796066953\n",
      "Epoch: 228 Training Loss: 0.21731620814503316 Test Loss: 0.2140465532124416\n",
      "Epoch: 229 Training Loss: 0.21731673830180967 Test Loss: 0.2139830227248593\n",
      "Epoch: 230 Training Loss: 0.21731856841006722 Test Loss: 0.2141023967832444\n",
      "Epoch: 231 Training Loss: 0.21731970759577127 Test Loss: 0.21402773128467797\n",
      "Epoch: 232 Training Loss: 0.21731801387527788 Test Loss: 0.21484192225081633\n",
      "Epoch: 233 Training Loss: 0.2173174800694204 Test Loss: 0.21494274659940135\n",
      "Epoch: 234 Training Loss: 0.21731828365622874 Test Loss: 0.21453010305498035\n",
      "Epoch: 235 Training Loss: 0.2173173986778793 Test Loss: 0.21423386509686604\n",
      "Epoch: 236 Training Loss: 0.21731842121851286 Test Loss: 0.2140888766243894\n",
      "Epoch: 237 Training Loss: 0.2173183802806657 Test Loss: 0.21413396110425295\n",
      "Epoch: 238 Training Loss: 0.2173176017353394 Test Loss: 0.21402686277974672\n",
      "Epoch: 239 Training Loss: 0.21731975851255492 Test Loss: 0.21405732526614102\n",
      "Epoch: 240 Training Loss: 0.21731609615321337 Test Loss: 0.21404738282909233\n",
      "Epoch: 241 Training Loss: 0.21731837366390458 Test Loss: 0.21388791495350828\n",
      "Epoch: 242 Training Loss: 0.2173202416975002 Test Loss: 0.21401195560555378\n",
      "Epoch: 243 Training Loss: 0.21731810966589535 Test Loss: 0.2139616989843833\n",
      "Epoch: 244 Training Loss: 0.21731977027568578 Test Loss: 0.21989863017846714\n",
      "Epoch: 245 Training Loss: 0.21732071061785616 Test Loss: 0.21427760144967214\n",
      "Epoch: 246 Training Loss: 0.2173178783213332 Test Loss: 0.21409063955977223\n",
      "Epoch: 247 Training Loss: 0.21732269345853303 Test Loss: 0.21394882696353668\n",
      "Epoch: 248 Training Loss: 0.21732140862339241 Test Loss: 0.21409054882045106\n",
      "Epoch: 249 Training Loss: 0.2173198371336635 Test Loss: 0.21397152475659048\n",
      "Epoch: 250 Training Loss: 0.21732199734478078 Test Loss: 0.21402774424743815\n",
      "Epoch: 251 Training Loss: 0.21731999004539887 Test Loss: 0.21404978093972338\n",
      "Epoch: 252 Training Loss: 0.21732198599407676 Test Loss: 0.21400303722655836\n",
      "Epoch: 253 Training Loss: 0.21732066469502362 Test Loss: 0.21402105546319156\n",
      "Epoch: 254 Training Loss: 0.21732165472566292 Test Loss: 0.21398188200196452\n",
      "Epoch: 255 Training Loss: 0.21732074351337985 Test Loss: 0.213946454778426\n",
      "Epoch: 256 Training Loss: 0.21732177477773776 Test Loss: 0.21394937139946374\n",
      "Epoch: 257 Training Loss: 0.21732088174809905 Test Loss: 0.21807264393019274\n",
      "Epoch: 258 Training Loss: 0.21732282077290666 Test Loss: 0.214177230797693\n",
      "Epoch: 259 Training Loss: 0.21732263596085147 Test Loss: 0.21401282411048503\n",
      "Epoch: 260 Training Loss: 0.21732311932511278 Test Loss: 0.2139423455834528\n",
      "Epoch: 261 Training Loss: 0.21732444174489102 Test Loss: 0.21389135008495275\n",
      "Epoch: 262 Training Loss: 0.21732563546059652 Test Loss: 0.2139851875058073\n",
      "Epoch: 263 Training Loss: 0.21732339818842 Test Loss: 0.2139645637543804\n",
      "Epoch: 264 Training Loss: 0.2173253107910122 Test Loss: 0.21395991012348015\n",
      "Epoch: 265 Training Loss: 0.2173271443869663 Test Loss: 0.21402380356834713\n",
      "Epoch: 266 Training Loss: 0.2173256223077665 Test Loss: 0.21404650136140094\n",
      "Epoch: 267 Training Loss: 0.21732585733727283 Test Loss: 0.2140102963722523\n",
      "Epoch: 268 Training Loss: 0.21732852906526642 Test Loss: 0.2140055520020309\n",
      "Epoch: 269 Training Loss: 0.21732686720922967 Test Loss: 0.2140305312408742\n",
      "Epoch: 270 Training Loss: 0.21732865182501315 Test Loss: 0.21403683114231575\n",
      "Epoch: 271 Training Loss: 0.2173287394029569 Test Loss: 0.21395515279049857\n",
      "Epoch: 272 Training Loss: 0.2173301734290255 Test Loss: 0.2139612193622571\n",
      "Epoch: 273 Training Loss: 0.2173274472875859 Test Loss: 0.21400550015099024\n",
      "Epoch: 274 Training Loss: 0.2173300228304668 Test Loss: 0.21400183168986275\n",
      "Epoch: 275 Training Loss: 0.21732762247933665 Test Loss: 0.21394027154182596\n",
      "Epoch: 276 Training Loss: 0.2173254100693262 Test Loss: 0.21393983080798024\n",
      "Epoch: 277 Training Loss: 0.2173158533503576 Test Loss: 0.21565179661781883\n",
      "Epoch: 278 Training Loss: 0.2173042284542186 Test Loss: 0.21545963666109164\n",
      "Epoch: 279 Training Loss: 0.21730322007058472 Test Loss: 0.2145523341886681\n",
      "Epoch: 280 Training Loss: 0.21730176694736011 Test Loss: 0.214326652534147\n",
      "Epoch: 281 Training Loss: 0.2172983609292338 Test Loss: 0.2147847694412366\n",
      "Epoch: 282 Training Loss: 0.21730004440181666 Test Loss: 0.2140069260546087\n",
      "Epoch: 283 Training Loss: 0.21729682945387457 Test Loss: 0.21400105392425267\n",
      "Epoch: 284 Training Loss: 0.21729500341609068 Test Loss: 0.2138720096467824\n",
      "Epoch: 285 Training Loss: 0.2172961714089132 Test Loss: 0.21392220145415206\n",
      "Epoch: 286 Training Loss: 0.21729442162526644 Test Loss: 0.21396499152546591\n",
      "Epoch: 287 Training Loss: 0.21729407497153816 Test Loss: 0.21389811664576033\n",
      "Epoch: 288 Training Loss: 0.21729464410265142 Test Loss: 0.21391343862827863\n",
      "Epoch: 289 Training Loss: 0.2172964547371893 Test Loss: 0.2139560990719908\n",
      "Epoch: 290 Training Loss: 0.2172936330651405 Test Loss: 0.21380070150309946\n",
      "Epoch: 291 Training Loss: 0.2172939006674017 Test Loss: 0.2138389805338749\n",
      "Epoch: 292 Training Loss: 0.21729566243832435 Test Loss: 0.21385418585155172\n",
      "Epoch: 293 Training Loss: 0.2172947913838638 Test Loss: 0.21379679971228893\n",
      "Epoch: 294 Training Loss: 0.2172948962568376 Test Loss: 0.21399868173914197\n",
      "Epoch: 295 Training Loss: 0.21729265772944897 Test Loss: 0.21418430846474462\n",
      "Epoch: 296 Training Loss: 0.21729476470164014 Test Loss: 0.21405654750053096\n",
      "Epoch: 297 Training Loss: 0.21729391495888845 Test Loss: 0.21398259495377375\n",
      "Epoch: 298 Training Loss: 0.21729390759796585 Test Loss: 0.21390357396779097\n",
      "Epoch: 299 Training Loss: 0.21729327846771257 Test Loss: 0.21376769831571227\n",
      "Epoch: 300 Training Loss: 0.21729431593640472 Test Loss: 0.21384846927431772\n",
      "Epoch: 301 Training Loss: 0.21729419092624197 Test Loss: 0.2138221289456568\n",
      "Epoch: 302 Training Loss: 0.21729258935625084 Test Loss: 0.21381544016141021\n",
      "Epoch: 303 Training Loss: 0.21729419885201 Test Loss: 0.21386543752737736\n",
      "Epoch: 304 Training Loss: 0.2172918680844611 Test Loss: 0.21381875862801317\n",
      "Epoch: 305 Training Loss: 0.21729305869799945 Test Loss: 0.21395631943891366\n",
      "Epoch: 306 Training Loss: 0.21729538283085564 Test Loss: 0.21396488782338458\n",
      "Epoch: 307 Training Loss: 0.2172928075748804 Test Loss: 0.213950473234078\n",
      "Epoch: 308 Training Loss: 0.2172936086960935 Test Loss: 0.21433245985070218\n",
      "Epoch: 309 Training Loss: 0.21728983311661745 Test Loss: 0.21462086830167515\n",
      "Epoch: 310 Training Loss: 0.21729330819830855 Test Loss: 0.21420188596753212\n",
      "Epoch: 311 Training Loss: 0.21729238359111885 Test Loss: 0.21397680059997876\n",
      "Epoch: 312 Training Loss: 0.21729288952230116 Test Loss: 0.214033758968156\n",
      "Epoch: 313 Training Loss: 0.217292676647289 Test Loss: 0.21450687378875968\n",
      "Epoch: 314 Training Loss: 0.21729293762382332 Test Loss: 0.21392808654726825\n",
      "Epoch: 315 Training Loss: 0.21729382810717454 Test Loss: 0.214035768195982\n",
      "Epoch: 316 Training Loss: 0.21729175902445824 Test Loss: 0.21396417487157535\n",
      "Epoch: 317 Training Loss: 0.217293039484288 Test Loss: 0.2139916299976107\n",
      "Epoch: 318 Training Loss: 0.2172935724294286 Test Loss: 0.21395738238524742\n",
      "Epoch: 319 Training Loss: 0.21729165418734764 Test Loss: 0.21389264636096952\n",
      "Epoch: 320 Training Loss: 0.21729223127112635 Test Loss: 0.21384612301472736\n",
      "Epoch: 321 Training Loss: 0.21729457112999742 Test Loss: 0.21395633240167383\n",
      "Epoch: 322 Training Loss: 0.21729373736430305 Test Loss: 0.21402701833286875\n",
      "Epoch: 323 Training Loss: 0.21729123142293502 Test Loss: 0.21395485464701472\n",
      "Epoch: 324 Training Loss: 0.21729061019155235 Test Loss: 0.21423107810342998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 Training Loss: 0.21729268748694236 Test Loss: 0.21403928110398748\n",
      "Epoch: 326 Training Loss: 0.21729357675094466 Test Loss: 0.21393181982219656\n",
      "Epoch: 327 Training Loss: 0.217290408541723 Test Loss: 0.21385917651421632\n",
      "Epoch: 328 Training Loss: 0.21729526114700504 Test Loss: 0.21384612301472736\n",
      "Epoch: 329 Training Loss: 0.21729384105379113 Test Loss: 0.2138823150411158\n",
      "Epoch: 330 Training Loss: 0.21729304347406944 Test Loss: 0.21398781894612137\n",
      "Epoch: 331 Training Loss: 0.21729205681457128 Test Loss: 0.2139453659065719\n",
      "Epoch: 332 Training Loss: 0.21729226279488253 Test Loss: 0.21397144698002946\n",
      "Epoch: 333 Training Loss: 0.2172918127565034 Test Loss: 0.21394331779046538\n",
      "Epoch: 334 Training Loss: 0.21729061195781516 Test Loss: 0.21392977170609004\n",
      "Epoch: 335 Training Loss: 0.2172936592990741 Test Loss: 0.2138907278724647\n",
      "Epoch: 336 Training Loss: 0.2172942483073681 Test Loss: 0.21388398723717744\n",
      "Epoch: 337 Training Loss: 0.2172938080596436 Test Loss: 0.21384691374309758\n",
      "Epoch: 338 Training Loss: 0.21729449040192558 Test Loss: 0.21383631020528035\n",
      "Epoch: 339 Training Loss: 0.21729222675236268 Test Loss: 0.21394492517272617\n",
      "Epoch: 340 Training Loss: 0.2172921623599802 Test Loss: 0.21395017509059414\n",
      "Epoch: 341 Training Loss: 0.217290086786024 Test Loss: 0.21397704689242195\n",
      "Epoch: 342 Training Loss: 0.2172907648964479 Test Loss: 0.2139474529109589\n",
      "Epoch: 343 Training Loss: 0.21728803991159665 Test Loss: 0.21515523216407162\n",
      "Epoch: 344 Training Loss: 0.21729407320527536 Test Loss: 0.21424790376612773\n",
      "Epoch: 345 Training Loss: 0.21729259313981886 Test Loss: 0.2140779360548078\n",
      "Epoch: 346 Training Loss: 0.21729141450459058 Test Loss: 0.21393218277948126\n",
      "Epoch: 347 Training Loss: 0.2172931040201233 Test Loss: 0.21397880982780476\n",
      "Epoch: 348 Training Loss: 0.21729301018405048 Test Loss: 0.2139171200521663\n",
      "Epoch: 349 Training Loss: 0.2172925786331529 Test Loss: 0.21394086782879368\n",
      "Epoch: 350 Training Loss: 0.2172920456790465 Test Loss: 0.2139903077960736\n",
      "Epoch: 351 Training Loss: 0.21729194395306883 Test Loss: 0.21392199404998938\n",
      "Epoch: 352 Training Loss: 0.2172913547923562 Test Loss: 0.21394972139398827\n",
      "Epoch: 353 Training Loss: 0.217291962368824 Test Loss: 0.2145736968174246\n",
      "Epoch: 354 Training Loss: 0.21729354199053436 Test Loss: 0.21389409819010832\n",
      "Epoch: 355 Training Loss: 0.21729184739139248 Test Loss: 0.21403767372172666\n",
      "Epoch: 356 Training Loss: 0.21729207367027709 Test Loss: 0.21391613488239355\n",
      "Epoch: 357 Training Loss: 0.21729266119024815 Test Loss: 0.21393697900074335\n",
      "Epoch: 358 Training Loss: 0.21729327868289178 Test Loss: 0.2138563247069794\n",
      "Epoch: 359 Training Loss: 0.21729127544501772 Test Loss: 0.22081216773853138\n",
      "Epoch: 360 Training Loss: 0.21729200620262487 Test Loss: 0.21432109151003503\n",
      "Epoch: 361 Training Loss: 0.21728874212209354 Test Loss: 0.2139440307422746\n",
      "Epoch: 362 Training Loss: 0.2172916567157035 Test Loss: 0.21390667206747108\n",
      "Epoch: 363 Training Loss: 0.21729334944995876 Test Loss: 0.21397985981137835\n",
      "Epoch: 364 Training Loss: 0.21729185486887048 Test Loss: 0.21396566558899463\n",
      "Epoch: 365 Training Loss: 0.217291837143482 Test Loss: 0.2139675711147393\n",
      "Epoch: 366 Training Loss: 0.21729194121849954 Test Loss: 0.21390347026570963\n",
      "Epoch: 367 Training Loss: 0.21729260033039122 Test Loss: 0.21399310775226982\n",
      "Epoch: 368 Training Loss: 0.2172924158859339 Test Loss: 0.21388546499183658\n",
      "Epoch: 369 Training Loss: 0.21729336820641437 Test Loss: 0.21388940567092757\n",
      "Epoch: 370 Training Loss: 0.21729481163760817 Test Loss: 0.21399746323968621\n",
      "Epoch: 371 Training Loss: 0.21729189867577398 Test Loss: 0.2139992391378292\n",
      "Epoch: 372 Training Loss: 0.21728913219719587 Test Loss: 0.21391706820112563\n",
      "Epoch: 373 Training Loss: 0.21729036437618746 Test Loss: 0.21393692714970267\n",
      "Epoch: 374 Training Loss: 0.2172918306432763 Test Loss: 0.21389469447707604\n",
      "Epoch: 375 Training Loss: 0.217292241151439 Test Loss: 0.21403964406127216\n",
      "Epoch: 376 Training Loss: 0.21729032321419525 Test Loss: 0.21391419046836838\n",
      "Epoch: 377 Training Loss: 0.2172887679794635 Test Loss: 0.21423501878252096\n",
      "Epoch: 378 Training Loss: 0.217293866104239 Test Loss: 0.21387750585709356\n",
      "Epoch: 379 Training Loss: 0.21729010557834277 Test Loss: 0.21388525758767388\n",
      "Epoch: 380 Training Loss: 0.21729317465270326 Test Loss: 0.21388484277934852\n",
      "Epoch: 381 Training Loss: 0.21728957609400135 Test Loss: 0.21422944479564882\n",
      "Epoch: 382 Training Loss: 0.21729392613924223 Test Loss: 0.21389308709481522\n",
      "Epoch: 383 Training Loss: 0.21729144039782375 Test Loss: 0.21392749026030053\n",
      "Epoch: 384 Training Loss: 0.21729233441370058 Test Loss: 0.2139626971169162\n",
      "Epoch: 385 Training Loss: 0.21729306986042166 Test Loss: 0.21396150454298077\n",
      "Epoch: 386 Training Loss: 0.21729211043006103 Test Loss: 0.2139857967555352\n",
      "Epoch: 387 Training Loss: 0.21729214193588559 Test Loss: 0.2139905411257566\n",
      "Epoch: 388 Training Loss: 0.21729247257669337 Test Loss: 0.2140407977469271\n",
      "Epoch: 389 Training Loss: 0.21729396081896035 Test Loss: 0.21402421837667251\n",
      "Epoch: 390 Training Loss: 0.21729190606359397 Test Loss: 0.21393210500292026\n",
      "Epoch: 391 Training Loss: 0.21729078425361217 Test Loss: 0.2139361493840926\n",
      "Epoch: 392 Training Loss: 0.21729180369207862 Test Loss: 0.21394060857359032\n",
      "Epoch: 393 Training Loss: 0.21729097887425358 Test Loss: 0.21454167879981018\n",
      "Epoch: 394 Training Loss: 0.21729392458815866 Test Loss: 0.213973961755502\n",
      "Epoch: 395 Training Loss: 0.21729063279433658 Test Loss: 0.21395792682117448\n",
      "Epoch: 396 Training Loss: 0.2172915737372156 Test Loss: 0.2138503229490217\n",
      "Epoch: 397 Training Loss: 0.2172946960415364 Test Loss: 0.21396937293840262\n",
      "Epoch: 398 Training Loss: 0.2172913154593874 Test Loss: 0.2139519639514973\n",
      "Epoch: 399 Training Loss: 0.2172924355479354 Test Loss: 0.21394462702924233\n",
      "Epoch: 400 Training Loss: 0.21729274268937884 Test Loss: 0.21388110950442019\n",
      "Epoch: 401 Training Loss: 0.21729221654031539 Test Loss: 0.21394594923077945\n",
      "Epoch: 402 Training Loss: 0.21729046498144003 Test Loss: 0.2139664044663242\n",
      "Epoch: 403 Training Loss: 0.21729164762438133 Test Loss: 0.21396714334365377\n",
      "Epoch: 404 Training Loss: 0.21729317604240242 Test Loss: 0.21391477379257592\n",
      "Epoch: 405 Training Loss: 0.21729094750291603 Test Loss: 0.21397003403917117\n",
      "Epoch: 406 Training Loss: 0.21729246525163398 Test Loss: 0.2139790301947276\n",
      "Epoch: 407 Training Loss: 0.21729251527183754 Test Loss: 0.213979367226492\n",
      "Epoch: 408 Training Loss: 0.2172932340242372 Test Loss: 0.21398490232508363\n",
      "Epoch: 409 Training Loss: 0.21728997875708822 Test Loss: 0.21393060132274078\n",
      "Epoch: 410 Training Loss: 0.21729204399347593 Test Loss: 0.2139087461090979\n",
      "Epoch: 411 Training Loss: 0.2172941734429301 Test Loss: 0.21391910335447198\n",
      "Epoch: 412 Training Loss: 0.2172912741808398 Test Loss: 0.213967856295463\n",
      "Epoch: 413 Training Loss: 0.21729209858623796 Test Loss: 0.21386687639375598\n",
      "Epoch: 414 Training Loss: 0.21729364800216489 Test Loss: 0.21386441346932408\n",
      "Epoch: 415 Training Loss: 0.21729167300656385 Test Loss: 0.2155661127731098\n",
      "Epoch: 416 Training Loss: 0.21729060797699953 Test Loss: 0.21396381191429065\n",
      "Epoch: 417 Training Loss: 0.21729188731610416 Test Loss: 0.21415306821274024\n",
      "Epoch: 418 Training Loss: 0.2172920118421137 Test Loss: 0.21429236603350324\n",
      "Epoch: 419 Training Loss: 0.21729034594250068 Test Loss: 0.2140132778070909\n",
      "Epoch: 420 Training Loss: 0.21729236285322123 Test Loss: 0.2139741302713842\n",
      "Epoch: 421 Training Loss: 0.2172930825649616 Test Loss: 0.213971874751115\n",
      "Epoch: 422 Training Loss: 0.21728966238983558 Test Loss: 0.2139649526371854\n",
      "Epoch: 423 Training Loss: 0.2172892634206587 Test Loss: 0.21394067338739114\n",
      "Epoch: 424 Training Loss: 0.21729076906554534 Test Loss: 0.2140029724127575\n",
      "Epoch: 425 Training Loss: 0.21729317458994266 Test Loss: 0.21396280081899754\n",
      "Epoch: 426 Training Loss: 0.21729190471872384 Test Loss: 0.2138990110762119\n",
      "Epoch: 427 Training Loss: 0.21729108708250536 Test Loss: 0.21396907479491875\n",
      "Epoch: 428 Training Loss: 0.21729271109389628 Test Loss: 0.21405080499777662\n",
      "Epoch: 429 Training Loss: 0.21729265055680821 Test Loss: 0.21390278323942072\n",
      "Epoch: 430 Training Loss: 0.21729386101166406 Test Loss: 0.2141355425609934\n",
      "Epoch: 431 Training Loss: 0.21729248539778873 Test Loss: 0.21402826275784487\n",
      "Epoch: 432 Training Loss: 0.21729237219558584 Test Loss: 0.2139097183161105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 433 Training Loss: 0.21729307464815936 Test Loss: 0.21392155331614368\n",
      "Epoch: 434 Training Loss: 0.21729190759674594 Test Loss: 0.21383562317899146\n",
      "Epoch: 435 Training Loss: 0.21729240707255157 Test Loss: 0.21392726989337768\n",
      "Epoch: 436 Training Loss: 0.2172920212920679 Test Loss: 0.21400428165153446\n",
      "Epoch: 437 Training Loss: 0.21729338205857684 Test Loss: 0.2139349827356775\n",
      "Epoch: 438 Training Loss: 0.21729270796483174 Test Loss: 0.21415384597835033\n",
      "Epoch: 439 Training Loss: 0.21729022001782622 Test Loss: 0.21444252664728683\n",
      "Epoch: 440 Training Loss: 0.21729009391383575 Test Loss: 0.21406873249508868\n",
      "Epoch: 441 Training Loss: 0.21729190770433554 Test Loss: 0.21398263384205427\n",
      "Epoch: 442 Training Loss: 0.21729033389246422 Test Loss: 0.21389184266983913\n",
      "Epoch: 443 Training Loss: 0.21729273062141077 Test Loss: 0.21399703546860066\n",
      "Epoch: 444 Training Loss: 0.2172887089844932 Test Loss: 0.21474471451231814\n",
      "Epoch: 445 Training Loss: 0.2172931020476471 Test Loss: 0.21406702141074652\n",
      "Epoch: 446 Training Loss: 0.21728980163769027 Test Loss: 0.2140714546747239\n",
      "Epoch: 447 Training Loss: 0.21729066745612308 Test Loss: 0.21405841413799512\n",
      "Epoch: 448 Training Loss: 0.2172925018321019 Test Loss: 0.21414249060044335\n",
      "Epoch: 449 Training Loss: 0.21729188940513577 Test Loss: 0.21412478347005415\n",
      "Epoch: 450 Training Loss: 0.21729202981854462 Test Loss: 0.21404838096162526\n",
      "Epoch: 451 Training Loss: 0.21729184590306955 Test Loss: 0.2145621469981151\n",
      "Epoch: 452 Training Loss: 0.2172933285327451 Test Loss: 0.21403767372172666\n",
      "Epoch: 453 Training Loss: 0.2172909554286841 Test Loss: 0.21404690320696612\n",
      "Epoch: 454 Training Loss: 0.2172902178032734 Test Loss: 0.21394960472914676\n",
      "Epoch: 455 Training Loss: 0.21729367495336258 Test Loss: 0.21395482872149438\n",
      "Epoch: 456 Training Loss: 0.21729261142108702 Test Loss: 0.21389968513974064\n",
      "Epoch: 457 Training Loss: 0.2172917125726435 Test Loss: 0.21394296779594085\n",
      "Epoch: 458 Training Loss: 0.2172924523319148 Test Loss: 0.21404918465275566\n",
      "Epoch: 459 Training Loss: 0.21729221439748894 Test Loss: 0.21399087815752096\n",
      "Epoch: 460 Training Loss: 0.21729184272917598 Test Loss: 0.21394929362290274\n",
      "Epoch: 461 Training Loss: 0.2172929880385221 Test Loss: 0.21404670876556361\n",
      "Epoch: 462 Training Loss: 0.21729405553368167 Test Loss: 0.21403505524417277\n",
      "Epoch: 463 Training Loss: 0.21729256203745534 Test Loss: 0.21401497592867288\n",
      "Epoch: 464 Training Loss: 0.21729424337617756 Test Loss: 0.21423505767080148\n",
      "Epoch: 465 Training Loss: 0.21728860272182024 Test Loss: 0.21401812587939364\n",
      "Epoch: 466 Training Loss: 0.2172908573876506 Test Loss: 0.21392112554505813\n",
      "Epoch: 467 Training Loss: 0.21729111212398744 Test Loss: 0.2139390919306507\n",
      "Epoch: 468 Training Loss: 0.21729010241341504 Test Loss: 0.2139381975001991\n",
      "Epoch: 469 Training Loss: 0.2172945765632728 Test Loss: 0.21426469054054503\n",
      "Epoch: 470 Training Loss: 0.21729258383331745 Test Loss: 0.2140527494118018\n",
      "Epoch: 471 Training Loss: 0.2172923505521423 Test Loss: 0.21401835920907666\n",
      "Epoch: 472 Training Loss: 0.21729306229328563 Test Loss: 0.21407303613146436\n",
      "Epoch: 473 Training Loss: 0.21729265489625588 Test Loss: 0.2139540898441648\n",
      "Epoch: 474 Training Loss: 0.21729335212176745 Test Loss: 0.21398175237436284\n",
      "Epoch: 475 Training Loss: 0.21729323544979956 Test Loss: 0.21425280368947117\n",
      "Epoch: 476 Training Loss: 0.21729451225158253 Test Loss: 0.21422057826769406\n",
      "Epoch: 477 Training Loss: 0.21729098485444284 Test Loss: 0.21446611887079217\n",
      "Epoch: 478 Training Loss: 0.21729275204967505 Test Loss: 0.21407534350277424\n",
      "Epoch: 479 Training Loss: 0.21729341585964645 Test Loss: 0.21402633130657986\n",
      "Epoch: 480 Training Loss: 0.2172926486650242 Test Loss: 0.2140322682507367\n",
      "Epoch: 481 Training Loss: 0.21729512932283354 Test Loss: 0.21397283399536743\n",
      "Epoch: 482 Training Loss: 0.21729492319010368 Test Loss: 0.2139995113557927\n",
      "Epoch: 483 Training Loss: 0.21729348020719993 Test Loss: 0.21397595802056785\n",
      "Epoch: 484 Training Loss: 0.217293250610969 Test Loss: 0.21393627901169426\n",
      "Epoch: 485 Training Loss: 0.21729388633108598 Test Loss: 0.2140078075223001\n",
      "Epoch: 486 Training Loss: 0.21729267190438029 Test Loss: 0.21639605349285201\n",
      "Epoch: 487 Training Loss: 0.2172935190918787 Test Loss: 0.21403302009082642\n",
      "Epoch: 488 Training Loss: 0.21729419594709048 Test Loss: 0.21387327999727887\n",
      "Epoch: 489 Training Loss: 0.21729411338102947 Test Loss: 0.2139690618321586\n",
      "Epoch: 490 Training Loss: 0.21729406742233376 Test Loss: 0.21401154079722842\n",
      "Epoch: 491 Training Loss: 0.21729206141402718 Test Loss: 0.21433356168531645\n",
      "Epoch: 492 Training Loss: 0.21729205807874924 Test Loss: 0.21414481093451337\n",
      "Epoch: 493 Training Loss: 0.2172935789923949 Test Loss: 0.21394638996462514\n",
      "Epoch: 494 Training Loss: 0.2172915763641953 Test Loss: 0.21390773501380483\n",
      "Epoch: 495 Training Loss: 0.21729449585313257 Test Loss: 0.2160577643407534\n",
      "Epoch: 496 Training Loss: 0.21729502280911814 Test Loss: 0.2139389234147685\n",
      "Epoch: 497 Training Loss: 0.21729423231237918 Test Loss: 0.21379559417559335\n",
      "Epoch: 498 Training Loss: 0.21729516286389491 Test Loss: 0.21390638688674737\n",
      "Epoch: 499 Training Loss: 0.2172950346439754 Test Loss: 0.2139349827356775\n",
      "Epoch: 500 Training Loss: 0.21729555108307644 Test Loss: 0.21393389386382342\n",
      "Epoch: 501 Training Loss: 0.21729520524523604 Test Loss: 0.21391263493714824\n",
      "Epoch: 502 Training Loss: 0.21729159547928295 Test Loss: 0.2139250791869093\n",
      "Epoch: 503 Training Loss: 0.21729728029021 Test Loss: 0.2139705525495779\n",
      "Epoch: 504 Training Loss: 0.217293939076893 Test Loss: 0.21414936086333228\n",
      "Epoch: 505 Training Loss: 0.2172926991514494 Test Loss: 0.21399886321778433\n",
      "Epoch: 506 Training Loss: 0.21729335015825704 Test Loss: 0.21398884300417464\n",
      "Epoch: 507 Training Loss: 0.21729155608355352 Test Loss: 0.21391188309705852\n",
      "Epoch: 508 Training Loss: 0.21729188413324482 Test Loss: 0.2139066979929914\n",
      "Epoch: 509 Training Loss: 0.21729622133945872 Test Loss: 0.21392227923071308\n",
      "Epoch: 510 Training Loss: 0.21729548372301383 Test Loss: 0.21385891725901296\n",
      "Epoch: 511 Training Loss: 0.21729777407273201 Test Loss: 0.2138612246303228\n",
      "Epoch: 512 Training Loss: 0.21729643646488694 Test Loss: 0.2138897945537326\n",
      "Epoch: 513 Training Loss: 0.2172968232495403 Test Loss: 0.2139626971169162\n",
      "Epoch: 514 Training Loss: 0.21729634417989768 Test Loss: 0.2139771117062228\n",
      "Epoch: 515 Training Loss: 0.21729586553164768 Test Loss: 0.21385795801476054\n",
      "Epoch: 516 Training Loss: 0.21729665398418402 Test Loss: 0.2138930222810144\n",
      "Epoch: 517 Training Loss: 0.2172959611698465 Test Loss: 0.2139420992910096\n",
      "Epoch: 518 Training Loss: 0.21729603105826498 Test Loss: 0.21383485837614155\n",
      "Epoch: 519 Training Loss: 0.21729629513696644 Test Loss: 0.2142840957925162\n",
      "Epoch: 520 Training Loss: 0.2172945203118376 Test Loss: 0.21452954565629315\n",
      "Epoch: 521 Training Loss: 0.217295853669893 Test Loss: 0.21444631177325582\n",
      "Epoch: 522 Training Loss: 0.21729373089996057 Test Loss: 0.21399139666792769\n",
      "Epoch: 523 Training Loss: 0.21729785363524973 Test Loss: 0.21395066767548052\n",
      "Epoch: 524 Training Loss: 0.21729639522220254 Test Loss: 0.21395497131185623\n",
      "Epoch: 525 Training Loss: 0.21729702660287187 Test Loss: 0.213933323502376\n",
      "Epoch: 526 Training Loss: 0.2172967717948086 Test Loss: 0.21393596790545025\n",
      "Epoch: 527 Training Loss: 0.2172973945145142 Test Loss: 0.2138809539512982\n",
      "Epoch: 528 Training Loss: 0.2172951766981258 Test Loss: 0.21392997911025274\n",
      "Epoch: 529 Training Loss: 0.21729550911416215 Test Loss: 0.21394882696353668\n",
      "Epoch: 530 Training Loss: 0.2172972296424004 Test Loss: 0.2154555533916388\n",
      "Epoch: 531 Training Loss: 0.21729993203343323 Test Loss: 0.21393254573676596\n",
      "Epoch: 532 Training Loss: 0.21729870750226987 Test Loss: 0.21388604831604413\n",
      "Epoch: 533 Training Loss: 0.21729891145631008 Test Loss: 0.2139112479218103\n",
      "Epoch: 534 Training Loss: 0.21729754997253706 Test Loss: 0.213987559690918\n",
      "Epoch: 535 Training Loss: 0.21729813134196863 Test Loss: 0.21395271579158703\n",
      "Epoch: 536 Training Loss: 0.2172982918298058 Test Loss: 0.21399886321778433\n",
      "Epoch: 537 Training Loss: 0.2172976505329606 Test Loss: 0.21392957726468753\n",
      "Epoch: 538 Training Loss: 0.21729602360768438 Test Loss: 0.21399724287276337\n",
      "Epoch: 539 Training Loss: 0.21729665972229661 Test Loss: 0.21415741073739647\n",
      "Epoch: 540 Training Loss: 0.21729825387757035 Test Loss: 0.2140184369856377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 541 Training Loss: 0.21729642468382449 Test Loss: 0.2144775779507805\n",
      "Epoch: 542 Training Loss: 0.21730030727910077 Test Loss: 0.21406505107120102\n",
      "Epoch: 543 Training Loss: 0.21730019118094415 Test Loss: 0.21397648949373474\n",
      "Epoch: 544 Training Loss: 0.21730066951539112 Test Loss: 0.2139626193403552\n",
      "Epoch: 545 Training Loss: 0.21730243764306664 Test Loss: 0.21401192968003346\n",
      "Epoch: 546 Training Loss: 0.21730241414370233 Test Loss: 0.21397327472921313\n",
      "Epoch: 547 Training Loss: 0.21730113807711504 Test Loss: 0.21398341160766432\n",
      "Epoch: 548 Training Loss: 0.21730108118910796 Test Loss: 0.21409629132320537\n",
      "Epoch: 549 Training Loss: 0.21730268761856317 Test Loss: 0.21390869425805722\n",
      "Epoch: 550 Training Loss: 0.21730409779560123 Test Loss: 0.21480353951795955\n",
      "Epoch: 551 Training Loss: 0.21730264372200167 Test Loss: 0.2145723227648468\n",
      "Epoch: 552 Training Loss: 0.21730481535554935 Test Loss: 0.21419580643301345\n",
      "Epoch: 553 Training Loss: 0.21730221718298687 Test Loss: 0.21396168602162313\n",
      "Epoch: 554 Training Loss: 0.21730549502602267 Test Loss: 0.21397597098332802\n",
      "Epoch: 555 Training Loss: 0.2173052672946783 Test Loss: 0.21397448026590873\n",
      "Epoch: 556 Training Loss: 0.2173060515063908 Test Loss: 0.2138598246522247\n",
      "Epoch: 557 Training Loss: 0.21730565840084773 Test Loss: 0.21733796653492354\n",
      "Epoch: 558 Training Loss: 0.21730689332337444 Test Loss: 0.21406359924206222\n",
      "Epoch: 559 Training Loss: 0.21730394407694378 Test Loss: 0.21395528241810025\n",
      "Epoch: 560 Training Loss: 0.21729646402575908 Test Loss: 0.21390184992068864\n",
      "Epoch: 561 Training Loss: 0.2172883052634418 Test Loss: 0.21398553750033183\n",
      "Epoch: 562 Training Loss: 0.21728105930035438 Test Loss: 0.21387502996990151\n",
      "Epoch: 563 Training Loss: 0.21727575680014602 Test Loss: 0.21393218277948126\n",
      "Epoch: 564 Training Loss: 0.21727306239764183 Test Loss: 0.2139056091211373\n",
      "Epoch: 565 Training Loss: 0.21727046928179106 Test Loss: 0.2139570323907229\n",
      "Epoch: 566 Training Loss: 0.2172684791339864 Test Loss: 0.21390076104883454\n",
      "Epoch: 567 Training Loss: 0.21726808546359788 Test Loss: 0.21395813422533716\n",
      "Epoch: 568 Training Loss: 0.21726740539862935 Test Loss: 0.21383585650867448\n",
      "Epoch: 569 Training Loss: 0.21726815879488393 Test Loss: 0.21385896911005361\n",
      "Epoch: 570 Training Loss: 0.21726668669105867 Test Loss: 0.21391951816279733\n",
      "Epoch: 571 Training Loss: 0.21726819064140893 Test Loss: 0.21385845059964692\n",
      "Epoch: 572 Training Loss: 0.21726367623510487 Test Loss: 0.21387132262049352\n",
      "Epoch: 573 Training Loss: 0.21726948073947472 Test Loss: 0.21395201580253798\n",
      "Epoch: 574 Training Loss: 0.21726639974059897 Test Loss: 0.21402170360119996\n",
      "Epoch: 575 Training Loss: 0.21726432483907784 Test Loss: 0.21385912466317564\n",
      "Epoch: 576 Training Loss: 0.21726527013933616 Test Loss: 0.21391054793276124\n",
      "Epoch: 577 Training Loss: 0.21726552284043618 Test Loss: 0.21383531207274742\n",
      "Epoch: 578 Training Loss: 0.21726689007128763 Test Loss: 0.2138015311197502\n",
      "Epoch: 579 Training Loss: 0.21726525989142567 Test Loss: 0.21396641742908437\n",
      "Epoch: 580 Training Loss: 0.2172651051417011 Test Loss: 0.2139812597894765\n",
      "Epoch: 581 Training Loss: 0.21726480709157983 Test Loss: 0.2139322475932821\n",
      "Epoch: 582 Training Loss: 0.21726375587831478 Test Loss: 0.2138389805338749\n",
      "Epoch: 583 Training Loss: 0.2172640035944295 Test Loss: 0.2138850112952307\n",
      "Epoch: 584 Training Loss: 0.21726564301803225 Test Loss: 0.2140054742254699\n",
      "Epoch: 585 Training Loss: 0.21726619863768346 Test Loss: 0.21394631218806415\n",
      "Epoch: 586 Training Loss: 0.21726324592148785 Test Loss: 0.21462444602348146\n",
      "Epoch: 587 Training Loss: 0.21726498710693148 Test Loss: 0.21383453430713736\n",
      "Epoch: 588 Training Loss: 0.2172656461560626 Test Loss: 0.21392194219894872\n",
      "Epoch: 589 Training Loss: 0.21726436061262366 Test Loss: 0.21395289727022937\n",
      "Epoch: 590 Training Loss: 0.2172653238086207 Test Loss: 0.21390033327774902\n",
      "Epoch: 591 Training Loss: 0.2172649985114303 Test Loss: 0.2138965092634995\n",
      "Epoch: 592 Training Loss: 0.2172655005155918 Test Loss: 0.21390024253842785\n",
      "Epoch: 593 Training Loss: 0.21726571443063689 Test Loss: 0.21394668810810902\n",
      "Epoch: 594 Training Loss: 0.21726603931540042 Test Loss: 0.21763854701769406\n",
      "Epoch: 595 Training Loss: 0.21726361414693324 Test Loss: 0.21430272327887728\n",
      "Epoch: 596 Training Loss: 0.21726488744308803 Test Loss: 0.2139265310160481\n",
      "Epoch: 597 Training Loss: 0.21726329919627715 Test Loss: 0.21389013158549697\n",
      "Epoch: 598 Training Loss: 0.21726485301441234 Test Loss: 0.2138192771384199\n",
      "Epoch: 599 Training Loss: 0.21726396028064504 Test Loss: 0.21377335007914544\n",
      "Epoch: 600 Training Loss: 0.21726123684687734 Test Loss: 0.21474652929874163\n",
      "Epoch: 601 Training Loss: 0.2172671581756337 Test Loss: 0.2139607786284114\n",
      "Epoch: 602 Training Loss: 0.21726334063620917 Test Loss: 0.21386897636090316\n",
      "Epoch: 603 Training Loss: 0.21726604145822687 Test Loss: 0.2139187663227076\n",
      "Epoch: 604 Training Loss: 0.21726495782462554 Test Loss: 0.21389049454278167\n",
      "Epoch: 605 Training Loss: 0.21726492425666677 Test Loss: 0.21386145796000583\n",
      "Epoch: 606 Training Loss: 0.21726503271596095 Test Loss: 0.21404109589041095\n",
      "Epoch: 607 Training Loss: 0.21726524191499474 Test Loss: 0.21380408478350324\n",
      "Epoch: 608 Training Loss: 0.2172639549549593 Test Loss: 0.21392023111460656\n",
      "Epoch: 609 Training Loss: 0.21726433620771346 Test Loss: 0.2140456328564697\n",
      "Epoch: 610 Training Loss: 0.21726561231912975 Test Loss: 0.2139404530204683\n",
      "Epoch: 611 Training Loss: 0.21726536927419737 Test Loss: 0.21381349574738504\n",
      "Epoch: 612 Training Loss: 0.21726439576752923 Test Loss: 0.21383953793256213\n",
      "Epoch: 613 Training Loss: 0.21726531606216867 Test Loss: 0.21377408895647498\n",
      "Epoch: 614 Training Loss: 0.21726587553711432 Test Loss: 0.21372743598263114\n",
      "Epoch: 615 Training Loss: 0.21726510393131798 Test Loss: 0.2137008234360067\n",
      "Epoch: 616 Training Loss: 0.21726600242112948 Test Loss: 0.21381470128408064\n",
      "Epoch: 617 Training Loss: 0.2172633071489426 Test Loss: 0.21390636096122703\n",
      "Epoch: 618 Training Loss: 0.21726251106650965 Test Loss: 0.2138418841921525\n",
      "Epoch: 619 Training Loss: 0.2172622244567504 Test Loss: 0.21380823286675693\n",
      "Epoch: 620 Training Loss: 0.2172663298521805 Test Loss: 0.2140110222868217\n",
      "Epoch: 621 Training Loss: 0.21726449290301683 Test Loss: 0.2139889337434958\n",
      "Epoch: 622 Training Loss: 0.21726528027069125 Test Loss: 0.21393874193612616\n",
      "Epoch: 623 Training Loss: 0.21726425559619703 Test Loss: 0.21382994549003798\n",
      "Epoch: 624 Training Loss: 0.21726495604939697 Test Loss: 0.21390095549023708\n",
      "Epoch: 625 Training Loss: 0.2172653828753174 Test Loss: 0.21383515651962542\n",
      "Epoch: 626 Training Loss: 0.21726578642601857 Test Loss: 0.21380779213291123\n",
      "Epoch: 627 Training Loss: 0.217265027713044 Test Loss: 0.21549551758123606\n",
      "Epoch: 628 Training Loss: 0.21726495314447744 Test Loss: 0.21386596900054422\n",
      "Epoch: 629 Training Loss: 0.21726603374763803 Test Loss: 0.21388402612545795\n",
      "Epoch: 630 Training Loss: 0.2172647020303242 Test Loss: 0.2138406268044162\n",
      "Epoch: 631 Training Loss: 0.21726569367480766 Test Loss: 0.21379304051184028\n",
      "Epoch: 632 Training Loss: 0.21726601931269848 Test Loss: 0.2138559617496947\n",
      "Epoch: 633 Training Loss: 0.21726572708138203 Test Loss: 0.21386968931271239\n",
      "Epoch: 634 Training Loss: 0.21726545175060039 Test Loss: 0.21381614015045927\n",
      "Epoch: 635 Training Loss: 0.2172665213796206 Test Loss: 0.21381586793249574\n",
      "Epoch: 636 Training Loss: 0.21726609775449107 Test Loss: 0.2138895093730089\n",
      "Epoch: 637 Training Loss: 0.2172657707986275 Test Loss: 0.213723793447024\n",
      "Epoch: 638 Training Loss: 0.2172653202312661 Test Loss: 0.21377907961913958\n",
      "Epoch: 639 Training Loss: 0.2172641654988632 Test Loss: 0.2137920294165472\n",
      "Epoch: 640 Training Loss: 0.2172632230497296 Test Loss: 0.21399544104910004\n",
      "Epoch: 641 Training Loss: 0.21726371603429534 Test Loss: 0.21388525758767388\n",
      "Epoch: 642 Training Loss: 0.21726366826450783 Test Loss: 0.21373560252153684\n",
      "Epoch: 643 Training Loss: 0.21726699956164894 Test Loss: 0.2141749752774238\n",
      "Epoch: 644 Training Loss: 0.2172657503924645 Test Loss: 0.21380967173313556\n",
      "Epoch: 645 Training Loss: 0.2172645964490521 Test Loss: 0.21370921034183524\n",
      "Epoch: 646 Training Loss: 0.2172659624246914 Test Loss: 0.21374271907686895\n",
      "Epoch: 647 Training Loss: 0.2172645029447139 Test Loss: 0.21373631547334607\n",
      "Epoch: 648 Training Loss: 0.21726671836723346 Test Loss: 0.2138117068864819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 649 Training Loss: 0.21726361023784402 Test Loss: 0.2191615935608474\n",
      "Epoch: 650 Training Loss: 0.21726592538696765 Test Loss: 0.2141519274898455\n",
      "Epoch: 651 Training Loss: 0.21726445853710175 Test Loss: 0.21382927142650923\n",
      "Epoch: 652 Training Loss: 0.2172647985113083 Test Loss: 0.21472770737097802\n",
      "Epoch: 653 Training Loss: 0.21726355206772738 Test Loss: 0.21481702078853404\n",
      "Epoch: 654 Training Loss: 0.2172651276996563 Test Loss: 0.21421305986679676\n",
      "Epoch: 655 Training Loss: 0.21726339457446775 Test Loss: 0.2139665211311657\n",
      "Epoch: 656 Training Loss: 0.21726380322670966 Test Loss: 0.21382511038049537\n",
      "Epoch: 657 Training Loss: 0.2172621827299127 Test Loss: 0.2137202935017787\n",
      "Epoch: 658 Training Loss: 0.21726658354848444 Test Loss: 0.21374273203962912\n",
      "Epoch: 659 Training Loss: 0.21726568325654694 Test Loss: 0.21382918068718806\n",
      "Epoch: 660 Training Loss: 0.21726462795487667 Test Loss: 0.21360246201185357\n",
      "Epoch: 661 Training Loss: 0.2172675366489896 Test Loss: 0.21376566316236592\n",
      "Epoch: 662 Training Loss: 0.21726601211316032 Test Loss: 0.21370595668903314\n",
      "Epoch: 663 Training Loss: 0.2172661175958086 Test Loss: 0.21368827548416428\n",
      "Epoch: 664 Training Loss: 0.2172668852028577 Test Loss: 0.2136914902486859\n",
      "Epoch: 665 Training Loss: 0.2172672282344024 Test Loss: 0.21420906733666506\n",
      "Epoch: 666 Training Loss: 0.21726754098843726 Test Loss: 0.21379178312410402\n",
      "Epoch: 667 Training Loss: 0.21726664790500372 Test Loss: 0.21370659186428134\n",
      "Epoch: 668 Training Loss: 0.21726615097548557 Test Loss: 0.21376187803639693\n",
      "Epoch: 669 Training Loss: 0.21726649391737224 Test Loss: 0.21373701546239512\n",
      "Epoch: 670 Training Loss: 0.21726610650511283 Test Loss: 0.2136998901172746\n",
      "Epoch: 671 Training Loss: 0.2172652809700237 Test Loss: 0.2137430561086333\n",
      "Epoch: 672 Training Loss: 0.21726608568652298 Test Loss: 0.2137362636223054\n",
      "Epoch: 673 Training Loss: 0.2172671272884494 Test Loss: 0.21379622935084155\n",
      "Epoch: 674 Training Loss: 0.21726570852217406 Test Loss: 0.21397177104903367\n",
      "Epoch: 675 Training Loss: 0.21726764103781016 Test Loss: 0.2142536851571626\n",
      "Epoch: 676 Training Loss: 0.2172644711071547 Test Loss: 0.21383562317899146\n",
      "Epoch: 677 Training Loss: 0.2172668121943405 Test Loss: 0.21895693750331846\n",
      "Epoch: 678 Training Loss: 0.2172667843375969 Test Loss: 0.21397753947730833\n",
      "Epoch: 679 Training Loss: 0.21726528443082288 Test Loss: 0.21378584617994717\n",
      "Epoch: 680 Training Loss: 0.21726601695469283 Test Loss: 0.2137074603692126\n",
      "Epoch: 681 Training Loss: 0.21726596132189788 Test Loss: 0.21368695328262716\n",
      "Epoch: 682 Training Loss: 0.21726622956969677 Test Loss: 0.21367927932860784\n",
      "Epoch: 683 Training Loss: 0.21726601471324258 Test Loss: 0.21375101524337634\n",
      "Epoch: 684 Training Loss: 0.2172657894295619 Test Loss: 0.21368966249950222\n",
      "Epoch: 685 Training Loss: 0.21726726827566947 Test Loss: 0.21373154517760434\n",
      "Epoch: 686 Training Loss: 0.21726648735440596 Test Loss: 0.21369224208877563\n",
      "Epoch: 687 Training Loss: 0.21726471385621565 Test Loss: 0.21744161676522514\n",
      "Epoch: 688 Training Loss: 0.21726690529521767 Test Loss: 0.2138650097562918\n",
      "Epoch: 689 Training Loss: 0.217266504828752 Test Loss: 0.21378640357863438\n",
      "Epoch: 690 Training Loss: 0.21726634647477547 Test Loss: 0.2138232567057914\n",
      "Epoch: 691 Training Loss: 0.21726560674240156 Test Loss: 0.21373947838682703\n",
      "Epoch: 692 Training Loss: 0.21726468640293312 Test Loss: 0.21367514420811431\n",
      "Epoch: 693 Training Loss: 0.21726734723747854 Test Loss: 0.2137062159442365\n",
      "Epoch: 694 Training Loss: 0.21726517728950143 Test Loss: 0.21373175258176702\n",
      "Epoch: 695 Training Loss: 0.21726581578901671 Test Loss: 0.21373008038570537\n",
      "Epoch: 696 Training Loss: 0.217265915820458 Test Loss: 0.21378646839243523\n",
      "Epoch: 697 Training Loss: 0.21726771601880357 Test Loss: 0.21383096954809122\n",
      "Epoch: 698 Training Loss: 0.2172636990261709 Test Loss: 0.2137412153966895\n",
      "Epoch: 699 Training Loss: 0.2172671134631843 Test Loss: 0.2138540302984297\n",
      "Epoch: 700 Training Loss: 0.21726598775307912 Test Loss: 0.2137769796519924\n",
      "Epoch: 701 Training Loss: 0.21726404513298533 Test Loss: 0.21378697394008175\n",
      "Epoch: 702 Training Loss: 0.21726606351409722 Test Loss: 0.2138337435787671\n",
      "Epoch: 703 Training Loss: 0.21726527210284657 Test Loss: 0.21373580992569954\n",
      "Epoch: 704 Training Loss: 0.21726575521606545 Test Loss: 0.2137212138577506\n",
      "Epoch: 705 Training Loss: 0.2172671918153189 Test Loss: 0.21381160318440054\n",
      "Epoch: 706 Training Loss: 0.21726594705730856 Test Loss: 0.2137108047613359\n",
      "Epoch: 707 Training Loss: 0.21726447245202482 Test Loss: 0.21370551595518741\n",
      "Epoch: 708 Training Loss: 0.21726616106201166 Test Loss: 0.21368560515556972\n",
      "Epoch: 709 Training Loss: 0.21726409648909323 Test Loss: 0.2142233004473293\n",
      "Epoch: 710 Training Loss: 0.21726705013773215 Test Loss: 0.21376934458625357\n",
      "Epoch: 711 Training Loss: 0.2172652884206043 Test Loss: 0.21374087836492514\n",
      "Epoch: 712 Training Loss: 0.21726467131349012 Test Loss: 0.21371328064852793\n",
      "Epoch: 713 Training Loss: 0.2172687875575394 Test Loss: 0.2138282862567365\n",
      "Epoch: 714 Training Loss: 0.21726437745936367 Test Loss: 0.213956267587873\n",
      "Epoch: 715 Training Loss: 0.2172639270175235 Test Loss: 0.21375897437811936\n",
      "Epoch: 716 Training Loss: 0.21726325239479613 Test Loss: 0.21493148196081555\n",
      "Epoch: 717 Training Loss: 0.2172634147654515 Test Loss: 0.213816736437427\n",
      "Epoch: 718 Training Loss: 0.21726205776457896 Test Loss: 0.2145238031535388\n",
      "Epoch: 719 Training Loss: 0.2172661307845018 Test Loss: 0.2138784521385858\n",
      "Epoch: 720 Training Loss: 0.21726597099599712 Test Loss: 0.21389367041902277\n",
      "Epoch: 721 Training Loss: 0.2172655193886028 Test Loss: 0.21378913872102978\n",
      "Epoch: 722 Training Loss: 0.21726711739917093 Test Loss: 0.2138238141044786\n",
      "Epoch: 723 Training Loss: 0.21726428124735359 Test Loss: 0.21377813333764734\n",
      "Epoch: 724 Training Loss: 0.21726459292549233 Test Loss: 0.21374675049528113\n",
      "Epoch: 725 Training Loss: 0.21726768011077072 Test Loss: 0.21381999009022912\n",
      "Epoch: 726 Training Loss: 0.217264870282545 Test Loss: 0.21371234732979585\n",
      "Epoch: 727 Training Loss: 0.2172671628557818 Test Loss: 0.21380229592260008\n",
      "Epoch: 728 Training Loss: 0.21726605375033997 Test Loss: 0.21374549310754487\n",
      "Epoch: 729 Training Loss: 0.21726590459527523 Test Loss: 0.21373704138791547\n",
      "Epoch: 730 Training Loss: 0.2172669479096696 Test Loss: 0.2137128528774424\n",
      "Epoch: 731 Training Loss: 0.21726601915131408 Test Loss: 0.21382435854040566\n",
      "Epoch: 732 Training Loss: 0.21726587757235113 Test Loss: 0.2136454983756106\n",
      "Epoch: 733 Training Loss: 0.2172680697913778 Test Loss: 0.21377935183710312\n",
      "Epoch: 734 Training Loss: 0.2172645386375675 Test Loss: 0.2137188027843594\n",
      "Epoch: 735 Training Loss: 0.21726556890672152 Test Loss: 0.21377049827190853\n",
      "Epoch: 736 Training Loss: 0.21726549085942418 Test Loss: 0.21373925801990415\n",
      "Epoch: 737 Training Loss: 0.2172647985740689 Test Loss: 0.21373995800895323\n",
      "Epoch: 738 Training Loss: 0.21726388535344643 Test Loss: 0.2141110818325568\n",
      "Epoch: 739 Training Loss: 0.21726459514004515 Test Loss: 0.21372589341417117\n",
      "Epoch: 740 Training Loss: 0.21726496504209533 Test Loss: 0.2137382339618509\n",
      "Epoch: 741 Training Loss: 0.2172654612991784 Test Loss: 0.21379184793790484\n",
      "Epoch: 742 Training Loss: 0.21726570183368654 Test Loss: 0.2137189713002416\n",
      "Epoch: 743 Training Loss: 0.21726562612646325 Test Loss: 0.2137758907801383\n",
      "Epoch: 744 Training Loss: 0.21726340755694754 Test Loss: 0.21569284967927022\n",
      "Epoch: 745 Training Loss: 0.21726597292364433 Test Loss: 0.2138541988143119\n",
      "Epoch: 746 Training Loss: 0.2172650060337373 Test Loss: 0.21375721144273654\n",
      "Epoch: 747 Training Loss: 0.2172650809250727 Test Loss: 0.21369779015012744\n",
      "Epoch: 748 Training Loss: 0.21726470180617916 Test Loss: 0.21371754539662313\n",
      "Epoch: 749 Training Loss: 0.21726568104199412 Test Loss: 0.2137063714973585\n",
      "Epoch: 750 Training Loss: 0.2172664074601536 Test Loss: 0.21372957483805882\n",
      "Epoch: 751 Training Loss: 0.21726397536112227 Test Loss: 0.21378050552275804\n",
      "Epoch: 752 Training Loss: 0.21726355724099455 Test Loss: 0.21371535469015476\n",
      "Epoch: 753 Training Loss: 0.21726347673706772 Test Loss: 0.21382845477261866\n",
      "Epoch: 754 Training Loss: 0.21726529361180308 Test Loss: 0.21382975104863544\n",
      "Epoch: 755 Training Loss: 0.21726497245681273 Test Loss: 0.2137791962839811\n",
      "Epoch: 756 Training Loss: 0.21726558153056927 Test Loss: 0.2137831369630721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757 Training Loss: 0.2172660260639466 Test Loss: 0.21372169347987682\n",
      "Epoch: 758 Training Loss: 0.21726488780172007 Test Loss: 0.21374175983261653\n",
      "Epoch: 759 Training Loss: 0.2172654755099729 Test Loss: 0.21374699678772432\n",
      "Epoch: 760 Training Loss: 0.21726558857768882 Test Loss: 0.21370880849627005\n",
      "Epoch: 761 Training Loss: 0.2172652151789763 Test Loss: 0.21376224099368163\n",
      "Epoch: 762 Training Loss: 0.2172666252125615 Test Loss: 0.21375143005170172\n",
      "Epoch: 763 Training Loss: 0.2172663242933839 Test Loss: 0.2137457264372279\n",
      "Epoch: 764 Training Loss: 0.21726551006416983 Test Loss: 0.2138778947398986\n",
      "Epoch: 765 Training Loss: 0.21726487834280006 Test Loss: 0.21382412521072264\n",
      "Epoch: 766 Training Loss: 0.2172657987360633 Test Loss: 0.21371976202861181\n",
      "Epoch: 767 Training Loss: 0.2172651788316192 Test Loss: 0.2137313507362018\n",
      "Epoch: 768 Training Loss: 0.2172643123945461 Test Loss: 0.21382053452615615\n",
      "Epoch: 769 Training Loss: 0.21726543582733787 Test Loss: 0.2138682763718541\n",
      "Epoch: 770 Training Loss: 0.21726456451286907 Test Loss: 0.21378187957533584\n",
      "Epoch: 771 Training Loss: 0.21726454897513603 Test Loss: 0.21381632162910164\n",
      "Epoch: 772 Training Loss: 0.21726442138282256 Test Loss: 0.21376962976697728\n",
      "Epoch: 773 Training Loss: 0.21726495679355845 Test Loss: 0.21369391428483725\n",
      "Epoch: 774 Training Loss: 0.21726369495569728 Test Loss: 0.21367620715444807\n",
      "Epoch: 775 Training Loss: 0.21726407336629255 Test Loss: 0.213789773896278\n",
      "Epoch: 776 Training Loss: 0.21726374034058174 Test Loss: 0.213890598244863\n",
      "Epoch: 777 Training Loss: 0.21726103840680472 Test Loss: 0.21413778511850246\n",
      "Epoch: 778 Training Loss: 0.21726353130293236 Test Loss: 0.21404810874366173\n",
      "Epoch: 779 Training Loss: 0.2172641598414428 Test Loss: 0.2139047276534459\n",
      "Epoch: 780 Training Loss: 0.21726401371681878 Test Loss: 0.2137053992903459\n",
      "Epoch: 781 Training Loss: 0.2172642373418263 Test Loss: 0.21372711191362695\n",
      "Epoch: 782 Training Loss: 0.2172639353557184 Test Loss: 0.21373954320062785\n",
      "Epoch: 783 Training Loss: 0.21726527944583754 Test Loss: 0.21376729647014708\n",
      "Epoch: 784 Training Loss: 0.2172631061625825 Test Loss: 0.2137414616891327\n",
      "Epoch: 785 Training Loss: 0.21726557169508562 Test Loss: 0.21487518469340688\n",
      "Epoch: 786 Training Loss: 0.21726496835047587 Test Loss: 0.21390467580240521\n",
      "Epoch: 787 Training Loss: 0.2172631322530633 Test Loss: 0.21385593582417436\n",
      "Epoch: 788 Training Loss: 0.21726318799344785 Test Loss: 0.21372661932874057\n",
      "Epoch: 789 Training Loss: 0.21726456167071018 Test Loss: 0.21378930723691197\n",
      "Epoch: 790 Training Loss: 0.21726406037484697 Test Loss: 0.21377447783928002\n",
      "Epoch: 791 Training Loss: 0.21726446427521437 Test Loss: 0.21418827506935595\n",
      "Epoch: 792 Training Loss: 0.2172642085347078 Test Loss: 0.21374811158509877\n",
      "Epoch: 793 Training Loss: 0.21726587955379315 Test Loss: 0.21374518200130083\n",
      "Epoch: 794 Training Loss: 0.2172631567476315 Test Loss: 0.21373507104836997\n",
      "Epoch: 795 Training Loss: 0.21726283847066327 Test Loss: 0.21375949288852608\n",
      "Epoch: 796 Training Loss: 0.21726578795020474 Test Loss: 0.21383973237396464\n",
      "Epoch: 797 Training Loss: 0.21726491316597096 Test Loss: 0.2137771870561551\n",
      "Epoch: 798 Training Loss: 0.21726494746015965 Test Loss: 0.21376360208349926\n",
      "Epoch: 799 Training Loss: 0.21726368549677727 Test Loss: 0.2137089121983514\n",
      "Epoch: 800 Training Loss: 0.21726663280659492 Test Loss: 0.21373954320062785\n",
      "Epoch: 801 Training Loss: 0.21726437440202553 Test Loss: 0.21379576269147552\n",
      "Epoch: 802 Training Loss: 0.21726551569469282 Test Loss: 0.21374977081840024\n",
      "Epoch: 803 Training Loss: 0.21726551201871444 Test Loss: 0.21373052111955107\n",
      "Epoch: 804 Training Loss: 0.21726388581966807 Test Loss: 0.21664287740920676\n",
      "Epoch: 805 Training Loss: 0.21726714283514825 Test Loss: 0.21377438709995886\n",
      "Epoch: 806 Training Loss: 0.2172662531587191 Test Loss: 0.2137721186169295\n",
      "Epoch: 807 Training Loss: 0.2172646113322817 Test Loss: 0.21367988857833572\n",
      "Epoch: 808 Training Loss: 0.21726910216749504 Test Loss: 0.21372718969018795\n",
      "Epoch: 809 Training Loss: 0.21726586863344757 Test Loss: 0.2137499393342824\n",
      "Epoch: 810 Training Loss: 0.21726515880201985 Test Loss: 0.21370642334839918\n",
      "Epoch: 811 Training Loss: 0.21726570047088478 Test Loss: 0.2137453634799432\n",
      "Epoch: 812 Training Loss: 0.21726486064430894 Test Loss: 0.2136723312891579\n",
      "Epoch: 813 Training Loss: 0.21726629300273856 Test Loss: 0.21366092406021026\n",
      "Epoch: 814 Training Loss: 0.21726664345796645 Test Loss: 0.21373479883040644\n",
      "Epoch: 815 Training Loss: 0.21726528593707745 Test Loss: 0.2137435486935197\n",
      "Epoch: 816 Training Loss: 0.2172640299987133 Test Loss: 0.21359023812901534\n",
      "Epoch: 817 Training Loss: 0.21726381035452141 Test Loss: 0.2142969289250823\n",
      "Epoch: 818 Training Loss: 0.21726494872433758 Test Loss: 0.2137633039400154\n",
      "Epoch: 819 Training Loss: 0.21726540467117955 Test Loss: 0.213832188047547\n",
      "Epoch: 820 Training Loss: 0.21726430885305473 Test Loss: 0.21385268217137227\n",
      "Epoch: 821 Training Loss: 0.21726278875529695 Test Loss: 0.2139209829546963\n",
      "Epoch: 822 Training Loss: 0.21726331340707167 Test Loss: 0.2137675297998301\n",
      "Epoch: 823 Training Loss: 0.21726576281009885 Test Loss: 0.2137698630966603\n",
      "Epoch: 824 Training Loss: 0.21726327106159374 Test Loss: 0.21391597932927153\n",
      "Epoch: 825 Training Loss: 0.21726422474487594 Test Loss: 0.21387999470704577\n",
      "Epoch: 826 Training Loss: 0.21726447472933827 Test Loss: 0.21381912158529787\n",
      "Epoch: 827 Training Loss: 0.21726479975755464 Test Loss: 0.2138530321658968\n",
      "Epoch: 828 Training Loss: 0.21726427003113657 Test Loss: 0.21372668414254142\n",
      "Epoch: 829 Training Loss: 0.21726501745616772 Test Loss: 0.21376090582938437\n",
      "Epoch: 830 Training Loss: 0.21726492200625072 Test Loss: 0.21369110136588085\n",
      "Epoch: 831 Training Loss: 0.21726550001350695 Test Loss: 0.21376098360594536\n",
      "Epoch: 832 Training Loss: 0.21726456166174438 Test Loss: 0.21374336721487736\n",
      "Epoch: 833 Training Loss: 0.21726530823502443 Test Loss: 0.21375973918096927\n",
      "Epoch: 834 Training Loss: 0.217264511542917 Test Loss: 0.21371318990920676\n",
      "Epoch: 835 Training Loss: 0.21726690247099034 Test Loss: 0.2138133531570232\n",
      "Epoch: 836 Training Loss: 0.21726455216696117 Test Loss: 0.21395320837647339\n",
      "Epoch: 837 Training Loss: 0.21726547843282404 Test Loss: 0.21381563460281272\n",
      "Epoch: 838 Training Loss: 0.21726606881288557 Test Loss: 0.21386393384719787\n",
      "Epoch: 839 Training Loss: 0.21726429238287837 Test Loss: 0.21375574665083757\n",
      "Epoch: 840 Training Loss: 0.21726497297682917 Test Loss: 0.21375667996956965\n",
      "Epoch: 841 Training Loss: 0.2172651041285656 Test Loss: 0.21378714245596395\n",
      "Epoch: 842 Training Loss: 0.2172647575286321 Test Loss: 0.21368712179850935\n",
      "Epoch: 843 Training Loss: 0.21726759081139319 Test Loss: 0.21379864042423277\n",
      "Epoch: 844 Training Loss: 0.2172654414399293 Test Loss: 0.21376191692467744\n",
      "Epoch: 845 Training Loss: 0.21726462025325363 Test Loss: 0.21370209378650312\n",
      "Epoch: 846 Training Loss: 0.21726503182834667 Test Loss: 0.21376068546246152\n",
      "Epoch: 847 Training Loss: 0.2172646984619354 Test Loss: 0.21372096756530742\n",
      "Epoch: 848 Training Loss: 0.21726825092745458 Test Loss: 0.21377731668375677\n",
      "Epoch: 849 Training Loss: 0.21726596188674335 Test Loss: 0.21370739555541177\n",
      "Epoch: 850 Training Loss: 0.21726387056884067 Test Loss: 0.21374362647008072\n",
      "Epoch: 851 Training Loss: 0.21726426264331658 Test Loss: 0.21369535315121588\n",
      "Epoch: 852 Training Loss: 0.21726521187956152 Test Loss: 0.2143518910281937\n",
      "Epoch: 853 Training Loss: 0.21726559089086547 Test Loss: 0.21381346982186472\n",
      "Epoch: 854 Training Loss: 0.21726411928015926 Test Loss: 0.2138435693509743\n",
      "Epoch: 855 Training Loss: 0.21726508783770526 Test Loss: 0.21383480652510087\n",
      "Epoch: 856 Training Loss: 0.2172647938132286 Test Loss: 0.21374594680415074\n",
      "Epoch: 857 Training Loss: 0.21726521046296499 Test Loss: 0.21372510268580094\n",
      "Epoch: 858 Training Loss: 0.21726472435516858 Test Loss: 0.21372991186982318\n",
      "Epoch: 859 Training Loss: 0.21726675579048665 Test Loss: 0.21383076214392854\n",
      "Epoch: 860 Training Loss: 0.21726495701770346 Test Loss: 0.21377550189733327\n",
      "Epoch: 861 Training Loss: 0.2172652013268138 Test Loss: 0.21379580157975603\n",
      "Epoch: 862 Training Loss: 0.21726330563372223 Test Loss: 0.21387372073112457\n",
      "Epoch: 863 Training Loss: 0.21726496159026196 Test Loss: 0.2138323047123885\n",
      "Epoch: 864 Training Loss: 0.21726349812946882 Test Loss: 0.213810942083632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 865 Training Loss: 0.2172655585601872 Test Loss: 0.21383426208917383\n",
      "Epoch: 866 Training Loss: 0.21726473001258897 Test Loss: 0.2137462838359151\n",
      "Epoch: 867 Training Loss: 0.2172650640872985 Test Loss: 0.2137585206815135\n",
      "Epoch: 868 Training Loss: 0.21726503851683418 Test Loss: 0.21374098206700648\n",
      "Epoch: 869 Training Loss: 0.21726387083781468 Test Loss: 0.21371186770766964\n",
      "Epoch: 870 Training Loss: 0.21726131726114614 Test Loss: 0.21475892169746202\n",
      "Epoch: 871 Training Loss: 0.21726850648864512 Test Loss: 0.21387234667854677\n",
      "Epoch: 872 Training Loss: 0.21726601876578464 Test Loss: 0.21387431701809229\n",
      "Epoch: 873 Training Loss: 0.21726429842582823 Test Loss: 0.21379922374844032\n",
      "Epoch: 874 Training Loss: 0.21726524504405928 Test Loss: 0.21373958208890836\n",
      "Epoch: 875 Training Loss: 0.21726545925497578 Test Loss: 0.2138978055395163\n",
      "Epoch: 876 Training Loss: 0.21726398542971675 Test Loss: 0.21397109698550493\n",
      "Epoch: 877 Training Loss: 0.21726526478675298 Test Loss: 0.21382822144293565\n",
      "Epoch: 878 Training Loss: 0.21726386501900985 Test Loss: 0.2137857943289065\n",
      "Epoch: 879 Training Loss: 0.21726584986802616 Test Loss: 0.2138648542031698\n",
      "Epoch: 880 Training Loss: 0.21726470893399094 Test Loss: 0.2137057363221103\n",
      "Epoch: 881 Training Loss: 0.21726325970192392 Test Loss: 0.21371430470658118\n",
      "Epoch: 882 Training Loss: 0.2172639309176469 Test Loss: 0.21363263931752416\n",
      "Epoch: 883 Training Loss: 0.21726603461732072 Test Loss: 0.21367165722562917\n",
      "Epoch: 884 Training Loss: 0.21726405180354125 Test Loss: 0.2138158031186949\n",
      "Epoch: 885 Training Loss: 0.2172644635669161 Test Loss: 0.21377471116896304\n",
      "Epoch: 886 Training Loss: 0.21726504955373516 Test Loss: 0.21369011619610812\n",
      "Epoch: 887 Training Loss: 0.21726471861705596 Test Loss: 0.21368578663421206\n",
      "Epoch: 888 Training Loss: 0.21726604434521476 Test Loss: 0.21363352078521558\n",
      "Epoch: 889 Training Loss: 0.21726410796531845 Test Loss: 0.21411818542512875\n",
      "Epoch: 890 Training Loss: 0.21726427480094268 Test Loss: 0.21447967791792769\n",
      "Epoch: 891 Training Loss: 0.21726260678540069 Test Loss: 0.21401055562745566\n",
      "Epoch: 892 Training Loss: 0.21726605514900493 Test Loss: 0.2138164901449838\n",
      "Epoch: 893 Training Loss: 0.21726317579995855 Test Loss: 0.21376122989838856\n",
      "Epoch: 894 Training Loss: 0.21726267582206807 Test Loss: 0.21368684958054582\n",
      "Epoch: 895 Training Loss: 0.21726476315018933 Test Loss: 0.213727500796432\n",
      "Epoch: 896 Training Loss: 0.21726620311161815 Test Loss: 0.21374233019406394\n",
      "Epoch: 897 Training Loss: 0.21726347571496643 Test Loss: 0.2137174416945418\n",
      "Epoch: 898 Training Loss: 0.2172663110150327 Test Loss: 0.21364282804701604\n",
      "Epoch: 899 Training Loss: 0.21726653688149045 Test Loss: 0.21375415223133695\n",
      "Epoch: 900 Training Loss: 0.2172656202000688 Test Loss: 0.2137638224504221\n",
      "Epoch: 901 Training Loss: 0.21726522126675513 Test Loss: 0.21387714289980886\n",
      "Epoch: 902 Training Loss: 0.21726479437807406 Test Loss: 0.2137556559115164\n",
      "Epoch: 903 Training Loss: 0.2172646889492206 Test Loss: 0.21376806127299697\n",
      "Epoch: 904 Training Loss: 0.21726516272904067 Test Loss: 0.2137642372587475\n",
      "Epoch: 905 Training Loss: 0.21726597045804907 Test Loss: 0.21379406456989355\n",
      "Epoch: 906 Training Loss: 0.21726367350950138 Test Loss: 0.2140011317008137\n",
      "Epoch: 907 Training Loss: 0.2172632070547407 Test Loss: 0.21382146784488823\n",
      "Epoch: 908 Training Loss: 0.21726416511333377 Test Loss: 0.21369576795954126\n",
      "Epoch: 909 Training Loss: 0.21726400801456935 Test Loss: 0.21372529712720345\n",
      "Epoch: 910 Training Loss: 0.2172666766583274 Test Loss: 0.21375319298708453\n",
      "Epoch: 911 Training Loss: 0.21726725008405934 Test Loss: 0.21377355748330812\n",
      "Epoch: 912 Training Loss: 0.21726528679779433 Test Loss: 0.2137382858128916\n",
      "Epoch: 913 Training Loss: 0.21726529633740657 Test Loss: 0.21369359021583306\n",
      "Epoch: 914 Training Loss: 0.21726533853943167 Test Loss: 0.21374094317872597\n",
      "Epoch: 915 Training Loss: 0.2172650200203868 Test Loss: 0.21372931558285546\n",
      "Epoch: 916 Training Loss: 0.21726462028015103 Test Loss: 0.21369794570324943\n",
      "Epoch: 917 Training Loss: 0.21726480350525945 Test Loss: 0.21373573214913852\n",
      "Epoch: 918 Training Loss: 0.2172634531839086 Test Loss: 0.21374258944926727\n",
      "Epoch: 919 Training Loss: 0.21726561258810378 Test Loss: 0.21418174183823138\n",
      "Epoch: 920 Training Loss: 0.2172637328182747 Test Loss: 0.21376392615250345\n",
      "Epoch: 921 Training Loss: 0.21726254289510305 Test Loss: 0.21373758582384253\n",
      "Epoch: 922 Training Loss: 0.21726585817035785 Test Loss: 0.2137564596026468\n",
      "Epoch: 923 Training Loss: 0.21726470091856487 Test Loss: 0.21371987869345332\n",
      "Epoch: 924 Training Loss: 0.21726484418309838 Test Loss: 0.2137598039947701\n",
      "Epoch: 925 Training Loss: 0.2172658092260504 Test Loss: 0.21379003315148137\n",
      "Epoch: 926 Training Loss: 0.21726375481138446 Test Loss: 0.2137383117384119\n",
      "Epoch: 927 Training Loss: 0.21726433984782864 Test Loss: 0.21383336765872227\n",
      "Epoch: 928 Training Loss: 0.21726382761368826 Test Loss: 0.21375897437811936\n",
      "Epoch: 929 Training Loss: 0.21726554424180308 Test Loss: 0.21370541225310608\n",
      "Epoch: 930 Training Loss: 0.21726417209769272 Test Loss: 0.21387782992609775\n",
      "Epoch: 931 Training Loss: 0.2172662371816618 Test Loss: 0.21382793626221197\n",
      "Epoch: 932 Training Loss: 0.21726469015960373 Test Loss: 0.21364334655742276\n",
      "Epoch: 933 Training Loss: 0.21726545818804546 Test Loss: 0.21364551133837076\n",
      "Epoch: 934 Training Loss: 0.21726487993871263 Test Loss: 0.213683388523581\n",
      "Epoch: 935 Training Loss: 0.21726395479357485 Test Loss: 0.2136887421435303\n",
      "Epoch: 936 Training Loss: 0.21726600945031743 Test Loss: 0.2137951664045078\n",
      "Epoch: 937 Training Loss: 0.217264064337731 Test Loss: 0.21373816914805008\n",
      "Epoch: 938 Training Loss: 0.21726480706468243 Test Loss: 0.21368997360574626\n",
      "Epoch: 939 Training Loss: 0.21726298025583962 Test Loss: 0.21470948173018212\n",
      "Epoch: 940 Training Loss: 0.2172646506204215 Test Loss: 0.21379446641545874\n",
      "Epoch: 941 Training Loss: 0.2172636748095425 Test Loss: 0.21374209686438092\n",
      "Epoch: 942 Training Loss: 0.21726401813695864 Test Loss: 0.21368366074154455\n",
      "Epoch: 943 Training Loss: 0.21726261052413967 Test Loss: 0.21374590791587023\n",
      "Epoch: 944 Training Loss: 0.2172640814265476 Test Loss: 0.21376415948218647\n",
      "Epoch: 945 Training Loss: 0.21726019134482752 Test Loss: 0.21445603384338166\n",
      "Epoch: 946 Training Loss: 0.21726634413470144 Test Loss: 0.21379081091709143\n",
      "Epoch: 947 Training Loss: 0.21726509029433472 Test Loss: 0.2138877853259066\n",
      "Epoch: 948 Training Loss: 0.21726366654307402 Test Loss: 0.2137668816618217\n",
      "Epoch: 949 Training Loss: 0.21726495156649647 Test Loss: 0.21378010367719286\n",
      "Epoch: 950 Training Loss: 0.21726371234935113 Test Loss: 0.2137695779159366\n",
      "Epoch: 951 Training Loss: 0.21726397022371832 Test Loss: 0.2137998329981682\n",
      "Epoch: 952 Training Loss: 0.21726387887117235 Test Loss: 0.2137175583593833\n",
      "Epoch: 953 Training Loss: 0.2172643609712557 Test Loss: 0.21365520748297626\n",
      "Epoch: 954 Training Loss: 0.21726392024834376 Test Loss: 0.21396098603257407\n",
      "Epoch: 955 Training Loss: 0.21726531292413834 Test Loss: 0.21377232602109217\n",
      "Epoch: 956 Training Loss: 0.2172652874702294 Test Loss: 0.21374912268039184\n",
      "Epoch: 957 Training Loss: 0.21726609557580145 Test Loss: 0.21378785540777318\n",
      "Epoch: 958 Training Loss: 0.21726473419065223 Test Loss: 0.213731441475523\n",
      "Epoch: 959 Training Loss: 0.21726497194576205 Test Loss: 0.2136998901172746\n",
      "Epoch: 960 Training Loss: 0.21726439476335951 Test Loss: 0.2136765182606921\n",
      "Epoch: 961 Training Loss: 0.21726465561437264 Test Loss: 0.21360568973913535\n",
      "Epoch: 962 Training Loss: 0.2172664448027146 Test Loss: 0.21372888781176994\n",
      "Epoch: 963 Training Loss: 0.21726460349617166 Test Loss: 0.21393231240708294\n",
      "Epoch: 964 Training Loss: 0.21726364717694396 Test Loss: 0.2137449227460975\n",
      "Epoch: 965 Training Loss: 0.21726498061569158 Test Loss: 0.21376517057747957\n",
      "Epoch: 966 Training Loss: 0.21726454982688712 Test Loss: 0.213712126962873\n",
      "Epoch: 967 Training Loss: 0.21726523087809377 Test Loss: 0.21396925627356111\n",
      "Epoch: 968 Training Loss: 0.2172626090268509 Test Loss: 0.2152248162606523\n",
      "Epoch: 969 Training Loss: 0.21726443246455257 Test Loss: 0.21404967723764204\n",
      "Epoch: 970 Training Loss: 0.21726211168490592 Test Loss: 0.21480479690569582\n",
      "Epoch: 971 Training Loss: 0.21726503607813633 Test Loss: 0.21400439831637597\n",
      "Epoch: 972 Training Loss: 0.21726312529560174 Test Loss: 0.21385357660182383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 973 Training Loss: 0.21726467258663384 Test Loss: 0.21377748519963896\n",
      "Epoch: 974 Training Loss: 0.21726377959285834 Test Loss: 0.21373118222031964\n",
      "Epoch: 975 Training Loss: 0.21726576217352697 Test Loss: 0.21549970455277026\n",
      "Epoch: 976 Training Loss: 0.21726403302915404 Test Loss: 0.21374323758727568\n",
      "Epoch: 977 Training Loss: 0.21726678645352596 Test Loss: 0.21377399821715382\n",
      "Epoch: 978 Training Loss: 0.217265208795326 Test Loss: 0.2137925479269539\n",
      "Epoch: 979 Training Loss: 0.2172646824579807 Test Loss: 0.21369651979963097\n",
      "Epoch: 980 Training Loss: 0.21726512425678876 Test Loss: 0.2137635891207391\n",
      "Epoch: 981 Training Loss: 0.2172655828306104 Test Loss: 0.21374660790491928\n",
      "Epoch: 982 Training Loss: 0.21726454936066547 Test Loss: 0.21509551272797867\n",
      "Epoch: 983 Training Loss: 0.2172592972751559 Test Loss: 0.21486261081604413\n",
      "Epoch: 984 Training Loss: 0.2172628967663011 Test Loss: 0.21439420147738134\n",
      "Epoch: 985 Training Loss: 0.21726434329966202 Test Loss: 0.2139121293895017\n",
      "Epoch: 986 Training Loss: 0.21726420879471603 Test Loss: 0.21393390682658356\n",
      "Epoch: 987 Training Loss: 0.21726267034396368 Test Loss: 0.21385788023819954\n",
      "Epoch: 988 Training Loss: 0.21726420250072376 Test Loss: 0.21370466041301636\n",
      "Epoch: 989 Training Loss: 0.2172646170255653 Test Loss: 0.21375337446572687\n",
      "Epoch: 990 Training Loss: 0.21726455461462482 Test Loss: 0.21369781607564778\n",
      "Epoch: 991 Training Loss: 0.21726532538660168 Test Loss: 0.2137542040823776\n",
      "Epoch: 992 Training Loss: 0.21726408466320177 Test Loss: 0.21369766052252576\n",
      "Epoch: 993 Training Loss: 0.21726496991052524 Test Loss: 0.21364596503497663\n",
      "Epoch: 994 Training Loss: 0.21726540580087048 Test Loss: 0.21362456351793963\n",
      "Epoch: 995 Training Loss: 0.21726347151000577 Test Loss: 0.21364636688054184\n",
      "Epoch: 996 Training Loss: 0.21726214200724478 Test Loss: 0.21374575236274823\n",
      "Epoch: 997 Training Loss: 0.2172633520765712 Test Loss: 0.2136977642246071\n",
      "Epoch: 998 Training Loss: 0.21726438114430788 Test Loss: 0.21373221924113306\n",
      "Epoch: 999 Training Loss: 0.2172654153853117 Test Loss: 0.213689105100815\n",
      "Epoch: 1000 Training Loss: 0.21726483343310304 Test Loss: 0.21375600590604094\n",
      "Epoch: 1001 Training Loss: 0.2172646579723783 Test Loss: 0.21375767810210258\n",
      "Epoch: 1002 Training Loss: 0.21726475111808444 Test Loss: 0.21370458263645534\n",
      "Epoch: 1003 Training Loss: 0.21726455786921056 Test Loss: 0.21367287572508495\n",
      "Epoch: 1004 Training Loss: 0.21726500813173474 Test Loss: 0.21374868194654614\n",
      "Epoch: 1005 Training Loss: 0.2172650246198427 Test Loss: 0.21383229174962834\n",
      "Epoch: 1006 Training Loss: 0.21726544950018434 Test Loss: 0.2136965586879115\n",
      "Epoch: 1007 Training Loss: 0.2172647555292585 Test Loss: 0.21365910927378676\n",
      "Epoch: 1008 Training Loss: 0.21726473792042542 Test Loss: 0.21374043763107944\n",
      "Epoch: 1009 Training Loss: 0.21726459391173042 Test Loss: 0.21392759396238187\n",
      "Epoch: 1010 Training Loss: 0.2172656043664643 Test Loss: 0.2138490396357651\n",
      "Epoch: 1011 Training Loss: 0.21726372752845216 Test Loss: 0.21379376642640968\n",
      "Epoch: 1012 Training Loss: 0.21726348840157478 Test Loss: 0.21375731514481788\n",
      "Epoch: 1013 Training Loss: 0.2172660409651078 Test Loss: 0.21369014212162843\n",
      "Epoch: 1014 Training Loss: 0.21726466157663027 Test Loss: 0.2137219786606005\n",
      "Epoch: 1015 Training Loss: 0.21726437142537963 Test Loss: 0.21372833041308273\n",
      "Epoch: 1016 Training Loss: 0.21726567882744127 Test Loss: 0.2138074032501062\n",
      "Epoch: 1017 Training Loss: 0.2172633652294012 Test Loss: 0.2137390376529813\n",
      "Epoch: 1018 Training Loss: 0.21726607489169864 Test Loss: 0.21382035304751382\n",
      "Epoch: 1019 Training Loss: 0.2172642777327596 Test Loss: 0.21376916310761124\n",
      "Epoch: 1020 Training Loss: 0.21726518787811236 Test Loss: 0.2137192824064856\n",
      "Epoch: 1021 Training Loss: 0.21726444436217043 Test Loss: 0.2141279852718156\n",
      "Epoch: 1022 Training Loss: 0.217263806454398 Test Loss: 0.21410679115894127\n",
      "Epoch: 1023 Training Loss: 0.21726586872310558 Test Loss: 0.21390573874873897\n",
      "Epoch: 1024 Training Loss: 0.2172640813458554 Test Loss: 0.21387320222071785\n",
      "Epoch: 1025 Training Loss: 0.2172653266956086 Test Loss: 0.21375564294875624\n",
      "Epoch: 1026 Training Loss: 0.21726458487420305 Test Loss: 0.21370180860577945\n",
      "Epoch: 1027 Training Loss: 0.21726491309424456 Test Loss: 0.21386270238498195\n",
      "Epoch: 1028 Training Loss: 0.2172642569769304 Test Loss: 0.21376714091702506\n",
      "Epoch: 1029 Training Loss: 0.2172623290876476 Test Loss: 0.21376168359499442\n",
      "Epoch: 1030 Training Loss: 0.217263720373743 Test Loss: 0.21375073006265266\n",
      "Epoch: 1031 Training Loss: 0.21726652267966173 Test Loss: 0.2138524618044494\n",
      "Epoch: 1032 Training Loss: 0.2172637251614807 Test Loss: 0.2137545670396623\n",
      "Epoch: 1033 Training Loss: 0.21726228432140338 Test Loss: 0.21381039764770496\n",
      "Epoch: 1034 Training Loss: 0.21726501557334954 Test Loss: 0.21377411488199533\n",
      "Epoch: 1035 Training Loss: 0.21726540815887613 Test Loss: 0.21386293571466497\n",
      "Epoch: 1036 Training Loss: 0.21726312932124636 Test Loss: 0.21377731668375677\n",
      "Epoch: 1037 Training Loss: 0.2172668842704144 Test Loss: 0.2138955889075276\n",
      "Epoch: 1038 Training Loss: 0.21726298799332586 Test Loss: 0.21389482410467772\n",
      "Epoch: 1039 Training Loss: 0.21726167918363345 Test Loss: 0.21473014436988958\n",
      "Epoch: 1040 Training Loss: 0.21726916413911126 Test Loss: 0.21396356562184746\n",
      "Epoch: 1041 Training Loss: 0.21726504653226023 Test Loss: 0.21393023836545608\n",
      "Epoch: 1042 Training Loss: 0.21726431190142706 Test Loss: 0.2138612246303228\n",
      "Epoch: 1043 Training Loss: 0.2172657838617995 Test Loss: 0.21383279729727486\n",
      "Epoch: 1044 Training Loss: 0.2172653995875704 Test Loss: 0.2137904609225669\n",
      "Epoch: 1045 Training Loss: 0.21726562651199266 Test Loss: 0.21376111323354705\n",
      "Epoch: 1046 Training Loss: 0.2172646329757252 Test Loss: 0.21369995493107544\n",
      "Epoch: 1047 Training Loss: 0.21726511923594022 Test Loss: 0.21369833458605447\n",
      "Epoch: 1048 Training Loss: 0.21726436324856915 Test Loss: 0.21372699524878544\n",
      "Epoch: 1049 Training Loss: 0.21726527164559073 Test Loss: 0.21365156494736912\n",
      "Epoch: 1050 Training Loss: 0.21726597584649543 Test Loss: 0.21373824692461107\n",
      "Epoch: 1051 Training Loss: 0.21726569614040292 Test Loss: 0.2137635891207391\n",
      "Epoch: 1052 Training Loss: 0.21726386010575094 Test Loss: 0.21371898426300176\n",
      "Epoch: 1053 Training Loss: 0.21726330482680015 Test Loss: 0.2137717556596448\n",
      "Epoch: 1054 Training Loss: 0.21726472781596776 Test Loss: 0.213708497390026\n",
      "Epoch: 1055 Training Loss: 0.21726323377282755 Test Loss: 0.2136942253910813\n",
      "Epoch: 1056 Training Loss: 0.21726819044416132 Test Loss: 0.21376882607584688\n",
      "Epoch: 1057 Training Loss: 0.21726377824798818 Test Loss: 0.21375286891808035\n",
      "Epoch: 1058 Training Loss: 0.21726306440884743 Test Loss: 0.21371158252694594\n",
      "Epoch: 1059 Training Loss: 0.21726505516632655 Test Loss: 0.21370663075256185\n",
      "Epoch: 1060 Training Loss: 0.21726568619732967 Test Loss: 0.21393581235232823\n",
      "Epoch: 1061 Training Loss: 0.21726498036464917 Test Loss: 0.21376742609774876\n",
      "Epoch: 1062 Training Loss: 0.21726427505198512 Test Loss: 0.2137292637318148\n",
      "Epoch: 1063 Training Loss: 0.21726466158559607 Test Loss: 0.2138063532665326\n",
      "Epoch: 1064 Training Loss: 0.21726506447282795 Test Loss: 0.2137816073573723\n",
      "Epoch: 1065 Training Loss: 0.2172655751558848 Test Loss: 0.21381817530380562\n",
      "Epoch: 1066 Training Loss: 0.21726645096221986 Test Loss: 0.2137980182117447\n",
      "Epoch: 1067 Training Loss: 0.21726598705374664 Test Loss: 0.21374279685342998\n",
      "Epoch: 1068 Training Loss: 0.2172649816915877 Test Loss: 0.2137178824283875\n",
      "Epoch: 1069 Training Loss: 0.21726537102252855 Test Loss: 0.21364514838108606\n",
      "Epoch: 1070 Training Loss: 0.21726719398504274 Test Loss: 0.2137966960102076\n",
      "Epoch: 1071 Training Loss: 0.217265568028073 Test Loss: 0.21411775765404323\n",
      "Epoch: 1072 Training Loss: 0.2172621189113415 Test Loss: 0.2143137416250199\n",
      "Epoch: 1073 Training Loss: 0.2172637477642649 Test Loss: 0.21400305018931853\n",
      "Epoch: 1074 Training Loss: 0.21726274082412503 Test Loss: 0.21389846664028486\n",
      "Epoch: 1075 Training Loss: 0.21726172220154646 Test Loss: 0.21376886496412736\n",
      "Epoch: 1076 Training Loss: 0.21726346497393687 Test Loss: 0.2137612558239089\n",
      "Epoch: 1077 Training Loss: 0.2172662073165788 Test Loss: 0.21387304666759585\n",
      "Epoch: 1078 Training Loss: 0.21726540270766914 Test Loss: 0.21387657253836148\n",
      "Epoch: 1079 Training Loss: 0.2172646626973554 Test Loss: 0.21368601996389508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1080 Training Loss: 0.21726479874441915 Test Loss: 0.21394602700734044\n",
      "Epoch: 1081 Training Loss: 0.21726505503183954 Test Loss: 0.21377922220950143\n",
      "Epoch: 1082 Training Loss: 0.21726392926793953 Test Loss: 0.21374860416998515\n",
      "Epoch: 1083 Training Loss: 0.21726516167107615 Test Loss: 0.2138121476203276\n",
      "Epoch: 1084 Training Loss: 0.2172640082745776 Test Loss: 0.21378534063230062\n",
      "Epoch: 1085 Training Loss: 0.21726475691895766 Test Loss: 0.21367798305259106\n",
      "Epoch: 1086 Training Loss: 0.21726471078991172 Test Loss: 0.21375031525432728\n",
      "Epoch: 1087 Training Loss: 0.217263660553919 Test Loss: 0.21378754430152916\n",
      "Epoch: 1088 Training Loss: 0.21726462704933078 Test Loss: 0.21376364097177977\n",
      "Epoch: 1089 Training Loss: 0.21726426973526516 Test Loss: 0.21368456813475628\n",
      "Epoch: 1090 Training Loss: 0.21726368093318457 Test Loss: 0.21405369569329405\n",
      "Epoch: 1091 Training Loss: 0.21726388710177763 Test Loss: 0.21374063207248195\n",
      "Epoch: 1092 Training Loss: 0.21726589279628117 Test Loss: 0.2139275032230607\n",
      "Epoch: 1093 Training Loss: 0.2172635624232275 Test Loss: 0.2141808085194993\n",
      "Epoch: 1094 Training Loss: 0.21726337616767838 Test Loss: 0.2138837668702546\n",
      "Epoch: 1095 Training Loss: 0.2172647713897604 Test Loss: 0.2138477563225085\n",
      "Epoch: 1096 Training Loss: 0.21726496548141958 Test Loss: 0.2138570506215488\n",
      "Epoch: 1097 Training Loss: 0.2172640121119404 Test Loss: 0.2139147349042954\n",
      "Epoch: 1098 Training Loss: 0.21726452534128468 Test Loss: 0.21375275225323884\n",
      "Epoch: 1099 Training Loss: 0.21726531056613269 Test Loss: 0.213747126415326\n",
      "Epoch: 1100 Training Loss: 0.2172646731245819 Test Loss: 0.213762422472324\n",
      "Epoch: 1101 Training Loss: 0.21726474295023976 Test Loss: 0.21374331536383667\n",
      "Epoch: 1102 Training Loss: 0.2172656892725994 Test Loss: 0.21394356408290857\n",
      "Epoch: 1103 Training Loss: 0.2172655782042571 Test Loss: 0.21376391318974328\n",
      "Epoch: 1104 Training Loss: 0.217264741273635 Test Loss: 0.21382657517239434\n",
      "Epoch: 1105 Training Loss: 0.2172645972918374 Test Loss: 0.2137963719412034\n",
      "Epoch: 1106 Training Loss: 0.217264477625292 Test Loss: 0.2136823515027676\n",
      "Epoch: 1107 Training Loss: 0.2172654098534125 Test Loss: 0.2163903109900977\n",
      "Epoch: 1108 Training Loss: 0.21725934470424296 Test Loss: 0.21549782495254594\n",
      "Epoch: 1109 Training Loss: 0.2172637577790646 Test Loss: 0.21445978008107014\n",
      "Epoch: 1110 Training Loss: 0.21726182285162804 Test Loss: 0.2140720639244518\n",
      "Epoch: 1111 Training Loss: 0.21726350584902346 Test Loss: 0.21395765460321095\n",
      "Epoch: 1112 Training Loss: 0.21726328084328259 Test Loss: 0.21385593582417436\n",
      "Epoch: 1113 Training Loss: 0.21726035682661582 Test Loss: 0.21494720578889906\n",
      "Epoch: 1114 Training Loss: 0.2172666747934408 Test Loss: 0.2138363361308007\n",
      "Epoch: 1115 Training Loss: 0.21726391188325148 Test Loss: 0.21380936062689151\n",
      "Epoch: 1116 Training Loss: 0.2172628985146323 Test Loss: 0.21377469820620287\n",
      "Epoch: 1117 Training Loss: 0.21726441850480047 Test Loss: 0.2137687742248062\n",
      "Epoch: 1118 Training Loss: 0.21726553021032458 Test Loss: 0.21370182156853962\n",
      "Epoch: 1119 Training Loss: 0.2172669776402656 Test Loss: 0.2138293621658304\n",
      "Epoch: 1120 Training Loss: 0.21726470177928175 Test Loss: 0.213782125867779\n",
      "Epoch: 1121 Training Loss: 0.21726492875749884 Test Loss: 0.213719930544494\n",
      "Epoch: 1122 Training Loss: 0.21726651163379496 Test Loss: 0.2137113621600231\n",
      "Epoch: 1123 Training Loss: 0.21726469726051809 Test Loss: 0.21375285595532018\n",
      "Epoch: 1124 Training Loss: 0.2172646002774491 Test Loss: 0.2137346951283251\n",
      "Epoch: 1125 Training Loss: 0.2172651040478734 Test Loss: 0.2147725325956382\n",
      "Epoch: 1126 Training Loss: 0.21726607953598354 Test Loss: 0.21364689835370873\n",
      "Epoch: 1127 Training Loss: 0.21726561763584973 Test Loss: 0.2136184580579006\n",
      "Epoch: 1128 Training Loss: 0.21726662553533033 Test Loss: 0.21379835524350907\n",
      "Epoch: 1129 Training Loss: 0.21726533190473896 Test Loss: 0.21377244268593368\n",
      "Epoch: 1130 Training Loss: 0.2172665951591967 Test Loss: 0.2137618909991571\n",
      "Epoch: 1131 Training Loss: 0.21726475404990137 Test Loss: 0.21377446487651985\n",
      "Epoch: 1132 Training Loss: 0.2172641898589444 Test Loss: 0.21382682146483753\n",
      "Epoch: 1133 Training Loss: 0.21726322585602528 Test Loss: 0.21396067492633003\n",
      "Epoch: 1134 Training Loss: 0.21726223358393576 Test Loss: 0.21372887484900976\n",
      "Epoch: 1135 Training Loss: 0.21726245416057097 Test Loss: 0.2137119195587103\n",
      "Epoch: 1136 Training Loss: 0.21726264419968808 Test Loss: 0.21370595668903314\n",
      "Epoch: 1137 Training Loss: 0.217264913479774 Test Loss: 0.21377071863883138\n",
      "Epoch: 1138 Training Loss: 0.21726522815249028 Test Loss: 0.2137431468479545\n",
      "Epoch: 1139 Training Loss: 0.21726571588309662 Test Loss: 0.2136661869408384\n",
      "Epoch: 1140 Training Loss: 0.21726259278081958 Test Loss: 0.21372724154122863\n",
      "Epoch: 1141 Training Loss: 0.21726412916943771 Test Loss: 0.2137965404570856\n",
      "Epoch: 1142 Training Loss: 0.21726400453583858 Test Loss: 0.21374174686985636\n",
      "Epoch: 1143 Training Loss: 0.21726480727986164 Test Loss: 0.21383944719324094\n",
      "Epoch: 1144 Training Loss: 0.21726412348511992 Test Loss: 0.21375985584581078\n",
      "Epoch: 1145 Training Loss: 0.21726460719904744 Test Loss: 0.21383571391831263\n",
      "Epoch: 1146 Training Loss: 0.21726466860581822 Test Loss: 0.21370224933962514\n",
      "Epoch: 1147 Training Loss: 0.2172645872053113 Test Loss: 0.21368226076344643\n",
      "Epoch: 1148 Training Loss: 0.21726552093072057 Test Loss: 0.21368677180398482\n",
      "Epoch: 1149 Training Loss: 0.21726552294802579 Test Loss: 0.21374401535288573\n",
      "Epoch: 1150 Training Loss: 0.21726552709022584 Test Loss: 0.21373999689723372\n",
      "Epoch: 1151 Training Loss: 0.21726540407047087 Test Loss: 0.21374204501334024\n",
      "Epoch: 1152 Training Loss: 0.21726602283625826 Test Loss: 0.21372184903299885\n",
      "Epoch: 1153 Training Loss: 0.2172648040073443 Test Loss: 0.2138047069959913\n",
      "Epoch: 1154 Training Loss: 0.21726505501390794 Test Loss: 0.21376817793783848\n",
      "Epoch: 1155 Training Loss: 0.21726483793393514 Test Loss: 0.2137519485621084\n",
      "Epoch: 1156 Training Loss: 0.217265350912237 Test Loss: 0.2137465301283583\n",
      "Epoch: 1157 Training Loss: 0.21726226595944303 Test Loss: 0.21366149442165763\n",
      "Epoch: 1158 Training Loss: 0.21726290844873974 Test Loss: 0.213752998545682\n",
      "Epoch: 1159 Training Loss: 0.2172639345667279 Test Loss: 0.2137021067492633\n",
      "Epoch: 1160 Training Loss: 0.21726442024416584 Test Loss: 0.21370799184237949\n",
      "Epoch: 1161 Training Loss: 0.21726323299280287 Test Loss: 0.21373615992022407\n",
      "Epoch: 1162 Training Loss: 0.21726371835643776 Test Loss: 0.21365466304704922\n",
      "Epoch: 1163 Training Loss: 0.2172667195776166 Test Loss: 0.21372643785009823\n",
      "Epoch: 1164 Training Loss: 0.21726483044749131 Test Loss: 0.2137614502653114\n",
      "Epoch: 1165 Training Loss: 0.21726409561941054 Test Loss: 0.21372965261461985\n",
      "Epoch: 1166 Training Loss: 0.2172655276461055 Test Loss: 0.2138262251778698\n",
      "Epoch: 1167 Training Loss: 0.21726479063933507 Test Loss: 0.21372739709435065\n",
      "Epoch: 1168 Training Loss: 0.21726527412015179 Test Loss: 0.21380299591164914\n",
      "Epoch: 1169 Training Loss: 0.21726475199673292 Test Loss: 0.21375442444930046\n",
      "Epoch: 1170 Training Loss: 0.21726483680424422 Test Loss: 0.21371024736264868\n",
      "Epoch: 1171 Training Loss: 0.21726476709514175 Test Loss: 0.21367632381928958\n",
      "Epoch: 1172 Training Loss: 0.2172651369075339 Test Loss: 0.2137080566561803\n",
      "Epoch: 1173 Training Loss: 0.21726547984942057 Test Loss: 0.21371491395630907\n",
      "Epoch: 1174 Training Loss: 0.21726495154856487 Test Loss: 0.2137813221766486\n",
      "Epoch: 1175 Training Loss: 0.21726458138650648 Test Loss: 0.21371219177667383\n",
      "Epoch: 1176 Training Loss: 0.2172634607420788 Test Loss: 0.2136928513385035\n",
      "Epoch: 1177 Training Loss: 0.21726460634729636 Test Loss: 0.21381309390181985\n",
      "Epoch: 1178 Training Loss: 0.21726352753729597 Test Loss: 0.21398068942802909\n",
      "Epoch: 1179 Training Loss: 0.21726338990328545 Test Loss: 0.21382845477261866\n",
      "Epoch: 1180 Training Loss: 0.21726671841206247 Test Loss: 0.21380578290508523\n",
      "Epoch: 1181 Training Loss: 0.2172646114219397 Test Loss: 0.21375758736278141\n",
      "Epoch: 1182 Training Loss: 0.21726362364171645 Test Loss: 0.21370262525967001\n",
      "Epoch: 1183 Training Loss: 0.21726504312525588 Test Loss: 0.21373507104836997\n",
      "Epoch: 1184 Training Loss: 0.21726487776898878 Test Loss: 0.21391356825588032\n",
      "Epoch: 1185 Training Loss: 0.21726385952297386 Test Loss: 0.21371932129476612\n",
      "Epoch: 1186 Training Loss: 0.21726257949350256 Test Loss: 0.2137186472312374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1187 Training Loss: 0.2172652423901822 Test Loss: 0.21370332524871907\n",
      "Epoch: 1188 Training Loss: 0.21726472342272526 Test Loss: 0.21366963503504302\n",
      "Epoch: 1189 Training Loss: 0.2172662381947973 Test Loss: 0.21387566514514972\n",
      "Epoch: 1190 Training Loss: 0.2172648106241054 Test Loss: 0.2138732670345187\n",
      "Epoch: 1191 Training Loss: 0.21726320884790087 Test Loss: 0.21382158450972974\n",
      "Epoch: 1192 Training Loss: 0.2172647891599779 Test Loss: 0.21381380685362908\n",
      "Epoch: 1193 Training Loss: 0.21726364690796993 Test Loss: 0.21384765262042715\n",
      "Epoch: 1194 Training Loss: 0.21726589190866685 Test Loss: 0.21376545575820324\n",
      "Epoch: 1195 Training Loss: 0.21726438470373086 Test Loss: 0.21368725142611103\n",
      "Epoch: 1196 Training Loss: 0.21726563453638453 Test Loss: 0.21368161262543803\n",
      "Epoch: 1197 Training Loss: 0.21726412816526802 Test Loss: 0.2137105584688927\n",
      "Epoch: 1198 Training Loss: 0.21726681944767348 Test Loss: 0.21366463140961825\n",
      "Epoch: 1199 Training Loss: 0.2172625358928125 Test Loss: 0.21370773258717612\n",
      "Epoch: 1200 Training Loss: 0.21726320293943804 Test Loss: 0.21377732964651694\n",
      "Epoch: 1201 Training Loss: 0.21726636930170473 Test Loss: 0.21378505545157694\n",
      "Epoch: 1202 Training Loss: 0.2172649198992875 Test Loss: 0.21372996372086386\n",
      "Epoch: 1203 Training Loss: 0.2172645067013845 Test Loss: 0.21375700403857387\n",
      "Epoch: 1204 Training Loss: 0.21726415213085398 Test Loss: 0.21380391626762105\n",
      "Epoch: 1205 Training Loss: 0.21726621164706067 Test Loss: 0.21374392461356456\n",
      "Epoch: 1206 Training Loss: 0.21726437253713896 Test Loss: 0.2137921849696692\n",
      "Epoch: 1207 Training Loss: 0.2172650247991587 Test Loss: 0.21374621902211427\n",
      "Epoch: 1208 Training Loss: 0.2172652453040675 Test Loss: 0.21391268678818892\n",
      "Epoch: 1209 Training Loss: 0.2172637223462192 Test Loss: 0.2141120151512889\n",
      "Epoch: 1210 Training Loss: 0.2172646265472459 Test Loss: 0.21443480084222682\n",
      "Epoch: 1211 Training Loss: 0.2172649928809073 Test Loss: 0.21390951091194782\n",
      "Epoch: 1212 Training Loss: 0.21726383417665457 Test Loss: 0.21383993977812732\n",
      "Epoch: 1213 Training Loss: 0.2172653077419054 Test Loss: 0.21378366843623897\n",
      "Epoch: 1214 Training Loss: 0.21726447426311662 Test Loss: 0.21421429132901268\n",
      "Epoch: 1215 Training Loss: 0.2172623031137222 Test Loss: 0.21391164976737548\n",
      "Epoch: 1216 Training Loss: 0.21726261218281287 Test Loss: 0.2138585802272486\n",
      "Epoch: 1217 Training Loss: 0.21726384793915904 Test Loss: 0.21381372907706808\n",
      "Epoch: 1218 Training Loss: 0.2172642505663827 Test Loss: 0.21372210828820218\n",
      "Epoch: 1219 Training Loss: 0.21726439342745518 Test Loss: 0.21377626670018318\n",
      "Epoch: 1220 Training Loss: 0.21726732223185966 Test Loss: 0.21470350589774476\n",
      "Epoch: 1221 Training Loss: 0.21726452195221194 Test Loss: 0.21374838380306227\n",
      "Epoch: 1222 Training Loss: 0.2172636604821926 Test Loss: 0.21366362031432515\n",
      "Epoch: 1223 Training Loss: 0.21726756469401498 Test Loss: 0.2136745868094271\n",
      "Epoch: 1224 Training Loss: 0.21726482251275747 Test Loss: 0.21370919737907507\n",
      "Epoch: 1225 Training Loss: 0.21726580239411009 Test Loss: 0.21372965261461985\n",
      "Epoch: 1226 Training Loss: 0.21726151445497244 Test Loss: 0.21366530547314697\n",
      "Epoch: 1227 Training Loss: 0.21726578142310166 Test Loss: 0.2136858903362934\n",
      "Epoch: 1228 Training Loss: 0.21726566452698873 Test Loss: 0.21381919936185886\n",
      "Epoch: 1229 Training Loss: 0.2172655118304326 Test Loss: 0.21372743598263114\n",
      "Epoch: 1230 Training Loss: 0.2172643439720971 Test Loss: 0.21379524418106882\n",
      "Epoch: 1231 Training Loss: 0.21726205546036811 Test Loss: 0.21369664942723265\n",
      "Epoch: 1232 Training Loss: 0.21726318632580888 Test Loss: 0.21374979674392056\n",
      "Epoch: 1233 Training Loss: 0.2172644825295851 Test Loss: 0.213830178819721\n",
      "Epoch: 1234 Training Loss: 0.21726420907265587 Test Loss: 0.21379376642640968\n",
      "Epoch: 1235 Training Loss: 0.2172644944361688 Test Loss: 0.21431022871701444\n",
      "Epoch: 1236 Training Loss: 0.21726128624844063 Test Loss: 0.2145664506344908\n",
      "Epoch: 1237 Training Loss: 0.21726247907653182 Test Loss: 0.21416354212295582\n",
      "Epoch: 1238 Training Loss: 0.21726441443432684 Test Loss: 0.21394763438960127\n",
      "Epoch: 1239 Training Loss: 0.2172637689863158 Test Loss: 0.2138333546959621\n",
      "Epoch: 1240 Training Loss: 0.2172638961124076 Test Loss: 0.21370511410962223\n",
      "Epoch: 1241 Training Loss: 0.21726383580843034 Test Loss: 0.21381756605407773\n",
      "Epoch: 1242 Training Loss: 0.21726449299267483 Test Loss: 0.2137331655226253\n",
      "Epoch: 1243 Training Loss: 0.21726334445564038 Test Loss: 0.21381136985471752\n",
      "Epoch: 1244 Training Loss: 0.21726382005551806 Test Loss: 0.21369260504606033\n",
      "Epoch: 1245 Training Loss: 0.2172665691225107 Test Loss: 0.2137635372696984\n",
      "Epoch: 1246 Training Loss: 0.21726435923189033 Test Loss: 0.21375325780088536\n",
      "Epoch: 1247 Training Loss: 0.21726423785287693 Test Loss: 0.2137467634580413\n",
      "Epoch: 1248 Training Loss: 0.2172649854303267 Test Loss: 0.21376562427408544\n",
      "Epoch: 1249 Training Loss: 0.21726408206311948 Test Loss: 0.21370948255979877\n",
      "Epoch: 1250 Training Loss: 0.21726266133333372 Test Loss: 0.21505692259095918\n",
      "Epoch: 1251 Training Loss: 0.21726438724105251 Test Loss: 0.214277977369717\n",
      "Epoch: 1252 Training Loss: 0.21726692233920528 Test Loss: 0.21382604369922745\n",
      "Epoch: 1253 Training Loss: 0.21726420184622028 Test Loss: 0.21386814674425242\n",
      "Epoch: 1254 Training Loss: 0.21726427355469635 Test Loss: 0.21385728395123182\n",
      "Epoch: 1255 Training Loss: 0.21726360137963266 Test Loss: 0.21383929164011894\n",
      "Epoch: 1256 Training Loss: 0.21726361808291986 Test Loss: 0.21367037391237256\n",
      "Epoch: 1257 Training Loss: 0.21726191460763505 Test Loss: 0.21368077004602712\n",
      "Epoch: 1258 Training Loss: 0.21726309813819064 Test Loss: 0.2138893927081674\n",
      "Epoch: 1259 Training Loss: 0.21726265685043322 Test Loss: 0.21391196087361952\n",
      "Epoch: 1260 Training Loss: 0.21726347809090368 Test Loss: 0.21411863912173462\n",
      "Epoch: 1261 Training Loss: 0.21726524853175586 Test Loss: 0.21386218387457523\n",
      "Epoch: 1262 Training Loss: 0.21726351150644385 Test Loss: 0.213807156957663\n",
      "Epoch: 1263 Training Loss: 0.21726390527545617 Test Loss: 0.21379568491491452\n",
      "Epoch: 1264 Training Loss: 0.21726319189357127 Test Loss: 0.21375949288852608\n",
      "Epoch: 1265 Training Loss: 0.21726459210063864 Test Loss: 0.21371753243386296\n",
      "Epoch: 1266 Training Loss: 0.21726445794535887 Test Loss: 0.21373137666172215\n",
      "Epoch: 1267 Training Loss: 0.21726494264552454 Test Loss: 0.21378541840886164\n",
      "Epoch: 1268 Training Loss: 0.2172643637416882 Test Loss: 0.2137210971929091\n",
      "Epoch: 1269 Training Loss: 0.21726517257349012 Test Loss: 0.21376625944933364\n",
      "Epoch: 1270 Training Loss: 0.21726443887510025 Test Loss: 0.21372327493661727\n",
      "Epoch: 1271 Training Loss: 0.21726526889308984 Test Loss: 0.2137361080691834\n",
      "Epoch: 1272 Training Loss: 0.21726381282011667 Test Loss: 0.2137035585784021\n",
      "Epoch: 1273 Training Loss: 0.2172651779888339 Test Loss: 0.2137542559334183\n",
      "Epoch: 1274 Training Loss: 0.2172651257092485 Test Loss: 0.2137167157799724\n",
      "Epoch: 1275 Training Loss: 0.2172664926262969 Test Loss: 0.2137621372916003\n",
      "Epoch: 1276 Training Loss: 0.21726454364945025 Test Loss: 0.21368981805262424\n",
      "Epoch: 1277 Training Loss: 0.2172654611198624 Test Loss: 0.2139785505726014\n",
      "Epoch: 1278 Training Loss: 0.21726390341953536 Test Loss: 0.21430398066661357\n",
      "Epoch: 1279 Training Loss: 0.2172638617285609 Test Loss: 0.21406436404491214\n",
      "Epoch: 1280 Training Loss: 0.21726269225638123 Test Loss: 0.2139187274344271\n",
      "Epoch: 1281 Training Loss: 0.21726560012564045 Test Loss: 0.21379294977251911\n",
      "Epoch: 1282 Training Loss: 0.21726197632820884 Test Loss: 0.21383704908260992\n",
      "Epoch: 1283 Training Loss: 0.21726518222069194 Test Loss: 0.213667781360339\n",
      "Epoch: 1284 Training Loss: 0.2172646890747418 Test Loss: 0.213706099279395\n",
      "Epoch: 1285 Training Loss: 0.21726485396478723 Test Loss: 0.2137662724120938\n",
      "Epoch: 1286 Training Loss: 0.2172649507416428 Test Loss: 0.2136763108565294\n",
      "Epoch: 1287 Training Loss: 0.21726441101835667 Test Loss: 0.21374118947116916\n",
      "Epoch: 1288 Training Loss: 0.21726376606346468 Test Loss: 0.2137964497177644\n",
      "Epoch: 1289 Training Loss: 0.21726470723945454 Test Loss: 0.213826471470313\n",
      "Epoch: 1290 Training Loss: 0.21726503148764623 Test Loss: 0.21381410499711292\n",
      "Epoch: 1291 Training Loss: 0.2172654826467505 Test Loss: 0.21376828163991982\n",
      "Epoch: 1292 Training Loss: 0.21726542595599102 Test Loss: 0.21420170448888978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1293 Training Loss: 0.2172651545522302 Test Loss: 0.21377427043511735\n",
      "Epoch: 1294 Training Loss: 0.2172627601633577 Test Loss: 0.21378377213832034\n",
      "Epoch: 1295 Training Loss: 0.21726536245122283 Test Loss: 0.21377372599919028\n",
      "Epoch: 1296 Training Loss: 0.2172645955345404 Test Loss: 0.21398513565476665\n",
      "Epoch: 1297 Training Loss: 0.21726514798926388 Test Loss: 0.21376095768042502\n",
      "Epoch: 1298 Training Loss: 0.21726491858131475 Test Loss: 0.21373120814583996\n",
      "Epoch: 1299 Training Loss: 0.21726455984168677 Test Loss: 0.21372324901109696\n",
      "Epoch: 1300 Training Loss: 0.2172653192719254 Test Loss: 0.21374139687533183\n",
      "Epoch: 1301 Training Loss: 0.2172642054863355 Test Loss: 0.21494574099700012\n",
      "Epoch: 1302 Training Loss: 0.21726487315160128 Test Loss: 0.21369163283904774\n",
      "Epoch: 1303 Training Loss: 0.21726144694249122 Test Loss: 0.2137804018206767\n",
      "Epoch: 1304 Training Loss: 0.2172619179160156 Test Loss: 0.2138927759885712\n",
      "Epoch: 1305 Training Loss: 0.2172665998214132 Test Loss: 0.21362492647522432\n",
      "Epoch: 1306 Training Loss: 0.2172664457979185 Test Loss: 0.2136949513056507\n",
      "Epoch: 1307 Training Loss: 0.2172651471195812 Test Loss: 0.21368328482149967\n",
      "Epoch: 1308 Training Loss: 0.2172660765324402 Test Loss: 0.21369483464080918\n",
      "Epoch: 1309 Training Loss: 0.21726423536038428 Test Loss: 0.21886801296856748\n",
      "Epoch: 1310 Training Loss: 0.21726646034941346 Test Loss: 0.21396363043564828\n",
      "Epoch: 1311 Training Loss: 0.2172635036882654 Test Loss: 0.21377716113063475\n",
      "Epoch: 1312 Training Loss: 0.2172641852325911 Test Loss: 0.2137079011030583\n",
      "Epoch: 1313 Training Loss: 0.21726527200422274 Test Loss: 0.21381223835964877\n",
      "Epoch: 1314 Training Loss: 0.217265040552071 Test Loss: 0.21375752254898056\n",
      "Epoch: 1315 Training Loss: 0.21726585693307732 Test Loss: 0.21363873181480303\n",
      "Epoch: 1316 Training Loss: 0.21726570290061684 Test Loss: 0.21372411751602818\n",
      "Epoch: 1317 Training Loss: 0.2172636184505177 Test Loss: 0.21372953594977834\n",
      "Epoch: 1318 Training Loss: 0.21726569607764232 Test Loss: 0.21368799030344057\n",
      "Epoch: 1319 Training Loss: 0.2172675330178402 Test Loss: 0.2137680094219563\n",
      "Epoch: 1320 Training Loss: 0.21726325560455287 Test Loss: 0.21370460856197568\n",
      "Epoch: 1321 Training Loss: 0.2172660233383431 Test Loss: 0.21376492428503638\n",
      "Epoch: 1322 Training Loss: 0.21726490624437264 Test Loss: 0.213685008868602\n",
      "Epoch: 1323 Training Loss: 0.21726742047910655 Test Loss: 0.21378194438913667\n",
      "Epoch: 1324 Training Loss: 0.21726467738333735 Test Loss: 0.21369167172732823\n",
      "Epoch: 1325 Training Loss: 0.21726622907657772 Test Loss: 0.21365302973926809\n",
      "Epoch: 1326 Training Loss: 0.21726579438764984 Test Loss: 0.21365354824967478\n",
      "Epoch: 1327 Training Loss: 0.21726412321614588 Test Loss: 0.21375238929595414\n",
      "Epoch: 1328 Training Loss: 0.21726437180194327 Test Loss: 0.21373911542954233\n",
      "Epoch: 1329 Training Loss: 0.2172654200923572 Test Loss: 0.21377447783928002\n",
      "Epoch: 1330 Training Loss: 0.21726534290577673 Test Loss: 0.21382428076384463\n",
      "Epoch: 1331 Training Loss: 0.21726444624498864 Test Loss: 0.21379749970133802\n",
      "Epoch: 1332 Training Loss: 0.2172648795800806 Test Loss: 0.21375530591699188\n",
      "Epoch: 1333 Training Loss: 0.21726517935163564 Test Loss: 0.21376349838141792\n",
      "Epoch: 1334 Training Loss: 0.21726384726672396 Test Loss: 0.21370968996396145\n",
      "Epoch: 1335 Training Loss: 0.21726532238305835 Test Loss: 0.2137986274614726\n",
      "Epoch: 1336 Training Loss: 0.21726425676175115 Test Loss: 0.2137369895368748\n",
      "Epoch: 1337 Training Loss: 0.21726296374083426 Test Loss: 0.2138564543345811\n",
      "Epoch: 1338 Training Loss: 0.21726383647189962 Test Loss: 0.21374481904401615\n",
      "Epoch: 1339 Training Loss: 0.21726412130643027 Test Loss: 0.21389654815178002\n",
      "Epoch: 1340 Training Loss: 0.2172644926430086 Test Loss: 0.21366861097698975\n",
      "Epoch: 1341 Training Loss: 0.21726352308129288 Test Loss: 0.2137563040495248\n",
      "Epoch: 1342 Training Loss: 0.21726481013098636 Test Loss: 0.21375097635509585\n",
      "Epoch: 1343 Training Loss: 0.2172624985323199 Test Loss: 0.21374850046790378\n",
      "Epoch: 1344 Training Loss: 0.21726419903992458 Test Loss: 0.21384419156346235\n",
      "Epoch: 1345 Training Loss: 0.21726419453909251 Test Loss: 0.2137429394437918\n",
      "Epoch: 1346 Training Loss: 0.21726530637013783 Test Loss: 0.21368185891788122\n",
      "Epoch: 1347 Training Loss: 0.21726477356845003 Test Loss: 0.21423306140573564\n",
      "Epoch: 1348 Training Loss: 0.21726540166763622 Test Loss: 0.2142795069754168\n",
      "Epoch: 1349 Training Loss: 0.21726271879515205 Test Loss: 0.21395057693615935\n",
      "Epoch: 1350 Training Loss: 0.21726474749590086 Test Loss: 0.21385478213851944\n",
      "Epoch: 1351 Training Loss: 0.2172644468277657 Test Loss: 0.2137600502872133\n",
      "Epoch: 1352 Training Loss: 0.21726519161685134 Test Loss: 0.21369777718736727\n",
      "Epoch: 1353 Training Loss: 0.21726345625917834 Test Loss: 0.21371741576902145\n",
      "Epoch: 1354 Training Loss: 0.21726178910435323 Test Loss: 0.21858972843328556\n",
      "Epoch: 1355 Training Loss: 0.21726366670445846 Test Loss: 0.2139497084312281\n",
      "Epoch: 1356 Training Loss: 0.21726568075508848 Test Loss: 0.2138582172699639\n",
      "Epoch: 1357 Training Loss: 0.21726363196197973 Test Loss: 0.2138066773355368\n",
      "Epoch: 1358 Training Loss: 0.21726476401987202 Test Loss: 0.21369098470103934\n",
      "Epoch: 1359 Training Loss: 0.21726541967096455 Test Loss: 0.21378316288859245\n",
      "Epoch: 1360 Training Loss: 0.21726558619278577 Test Loss: 0.21370227526514549\n",
      "Epoch: 1361 Training Loss: 0.21726514552366863 Test Loss: 0.21366385364400817\n",
      "Epoch: 1362 Training Loss: 0.21726606296718337 Test Loss: 0.2137460375434719\n",
      "Epoch: 1363 Training Loss: 0.21726660789063404 Test Loss: 0.21401793143799114\n",
      "Epoch: 1364 Training Loss: 0.217264137444872 Test Loss: 0.2174429908178029\n",
      "Epoch: 1365 Training Loss: 0.21726473723902456 Test Loss: 0.21388528351319422\n",
      "Epoch: 1366 Training Loss: 0.21726573982178518 Test Loss: 0.21365034644791334\n",
      "Epoch: 1367 Training Loss: 0.2172650323662947 Test Loss: 0.21368975323882342\n",
      "Epoch: 1368 Training Loss: 0.21726523347817603 Test Loss: 0.21369211246117395\n",
      "Epoch: 1369 Training Loss: 0.21726558618381997 Test Loss: 0.21371307324436525\n",
      "Epoch: 1370 Training Loss: 0.2172653482493941 Test Loss: 0.21365327603171128\n",
      "Epoch: 1371 Training Loss: 0.21726662194004415 Test Loss: 0.2137034289508004\n",
      "Epoch: 1372 Training Loss: 0.21726479695125894 Test Loss: 0.21370189934510062\n",
      "Epoch: 1373 Training Loss: 0.21726585387573918 Test Loss: 0.2137713149257991\n",
      "Epoch: 1374 Training Loss: 0.21726538199666892 Test Loss: 0.2136446946844802\n",
      "Epoch: 1375 Training Loss: 0.21726791810795718 Test Loss: 0.21367894229684348\n",
      "Epoch: 1376 Training Loss: 0.21726410858395873 Test Loss: 0.21385181366644102\n",
      "Epoch: 1377 Training Loss: 0.21726365334541503 Test Loss: 0.21411323365074467\n",
      "Epoch: 1378 Training Loss: 0.217264834921426 Test Loss: 0.2139173533818493\n",
      "Epoch: 1379 Training Loss: 0.21726592949330448 Test Loss: 0.21383147509573777\n",
      "Epoch: 1380 Training Loss: 0.21726607158331807 Test Loss: 0.21378299437271026\n",
      "Epoch: 1381 Training Loss: 0.21726409896365428 Test Loss: 0.21365717782252178\n",
      "Epoch: 1382 Training Loss: 0.21726696221012215 Test Loss: 0.21376259098820616\n",
      "Epoch: 1383 Training Loss: 0.21726478500881208 Test Loss: 0.2137326081239381\n",
      "Epoch: 1384 Training Loss: 0.21726471037748488 Test Loss: 0.21381405314607227\n",
      "Epoch: 1385 Training Loss: 0.2172652779754462 Test Loss: 0.21372035831557953\n",
      "Epoch: 1386 Training Loss: 0.21726471860809016 Test Loss: 0.21379503677690612\n",
      "Epoch: 1387 Training Loss: 0.2172637036794216 Test Loss: 0.2136995271599899\n",
      "Epoch: 1388 Training Loss: 0.2172661157219562 Test Loss: 0.21374818936165976\n",
      "Epoch: 1389 Training Loss: 0.2172642198405828 Test Loss: 0.21380491440015398\n",
      "Epoch: 1390 Training Loss: 0.2172644365081288 Test Loss: 0.2137902146301237\n",
      "Epoch: 1391 Training Loss: 0.21726479716643818 Test Loss: 0.2139263754629261\n",
      "Epoch: 1392 Training Loss: 0.21726415042735178 Test Loss: 0.21382470853493019\n",
      "Epoch: 1393 Training Loss: 0.21726633793933298 Test Loss: 0.21379799228622437\n",
      "Epoch: 1394 Training Loss: 0.21726443261697118 Test Loss: 0.21384862482743974\n",
      "Epoch: 1395 Training Loss: 0.2172638673142549 Test Loss: 0.21379429789957657\n",
      "Epoch: 1396 Training Loss: 0.21726431835680377 Test Loss: 0.2138335491373646\n",
      "Epoch: 1397 Training Loss: 0.21726383228487056 Test Loss: 0.2138162957035813\n",
      "Epoch: 1398 Training Loss: 0.21726442578503083 Test Loss: 0.2137359006650207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1399 Training Loss: 0.21726553535669432 Test Loss: 0.21375627812400447\n",
      "Epoch: 1400 Training Loss: 0.2172656912899046 Test Loss: 0.21371697503517575\n",
      "Epoch: 1401 Training Loss: 0.21726545767699482 Test Loss: 0.2137312729596408\n",
      "Epoch: 1402 Training Loss: 0.21726588989136164 Test Loss: 0.21369852902745698\n",
      "Epoch: 1403 Training Loss: 0.2172637962513165 Test Loss: 0.21368156077439737\n",
      "Epoch: 1404 Training Loss: 0.21726494154273102 Test Loss: 0.21378738874840714\n",
      "Epoch: 1405 Training Loss: 0.2172647058138922 Test Loss: 0.21373180443280768\n",
      "Epoch: 1406 Training Loss: 0.21726493636049807 Test Loss: 0.21372527120168314\n",
      "Epoch: 1407 Training Loss: 0.2172645337601718 Test Loss: 0.21374925230799352\n",
      "Epoch: 1408 Training Loss: 0.21726637606191865 Test Loss: 0.21379591824459754\n",
      "Epoch: 1409 Training Loss: 0.21726467077554204 Test Loss: 0.2137927942193971\n",
      "Epoch: 1410 Training Loss: 0.21726377717209205 Test Loss: 0.21378585914270734\n",
      "Epoch: 1411 Training Loss: 0.21726418291941446 Test Loss: 0.21379696822817112\n",
      "Epoch: 1412 Training Loss: 0.2172648339620853 Test Loss: 0.21377382970127162\n",
      "Epoch: 1413 Training Loss: 0.2172649088175575 Test Loss: 0.21377132788855926\n",
      "Epoch: 1414 Training Loss: 0.21726530361763693 Test Loss: 0.21379985892368855\n",
      "Epoch: 1415 Training Loss: 0.21726464084769845 Test Loss: 0.21422033197525087\n",
      "Epoch: 1416 Training Loss: 0.21726586860655017 Test Loss: 0.2137305081567909\n",
      "Epoch: 1417 Training Loss: 0.21726449405063936 Test Loss: 0.21376952606489594\n",
      "Epoch: 1418 Training Loss: 0.21726546476894337 Test Loss: 0.21389618519449533\n",
      "Epoch: 1419 Training Loss: 0.21726498477582323 Test Loss: 0.21407638052358766\n",
      "Epoch: 1420 Training Loss: 0.21726243179986338 Test Loss: 0.2137728445314989\n",
      "Epoch: 1421 Training Loss: 0.21726456348180198 Test Loss: 0.21372910817869278\n",
      "Epoch: 1422 Training Loss: 0.2172652714842063 Test Loss: 0.21374017837587608\n",
      "Epoch: 1423 Training Loss: 0.2172635436398745 Test Loss: 0.2137447153419348\n",
      "Epoch: 1424 Training Loss: 0.21726571954114343 Test Loss: 0.21375535776803256\n",
      "Epoch: 1425 Training Loss: 0.21726561569923672 Test Loss: 0.2137285378172454\n",
      "Epoch: 1426 Training Loss: 0.2172639701250945 Test Loss: 0.21371626208336653\n",
      "Epoch: 1427 Training Loss: 0.2172657389252051 Test Loss: 0.21467481930949348\n",
      "Epoch: 1428 Training Loss: 0.21726448192887646 Test Loss: 0.21370004567039663\n",
      "Epoch: 1429 Training Loss: 0.2172671544906895 Test Loss: 0.21377559263665447\n",
      "Epoch: 1430 Training Loss: 0.21726508231477187 Test Loss: 0.21371274917536104\n",
      "Epoch: 1431 Training Loss: 0.21726443051000796 Test Loss: 0.21375905215468036\n",
      "Epoch: 1432 Training Loss: 0.2172640644453206 Test Loss: 0.2137553448052724\n",
      "Epoch: 1433 Training Loss: 0.217264518885908 Test Loss: 0.21369215134945443\n",
      "Epoch: 1434 Training Loss: 0.21726540311113018 Test Loss: 0.21383076214392854\n",
      "Epoch: 1435 Training Loss: 0.21726504280248704 Test Loss: 0.21382004194126977\n",
      "Epoch: 1436 Training Loss: 0.2172652406328852 Test Loss: 0.2137567447833705\n",
      "Epoch: 1437 Training Loss: 0.21726484746458155 Test Loss: 0.21379328680428347\n",
      "Epoch: 1438 Training Loss: 0.21726433553527838 Test Loss: 0.21366823505694488\n",
      "Epoch: 1439 Training Loss: 0.21726596164466672 Test Loss: 0.21379800524898454\n",
      "Epoch: 1440 Training Loss: 0.2172654171246771 Test Loss: 0.21436743337763486\n",
      "Epoch: 1441 Training Loss: 0.2172631268646169 Test Loss: 0.2146742489480461\n",
      "Epoch: 1442 Training Loss: 0.21726474009014926 Test Loss: 0.2141083985412021\n",
      "Epoch: 1443 Training Loss: 0.21726255602103564 Test Loss: 0.21402821090680418\n",
      "Epoch: 1444 Training Loss: 0.21726622534680454 Test Loss: 0.2138253048218979\n",
      "Epoch: 1445 Training Loss: 0.21726417608747414 Test Loss: 0.2137999626257699\n",
      "Epoch: 1446 Training Loss: 0.2172633099283409 Test Loss: 0.2136827922366133\n",
      "Epoch: 1447 Training Loss: 0.21726683338949396 Test Loss: 0.21372901743937162\n",
      "Epoch: 1448 Training Loss: 0.21726543248309413 Test Loss: 0.21370831591138367\n",
      "Epoch: 1449 Training Loss: 0.21726551116696333 Test Loss: 0.21364527800868774\n",
      "Epoch: 1450 Training Loss: 0.21726476175152437 Test Loss: 0.213716223195086\n",
      "Epoch: 1451 Training Loss: 0.21726460846322537 Test Loss: 0.2137997163333267\n",
      "Epoch: 1452 Training Loss: 0.21726571224298144 Test Loss: 0.21374095614148614\n",
      "Epoch: 1453 Training Loss: 0.2172654715022599 Test Loss: 0.21374799492025726\n",
      "Epoch: 1454 Training Loss: 0.21726427618167604 Test Loss: 0.21368944213257937\n",
      "Epoch: 1455 Training Loss: 0.21726540542430683 Test Loss: 0.21379904226979798\n",
      "Epoch: 1456 Training Loss: 0.2172647517008615 Test Loss: 0.21379664415916694\n",
      "Epoch: 1457 Training Loss: 0.21726377290437082 Test Loss: 0.21376737424670808\n",
      "Epoch: 1458 Training Loss: 0.21726423868669642 Test Loss: 0.21377570930149598\n",
      "Epoch: 1459 Training Loss: 0.21726486725210425 Test Loss: 0.21371674170549274\n",
      "Epoch: 1460 Training Loss: 0.21726596402060397 Test Loss: 0.2137505874722908\n",
      "Epoch: 1461 Training Loss: 0.2172652639529335 Test Loss: 0.2137547355555445\n",
      "Epoch: 1462 Training Loss: 0.21726454913652044 Test Loss: 0.21373102666719762\n",
      "Epoch: 1463 Training Loss: 0.21726443487635302 Test Loss: 0.21375845586771264\n",
      "Epoch: 1464 Training Loss: 0.21726624155697266 Test Loss: 0.21374961526527822\n",
      "Epoch: 1465 Training Loss: 0.21726555955539112 Test Loss: 0.2137581966125093\n",
      "Epoch: 1466 Training Loss: 0.21726471219754248 Test Loss: 0.21377071863883138\n",
      "Epoch: 1467 Training Loss: 0.21726320282288264 Test Loss: 0.21377040753258733\n",
      "Epoch: 1468 Training Loss: 0.21726398699873192 Test Loss: 0.21379362383604783\n",
      "Epoch: 1469 Training Loss: 0.21726491592743766 Test Loss: 0.21371805094426968\n",
      "Epoch: 1470 Training Loss: 0.21726513263084685 Test Loss: 0.21373514882493097\n",
      "Epoch: 1471 Training Loss: 0.21726435571729635 Test Loss: 0.21369346058823138\n",
      "Epoch: 1472 Training Loss: 0.21726384441559926 Test Loss: 0.21369190505701124\n",
      "Epoch: 1473 Training Loss: 0.21726425849215075 Test Loss: 0.21371560098259795\n",
      "Epoch: 1474 Training Loss: 0.21726493203001618 Test Loss: 0.2138021662949984\n",
      "Epoch: 1475 Training Loss: 0.21726511433164708 Test Loss: 0.2136242524116956\n",
      "Epoch: 1476 Training Loss: 0.21726624580676232 Test Loss: 0.2137261267438542\n",
      "Epoch: 1477 Training Loss: 0.21726378903384674 Test Loss: 0.2137329710812228\n",
      "Epoch: 1478 Training Loss: 0.21726339070124173 Test Loss: 0.21377176862240496\n",
      "Epoch: 1479 Training Loss: 0.2172641684486117 Test Loss: 0.21380167371011202\n",
      "Epoch: 1480 Training Loss: 0.21726414632101496 Test Loss: 0.21372638599905755\n",
      "Epoch: 1481 Training Loss: 0.21726639018305516 Test Loss: 0.21373820803633056\n",
      "Epoch: 1482 Training Loss: 0.21726498527790808 Test Loss: 0.21367690714349713\n",
      "Epoch: 1483 Training Loss: 0.21726708793754898 Test Loss: 0.21372899151385127\n",
      "Epoch: 1484 Training Loss: 0.21726533991119923 Test Loss: 0.21373663954235028\n",
      "Epoch: 1485 Training Loss: 0.21726553259522763 Test Loss: 0.21394702513987338\n",
      "Epoch: 1486 Training Loss: 0.21726373762394405 Test Loss: 0.21418741952718487\n",
      "Epoch: 1487 Training Loss: 0.2172648414305975 Test Loss: 0.2138101772807821\n",
      "Epoch: 1488 Training Loss: 0.2172655248667072 Test Loss: 0.21395862681022354\n",
      "Epoch: 1489 Training Loss: 0.21726450450476326 Test Loss: 0.21372228976684454\n",
      "Epoch: 1490 Training Loss: 0.2172652359706687 Test Loss: 0.213696753129314\n",
      "Epoch: 1491 Training Loss: 0.2172624339875188 Test Loss: 0.21371097327721805\n",
      "Epoch: 1492 Training Loss: 0.21726420432078136 Test Loss: 0.21369213838669426\n",
      "Epoch: 1493 Training Loss: 0.2172659812439076 Test Loss: 0.21386576159638154\n",
      "Epoch: 1494 Training Loss: 0.21726535035635733 Test Loss: 0.21375604479432145\n",
      "Epoch: 1495 Training Loss: 0.21726496616282046 Test Loss: 0.213658046327453\n",
      "Epoch: 1496 Training Loss: 0.21726508048574847 Test Loss: 0.21381783827204126\n",
      "Epoch: 1497 Training Loss: 0.2172641709769676 Test Loss: 0.21386923561610652\n",
      "Epoch: 1498 Training Loss: 0.2172645218356565 Test Loss: 0.21370455671093502\n",
      "Epoch: 1499 Training Loss: 0.2172649761865859 Test Loss: 0.21378117958628676\n",
      "Epoch: 1500 Training Loss: 0.21726400033984375 Test Loss: 0.21370469930129685\n",
      "Epoch: 1501 Training Loss: 0.2172638840265079 Test Loss: 0.21385602656349553\n",
      "Epoch: 1502 Training Loss: 0.2172641197553467 Test Loss: 0.21376999272426198\n",
      "Epoch: 1503 Training Loss: 0.21726410321344394 Test Loss: 0.2137192824064856\n",
      "Epoch: 1504 Training Loss: 0.21726430310597633 Test Loss: 0.2136540667600815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1505 Training Loss: 0.21726419990064147 Test Loss: 0.21376900755448922\n",
      "Epoch: 1506 Training Loss: 0.2172641766612854 Test Loss: 0.21369528833741505\n",
      "Epoch: 1507 Training Loss: 0.21726565794609085 Test Loss: 0.21368708291022884\n",
      "Epoch: 1508 Training Loss: 0.21726638461529277 Test Loss: 0.21374702271324467\n",
      "Epoch: 1509 Training Loss: 0.2172639980535645 Test Loss: 0.21375723736825689\n",
      "Epoch: 1510 Training Loss: 0.21726581326962666 Test Loss: 0.2136943420559228\n",
      "Epoch: 1511 Training Loss: 0.2172649321645032 Test Loss: 0.21365055385207604\n",
      "Epoch: 1512 Training Loss: 0.21726503721679305 Test Loss: 0.21368084782258814\n",
      "Epoch: 1513 Training Loss: 0.2172651302728412 Test Loss: 0.21398203755508655\n",
      "Epoch: 1514 Training Loss: 0.21726471511142778 Test Loss: 0.21367143685870632\n",
      "Epoch: 1515 Training Loss: 0.21726579311450608 Test Loss: 0.21373999689723372\n",
      "Epoch: 1516 Training Loss: 0.21726458038233679 Test Loss: 0.21370806961894048\n",
      "Epoch: 1517 Training Loss: 0.21726426485786943 Test Loss: 0.2137454412565042\n",
      "Epoch: 1518 Training Loss: 0.21726640725394017 Test Loss: 0.2137969293398906\n",
      "Epoch: 1519 Training Loss: 0.217263385734188 Test Loss: 0.21381781234652092\n",
      "Epoch: 1520 Training Loss: 0.21726406863234965 Test Loss: 0.21384302491504725\n",
      "Epoch: 1521 Training Loss: 0.21726336432385532 Test Loss: 0.21376182618535627\n",
      "Epoch: 1522 Training Loss: 0.21726312531353334 Test Loss: 0.21374791714369623\n",
      "Epoch: 1523 Training Loss: 0.2172655265522778 Test Loss: 0.21372868040760726\n",
      "Epoch: 1524 Training Loss: 0.21726644702623324 Test Loss: 0.21381770864443958\n",
      "Epoch: 1525 Training Loss: 0.21726531788222625 Test Loss: 0.21376412059390595\n",
      "Epoch: 1526 Training Loss: 0.21726479020897663 Test Loss: 0.2137399320834329\n",
      "Epoch: 1527 Training Loss: 0.21726458473971605 Test Loss: 0.21375906511744053\n",
      "Epoch: 1528 Training Loss: 0.21726435027505517 Test Loss: 0.21379073314053043\n",
      "Epoch: 1529 Training Loss: 0.21726362647490954 Test Loss: 0.214057091936458\n",
      "Epoch: 1530 Training Loss: 0.21726307466572373 Test Loss: 0.21381926417565972\n",
      "Epoch: 1531 Training Loss: 0.21726405187526765 Test Loss: 0.2137189713002416\n",
      "Epoch: 1532 Training Loss: 0.2172658080784279 Test Loss: 0.213886424236089\n",
      "Epoch: 1533 Training Loss: 0.21726358625432646 Test Loss: 0.2138159327462966\n",
      "Epoch: 1534 Training Loss: 0.21726425644794814 Test Loss: 0.21375684848545184\n",
      "Epoch: 1535 Training Loss: 0.2172656790784837 Test Loss: 0.21367419792662207\n",
      "Epoch: 1536 Training Loss: 0.21726522610828763 Test Loss: 0.21371539357843528\n",
      "Epoch: 1537 Training Loss: 0.2172666608516203 Test Loss: 0.2137538151995726\n",
      "Epoch: 1538 Training Loss: 0.21726438186157193 Test Loss: 0.21373157110312466\n",
      "Epoch: 1539 Training Loss: 0.21726179739771911 Test Loss: 0.2150850777060436\n",
      "Epoch: 1540 Training Loss: 0.21726297264387462 Test Loss: 0.2137770574285534\n",
      "Epoch: 1541 Training Loss: 0.21726453897826795 Test Loss: 0.21371181585662896\n",
      "Epoch: 1542 Training Loss: 0.2172622255326465 Test Loss: 0.2139161089568732\n",
      "Epoch: 1543 Training Loss: 0.21726465926345362 Test Loss: 0.21377538523249176\n",
      "Epoch: 1544 Training Loss: 0.2172634755177188 Test Loss: 0.21374763196297256\n",
      "Epoch: 1545 Training Loss: 0.2172663754970732 Test Loss: 0.21378676653591908\n",
      "Epoch: 1546 Training Loss: 0.21726471923569624 Test Loss: 0.213774166733036\n",
      "Epoch: 1547 Training Loss: 0.21726467386874337 Test Loss: 0.21586324516167568\n",
      "Epoch: 1548 Training Loss: 0.217266145640834 Test Loss: 0.21373342477782867\n",
      "Epoch: 1549 Training Loss: 0.21726558894528666 Test Loss: 0.21375426889617846\n",
      "Epoch: 1550 Training Loss: 0.217264245957961 Test Loss: 0.21370232711618614\n",
      "Epoch: 1551 Training Loss: 0.21726586830171293 Test Loss: 0.21373230998045423\n",
      "Epoch: 1552 Training Loss: 0.21726487325919092 Test Loss: 0.2137082251720625\n",
      "Epoch: 1553 Training Loss: 0.21726572919731105 Test Loss: 0.21364346322226427\n",
      "Epoch: 1554 Training Loss: 0.2172625229461959 Test Loss: 0.21449787763320324\n",
      "Epoch: 1555 Training Loss: 0.21726890324326917 Test Loss: 0.2137913035019778\n",
      "Epoch: 1556 Training Loss: 0.21726615157619422 Test Loss: 0.21379022759288388\n",
      "Epoch: 1557 Training Loss: 0.21726498744763192 Test Loss: 0.21379385716573085\n",
      "Epoch: 1558 Training Loss: 0.2172637158011845 Test Loss: 0.2137797536826683\n",
      "Epoch: 1559 Training Loss: 0.21726579729256934 Test Loss: 0.21389640556141817\n",
      "Epoch: 1560 Training Loss: 0.21726423770045833 Test Loss: 0.213782048091218\n",
      "Epoch: 1561 Training Loss: 0.21726405639403132 Test Loss: 0.2136940568751991\n",
      "Epoch: 1562 Training Loss: 0.217263645823108 Test Loss: 0.21373953023786768\n",
      "Epoch: 1563 Training Loss: 0.21726566683119958 Test Loss: 0.2137797925709488\n",
      "Epoch: 1564 Training Loss: 0.21726462739003122 Test Loss: 0.21379904226979798\n",
      "Epoch: 1565 Training Loss: 0.21726421602115162 Test Loss: 0.21366238885210923\n",
      "Epoch: 1566 Training Loss: 0.2172650236425704 Test Loss: 0.21377406303095467\n",
      "Epoch: 1567 Training Loss: 0.21726481986784618 Test Loss: 0.21378033700687588\n",
      "Epoch: 1568 Training Loss: 0.21726332507157872 Test Loss: 0.21372505083476026\n",
      "Epoch: 1569 Training Loss: 0.21726322339939583 Test Loss: 0.2137589614153592\n",
      "Epoch: 1570 Training Loss: 0.2172652781547622 Test Loss: 0.21383436579125517\n",
      "Epoch: 1571 Training Loss: 0.21726262948680872 Test Loss: 0.21372865448208692\n",
      "Epoch: 1572 Training Loss: 0.21726302939739467 Test Loss: 0.21435195584199454\n",
      "Epoch: 1573 Training Loss: 0.21726282219773452 Test Loss: 0.21434898736991612\n",
      "Epoch: 1574 Training Loss: 0.21726421162790913 Test Loss: 0.21394349926910772\n",
      "Epoch: 1575 Training Loss: 0.21726339449377555 Test Loss: 0.21380685881417916\n",
      "Epoch: 1576 Training Loss: 0.21726252396829723 Test Loss: 0.2136769201062573\n",
      "Epoch: 1577 Training Loss: 0.21726555129788844 Test Loss: 0.2138192252873792\n",
      "Epoch: 1578 Training Loss: 0.21726453975829263 Test Loss: 0.2137134102761296\n",
      "Epoch: 1579 Training Loss: 0.21726518244483697 Test Loss: 0.2137300674229452\n",
      "Epoch: 1580 Training Loss: 0.21726425934390184 Test Loss: 0.21382733997524425\n",
      "Epoch: 1581 Training Loss: 0.21726452047285477 Test Loss: 0.21371411026517867\n",
      "Epoch: 1582 Training Loss: 0.21726566933265806 Test Loss: 0.21374253759822662\n",
      "Epoch: 1583 Training Loss: 0.2172641609890653 Test Loss: 0.21366093702297043\n",
      "Epoch: 1584 Training Loss: 0.2172649668083581 Test Loss: 0.21377268897837687\n",
      "Epoch: 1585 Training Loss: 0.21726496245097884 Test Loss: 0.21378701282836227\n",
      "Epoch: 1586 Training Loss: 0.21726445305003156 Test Loss: 0.21375228559387277\n",
      "Epoch: 1587 Training Loss: 0.21726450654896587 Test Loss: 0.21483805934828634\n",
      "Epoch: 1588 Training Loss: 0.21726296465534595 Test Loss: 0.21395582685402728\n",
      "Epoch: 1589 Training Loss: 0.21726295969725803 Test Loss: 0.21386005798190771\n",
      "Epoch: 1590 Training Loss: 0.2172647507863498 Test Loss: 0.21376750387430976\n",
      "Epoch: 1591 Training Loss: 0.21726376025362565 Test Loss: 0.21371277510088138\n",
      "Epoch: 1592 Training Loss: 0.21726329343126713 Test Loss: 0.2137532059498447\n",
      "Epoch: 1593 Training Loss: 0.21726397138927245 Test Loss: 0.21378943686451365\n",
      "Epoch: 1594 Training Loss: 0.21726396115929356 Test Loss: 0.21378860724786292\n",
      "Epoch: 1595 Training Loss: 0.21726434947709888 Test Loss: 0.21380645696861394\n",
      "Epoch: 1596 Training Loss: 0.21726388357821785 Test Loss: 0.21374105984356748\n",
      "Epoch: 1597 Training Loss: 0.21726510076639025 Test Loss: 0.21370996218192498\n",
      "Epoch: 1598 Training Loss: 0.21726477863412758 Test Loss: 0.21375670589509\n",
      "Epoch: 1599 Training Loss: 0.2172650330566614 Test Loss: 0.21375223374283212\n",
      "Epoch: 1600 Training Loss: 0.21726330096253993 Test Loss: 0.2137357840001792\n",
      "Epoch: 1601 Training Loss: 0.2172660774648835 Test Loss: 0.21383068436736752\n",
      "Epoch: 1602 Training Loss: 0.21726477533471283 Test Loss: 0.21386607270262556\n",
      "Epoch: 1603 Training Loss: 0.21726299516596662 Test Loss: 0.2137719889893278\n",
      "Epoch: 1604 Training Loss: 0.2172655894563373 Test Loss: 0.21378769985465115\n",
      "Epoch: 1605 Training Loss: 0.21726393246873046 Test Loss: 0.21381748827751673\n",
      "Epoch: 1606 Training Loss: 0.21726316631414114 Test Loss: 0.21377506116348757\n",
      "Epoch: 1607 Training Loss: 0.21726414930662666 Test Loss: 0.21412898340434852\n",
      "Epoch: 1608 Training Loss: 0.21726568531868115 Test Loss: 0.21373472105384544\n",
      "Epoch: 1609 Training Loss: 0.2172641422057123 Test Loss: 0.21367340719825184\n",
      "Epoch: 1610 Training Loss: 0.21726458114442987 Test Loss: 0.21377084826643306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1611 Training Loss: 0.21726476158117417 Test Loss: 0.2137810888469656\n",
      "Epoch: 1612 Training Loss: 0.21726336839432894 Test Loss: 0.2137005771435635\n",
      "Epoch: 1613 Training Loss: 0.21726498573516392 Test Loss: 0.21371685837033425\n",
      "Epoch: 1614 Training Loss: 0.21726417055557495 Test Loss: 0.2137323618314949\n",
      "Epoch: 1615 Training Loss: 0.21726419519359597 Test Loss: 0.2137012512070922\n",
      "Epoch: 1616 Training Loss: 0.21726483335241084 Test Loss: 0.21924276636501805\n",
      "Epoch: 1617 Training Loss: 0.21726598221221413 Test Loss: 0.21390161659100562\n",
      "Epoch: 1618 Training Loss: 0.21726658427471432 Test Loss: 0.21386709676067883\n",
      "Epoch: 1619 Training Loss: 0.2172629585675671 Test Loss: 0.21372883596072928\n",
      "Epoch: 1620 Training Loss: 0.2172652828976709 Test Loss: 0.21371238621807634\n",
      "Epoch: 1621 Training Loss: 0.21726386569144493 Test Loss: 0.2137289526255708\n",
      "Epoch: 1622 Training Loss: 0.21726389572687815 Test Loss: 0.2138358046576338\n",
      "Epoch: 1623 Training Loss: 0.2172650909667698 Test Loss: 0.21374624494763458\n",
      "Epoch: 1624 Training Loss: 0.21726576373357634 Test Loss: 0.21367891637132314\n",
      "Epoch: 1625 Training Loss: 0.21726523198088726 Test Loss: 0.21372905632765213\n",
      "Epoch: 1626 Training Loss: 0.2172648477425214 Test Loss: 0.21459774273753585\n",
      "Epoch: 1627 Training Loss: 0.21726526514538502 Test Loss: 0.2137730778611819\n",
      "Epoch: 1628 Training Loss: 0.21726551874306516 Test Loss: 0.21376260395096633\n",
      "Epoch: 1629 Training Loss: 0.21726395643431645 Test Loss: 0.21380569216576403\n",
      "Epoch: 1630 Training Loss: 0.21726545548037357 Test Loss: 0.21376235765852314\n",
      "Epoch: 1631 Training Loss: 0.21726529818436155 Test Loss: 0.21384105457550176\n",
      "Epoch: 1632 Training Loss: 0.2172647864343744 Test Loss: 0.21375137820066104\n",
      "Epoch: 1633 Training Loss: 0.21726453783064542 Test Loss: 0.21374236908234442\n",
      "Epoch: 1634 Training Loss: 0.21726697995344224 Test Loss: 0.21371687133309442\n",
      "Epoch: 1635 Training Loss: 0.21726433219103464 Test Loss: 0.21373744323348068\n",
      "Epoch: 1636 Training Loss: 0.2172639158461355 Test Loss: 0.21371617134404533\n",
      "Epoch: 1637 Training Loss: 0.2172647713359656 Test Loss: 0.21371661207789105\n",
      "Epoch: 1638 Training Loss: 0.21726388341683342 Test Loss: 0.2137613724887504\n",
      "Epoch: 1639 Training Loss: 0.21726566424904892 Test Loss: 0.2138183438196878\n",
      "Epoch: 1640 Training Loss: 0.2172647254310647 Test Loss: 0.21389465558879553\n",
      "Epoch: 1641 Training Loss: 0.21726670616477836 Test Loss: 0.21371272324984072\n",
      "Epoch: 1642 Training Loss: 0.2172636922838886 Test Loss: 0.21588910586821042\n",
      "Epoch: 1643 Training Loss: 0.21726009717702005 Test Loss: 0.22090104042224168\n",
      "Epoch: 1644 Training Loss: 0.2172615158088084 Test Loss: 0.21461910536629233\n",
      "Epoch: 1645 Training Loss: 0.21726480644604215 Test Loss: 0.2140979764820272\n",
      "Epoch: 1646 Training Loss: 0.21726080404076767 Test Loss: 0.21396591188143782\n",
      "Epoch: 1647 Training Loss: 0.2172610389447528 Test Loss: 0.21383179916474196\n",
      "Epoch: 1648 Training Loss: 0.21726303764593158 Test Loss: 0.2138181882665658\n",
      "Epoch: 1649 Training Loss: 0.21726123603098946 Test Loss: 0.21373361921923117\n",
      "Epoch: 1650 Training Loss: 0.21726153019891892 Test Loss: 0.2137729223080599\n",
      "Epoch: 1651 Training Loss: 0.21726490894307873 Test Loss: 0.21373579696293937\n",
      "Epoch: 1652 Training Loss: 0.21726367035353944 Test Loss: 0.21375373742301157\n",
      "Epoch: 1653 Training Loss: 0.21726446216825113 Test Loss: 0.21379459604306042\n",
      "Epoch: 1654 Training Loss: 0.21726310666466736 Test Loss: 0.21375914289400155\n",
      "Epoch: 1655 Training Loss: 0.21726458888191608 Test Loss: 0.21379157571994134\n",
      "Epoch: 1656 Training Loss: 0.21726337660700262 Test Loss: 0.21376453540223134\n",
      "Epoch: 1657 Training Loss: 0.21726278338478217 Test Loss: 0.21381904380873687\n",
      "Epoch: 1658 Training Loss: 0.21726324245172288 Test Loss: 0.2145887854702599\n",
      "Epoch: 1659 Training Loss: 0.2172630193288002 Test Loss: 0.21376514465195923\n",
      "Epoch: 1660 Training Loss: 0.2172650256598756 Test Loss: 0.21372567304724832\n",
      "Epoch: 1661 Training Loss: 0.2172654481822116 Test Loss: 0.21376240950956382\n",
      "Epoch: 1662 Training Loss: 0.21726398838843106 Test Loss: 0.2137559410922401\n",
      "Epoch: 1663 Training Loss: 0.21726504325974288 Test Loss: 0.2138568821056666\n",
      "Epoch: 1664 Training Loss: 0.2172648853540564 Test Loss: 0.21377587781737814\n",
      "Epoch: 1665 Training Loss: 0.2172637307740721 Test Loss: 0.2149203080615509\n",
      "Epoch: 1666 Training Loss: 0.2172640767374337 Test Loss: 0.21380742917562653\n",
      "Epoch: 1667 Training Loss: 0.21726295816410607 Test Loss: 0.2136828829759345\n",
      "Epoch: 1668 Training Loss: 0.21726537021560646 Test Loss: 0.21374453386329245\n",
      "Epoch: 1669 Training Loss: 0.21726473713143493 Test Loss: 0.21374923934523335\n",
      "Epoch: 1670 Training Loss: 0.21726553178830554 Test Loss: 0.21370796591685914\n",
      "Epoch: 1671 Training Loss: 0.21726589311904998 Test Loss: 0.2137182453856722\n",
      "Epoch: 1672 Training Loss: 0.2172653881920374 Test Loss: 0.21371941203408729\n",
      "Epoch: 1673 Training Loss: 0.21726506545010027 Test Loss: 0.2137296266890995\n",
      "Epoch: 1674 Training Loss: 0.21726659326741268 Test Loss: 0.21376392615250345\n",
      "Epoch: 1675 Training Loss: 0.2172637205440932 Test Loss: 0.21373356736819052\n",
      "Epoch: 1676 Training Loss: 0.21726589808610372 Test Loss: 0.2137746074668817\n",
      "Epoch: 1677 Training Loss: 0.21726475839831483 Test Loss: 0.2137424598216656\n",
      "Epoch: 1678 Training Loss: 0.21726428025214967 Test Loss: 0.2137330877460643\n",
      "Epoch: 1679 Training Loss: 0.21726516510497793 Test Loss: 0.21371614541852502\n",
      "Epoch: 1680 Training Loss: 0.21726513443297285 Test Loss: 0.21397651541925508\n",
      "Epoch: 1681 Training Loss: 0.21726687813780654 Test Loss: 0.2139992002495487\n",
      "Epoch: 1682 Training Loss: 0.21726494265449034 Test Loss: 0.214324254423516\n",
      "Epoch: 1683 Training Loss: 0.21726170273679257 Test Loss: 0.21399551882566104\n",
      "Epoch: 1684 Training Loss: 0.21726550845929143 Test Loss: 0.2138488840826431\n",
      "Epoch: 1685 Training Loss: 0.21726513911312095 Test Loss: 0.21370112157949056\n",
      "Epoch: 1686 Training Loss: 0.2172665683783492 Test Loss: 0.21382638073099183\n",
      "Epoch: 1687 Training Loss: 0.21726399600039606 Test Loss: 0.21377486672208507\n",
      "Epoch: 1688 Training Loss: 0.21726515895443846 Test Loss: 0.2137919905282667\n",
      "Epoch: 1689 Training Loss: 0.21726424733869434 Test Loss: 0.21374143576361235\n",
      "Epoch: 1690 Training Loss: 0.21726514029660668 Test Loss: 0.2137989126421963\n",
      "Epoch: 1691 Training Loss: 0.21726440590785012 Test Loss: 0.213731597028645\n",
      "Epoch: 1692 Training Loss: 0.2172635383769493 Test Loss: 0.21378995537492035\n",
      "Epoch: 1693 Training Loss: 0.21726408466320177 Test Loss: 0.2137365099147486\n",
      "Epoch: 1694 Training Loss: 0.21726429986035636 Test Loss: 0.21377880740117605\n",
      "Epoch: 1695 Training Loss: 0.21726344312427995 Test Loss: 0.21369830866053413\n",
      "Epoch: 1696 Training Loss: 0.21726593666594524 Test Loss: 0.21389062417038335\n",
      "Epoch: 1697 Training Loss: 0.21726152712364918 Test Loss: 0.21414168690931296\n",
      "Epoch: 1698 Training Loss: 0.21726424785871082 Test Loss: 0.21396482300958372\n",
      "Epoch: 1699 Training Loss: 0.21726227866398298 Test Loss: 0.2138735133269619\n",
      "Epoch: 1700 Training Loss: 0.21726412295613765 Test Loss: 0.21385018035865988\n",
      "Epoch: 1701 Training Loss: 0.21726608077326406 Test Loss: 0.21531515373626156\n",
      "Epoch: 1702 Training Loss: 0.2172656669118918 Test Loss: 0.2137219008840395\n",
      "Epoch: 1703 Training Loss: 0.21726493255003265 Test Loss: 0.21366530547314697\n",
      "Epoch: 1704 Training Loss: 0.21726655473240017 Test Loss: 0.21372247124548688\n",
      "Epoch: 1705 Training Loss: 0.21726450771452 Test Loss: 0.21372200458612084\n",
      "Epoch: 1706 Training Loss: 0.21726442551605682 Test Loss: 0.21378419990940586\n",
      "Epoch: 1707 Training Loss: 0.21726442790992567 Test Loss: 0.2137293415083758\n",
      "Epoch: 1708 Training Loss: 0.2172649769128158 Test Loss: 0.21372879707244877\n",
      "Epoch: 1709 Training Loss: 0.2172636261969697 Test Loss: 0.21378775170569184\n",
      "Epoch: 1710 Training Loss: 0.21726348451938296 Test Loss: 0.2137227045751699\n",
      "Epoch: 1711 Training Loss: 0.21726522536412618 Test Loss: 0.21376677795974036\n",
      "Epoch: 1712 Training Loss: 0.21726571420649185 Test Loss: 0.21367354978861366\n",
      "Epoch: 1713 Training Loss: 0.21726473208368902 Test Loss: 0.21375902622916004\n",
      "Epoch: 1714 Training Loss: 0.21726410601077384 Test Loss: 0.21379733118545582\n",
      "Epoch: 1715 Training Loss: 0.2172637401433341 Test Loss: 0.21378407028180418\n",
      "Epoch: 1716 Training Loss: 0.21726487637928965 Test Loss: 0.21375596701776042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1717 Training Loss: 0.21726440335259684 Test Loss: 0.21373019705054688\n",
      "Epoch: 1718 Training Loss: 0.21726485401858206 Test Loss: 0.21374338017763753\n",
      "Epoch: 1719 Training Loss: 0.2172660043487767 Test Loss: 0.21388788902798794\n",
      "Epoch: 1720 Training Loss: 0.2172670739150363 Test Loss: 0.21442680281920332\n",
      "Epoch: 1721 Training Loss: 0.21726238229967626 Test Loss: 0.21383096954809122\n",
      "Epoch: 1722 Training Loss: 0.2172617525418169 Test Loss: 0.21370244378102765\n",
      "Epoch: 1723 Training Loss: 0.2172642534085416 Test Loss: 0.21378650728071572\n",
      "Epoch: 1724 Training Loss: 0.21726448668971676 Test Loss: 0.21375124857305935\n",
      "Epoch: 1725 Training Loss: 0.21726470271172507 Test Loss: 0.21374623198487444\n",
      "Epoch: 1726 Training Loss: 0.21726462335542077 Test Loss: 0.21372241939444622\n",
      "Epoch: 1727 Training Loss: 0.2172643513330197 Test Loss: 0.21380206259291706\n",
      "Epoch: 1728 Training Loss: 0.2172646236781896 Test Loss: 0.21376806127299697\n",
      "Epoch: 1729 Training Loss: 0.21726234967312658 Test Loss: 0.21431335274221489\n",
      "Epoch: 1730 Training Loss: 0.21726430631573307 Test Loss: 0.21389002788341563\n",
      "Epoch: 1731 Training Loss: 0.21726314025055773 Test Loss: 0.2137954515852315\n",
      "Epoch: 1732 Training Loss: 0.21726378320607612 Test Loss: 0.2137516374558644\n",
      "Epoch: 1733 Training Loss: 0.21726520087852375 Test Loss: 0.2137396469027092\n",
      "Epoch: 1734 Training Loss: 0.21726463434749274 Test Loss: 0.2137921849696692\n",
      "Epoch: 1735 Training Loss: 0.21726360554873012 Test Loss: 0.21373801359492806\n",
      "Epoch: 1736 Training Loss: 0.21726509435584254 Test Loss: 0.21372433788295106\n",
      "Epoch: 1737 Training Loss: 0.21726536262157306 Test Loss: 0.21405320310840767\n",
      "Epoch: 1738 Training Loss: 0.21726063565405984 Test Loss: 0.2145469416804383\n",
      "Epoch: 1739 Training Loss: 0.21726469237415655 Test Loss: 0.21400588903379528\n",
      "Epoch: 1740 Training Loss: 0.217264129877736 Test Loss: 0.21393093835450516\n",
      "Epoch: 1741 Training Loss: 0.2172614738757573 Test Loss: 0.2137679705336758\n",
      "Epoch: 1742 Training Loss: 0.21726540115658557 Test Loss: 0.21376332986553573\n",
      "Epoch: 1743 Training Loss: 0.21726326435517462 Test Loss: 0.21374818936165976\n",
      "Epoch: 1744 Training Loss: 0.2172654042228895 Test Loss: 0.21372213421372252\n",
      "Epoch: 1745 Training Loss: 0.21726528311285015 Test Loss: 0.21373818211081025\n",
      "Epoch: 1746 Training Loss: 0.21726287928298924 Test Loss: 0.21371346212717027\n",
      "Epoch: 1747 Training Loss: 0.21726534405339926 Test Loss: 0.213628763452234\n",
      "Epoch: 1748 Training Loss: 0.21726555354830446 Test Loss: 0.2136904791533928\n",
      "Epoch: 1749 Training Loss: 0.21726508868049055 Test Loss: 0.21370720111400923\n",
      "Epoch: 1750 Training Loss: 0.2172643580663362 Test Loss: 0.21375937622368457\n",
      "Epoch: 1751 Training Loss: 0.21726392398708277 Test Loss: 0.21375785958074492\n",
      "Epoch: 1752 Training Loss: 0.21726416520299177 Test Loss: 0.21380045521065627\n",
      "Epoch: 1753 Training Loss: 0.21726524761724414 Test Loss: 0.2137735963715886\n",
      "Epoch: 1754 Training Loss: 0.21726385728152364 Test Loss: 0.2137596095533676\n",
      "Epoch: 1755 Training Loss: 0.21726593402999977 Test Loss: 0.2138026459171246\n",
      "Epoch: 1756 Training Loss: 0.21726515597779256 Test Loss: 0.21367940895620952\n",
      "Epoch: 1757 Training Loss: 0.21726482906675798 Test Loss: 0.21372997668362403\n",
      "Epoch: 1758 Training Loss: 0.21726341062325147 Test Loss: 0.21371535469015476\n",
      "Epoch: 1759 Training Loss: 0.2172669094553493 Test Loss: 0.2145097255959966\n",
      "Epoch: 1760 Training Loss: 0.2172650040702269 Test Loss: 0.21369688275691567\n",
      "Epoch: 1761 Training Loss: 0.21726515995860818 Test Loss: 0.2137153417273946\n",
      "Epoch: 1762 Training Loss: 0.21726468192899845 Test Loss: 0.21363136896702772\n",
      "Epoch: 1763 Training Loss: 0.21726544249789378 Test Loss: 0.21371834908775353\n",
      "Epoch: 1764 Training Loss: 0.217264627309339 Test Loss: 0.21377379081299114\n",
      "Epoch: 1765 Training Loss: 0.21726517007203167 Test Loss: 0.21369023286094962\n",
      "Epoch: 1766 Training Loss: 0.21726510654036604 Test Loss: 0.21366079443260858\n",
      "Epoch: 1767 Training Loss: 0.2172667777029042 Test Loss: 0.2137299896463842\n",
      "Epoch: 1768 Training Loss: 0.21726437086949996 Test Loss: 0.21408238228154533\n",
      "Epoch: 1769 Training Loss: 0.2172640691702977 Test Loss: 0.21373656176578926\n",
      "Epoch: 1770 Training Loss: 0.2172659922359796 Test Loss: 0.21373202479973055\n",
      "Epoch: 1771 Training Loss: 0.21726669087808773 Test Loss: 0.21371681948205373\n",
      "Epoch: 1772 Training Loss: 0.21726484451483302 Test Loss: 0.21378614432343102\n",
      "Epoch: 1773 Training Loss: 0.21726641928604506 Test Loss: 0.21374295240655197\n",
      "Epoch: 1774 Training Loss: 0.2172660699156791 Test Loss: 0.21371565283363864\n",
      "Epoch: 1775 Training Loss: 0.217266175505917 Test Loss: 0.21367195536911304\n",
      "Epoch: 1776 Training Loss: 0.21726409667737503 Test Loss: 0.21369097173827917\n",
      "Epoch: 1777 Training Loss: 0.21726611738062937 Test Loss: 0.21376988902218064\n",
      "Epoch: 1778 Training Loss: 0.217264752803655 Test Loss: 0.21769480539682223\n",
      "Epoch: 1779 Training Loss: 0.2172667897888039 Test Loss: 0.21387776511229692\n",
      "Epoch: 1780 Training Loss: 0.21726358568051518 Test Loss: 0.2143042139962966\n",
      "Epoch: 1781 Training Loss: 0.21726499990112946 Test Loss: 0.21447466132974274\n",
      "Epoch: 1782 Training Loss: 0.21726298202210242 Test Loss: 0.21410215049080122\n",
      "Epoch: 1783 Training Loss: 0.21726485518413619 Test Loss: 0.21392650509052777\n",
      "Epoch: 1784 Training Loss: 0.21726707839793677 Test Loss: 0.21384460637178773\n",
      "Epoch: 1785 Training Loss: 0.2172656492223665 Test Loss: 0.21384548783947913\n",
      "Epoch: 1786 Training Loss: 0.21726458414797317 Test Loss: 0.21381419573643412\n",
      "Epoch: 1787 Training Loss: 0.2172643729316342 Test Loss: 0.21368845696280664\n",
      "Epoch: 1788 Training Loss: 0.21726576913995432 Test Loss: 0.21386930042990734\n",
      "Epoch: 1789 Training Loss: 0.2172649478456891 Test Loss: 0.21374795603197674\n",
      "Epoch: 1790 Training Loss: 0.21726497858942057 Test Loss: 0.21375736699585857\n",
      "Epoch: 1791 Training Loss: 0.21726471919086723 Test Loss: 0.21375516332663003\n",
      "Epoch: 1792 Training Loss: 0.21726377715416045 Test Loss: 0.2137219786606005\n",
      "Epoch: 1793 Training Loss: 0.21726516079242766 Test Loss: 0.21376968161801796\n",
      "Epoch: 1794 Training Loss: 0.21726360104789802 Test Loss: 0.2137395172751075\n",
      "Epoch: 1795 Training Loss: 0.21726216791840955 Test Loss: 0.21374362647008072\n",
      "Epoch: 1796 Training Loss: 0.21726310656604353 Test Loss: 0.21380115519970533\n",
      "Epoch: 1797 Training Loss: 0.21726477464434615 Test Loss: 0.2162585834212727\n",
      "Epoch: 1798 Training Loss: 0.21726520911809483 Test Loss: 0.21380487551187347\n",
      "Epoch: 1799 Training Loss: 0.21726433882572735 Test Loss: 0.21366406104817087\n",
      "Epoch: 1800 Training Loss: 0.21726558711626326 Test Loss: 0.21375708181513486\n",
      "Epoch: 1801 Training Loss: 0.21726434041267412 Test Loss: 0.21369002545678692\n",
      "Epoch: 1802 Training Loss: 0.21726470428074024 Test Loss: 0.21375885771327785\n",
      "Epoch: 1803 Training Loss: 0.2172644702643694 Test Loss: 0.21375143005170172\n",
      "Epoch: 1804 Training Loss: 0.2172645797816281 Test Loss: 0.21471681865243708\n",
      "Epoch: 1805 Training Loss: 0.2172635530718971 Test Loss: 0.2147634068124801\n",
      "Epoch: 1806 Training Loss: 0.21726243564619196 Test Loss: 0.21896017819336042\n",
      "Epoch: 1807 Training Loss: 0.21726863868938026 Test Loss: 0.21415122750079643\n",
      "Epoch: 1808 Training Loss: 0.2172624181897775 Test Loss: 0.21379724044613466\n",
      "Epoch: 1809 Training Loss: 0.21726445517492637 Test Loss: 0.21385710247258946\n",
      "Epoch: 1810 Training Loss: 0.21726372747465736 Test Loss: 0.21374476719297547\n",
      "Epoch: 1811 Training Loss: 0.2172644413317297 Test Loss: 0.2138237363279176\n",
      "Epoch: 1812 Training Loss: 0.21726312553767838 Test Loss: 0.21376391318974328\n",
      "Epoch: 1813 Training Loss: 0.21726392153045332 Test Loss: 0.2137591169684812\n",
      "Epoch: 1814 Training Loss: 0.21726393718474177 Test Loss: 0.2137733241536251\n",
      "Epoch: 1815 Training Loss: 0.2172654975658433 Test Loss: 0.213743419065918\n",
      "Epoch: 1816 Training Loss: 0.2172646703631152 Test Loss: 0.2137823332719417\n",
      "Epoch: 1817 Training Loss: 0.2172647460703385 Test Loss: 0.21372760449851333\n",
      "Epoch: 1818 Training Loss: 0.21726500203499008 Test Loss: 0.21369242356741797\n",
      "Epoch: 1819 Training Loss: 0.21726549445471036 Test Loss: 0.2136959494381836\n",
      "Epoch: 1820 Training Loss: 0.21726574383846403 Test Loss: 0.21364837610836784\n",
      "Epoch: 1821 Training Loss: 0.21726446147788445 Test Loss: 0.21377322045154376\n",
      "Epoch: 1822 Training Loss: 0.21726495426520256 Test Loss: 0.21368970138778273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1823 Training Loss: 0.21726476917520757 Test Loss: 0.2137776666782813\n",
      "Epoch: 1824 Training Loss: 0.21726432519770988 Test Loss: 0.21374892823898933\n",
      "Epoch: 1825 Training Loss: 0.2172633613292778 Test Loss: 0.2137523115193931\n",
      "Epoch: 1826 Training Loss: 0.21726433289933292 Test Loss: 0.2136519797556945\n",
      "Epoch: 1827 Training Loss: 0.21726587350187748 Test Loss: 0.21385763394575635\n",
      "Epoch: 1828 Training Loss: 0.2172643074543898 Test Loss: 0.21375078191369332\n",
      "Epoch: 1829 Training Loss: 0.2172635551609287 Test Loss: 0.2137411376201285\n",
      "Epoch: 1830 Training Loss: 0.21726511497718476 Test Loss: 0.21374172094433605\n",
      "Epoch: 1831 Training Loss: 0.21726406960962194 Test Loss: 0.21364758537999762\n",
      "Epoch: 1832 Training Loss: 0.21726561691858565 Test Loss: 0.21362138764169852\n",
      "Epoch: 1833 Training Loss: 0.2172667225273651 Test Loss: 0.2136981271818918\n",
      "Epoch: 1834 Training Loss: 0.21726526044730532 Test Loss: 0.2145595025950409\n",
      "Epoch: 1835 Training Loss: 0.21726510386855738 Test Loss: 0.21369246245569848\n",
      "Epoch: 1836 Training Loss: 0.21726509706351443 Test Loss: 0.21371519913703277\n",
      "Epoch: 1837 Training Loss: 0.21726495353000688 Test Loss: 0.21380062372653844\n",
      "Epoch: 1838 Training Loss: 0.21726409655185383 Test Loss: 0.21380571809128437\n",
      "Epoch: 1839 Training Loss: 0.21726489023145212 Test Loss: 0.2138153234965687\n",
      "Epoch: 1840 Training Loss: 0.21726344256840027 Test Loss: 0.21374485793229664\n",
      "Epoch: 1841 Training Loss: 0.21726155685424517 Test Loss: 0.2139027573139004\n",
      "Epoch: 1842 Training Loss: 0.2172630501621897 Test Loss: 0.2138027236936856\n",
      "Epoch: 1843 Training Loss: 0.21726337020542075 Test Loss: 0.21377268897837687\n",
      "Epoch: 1844 Training Loss: 0.2172636670541247 Test Loss: 0.21371882870987974\n",
      "Epoch: 1845 Training Loss: 0.21726673491810203 Test Loss: 0.21375828735183047\n",
      "Epoch: 1846 Training Loss: 0.21726652189963705 Test Loss: 0.21378055737379872\n",
      "Epoch: 1847 Training Loss: 0.21726680699417594 Test Loss: 0.21375145597722203\n",
      "Epoch: 1848 Training Loss: 0.2172638643555406 Test Loss: 0.2137728445314989\n",
      "Epoch: 1849 Training Loss: 0.21726568079095168 Test Loss: 0.21383726944953277\n",
      "Epoch: 1850 Training Loss: 0.2172659475683592 Test Loss: 0.2137455579213457\n",
      "Epoch: 1851 Training Loss: 0.21726418919547513 Test Loss: 0.2138408342085789\n",
      "Epoch: 1852 Training Loss: 0.21726187418980433 Test Loss: 0.21371764909870447\n",
      "Epoch: 1853 Training Loss: 0.21726453518573413 Test Loss: 0.21370442708333334\n",
      "Epoch: 1854 Training Loss: 0.21726546698349622 Test Loss: 0.2137401654131159\n",
      "Epoch: 1855 Training Loss: 0.21726442411739186 Test Loss: 0.21383015289420065\n",
      "Epoch: 1856 Training Loss: 0.2172645563001954 Test Loss: 0.21374977081840024\n",
      "Epoch: 1857 Training Loss: 0.21726372101031485 Test Loss: 0.21370904182595307\n",
      "Epoch: 1858 Training Loss: 0.21726647058835816 Test Loss: 0.21374647827731763\n",
      "Epoch: 1859 Training Loss: 0.21726464387813918 Test Loss: 0.21373499327180898\n",
      "Epoch: 1860 Training Loss: 0.2172658527819115 Test Loss: 0.21366019814564086\n",
      "Epoch: 1861 Training Loss: 0.21726248119246086 Test Loss: 0.21497621644615456\n",
      "Epoch: 1862 Training Loss: 0.21726701578974866 Test Loss: 0.2138505303531844\n",
      "Epoch: 1863 Training Loss: 0.2172635260400072 Test Loss: 0.21704395817155808\n",
      "Epoch: 1864 Training Loss: 0.21726599250495363 Test Loss: 0.21393522902812068\n",
      "Epoch: 1865 Training Loss: 0.21726460996947994 Test Loss: 0.21377325933982425\n",
      "Epoch: 1866 Training Loss: 0.21726419038792666 Test Loss: 0.21377035568154668\n",
      "Epoch: 1867 Training Loss: 0.21726442869891616 Test Loss: 0.2137506263605713\n",
      "Epoch: 1868 Training Loss: 0.21726459434208886 Test Loss: 0.21380391626762105\n",
      "Epoch: 1869 Training Loss: 0.2172642373238947 Test Loss: 0.21375708181513486\n",
      "Epoch: 1870 Training Loss: 0.2172646207732701 Test Loss: 0.21366988132748618\n",
      "Epoch: 1871 Training Loss: 0.2172638242066839 Test Loss: 0.21371456396178454\n",
      "Epoch: 1872 Training Loss: 0.2172651648270381 Test Loss: 0.21373740434520017\n",
      "Epoch: 1873 Training Loss: 0.21726608600929181 Test Loss: 0.2137775889017203\n",
      "Epoch: 1874 Training Loss: 0.21726420968233032 Test Loss: 0.21372665821702108\n",
      "Epoch: 1875 Training Loss: 0.21726537849104074 Test Loss: 0.21372039720386005\n",
      "Epoch: 1876 Training Loss: 0.21726636580504236 Test Loss: 0.21375587627843926\n",
      "Epoch: 1877 Training Loss: 0.2172656790695179 Test Loss: 0.21366363327708532\n",
      "Epoch: 1878 Training Loss: 0.21726461308957867 Test Loss: 0.2136993716068679\n",
      "Epoch: 1879 Training Loss: 0.2172652418253367 Test Loss: 0.21372707302534644\n",
      "Epoch: 1880 Training Loss: 0.21726458210377056 Test Loss: 0.21373514882493097\n",
      "Epoch: 1881 Training Loss: 0.21726689560318682 Test Loss: 0.21380050706169693\n",
      "Epoch: 1882 Training Loss: 0.21726417248322216 Test Loss: 0.21364741686411542\n",
      "Epoch: 1883 Training Loss: 0.21726602013755217 Test Loss: 0.21368119781711267\n",
      "Epoch: 1884 Training Loss: 0.21726537561301865 Test Loss: 0.2136653313986673\n",
      "Epoch: 1885 Training Loss: 0.2172654876407016 Test Loss: 0.21428331802690612\n",
      "Epoch: 1886 Training Loss: 0.21726568544420238 Test Loss: 0.21371083068685623\n",
      "Epoch: 1887 Training Loss: 0.21726361212066223 Test Loss: 0.21367041280065308\n",
      "Epoch: 1888 Training Loss: 0.21726530636117203 Test Loss: 0.2139728080698471\n",
      "Epoch: 1889 Training Loss: 0.2172605239401799 Test Loss: 0.21437067406767682\n",
      "Epoch: 1890 Training Loss: 0.21726053357841593 Test Loss: 0.213968011848585\n",
      "Epoch: 1891 Training Loss: 0.21726368614231492 Test Loss: 0.21380194592807555\n",
      "Epoch: 1892 Training Loss: 0.21726304125914936 Test Loss: 0.21379630712740258\n",
      "Epoch: 1893 Training Loss: 0.21726368841962837 Test Loss: 0.21374698382496415\n",
      "Epoch: 1894 Training Loss: 0.21726213161588145 Test Loss: 0.21367161833734868\n",
      "Epoch: 1895 Training Loss: 0.21726523594377128 Test Loss: 0.2137944016016579\n",
      "Epoch: 1896 Training Loss: 0.21726351534380667 Test Loss: 0.2136771145476598\n",
      "Epoch: 1897 Training Loss: 0.2172639352660604 Test Loss: 0.21363170599879208\n",
      "Epoch: 1898 Training Loss: 0.2172638692060389 Test Loss: 0.21364850573596952\n",
      "Epoch: 1899 Training Loss: 0.21726465683372156 Test Loss: 0.21380890693028565\n",
      "Epoch: 1900 Training Loss: 0.2172667241412093 Test Loss: 0.21367352386309335\n",
      "Epoch: 1901 Training Loss: 0.2172657717758998 Test Loss: 0.2137314803638035\n",
      "Epoch: 1902 Training Loss: 0.2172637598680962 Test Loss: 0.2137384802542941\n",
      "Epoch: 1903 Training Loss: 0.21726304552687062 Test Loss: 0.21374050244488027\n",
      "Epoch: 1904 Training Loss: 0.21726557690421597 Test Loss: 0.21368394592226825\n",
      "Epoch: 1905 Training Loss: 0.21726680767557682 Test Loss: 0.21377841851837104\n",
      "Epoch: 1906 Training Loss: 0.2172651573854233 Test Loss: 0.21374533755442285\n",
      "Epoch: 1907 Training Loss: 0.21726441570747057 Test Loss: 0.21372235458064537\n",
      "Epoch: 1908 Training Loss: 0.21726696149285807 Test Loss: 0.21379014981632288\n",
      "Epoch: 1909 Training Loss: 0.2172642272463344 Test Loss: 0.21370526966274425\n",
      "Epoch: 1910 Training Loss: 0.21726447794806084 Test Loss: 0.2136564389451922\n",
      "Epoch: 1911 Training Loss: 0.21726593012987636 Test Loss: 0.2137562651612443\n",
      "Epoch: 1912 Training Loss: 0.21726420111102462 Test Loss: 0.21368920880289635\n",
      "Epoch: 1913 Training Loss: 0.2172633408334568 Test Loss: 0.21375032821708745\n",
      "Epoch: 1914 Training Loss: 0.21726646594407326 Test Loss: 0.21365312047858925\n",
      "Epoch: 1915 Training Loss: 0.2172649218269347 Test Loss: 0.21372126570879127\n",
      "Epoch: 1916 Training Loss: 0.2172639701071629 Test Loss: 0.21369146432316555\n",
      "Epoch: 1917 Training Loss: 0.21726496084610047 Test Loss: 0.21367755528150553\n",
      "Epoch: 1918 Training Loss: 0.21726487082049303 Test Loss: 0.2137069029705254\n",
      "Epoch: 1919 Training Loss: 0.21726472596004695 Test Loss: 0.2136585000240589\n",
      "Epoch: 1920 Training Loss: 0.21726504919510312 Test Loss: 0.21368088671086863\n",
      "Epoch: 1921 Training Loss: 0.2172648284301861 Test Loss: 0.2137000197448763\n",
      "Epoch: 1922 Training Loss: 0.21726534041328407 Test Loss: 0.21374811158509877\n",
      "Epoch: 1923 Training Loss: 0.21726415949177655 Test Loss: 0.21375636886332564\n",
      "Epoch: 1924 Training Loss: 0.2172643226155592 Test Loss: 0.21380968469589573\n",
      "Epoch: 1925 Training Loss: 0.21726344948103282 Test Loss: 0.21375902622916004\n",
      "Epoch: 1926 Training Loss: 0.21726534321957977 Test Loss: 0.2137251934251221\n",
      "Epoch: 1927 Training Loss: 0.21726411386481548 Test Loss: 0.21371054550613253\n",
      "Epoch: 1928 Training Loss: 0.21726646420470788 Test Loss: 0.21370657890152117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1929 Training Loss: 0.21726351554105428 Test Loss: 0.21375233744491345\n",
      "Epoch: 1930 Training Loss: 0.21726407437942805 Test Loss: 0.21369712904935886\n",
      "Epoch: 1931 Training Loss: 0.2172655070516607 Test Loss: 0.21378334436723478\n",
      "Epoch: 1932 Training Loss: 0.21726559081913904 Test Loss: 0.21368828844692445\n",
      "Epoch: 1933 Training Loss: 0.21726348358693967 Test Loss: 0.21371068809649438\n",
      "Epoch: 1934 Training Loss: 0.21726687933025807 Test Loss: 0.2139380419470771\n",
      "Epoch: 1935 Training Loss: 0.21726480808678372 Test Loss: 0.21500763817680127\n",
      "Epoch: 1936 Training Loss: 0.21726169957186484 Test Loss: 0.21485222764514972\n",
      "Epoch: 1937 Training Loss: 0.21726486180089727 Test Loss: 0.21421828385914438\n",
      "Epoch: 1938 Training Loss: 0.21726573917624753 Test Loss: 0.21403158122444782\n",
      "Epoch: 1939 Training Loss: 0.21726001485303564 Test Loss: 0.2148151541510699\n",
      "Epoch: 1940 Training Loss: 0.2172651573854233 Test Loss: 0.21771152735743868\n",
      "Epoch: 1941 Training Loss: 0.21726495525144068 Test Loss: 0.21390655540262957\n",
      "Epoch: 1942 Training Loss: 0.21726436022709422 Test Loss: 0.21384946740685065\n",
      "Epoch: 1943 Training Loss: 0.2172645069165637 Test Loss: 0.21381441610335697\n",
      "Epoch: 1944 Training Loss: 0.21726497635693615 Test Loss: 0.21377726483271609\n",
      "Epoch: 1945 Training Loss: 0.217264492625077 Test Loss: 0.2136454983756106\n",
      "Epoch: 1946 Training Loss: 0.21726466225803115 Test Loss: 0.21369336984891021\n",
      "Epoch: 1947 Training Loss: 0.21726495014093414 Test Loss: 0.2136952235236142\n",
      "Epoch: 1948 Training Loss: 0.21726700070030566 Test Loss: 0.213625056102826\n",
      "Epoch: 1949 Training Loss: 0.21726811973088916 Test Loss: 0.21372639896181772\n",
      "Epoch: 1950 Training Loss: 0.21726457549597525 Test Loss: 0.21373914135506264\n",
      "Epoch: 1951 Training Loss: 0.21726655269716333 Test Loss: 0.21372228976684454\n",
      "Epoch: 1952 Training Loss: 0.2172642448999965 Test Loss: 0.21518726314444622\n",
      "Epoch: 1953 Training Loss: 0.21726335503528552 Test Loss: 0.215134323231921\n",
      "Epoch: 1954 Training Loss: 0.21726491504878917 Test Loss: 0.21426082763801502\n",
      "Epoch: 1955 Training Loss: 0.21726420340626965 Test Loss: 0.21399773545764972\n",
      "Epoch: 1956 Training Loss: 0.21726256077291015 Test Loss: 0.2138055106871217\n",
      "Epoch: 1957 Training Loss: 0.21726442959549624 Test Loss: 0.21379223682070989\n",
      "Epoch: 1958 Training Loss: 0.21726352158400414 Test Loss: 0.21369304577990603\n",
      "Epoch: 1959 Training Loss: 0.21726414805141453 Test Loss: 0.21430260661403577\n",
      "Epoch: 1960 Training Loss: 0.21726398301791627 Test Loss: 0.21374538940546353\n",
      "Epoch: 1961 Training Loss: 0.2172658754384905 Test Loss: 0.2136428410097762\n",
      "Epoch: 1962 Training Loss: 0.21726656406579894 Test Loss: 0.2137338525489142\n",
      "Epoch: 1963 Training Loss: 0.21726521708869187 Test Loss: 0.2137188416726399\n",
      "Epoch: 1964 Training Loss: 0.21726415314398947 Test Loss: 0.21371841390155438\n",
      "Epoch: 1965 Training Loss: 0.21726414621342535 Test Loss: 0.2136841144381504\n",
      "Epoch: 1966 Training Loss: 0.21726485395582143 Test Loss: 0.2138016866728722\n",
      "Epoch: 1967 Training Loss: 0.21726607594069736 Test Loss: 0.21375082080197383\n",
      "Epoch: 1968 Training Loss: 0.21726608926387758 Test Loss: 0.21374234315682408\n",
      "Epoch: 1969 Training Loss: 0.21726547750038072 Test Loss: 0.21378016849099368\n",
      "Epoch: 1970 Training Loss: 0.21726424854011167 Test Loss: 0.2137147454404269\n",
      "Epoch: 1971 Training Loss: 0.21726639571495435 Test Loss: 0.21383944719324094\n",
      "Epoch: 1972 Training Loss: 0.21726478504467528 Test Loss: 0.21374839676582244\n",
      "Epoch: 1973 Training Loss: 0.2172638242425471 Test Loss: 0.21370381783360545\n",
      "Epoch: 1974 Training Loss: 0.21726505283521833 Test Loss: 0.21371400656309733\n",
      "Epoch: 1975 Training Loss: 0.21726473254094486 Test Loss: 0.21381227724792928\n",
      "Epoch: 1976 Training Loss: 0.21726459512211355 Test Loss: 0.2136798496900552\n",
      "Epoch: 1977 Training Loss: 0.21726455426495858 Test Loss: 0.21360801007320537\n",
      "Epoch: 1978 Training Loss: 0.2172638671080415 Test Loss: 0.2137324914590966\n",
      "Epoch: 1979 Training Loss: 0.2172648751599407 Test Loss: 0.21374404127840607\n",
      "Epoch: 1980 Training Loss: 0.21726461445238043 Test Loss: 0.2137321544273322\n",
      "Epoch: 1981 Training Loss: 0.21726454555916586 Test Loss: 0.21378593691926834\n",
      "Epoch: 1982 Training Loss: 0.21726502528331196 Test Loss: 0.21372330086213762\n",
      "Epoch: 1983 Training Loss: 0.21726543062717332 Test Loss: 0.21380583475612588\n",
      "Epoch: 1984 Training Loss: 0.21726473508723232 Test Loss: 0.21373645806370792\n",
      "Epoch: 1985 Training Loss: 0.21726533116057747 Test Loss: 0.213677283063542\n",
      "Epoch: 1986 Training Loss: 0.21726498268679162 Test Loss: 0.21372502490923995\n",
      "Epoch: 1987 Training Loss: 0.21726396748018323 Test Loss: 0.21362040247192576\n",
      "Epoch: 1988 Training Loss: 0.21726610380640674 Test Loss: 0.21368127559367367\n",
      "Epoch: 1989 Training Loss: 0.21726533333030132 Test Loss: 0.2137377154514442\n",
      "Epoch: 1990 Training Loss: 0.21726498491927604 Test Loss: 0.21378220364434003\n",
      "Epoch: 1991 Training Loss: 0.21726552895511245 Test Loss: 0.21369890494750185\n",
      "Epoch: 1992 Training Loss: 0.21726496259443165 Test Loss: 0.2137211749694701\n",
      "Epoch: 1993 Training Loss: 0.21726446015991172 Test Loss: 0.21380111631142482\n",
      "Epoch: 1994 Training Loss: 0.21726601264214257 Test Loss: 0.21374415794324758\n",
      "Epoch: 1995 Training Loss: 0.21726516774988922 Test Loss: 0.21368316815665817\n",
      "Epoch: 1996 Training Loss: 0.21726601669468462 Test Loss: 0.21369706423555804\n",
      "Epoch: 1997 Training Loss: 0.21726561556474971 Test Loss: 0.21372826559928187\n",
      "Epoch: 1998 Training Loss: 0.2172648713584411 Test Loss: 0.2136988530964612\n",
      "Epoch: 1999 Training Loss: 0.21726351988946774 Test Loss: 0.21372357308010115\n",
      "Epoch: 2000 Training Loss: 0.2172633829996187 Test Loss: 0.21374557088410587\n",
      "Epoch: 2001 Training Loss: 0.21726537949521046 Test Loss: 0.21374614124555325\n",
      "Epoch: 2002 Training Loss: 0.21726580888535 Test Loss: 0.2138222067222178\n",
      "Epoch: 2003 Training Loss: 0.21726623025109765 Test Loss: 0.21378206105397818\n",
      "Epoch: 2004 Training Loss: 0.21726475041875196 Test Loss: 0.21372738413159048\n",
      "Epoch: 2005 Training Loss: 0.2172637260849582 Test Loss: 0.21377821111420833\n",
      "Epoch: 2006 Training Loss: 0.21726461914149434 Test Loss: 0.21373138962448232\n",
      "Epoch: 2007 Training Loss: 0.21726276797257033 Test Loss: 0.21418353069913454\n",
      "Epoch: 2008 Training Loss: 0.21726575189871908 Test Loss: 0.2137630187592917\n",
      "Epoch: 2009 Training Loss: 0.2172651284886468 Test Loss: 0.21375480036934533\n",
      "Epoch: 2010 Training Loss: 0.21726606294028594 Test Loss: 0.21367074983241743\n",
      "Epoch: 2011 Training Loss: 0.21726343414054738 Test Loss: 0.2137923923738319\n",
      "Epoch: 2012 Training Loss: 0.21726685661091844 Test Loss: 0.21386966338719204\n",
      "Epoch: 2013 Training Loss: 0.21726314008917333 Test Loss: 0.2137548911086665\n",
      "Epoch: 2014 Training Loss: 0.21726555348554386 Test Loss: 0.21375223374283212\n",
      "Epoch: 2015 Training Loss: 0.2172630828515 Test Loss: 0.2137709001174737\n",
      "Epoch: 2016 Training Loss: 0.21726507471177264 Test Loss: 0.21371290472848306\n",
      "Epoch: 2017 Training Loss: 0.21726492625604038 Test Loss: 0.21371233436703568\n",
      "Epoch: 2018 Training Loss: 0.21726529870437802 Test Loss: 0.21374331536383667\n",
      "Epoch: 2019 Training Loss: 0.21726624596814673 Test Loss: 0.21380447366630828\n",
      "Epoch: 2020 Training Loss: 0.21726447837841928 Test Loss: 0.21371127142070193\n",
      "Epoch: 2021 Training Loss: 0.21726441186114195 Test Loss: 0.21376742609774876\n",
      "Epoch: 2022 Training Loss: 0.21726396748018323 Test Loss: 0.21380984024901772\n",
      "Epoch: 2023 Training Loss: 0.21726199335426485 Test Loss: 0.21403163307548848\n",
      "Epoch: 2024 Training Loss: 0.21726282270878516 Test Loss: 0.21375741884689922\n",
      "Epoch: 2025 Training Loss: 0.21726685734611412 Test Loss: 0.21378903501894844\n",
      "Epoch: 2026 Training Loss: 0.21726408371282685 Test Loss: 0.2136727720230036\n",
      "Epoch: 2027 Training Loss: 0.21726392725063431 Test Loss: 0.2138028144330068\n",
      "Epoch: 2028 Training Loss: 0.21726608520236973 Test Loss: 0.21374321166175533\n",
      "Epoch: 2029 Training Loss: 0.2172646974756973 Test Loss: 0.21377163899480328\n",
      "Epoch: 2030 Training Loss: 0.2172651499617401 Test Loss: 0.21377539819525193\n",
      "Epoch: 2031 Training Loss: 0.21726348832984838 Test Loss: 0.21371254177119836\n",
      "Epoch: 2032 Training Loss: 0.21726445424248308 Test Loss: 0.21375979103200993\n",
      "Epoch: 2033 Training Loss: 0.21726415308122887 Test Loss: 0.21371946388512797\n",
      "Epoch: 2034 Training Loss: 0.21726532870394802 Test Loss: 0.21623180235876607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2035 Training Loss: 0.21726418282975646 Test Loss: 0.21377280564321838\n",
      "Epoch: 2036 Training Loss: 0.21726344617265225 Test Loss: 0.21380637919205295\n",
      "Epoch: 2037 Training Loss: 0.21726444368973535 Test Loss: 0.2138003515085749\n",
      "Epoch: 2038 Training Loss: 0.2172647863447164 Test Loss: 0.2137695779159366\n",
      "Epoch: 2039 Training Loss: 0.21726367239774205 Test Loss: 0.21368249409312945\n",
      "Epoch: 2040 Training Loss: 0.21726477170356345 Test Loss: 0.21370965107568096\n",
      "Epoch: 2041 Training Loss: 0.21726486531549125 Test Loss: 0.21377403710543433\n",
      "Epoch: 2042 Training Loss: 0.21726425227885068 Test Loss: 0.21369810125637145\n",
      "Epoch: 2043 Training Loss: 0.21726508683353554 Test Loss: 0.21367143685870632\n",
      "Epoch: 2044 Training Loss: 0.21726596319575028 Test Loss: 0.21379700711645164\n",
      "Epoch: 2045 Training Loss: 0.2172641420353621 Test Loss: 0.2137331266343448\n",
      "Epoch: 2046 Training Loss: 0.21726450997390184 Test Loss: 0.2138238529927591\n",
      "Epoch: 2047 Training Loss: 0.2172647376514514 Test Loss: 0.213805225506398\n",
      "Epoch: 2048 Training Loss: 0.21726405265529233 Test Loss: 0.21382988067623712\n",
      "Epoch: 2049 Training Loss: 0.21726697652850627 Test Loss: 0.2138057051285242\n",
      "Epoch: 2050 Training Loss: 0.21726508285271992 Test Loss: 0.21379848487111075\n",
      "Epoch: 2051 Training Loss: 0.2172630043110836 Test Loss: 0.2137523633704338\n",
      "Epoch: 2052 Training Loss: 0.2172636429271543 Test Loss: 0.2136957031457404\n",
      "Epoch: 2053 Training Loss: 0.21726498032878597 Test Loss: 0.21375897437811936\n",
      "Epoch: 2054 Training Loss: 0.2172651892140167 Test Loss: 0.21372630822249655\n",
      "Epoch: 2055 Training Loss: 0.21726497917219764 Test Loss: 0.21378417398388552\n",
      "Epoch: 2056 Training Loss: 0.2172643264708536 Test Loss: 0.21378404435628384\n",
      "Epoch: 2057 Training Loss: 0.21726383425734677 Test Loss: 0.2137602447286158\n",
      "Epoch: 2058 Training Loss: 0.21726366761897015 Test Loss: 0.21376377059938143\n",
      "Epoch: 2059 Training Loss: 0.21726491040450427 Test Loss: 0.2138159327462966\n",
      "Epoch: 2060 Training Loss: 0.21726502454811628 Test Loss: 0.21377968886886747\n",
      "Epoch: 2061 Training Loss: 0.2172645701882211 Test Loss: 0.2137268137701431\n",
      "Epoch: 2062 Training Loss: 0.21726574139080035 Test Loss: 0.21370613816767547\n",
      "Epoch: 2063 Training Loss: 0.21726551526433438 Test Loss: 0.21369584573610226\n",
      "Epoch: 2064 Training Loss: 0.2172646974936289 Test Loss: 0.2137161065302445\n",
      "Epoch: 2065 Training Loss: 0.21726575353946065 Test Loss: 0.21374761900021239\n",
      "Epoch: 2066 Training Loss: 0.21726396009236323 Test Loss: 0.21376047805829881\n",
      "Epoch: 2067 Training Loss: 0.21726281297192535 Test Loss: 0.2137909405446931\n",
      "Epoch: 2068 Training Loss: 0.2172637494408697 Test Loss: 0.21375193559934824\n",
      "Epoch: 2069 Training Loss: 0.21726438004151435 Test Loss: 0.21694500046043724\n",
      "Epoch: 2070 Training Loss: 0.2172661054830115 Test Loss: 0.21384347861165312\n",
      "Epoch: 2071 Training Loss: 0.2172646216250212 Test Loss: 0.21380258110332379\n",
      "Epoch: 2072 Training Loss: 0.2172633045219629 Test Loss: 0.2156731851720957\n",
      "Epoch: 2073 Training Loss: 0.21725885683018378 Test Loss: 0.21559205125620554\n",
      "Epoch: 2074 Training Loss: 0.21726292083051088 Test Loss: 0.21456966539901243\n",
      "Epoch: 2075 Training Loss: 0.21726326296547546 Test Loss: 0.21419526199708638\n",
      "Epoch: 2076 Training Loss: 0.21726227269275955 Test Loss: 0.21401929252780874\n",
      "Epoch: 2077 Training Loss: 0.21726502034315565 Test Loss: 0.21395326022751407\n",
      "Epoch: 2078 Training Loss: 0.2172634660319014 Test Loss: 0.21387142632257486\n",
      "Epoch: 2079 Training Loss: 0.21726398281170287 Test Loss: 0.213834197275373\n",
      "Epoch: 2080 Training Loss: 0.2172642300705617 Test Loss: 0.21375189671106776\n",
      "Epoch: 2081 Training Loss: 0.2172642487911541 Test Loss: 0.21368303852905648\n",
      "Epoch: 2082 Training Loss: 0.2172654734478387 Test Loss: 0.21378356473415763\n",
      "Epoch: 2083 Training Loss: 0.21726345676126319 Test Loss: 0.21371282695192206\n",
      "Epoch: 2084 Training Loss: 0.2172652416549865 Test Loss: 0.21368872918077014\n",
      "Epoch: 2085 Training Loss: 0.21726405923619024 Test Loss: 0.2138342361636535\n",
      "Epoch: 2086 Training Loss: 0.21726505866298892 Test Loss: 0.2137242341808697\n",
      "Epoch: 2087 Training Loss: 0.21726428899380562 Test Loss: 0.21374699678772432\n",
      "Epoch: 2088 Training Loss: 0.2172646330833148 Test Loss: 0.21371132327174258\n",
      "Epoch: 2089 Training Loss: 0.21726543830189896 Test Loss: 0.21365846113577838\n",
      "Epoch: 2090 Training Loss: 0.21726559418131441 Test Loss: 0.21378340918103564\n",
      "Epoch: 2091 Training Loss: 0.2172649349618331 Test Loss: 0.21395367503583945\n",
      "Epoch: 2092 Training Loss: 0.21726426400611834 Test Loss: 0.2139160311803122\n",
      "Epoch: 2093 Training Loss: 0.2172657382438042 Test Loss: 0.21388530943871456\n",
      "Epoch: 2094 Training Loss: 0.21726341023772203 Test Loss: 0.21382911587338724\n",
      "Epoch: 2095 Training Loss: 0.21726240092164487 Test Loss: 0.2145217939257128\n",
      "Epoch: 2096 Training Loss: 0.21726722623502878 Test Loss: 0.21386182091729053\n",
      "Epoch: 2097 Training Loss: 0.21726584378921313 Test Loss: 0.21385820430720373\n",
      "Epoch: 2098 Training Loss: 0.21726591478042512 Test Loss: 0.2137660390824108\n",
      "Epoch: 2099 Training Loss: 0.2172648263142571 Test Loss: 0.21373663954235028\n",
      "Epoch: 2100 Training Loss: 0.2172659372397565 Test Loss: 0.21370209378650312\n",
      "Epoch: 2101 Training Loss: 0.2172661710140507 Test Loss: 0.21379784969586255\n",
      "Epoch: 2102 Training Loss: 0.21726430696127072 Test Loss: 0.2137319599859297\n",
      "Epoch: 2103 Training Loss: 0.21726547872869548 Test Loss: 0.21422265230932092\n",
      "Epoch: 2104 Training Loss: 0.21726400319993425 Test Loss: 0.21376610389621165\n",
      "Epoch: 2105 Training Loss: 0.217266882217246 Test Loss: 0.21371411026517867\n",
      "Epoch: 2106 Training Loss: 0.21726430701506555 Test Loss: 0.21377069271331103\n",
      "Epoch: 2107 Training Loss: 0.21726447330377593 Test Loss: 0.21373929690818466\n",
      "Epoch: 2108 Training Loss: 0.21726317662481223 Test Loss: 0.21423924464233568\n",
      "Epoch: 2109 Training Loss: 0.21726637144453115 Test Loss: 0.2137678020177936\n",
      "Epoch: 2110 Training Loss: 0.21726487617307622 Test Loss: 0.2137610484197462\n",
      "Epoch: 2111 Training Loss: 0.21726610776929076 Test Loss: 0.2137527263277185\n",
      "Epoch: 2112 Training Loss: 0.2172656566550155 Test Loss: 0.21377494449864606\n",
      "Epoch: 2113 Training Loss: 0.21726443705504264 Test Loss: 0.21379563306387384\n",
      "Epoch: 2114 Training Loss: 0.21726119645594402 Test Loss: 0.21411143182708134\n",
      "Epoch: 2115 Training Loss: 0.2172638035494785 Test Loss: 0.2138745503477753\n",
      "Epoch: 2116 Training Loss: 0.2172651668084801 Test Loss: 0.21393054947170012\n",
      "Epoch: 2117 Training Loss: 0.2172641176573493 Test Loss: 0.21374601161795156\n",
      "Epoch: 2118 Training Loss: 0.21726237765539136 Test Loss: 0.21371676763101305\n",
      "Epoch: 2119 Training Loss: 0.21726487720414334 Test Loss: 0.21365625746654987\n",
      "Epoch: 2120 Training Loss: 0.2172655818981671 Test Loss: 0.2136885865904083\n",
      "Epoch: 2121 Training Loss: 0.21726462507685457 Test Loss: 0.21368353111394287\n",
      "Epoch: 2122 Training Loss: 0.21726365011772666 Test Loss: 0.2141002968160972\n",
      "Epoch: 2123 Training Loss: 0.21726434363139666 Test Loss: 0.21467466375637145\n",
      "Epoch: 2124 Training Loss: 0.2172649720892149 Test Loss: 0.21380553661264204\n",
      "Epoch: 2125 Training Loss: 0.21726558778869834 Test Loss: 0.21370114750501087\n",
      "Epoch: 2126 Training Loss: 0.2172655088806841 Test Loss: 0.21368353111394287\n",
      "Epoch: 2127 Training Loss: 0.21726407258626787 Test Loss: 0.21372497305819926\n",
      "Epoch: 2128 Training Loss: 0.21726467897028412 Test Loss: 0.21375249299803548\n",
      "Epoch: 2129 Training Loss: 0.21726532807634194 Test Loss: 0.21375067821161198\n",
      "Epoch: 2130 Training Loss: 0.21726431746918945 Test Loss: 0.21373862284465595\n",
      "Epoch: 2131 Training Loss: 0.2172635429584736 Test Loss: 0.21369436798144315\n",
      "Epoch: 2132 Training Loss: 0.21726396579461263 Test Loss: 0.2137030659935157\n",
      "Epoch: 2133 Training Loss: 0.21726514070006772 Test Loss: 0.213778029635566\n",
      "Epoch: 2134 Training Loss: 0.21726492157589228 Test Loss: 0.21381655495878465\n",
      "Epoch: 2135 Training Loss: 0.21726384933782397 Test Loss: 0.21482854468232318\n",
      "Epoch: 2136 Training Loss: 0.2172656454836275 Test Loss: 0.21373317848538548\n",
      "Epoch: 2137 Training Loss: 0.21726335245313483 Test Loss: 0.21365790373709118\n",
      "Epoch: 2138 Training Loss: 0.21726564672987383 Test Loss: 0.21371869908227806\n",
      "Epoch: 2139 Training Loss: 0.2172649331417755 Test Loss: 0.2137128528774424\n",
      "Epoch: 2140 Training Loss: 0.21726583088742554 Test Loss: 0.21384183234111181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2141 Training Loss: 0.2172650444790918 Test Loss: 0.21372193977232\n",
      "Epoch: 2142 Training Loss: 0.21726429296565544 Test Loss: 0.2136905569299538\n",
      "Epoch: 2143 Training Loss: 0.21726424453239865 Test Loss: 0.2136772441752615\n",
      "Epoch: 2144 Training Loss: 0.21726501674786947 Test Loss: 0.2137654039071626\n",
      "Epoch: 2145 Training Loss: 0.21726363419446418 Test Loss: 0.21375285595532018\n",
      "Epoch: 2146 Training Loss: 0.21726492796850835 Test Loss: 0.21376698536390304\n",
      "Epoch: 2147 Training Loss: 0.21726424256888824 Test Loss: 0.21381684013950833\n",
      "Epoch: 2148 Training Loss: 0.2172658370917598 Test Loss: 0.21374850046790378\n",
      "Epoch: 2149 Training Loss: 0.21726645669136668 Test Loss: 0.21373434513380057\n",
      "Epoch: 2150 Training Loss: 0.21726470837811127 Test Loss: 0.21377970183162764\n",
      "Epoch: 2151 Training Loss: 0.21726436239681807 Test Loss: 0.21384048421405438\n",
      "Epoch: 2152 Training Loss: 0.2172634904816406 Test Loss: 0.2137720408403685\n",
      "Epoch: 2153 Training Loss: 0.21726521595003517 Test Loss: 0.21373487660696744\n",
      "Epoch: 2154 Training Loss: 0.21726439852003013 Test Loss: 0.2137299507581037\n",
      "Epoch: 2155 Training Loss: 0.21726479786577063 Test Loss: 0.21365455934496788\n",
      "Epoch: 2156 Training Loss: 0.2172649499795497 Test Loss: 0.21361537292098068\n",
      "Epoch: 2157 Training Loss: 0.21726622165289453 Test Loss: 0.21373443587312174\n",
      "Epoch: 2158 Training Loss: 0.21726568902155696 Test Loss: 0.2137401265248354\n",
      "Epoch: 2159 Training Loss: 0.21726450456752386 Test Loss: 0.21377293527082006\n",
      "Epoch: 2160 Training Loss: 0.21726386779840817 Test Loss: 0.21377598151945948\n",
      "Epoch: 2161 Training Loss: 0.2172655950420313 Test Loss: 0.21374164316777503\n",
      "Epoch: 2162 Training Loss: 0.2172647722325457 Test Loss: 0.2136536649145163\n",
      "Epoch: 2163 Training Loss: 0.21726325006368788 Test Loss: 0.21365340565931296\n",
      "Epoch: 2164 Training Loss: 0.2172666197165255 Test Loss: 0.21374010059931506\n",
      "Epoch: 2165 Training Loss: 0.21726349166512632 Test Loss: 0.21376259098820616\n",
      "Epoch: 2166 Training Loss: 0.21726064603645737 Test Loss: 0.21489759730573696\n",
      "Epoch: 2167 Training Loss: 0.21726659627992181 Test Loss: 0.21387093373768848\n",
      "Epoch: 2168 Training Loss: 0.2172640075304161 Test Loss: 0.21379381827745036\n",
      "Epoch: 2169 Training Loss: 0.2172641494231821 Test Loss: 0.21380304776268982\n",
      "Epoch: 2170 Training Loss: 0.2172632556673135 Test Loss: 0.21378292955890943\n",
      "Epoch: 2171 Training Loss: 0.21726420263521076 Test Loss: 0.21379493307482478\n",
      "Epoch: 2172 Training Loss: 0.2172632100493182 Test Loss: 0.21374312092243417\n",
      "Epoch: 2173 Training Loss: 0.2172634874511999 Test Loss: 0.2136684683866279\n",
      "Epoch: 2174 Training Loss: 0.2172671452290171 Test Loss: 0.21389243895680685\n",
      "Epoch: 2175 Training Loss: 0.2172640468813165 Test Loss: 0.21377503523796723\n",
      "Epoch: 2176 Training Loss: 0.21726552884752282 Test Loss: 0.2137612558239089\n",
      "Epoch: 2177 Training Loss: 0.21726549857897878 Test Loss: 0.2137619817384783\n",
      "Epoch: 2178 Training Loss: 0.21726548040530025 Test Loss: 0.21381743642647605\n",
      "Epoch: 2179 Training Loss: 0.21726563721715902 Test Loss: 0.21371523802531325\n",
      "Epoch: 2180 Training Loss: 0.21726479900442736 Test Loss: 0.21375763921382207\n",
      "Epoch: 2181 Training Loss: 0.21726450973182523 Test Loss: 0.21489005297931932\n",
      "Epoch: 2182 Training Loss: 0.21726143810221146 Test Loss: 0.21501850096982186\n",
      "Epoch: 2183 Training Loss: 0.21726264689839417 Test Loss: 0.2143245266414795\n",
      "Epoch: 2184 Training Loss: 0.21726270850241255 Test Loss: 0.21406572513472974\n",
      "Epoch: 2185 Training Loss: 0.21726265919947307 Test Loss: 0.2139045332120434\n",
      "Epoch: 2186 Training Loss: 0.2172616449163422 Test Loss: 0.2137901368535627\n",
      "Epoch: 2187 Training Loss: 0.2172621946633938 Test Loss: 0.21372235458064537\n",
      "Epoch: 2188 Training Loss: 0.21726269717860594 Test Loss: 0.21375925955884306\n",
      "Epoch: 2189 Training Loss: 0.21726361522282933 Test Loss: 0.2136884310372863\n",
      "Epoch: 2190 Training Loss: 0.2172657520601035 Test Loss: 0.21377607225878067\n",
      "Epoch: 2191 Training Loss: 0.21726332747441338 Test Loss: 0.21366909059911596\n",
      "Epoch: 2192 Training Loss: 0.21726639955231716 Test Loss: 0.21375041895640862\n",
      "Epoch: 2193 Training Loss: 0.2172650190431145 Test Loss: 0.213747126415326\n",
      "Epoch: 2194 Training Loss: 0.21726508395551344 Test Loss: 0.2137469578994438\n",
      "Epoch: 2195 Training Loss: 0.21726381049797422 Test Loss: 0.21372791560475735\n",
      "Epoch: 2196 Training Loss: 0.2172644515258454 Test Loss: 0.21377277971769804\n",
      "Epoch: 2197 Training Loss: 0.21726497315614518 Test Loss: 0.2137662724120938\n",
      "Epoch: 2198 Training Loss: 0.21726456062171146 Test Loss: 0.21373796174388737\n",
      "Epoch: 2199 Training Loss: 0.21726503134419342 Test Loss: 0.21379184793790484\n",
      "Epoch: 2200 Training Loss: 0.21726400515447886 Test Loss: 0.21394462702924233\n",
      "Epoch: 2201 Training Loss: 0.21726286761848218 Test Loss: 0.21384401008482\n",
      "Epoch: 2202 Training Loss: 0.21726388168643385 Test Loss: 0.21387071337076563\n",
      "Epoch: 2203 Training Loss: 0.2172628275592835 Test Loss: 0.21371107697929942\n",
      "Epoch: 2204 Training Loss: 0.21726516462082468 Test Loss: 0.21378251475058405\n",
      "Epoch: 2205 Training Loss: 0.21726440052836954 Test Loss: 0.21368909213805484\n",
      "Epoch: 2206 Training Loss: 0.21726364142089974 Test Loss: 0.213805458836081\n",
      "Epoch: 2207 Training Loss: 0.21726730926731147 Test Loss: 0.21370167897817777\n",
      "Epoch: 2208 Training Loss: 0.21726737456523984 Test Loss: 0.21376182618535627\n",
      "Epoch: 2209 Training Loss: 0.21726639857504484 Test Loss: 0.2137571077406552\n",
      "Epoch: 2210 Training Loss: 0.21726539932756217 Test Loss: 0.21371943795960763\n",
      "Epoch: 2211 Training Loss: 0.21726375656868147 Test Loss: 0.2137719889893278\n",
      "Epoch: 2212 Training Loss: 0.21726582041537001 Test Loss: 0.21372983409326218\n",
      "Epoch: 2213 Training Loss: 0.21726504388734896 Test Loss: 0.21371971017757116\n",
      "Epoch: 2214 Training Loss: 0.2172643364946191 Test Loss: 0.21375515036386986\n",
      "Epoch: 2215 Training Loss: 0.21726508026160343 Test Loss: 0.21386882080778113\n",
      "Epoch: 2216 Training Loss: 0.21726465764960945 Test Loss: 0.213818745665253\n",
      "Epoch: 2217 Training Loss: 0.2172637261118556 Test Loss: 0.21377491857312572\n",
      "Epoch: 2218 Training Loss: 0.21726584896248027 Test Loss: 0.21381179762580307\n",
      "Epoch: 2219 Training Loss: 0.21726443693848724 Test Loss: 0.21374328943831633\n",
      "Epoch: 2220 Training Loss: 0.21726403023182414 Test Loss: 0.21368614959149676\n",
      "Epoch: 2221 Training Loss: 0.21726563375635985 Test Loss: 0.21369270874814167\n",
      "Epoch: 2222 Training Loss: 0.21726349318034668 Test Loss: 0.21391809225917888\n",
      "Epoch: 2223 Training Loss: 0.21726347218244085 Test Loss: 0.2138194197287817\n",
      "Epoch: 2224 Training Loss: 0.21726384364454038 Test Loss: 0.2137598817713311\n",
      "Epoch: 2225 Training Loss: 0.21726234449089363 Test Loss: 0.2138096587703754\n",
      "Epoch: 2226 Training Loss: 0.21726463541442306 Test Loss: 0.21384342676061247\n",
      "Epoch: 2227 Training Loss: 0.21726493357213394 Test Loss: 0.21390231658005468\n",
      "Epoch: 2228 Training Loss: 0.2172656566460497 Test Loss: 0.21382319189199056\n",
      "Epoch: 2229 Training Loss: 0.21726289063369325 Test Loss: 0.21376791868263512\n",
      "Epoch: 2230 Training Loss: 0.2172631510095189 Test Loss: 0.21375931140988372\n",
      "Epoch: 2231 Training Loss: 0.21726534527274818 Test Loss: 0.2137013030581329\n",
      "Epoch: 2232 Training Loss: 0.21726400618554595 Test Loss: 0.21364263360561353\n",
      "Epoch: 2233 Training Loss: 0.21726600107625935 Test Loss: 0.21391177939497716\n",
      "Epoch: 2234 Training Loss: 0.2172675031527572 Test Loss: 0.21425117038169003\n",
      "Epoch: 2235 Training Loss: 0.21726726915431796 Test Loss: 0.21395025286715513\n",
      "Epoch: 2236 Training Loss: 0.2172636933687505 Test Loss: 0.21378559988750398\n",
      "Epoch: 2237 Training Loss: 0.21726602900472933 Test Loss: 0.21373189517212887\n",
      "Epoch: 2238 Training Loss: 0.21726421620943343 Test Loss: 0.2136436058126261\n",
      "Epoch: 2239 Training Loss: 0.21726479978445204 Test Loss: 0.21379976818436736\n",
      "Epoch: 2240 Training Loss: 0.21726447203063218 Test Loss: 0.21371878982159923\n",
      "Epoch: 2241 Training Loss: 0.2172656313893884 Test Loss: 0.21364816870420517\n",
      "Epoch: 2242 Training Loss: 0.21726569345962843 Test Loss: 0.21374813751061908\n",
      "Epoch: 2243 Training Loss: 0.21726592719805943 Test Loss: 0.21378456286669056\n",
      "Epoch: 2244 Training Loss: 0.2172646657188303 Test Loss: 0.21368718661231018\n",
      "Epoch: 2245 Training Loss: 0.21726520936913726 Test Loss: 0.21369903457510353\n",
      "Epoch: 2246 Training Loss: 0.21726548327435655 Test Loss: 0.21509789787584951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2247 Training Loss: 0.2172654389922656 Test Loss: 0.2137108695751367\n",
      "Epoch: 2248 Training Loss: 0.2172652284842249 Test Loss: 0.21374520792682117\n",
      "Epoch: 2249 Training Loss: 0.21726422574904564 Test Loss: 0.21374315981071468\n",
      "Epoch: 2250 Training Loss: 0.21726593582315995 Test Loss: 0.21375496888522752\n",
      "Epoch: 2251 Training Loss: 0.2172631848016227 Test Loss: 0.21377157418100246\n",
      "Epoch: 2252 Training Loss: 0.21726445940678443 Test Loss: 0.21382561592814192\n",
      "Epoch: 2253 Training Loss: 0.2172630277207899 Test Loss: 0.21375744477241956\n",
      "Epoch: 2254 Training Loss: 0.21726503947617487 Test Loss: 0.2137669205501022\n",
      "Epoch: 2255 Training Loss: 0.2172645694619912 Test Loss: 0.21371982684241267\n",
      "Epoch: 2256 Training Loss: 0.21726515708955185 Test Loss: 0.21371539357843528\n",
      "Epoch: 2257 Training Loss: 0.2172646151786103 Test Loss: 0.21381393648123076\n",
      "Epoch: 2258 Training Loss: 0.2172635250717007 Test Loss: 0.21380061076377826\n",
      "Epoch: 2259 Training Loss: 0.2172637819329324 Test Loss: 0.2137844721273694\n",
      "Epoch: 2260 Training Loss: 0.21726241391309045 Test Loss: 0.2142229374900446\n",
      "Epoch: 2261 Training Loss: 0.21726601625536035 Test Loss: 0.21378869798718408\n",
      "Epoch: 2262 Training Loss: 0.2172658435202391 Test Loss: 0.21390340545190878\n",
      "Epoch: 2263 Training Loss: 0.21726485805319248 Test Loss: 0.2137030271052352\n",
      "Epoch: 2264 Training Loss: 0.21726408470803077 Test Loss: 0.213758792899477\n",
      "Epoch: 2265 Training Loss: 0.21726443832818637 Test Loss: 0.2136803422749416\n",
      "Epoch: 2266 Training Loss: 0.21726387886220655 Test Loss: 0.213686862543306\n",
      "Epoch: 2267 Training Loss: 0.2172626060591708 Test Loss: 0.21517026896586625\n",
      "Epoch: 2268 Training Loss: 0.21726271485916543 Test Loss: 0.21377367414814963\n",
      "Epoch: 2269 Training Loss: 0.2172636409098491 Test Loss: 0.21379228867175057\n",
      "Epoch: 2270 Training Loss: 0.21726455985065257 Test Loss: 0.21377612410982133\n",
      "Epoch: 2271 Training Loss: 0.21726411655455577 Test Loss: 0.21380986617453807\n",
      "Epoch: 2272 Training Loss: 0.21726504124243767 Test Loss: 0.2137336581075117\n",
      "Epoch: 2273 Training Loss: 0.21726407517738433 Test Loss: 0.2139101979382367\n",
      "Epoch: 2274 Training Loss: 0.21726727558279726 Test Loss: 0.21382415113624295\n",
      "Epoch: 2275 Training Loss: 0.21726407955269522 Test Loss: 0.21447908163095997\n",
      "Epoch: 2276 Training Loss: 0.2172658314343394 Test Loss: 0.21372349530354015\n",
      "Epoch: 2277 Training Loss: 0.2172665437851572 Test Loss: 0.2137638743014628\n",
      "Epoch: 2278 Training Loss: 0.21726499711276534 Test Loss: 0.213727500796432\n",
      "Epoch: 2279 Training Loss: 0.2172646310839412 Test Loss: 0.21370035677664065\n",
      "Epoch: 2280 Training Loss: 0.21726488990868328 Test Loss: 0.213733217373666\n",
      "Epoch: 2281 Training Loss: 0.21726442575813343 Test Loss: 0.21376270765304767\n",
      "Epoch: 2282 Training Loss: 0.2172645283717254 Test Loss: 0.21432059892514868\n",
      "Epoch: 2283 Training Loss: 0.21726555970780972 Test Loss: 0.21383305655247822\n",
      "Epoch: 2284 Training Loss: 0.21726248373874832 Test Loss: 0.2137745296903207\n",
      "Epoch: 2285 Training Loss: 0.2172667683157106 Test Loss: 0.21377793889624483\n",
      "Epoch: 2286 Training Loss: 0.21726446024956972 Test Loss: 0.2136865384743018\n",
      "Epoch: 2287 Training Loss: 0.21726450031773423 Test Loss: 0.2137178824283875\n",
      "Epoch: 2288 Training Loss: 0.21726508781977366 Test Loss: 0.21375268743943798\n",
      "Epoch: 2289 Training Loss: 0.2172666020001028 Test Loss: 0.2139559824071493\n",
      "Epoch: 2290 Training Loss: 0.21726341877316452 Test Loss: 0.21384744521626448\n",
      "Epoch: 2291 Training Loss: 0.2172657380824198 Test Loss: 0.21378191846361633\n",
      "Epoch: 2292 Training Loss: 0.2172635119995629 Test Loss: 0.21670673196579324\n",
      "Epoch: 2293 Training Loss: 0.2172643439810629 Test Loss: 0.21378162032013248\n",
      "Epoch: 2294 Training Loss: 0.21726431103174437 Test Loss: 0.2136969216451962\n",
      "Epoch: 2295 Training Loss: 0.21726525427883425 Test Loss: 0.21378038885791653\n",
      "Epoch: 2296 Training Loss: 0.21726370939960263 Test Loss: 0.21381555682625172\n",
      "Epoch: 2297 Training Loss: 0.21726337015162592 Test Loss: 0.21378351288311698\n",
      "Epoch: 2298 Training Loss: 0.21726292152984333 Test Loss: 0.21379025351840422\n",
      "Epoch: 2299 Training Loss: 0.2172645104311577 Test Loss: 0.21379511455346714\n",
      "Epoch: 2300 Training Loss: 0.21726413245092085 Test Loss: 0.21379205534206755\n",
      "Epoch: 2301 Training Loss: 0.2172637839143744 Test Loss: 0.21578518341994532\n",
      "Epoch: 2302 Training Loss: 0.2172657868025822 Test Loss: 0.21581584034774212\n",
      "Epoch: 2303 Training Loss: 0.21726092543771264 Test Loss: 0.21519662225728736\n",
      "Epoch: 2304 Training Loss: 0.21726434553214646 Test Loss: 0.21444880062320804\n",
      "Epoch: 2305 Training Loss: 0.21726365737105965 Test Loss: 0.2140623677798463\n",
      "Epoch: 2306 Training Loss: 0.21726439637720368 Test Loss: 0.21394826956484947\n",
      "Epoch: 2307 Training Loss: 0.21726227727428382 Test Loss: 0.2138711670673715\n",
      "Epoch: 2308 Training Loss: 0.2172621265681355 Test Loss: 0.21384926000268795\n",
      "Epoch: 2309 Training Loss: 0.2172635708062514 Test Loss: 0.21377237787213285\n",
      "Epoch: 2310 Training Loss: 0.21726524046253498 Test Loss: 0.21381341797082404\n",
      "Epoch: 2311 Training Loss: 0.2172645515303893 Test Loss: 0.21378899613066793\n",
      "Epoch: 2312 Training Loss: 0.21726515757370513 Test Loss: 0.21376017991481497\n",
      "Epoch: 2313 Training Loss: 0.21726411207165527 Test Loss: 0.21366463140961825\n",
      "Epoch: 2314 Training Loss: 0.217265633433591 Test Loss: 0.21369819199569262\n",
      "Epoch: 2315 Training Loss: 0.21726450594825722 Test Loss: 0.2144395192869279\n",
      "Epoch: 2316 Training Loss: 0.2172661522038003 Test Loss: 0.21364753352895693\n",
      "Epoch: 2317 Training Loss: 0.2172647461599965 Test Loss: 0.2137395691261482\n",
      "Epoch: 2318 Training Loss: 0.21726448496828296 Test Loss: 0.2137616447067139\n",
      "Epoch: 2319 Training Loss: 0.21726497660797855 Test Loss: 0.21376050398381916\n",
      "Epoch: 2320 Training Loss: 0.217263703840806 Test Loss: 0.2137504708074493\n",
      "Epoch: 2321 Training Loss: 0.21726398812842282 Test Loss: 0.21376633722589466\n",
      "Epoch: 2322 Training Loss: 0.21726494416971068 Test Loss: 0.21377440006271903\n",
      "Epoch: 2323 Training Loss: 0.2172685763232688 Test Loss: 0.2137571855172162\n",
      "Epoch: 2324 Training Loss: 0.21726520362205884 Test Loss: 0.2136807441205068\n",
      "Epoch: 2325 Training Loss: 0.2172655771193952 Test Loss: 0.21369256615777982\n",
      "Epoch: 2326 Training Loss: 0.21726545891427534 Test Loss: 0.21367601271304557\n",
      "Epoch: 2327 Training Loss: 0.21726444625395444 Test Loss: 0.21374277092790964\n",
      "Epoch: 2328 Training Loss: 0.21726459594696723 Test Loss: 0.21372030646453888\n",
      "Epoch: 2329 Training Loss: 0.2172658754474563 Test Loss: 0.213700460478722\n",
      "Epoch: 2330 Training Loss: 0.21726468209038285 Test Loss: 0.2137369506485943\n",
      "Epoch: 2331 Training Loss: 0.21726504670261046 Test Loss: 0.21367239610295874\n",
      "Epoch: 2332 Training Loss: 0.2172649162053775 Test Loss: 0.21370410301432913\n",
      "Epoch: 2333 Training Loss: 0.21726506257207814 Test Loss: 0.21431923783533105\n",
      "Epoch: 2334 Training Loss: 0.2172662519841992 Test Loss: 0.21379495900034512\n",
      "Epoch: 2335 Training Loss: 0.21726408533563682 Test Loss: 0.21375981695753027\n",
      "Epoch: 2336 Training Loss: 0.2172640074228265 Test Loss: 0.21374420979428826\n",
      "Epoch: 2337 Training Loss: 0.21726440910864106 Test Loss: 0.21371567875915898\n",
      "Epoch: 2338 Training Loss: 0.2172635381348727 Test Loss: 0.21370582706143146\n",
      "Epoch: 2339 Training Loss: 0.21726425252989312 Test Loss: 0.21367983672729504\n",
      "Epoch: 2340 Training Loss: 0.21726530196792956 Test Loss: 0.2136872255005907\n",
      "Epoch: 2341 Training Loss: 0.21726393055004908 Test Loss: 0.21371492691906924\n",
      "Epoch: 2342 Training Loss: 0.21726116920887492 Test Loss: 0.21375880586223717\n",
      "Epoch: 2343 Training Loss: 0.21726222800720757 Test Loss: 0.2137450005226585\n",
      "Epoch: 2344 Training Loss: 0.21726405019866288 Test Loss: 0.21450336088075422\n",
      "Epoch: 2345 Training Loss: 0.21726798528870378 Test Loss: 0.21388056506849315\n",
      "Epoch: 2346 Training Loss: 0.2172649620116546 Test Loss: 0.21377189825000664\n",
      "Epoch: 2347 Training Loss: 0.2172632100672498 Test Loss: 0.21378544433438196\n",
      "Epoch: 2348 Training Loss: 0.2172656845565881 Test Loss: 0.21380271073092547\n",
      "Epoch: 2349 Training Loss: 0.2172655193437738 Test Loss: 0.21376592241756928\n",
      "Epoch: 2350 Training Loss: 0.2172653640112722 Test Loss: 0.2137189194492009\n",
      "Epoch: 2351 Training Loss: 0.21726571039602643 Test Loss: 0.21368933843049803\n",
      "Epoch: 2352 Training Loss: 0.21726500965592088 Test Loss: 0.2137261267438542\n",
      "Epoch: 2353 Training Loss: 0.2172651011698513 Test Loss: 0.21371548431775644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2354 Training Loss: 0.21726474092396875 Test Loss: 0.213669557258482\n",
      "Epoch: 2355 Training Loss: 0.21726528893165498 Test Loss: 0.21368618847977727\n",
      "Epoch: 2356 Training Loss: 0.21726554786398666 Test Loss: 0.21370450485989434\n",
      "Epoch: 2357 Training Loss: 0.21726606373824223 Test Loss: 0.21366665360020443\n",
      "Epoch: 2358 Training Loss: 0.21726550378810916 Test Loss: 0.21380379960277954\n",
      "Epoch: 2359 Training Loss: 0.2172645311062947 Test Loss: 0.21372655451493974\n",
      "Epoch: 2360 Training Loss: 0.21726480464391618 Test Loss: 0.21373880432329828\n",
      "Epoch: 2361 Training Loss: 0.21726412402306797 Test Loss: 0.2137325173846169\n",
      "Epoch: 2362 Training Loss: 0.217265025731602 Test Loss: 0.21374755418641153\n",
      "Epoch: 2363 Training Loss: 0.21726137419398225 Test Loss: 0.21432937471378224\n",
      "Epoch: 2364 Training Loss: 0.21726609207017328 Test Loss: 0.21382395669484044\n",
      "Epoch: 2365 Training Loss: 0.21726381949963838 Test Loss: 0.213822530791222\n",
      "Epoch: 2366 Training Loss: 0.21726459589317243 Test Loss: 0.21382473446045053\n",
      "Epoch: 2367 Training Loss: 0.21726438010427496 Test Loss: 0.21375993362237178\n",
      "Epoch: 2368 Training Loss: 0.21726452270533922 Test Loss: 0.21376091879214454\n",
      "Epoch: 2369 Training Loss: 0.21726472801321536 Test Loss: 0.21390599800394233\n",
      "Epoch: 2370 Training Loss: 0.21726548116739333 Test Loss: 0.2138018033377137\n",
      "Epoch: 2371 Training Loss: 0.2172652975657213 Test Loss: 0.21376965569249762\n",
      "Epoch: 2372 Training Loss: 0.21726422864499936 Test Loss: 0.2139649526371854\n",
      "Epoch: 2373 Training Loss: 0.2172661570722302 Test Loss: 0.2137147454404269\n",
      "Epoch: 2374 Training Loss: 0.21726560374782403 Test Loss: 0.21372271753793007\n",
      "Epoch: 2375 Training Loss: 0.2172649881828276 Test Loss: 0.21377861295977355\n",
      "Epoch: 2376 Training Loss: 0.21726535810280936 Test Loss: 0.21377805556108634\n",
      "Epoch: 2377 Training Loss: 0.21726482674461553 Test Loss: 0.21377961109230648\n",
      "Epoch: 2378 Training Loss: 0.21726428748755106 Test Loss: 0.21377464635516222\n",
      "Epoch: 2379 Training Loss: 0.21726418210352658 Test Loss: 0.2137409691042463\n",
      "Epoch: 2380 Training Loss: 0.2172645085931685 Test Loss: 0.21400848158582883\n",
      "Epoch: 2381 Training Loss: 0.2172650246019111 Test Loss: 0.2142368594944648\n",
      "Epoch: 2382 Training Loss: 0.21726166502663374 Test Loss: 0.21394357704566874\n",
      "Epoch: 2383 Training Loss: 0.2172637485532554 Test Loss: 0.2138522544002867\n",
      "Epoch: 2384 Training Loss: 0.21726458076786623 Test Loss: 0.213843854531698\n",
      "Epoch: 2385 Training Loss: 0.21726455208626896 Test Loss: 0.21375560406047572\n",
      "Epoch: 2386 Training Loss: 0.21726407745469778 Test Loss: 0.21370511410962223\n",
      "Epoch: 2387 Training Loss: 0.21726652530664142 Test Loss: 0.2137060474283543\n",
      "Epoch: 2388 Training Loss: 0.21726465593714145 Test Loss: 0.21377620188638233\n",
      "Epoch: 2389 Training Loss: 0.21726269773448562 Test Loss: 0.21386298756570563\n",
      "Epoch: 2390 Training Loss: 0.21726193910220326 Test Loss: 0.21371302139332457\n",
      "Epoch: 2391 Training Loss: 0.21726400253646497 Test Loss: 0.2138082069412366\n",
      "Epoch: 2392 Training Loss: 0.2172624181897775 Test Loss: 0.21377281860597855\n",
      "Epoch: 2393 Training Loss: 0.21726365190192107 Test Loss: 0.2138370101943294\n",
      "Epoch: 2394 Training Loss: 0.21726410982123925 Test Loss: 0.21375327076364553\n",
      "Epoch: 2395 Training Loss: 0.2172622854152311 Test Loss: 0.21510230521430657\n",
      "Epoch: 2396 Training Loss: 0.21726428730823502 Test Loss: 0.21445084873931453\n",
      "Epoch: 2397 Training Loss: 0.21726371990752133 Test Loss: 0.21405167350270787\n",
      "Epoch: 2398 Training Loss: 0.217262936906192 Test Loss: 0.2139624248989527\n",
      "Epoch: 2399 Training Loss: 0.21726492503669145 Test Loss: 0.21382568074194278\n",
      "Epoch: 2400 Training Loss: 0.21726517191002084 Test Loss: 0.21384329713301078\n",
      "Epoch: 2401 Training Loss: 0.21726436414514924 Test Loss: 0.21376357615797892\n",
      "Epoch: 2402 Training Loss: 0.21726429163871688 Test Loss: 0.213820754893079\n",
      "Epoch: 2403 Training Loss: 0.21726427198568118 Test Loss: 0.21380718288318334\n",
      "Epoch: 2404 Training Loss: 0.21726351711006944 Test Loss: 0.21377322045154376\n",
      "Epoch: 2405 Training Loss: 0.2172643972020574 Test Loss: 0.21371873797055857\n",
      "Epoch: 2406 Training Loss: 0.21726535139639025 Test Loss: 0.21371471951490656\n",
      "Epoch: 2407 Training Loss: 0.2172647049442095 Test Loss: 0.21367366645345517\n",
      "Epoch: 2408 Training Loss: 0.21726475108222124 Test Loss: 0.21373521363873182\n",
      "Epoch: 2409 Training Loss: 0.21726488303191396 Test Loss: 0.21372393603738585\n",
      "Epoch: 2410 Training Loss: 0.21726385109512097 Test Loss: 0.2137940775326537\n",
      "Epoch: 2411 Training Loss: 0.21726427363538856 Test Loss: 0.21405919190360517\n",
      "Epoch: 2412 Training Loss: 0.2172620760817103 Test Loss: 0.213940893754314\n",
      "Epoch: 2413 Training Loss: 0.21726555952849372 Test Loss: 0.21381527164552802\n",
      "Epoch: 2414 Training Loss: 0.2172638691432783 Test Loss: 0.21384862482743974\n",
      "Epoch: 2415 Training Loss: 0.21726319735374405 Test Loss: 0.21371814168359085\n",
      "Epoch: 2416 Training Loss: 0.21726305840176077 Test Loss: 0.21370567150830944\n",
      "Epoch: 2417 Training Loss: 0.21726557410688607 Test Loss: 0.21372844707792424\n",
      "Epoch: 2418 Training Loss: 0.2172647900475922 Test Loss: 0.21385028406074122\n",
      "Epoch: 2419 Training Loss: 0.2172646283583377 Test Loss: 0.21379389605401136\n",
      "Epoch: 2420 Training Loss: 0.21726458557353553 Test Loss: 0.21372944521045714\n",
      "Epoch: 2421 Training Loss: 0.21726292367266978 Test Loss: 0.21372169347987682\n",
      "Epoch: 2422 Training Loss: 0.2172645339215562 Test Loss: 0.2150909887246801\n",
      "Epoch: 2423 Training Loss: 0.2172663832883542 Test Loss: 0.21373577103741903\n",
      "Epoch: 2424 Training Loss: 0.2172653313040303 Test Loss: 0.21377823703972867\n",
      "Epoch: 2425 Training Loss: 0.21726455517050447 Test Loss: 0.21374393757632473\n",
      "Epoch: 2426 Training Loss: 0.2172659623260676 Test Loss: 0.21370911960251407\n",
      "Epoch: 2427 Training Loss: 0.21726528021689642 Test Loss: 0.21375009488740443\n",
      "Epoch: 2428 Training Loss: 0.21726467853095988 Test Loss: 0.2137518059717466\n",
      "Epoch: 2429 Training Loss: 0.2172662028516099 Test Loss: 0.21378038885791653\n",
      "Epoch: 2430 Training Loss: 0.2172648209706397 Test Loss: 0.2145914817243748\n",
      "Epoch: 2431 Training Loss: 0.21726536452232284 Test Loss: 0.2136675869189365\n",
      "Epoch: 2432 Training Loss: 0.21726677735323796 Test Loss: 0.21365262789370287\n",
      "Epoch: 2433 Training Loss: 0.2172646254265208 Test Loss: 0.21366880541839228\n",
      "Epoch: 2434 Training Loss: 0.217265891316924 Test Loss: 0.21370704556088724\n",
      "Epoch: 2435 Training Loss: 0.21726477074422273 Test Loss: 0.2136608333208891\n",
      "Epoch: 2436 Training Loss: 0.2172649601288364 Test Loss: 0.21379536084591033\n",
      "Epoch: 2437 Training Loss: 0.21726363293028625 Test Loss: 0.21398074127906977\n",
      "Epoch: 2438 Training Loss: 0.217263954381148 Test Loss: 0.2138395897836028\n",
      "Epoch: 2439 Training Loss: 0.21726316945217147 Test Loss: 0.21378943686451365\n",
      "Epoch: 2440 Training Loss: 0.21726486085948818 Test Loss: 0.2136753127239965\n",
      "Epoch: 2441 Training Loss: 0.217265059200937 Test Loss: 0.21363892625620554\n",
      "Epoch: 2442 Training Loss: 0.2172664785499894 Test Loss: 0.2137532837264057\n",
      "Epoch: 2443 Training Loss: 0.21726524055219298 Test Loss: 0.21373102666719762\n",
      "Epoch: 2444 Training Loss: 0.2172648713225779 Test Loss: 0.21371637874820804\n",
      "Epoch: 2445 Training Loss: 0.21726642123162387 Test Loss: 0.21390125363372092\n",
      "Epoch: 2446 Training Loss: 0.21726321488188494 Test Loss: 0.21375275225323884\n",
      "Epoch: 2447 Training Loss: 0.21726593101749064 Test Loss: 0.21375769106486275\n",
      "Epoch: 2448 Training Loss: 0.2172638334593905 Test Loss: 0.21378116662352661\n",
      "Epoch: 2449 Training Loss: 0.21726492128898664 Test Loss: 0.21376904644276973\n",
      "Epoch: 2450 Training Loss: 0.2172647741422613 Test Loss: 0.21390732020547945\n",
      "Epoch: 2451 Training Loss: 0.21726278333995316 Test Loss: 0.21422638558424922\n",
      "Epoch: 2452 Training Loss: 0.21726582817975365 Test Loss: 0.21393101613106616\n",
      "Epoch: 2453 Training Loss: 0.21726510377889935 Test Loss: 0.21397077291650074\n",
      "Epoch: 2454 Training Loss: 0.21726483639181737 Test Loss: 0.2137950886279468\n",
      "Epoch: 2455 Training Loss: 0.21726409128892865 Test Loss: 0.21376890385240788\n",
      "Epoch: 2456 Training Loss: 0.21726468341732139 Test Loss: 0.21370346783908092\n",
      "Epoch: 2457 Training Loss: 0.2172646506742163 Test Loss: 0.213727500796432\n",
      "Epoch: 2458 Training Loss: 0.2172653276549493 Test Loss: 0.21378080366624191\n",
      "Epoch: 2459 Training Loss: 0.21726635650750675 Test Loss: 0.2136742238521424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2460 Training Loss: 0.21726541780607794 Test Loss: 0.21373617288298424\n",
      "Epoch: 2461 Training Loss: 0.21726570329511208 Test Loss: 0.21378751837600882\n",
      "Epoch: 2462 Training Loss: 0.21726513541024514 Test Loss: 0.213700460478722\n",
      "Epoch: 2463 Training Loss: 0.21726485302337814 Test Loss: 0.2136671850733713\n",
      "Epoch: 2464 Training Loss: 0.21726613201281655 Test Loss: 0.21374417090600775\n",
      "Epoch: 2465 Training Loss: 0.21726508788253426 Test Loss: 0.2137178824283875\n",
      "Epoch: 2466 Training Loss: 0.21726630655902962 Test Loss: 0.21390332767534778\n",
      "Epoch: 2467 Training Loss: 0.2172637774410661 Test Loss: 0.2137448320067763\n",
      "Epoch: 2468 Training Loss: 0.21726415528681592 Test Loss: 0.21371820649739168\n",
      "Epoch: 2469 Training Loss: 0.21726521135954507 Test Loss: 0.2138263029544308\n",
      "Epoch: 2470 Training Loss: 0.2172653152283492 Test Loss: 0.21376017991481497\n",
      "Epoch: 2471 Training Loss: 0.21726450975872263 Test Loss: 0.21373520067597165\n",
      "Epoch: 2472 Training Loss: 0.21726417939585468 Test Loss: 0.21375916881952187\n",
      "Epoch: 2473 Training Loss: 0.21726429493813165 Test Loss: 0.21384222122391686\n",
      "Epoch: 2474 Training Loss: 0.21726370601052986 Test Loss: 0.21379857561043192\n",
      "Epoch: 2475 Training Loss: 0.21726461388753496 Test Loss: 0.21379025351840422\n",
      "Epoch: 2476 Training Loss: 0.21726291335303288 Test Loss: 0.2139229014432011\n",
      "Epoch: 2477 Training Loss: 0.21726477353258683 Test Loss: 0.2138088032282043\n",
      "Epoch: 2478 Training Loss: 0.21726279358786366 Test Loss: 0.21383010104315997\n",
      "Epoch: 2479 Training Loss: 0.21726381480155868 Test Loss: 0.21383581762039397\n",
      "Epoch: 2480 Training Loss: 0.21726298924853799 Test Loss: 0.2138150123903247\n",
      "Epoch: 2481 Training Loss: 0.21726377112914222 Test Loss: 0.21376027065413614\n",
      "Epoch: 2482 Training Loss: 0.21726368359602746 Test Loss: 0.21379842005730992\n",
      "Epoch: 2483 Training Loss: 0.21726355681960188 Test Loss: 0.21868077886070406\n",
      "Epoch: 2484 Training Loss: 0.21726611704889473 Test Loss: 0.21413179632330492\n",
      "Epoch: 2485 Training Loss: 0.21726304954354944 Test Loss: 0.2138436471275353\n",
      "Epoch: 2486 Training Loss: 0.21726366422093157 Test Loss: 0.21378247586230353\n",
      "Epoch: 2487 Training Loss: 0.21726569745837565 Test Loss: 0.21373800063216788\n",
      "Epoch: 2488 Training Loss: 0.21726384615496463 Test Loss: 0.21378186661257567\n",
      "Epoch: 2489 Training Loss: 0.2172646356923629 Test Loss: 0.21380786990947223\n",
      "Epoch: 2490 Training Loss: 0.21726422591043007 Test Loss: 0.2138389805338749\n",
      "Epoch: 2491 Training Loss: 0.21726351042158193 Test Loss: 0.21376750387430976\n",
      "Epoch: 2492 Training Loss: 0.2172658119516539 Test Loss: 0.21382386595551928\n",
      "Epoch: 2493 Training Loss: 0.2172642317292349 Test Loss: 0.21375060043505098\n",
      "Epoch: 2494 Training Loss: 0.21726307052352367 Test Loss: 0.2137736482226293\n",
      "Epoch: 2495 Training Loss: 0.2172639906657445 Test Loss: 0.21373282849086095\n",
      "Epoch: 2496 Training Loss: 0.21726450531168534 Test Loss: 0.21378311103755177\n",
      "Epoch: 2497 Training Loss: 0.2172629266851789 Test Loss: 0.21386760230832536\n",
      "Epoch: 2498 Training Loss: 0.217265292894539 Test Loss: 0.21374240797062494\n",
      "Epoch: 2499 Training Loss: 0.21726518994921237 Test Loss: 0.21380491440015398\n",
      "Epoch: 2500 Training Loss: 0.21726622851173227 Test Loss: 0.2136967142410335\n",
      "Epoch: 2501 Training Loss: 0.21726581772562972 Test Loss: 0.21378843873198072\n",
      "Epoch: 2502 Training Loss: 0.2172668095763266 Test Loss: 0.2137790666563794\n",
      "Epoch: 2503 Training Loss: 0.2172652659792045 Test Loss: 0.21388016322292794\n",
      "Epoch: 2504 Training Loss: 0.2172634941396874 Test Loss: 0.2137733241536251\n",
      "Epoch: 2505 Training Loss: 0.21726553081999903 Test Loss: 0.2138334713608036\n",
      "Epoch: 2506 Training Loss: 0.2172630462799979 Test Loss: 0.21380991802557875\n",
      "Epoch: 2507 Training Loss: 0.2172636317109373 Test Loss: 0.2137105973571732\n",
      "Epoch: 2508 Training Loss: 0.21726425261955112 Test Loss: 0.21365560932854147\n",
      "Epoch: 2509 Training Loss: 0.2172660252211613 Test Loss: 0.21478208614988187\n",
      "Epoch: 2510 Training Loss: 0.21726371643775638 Test Loss: 0.2148829882750279\n",
      "Epoch: 2511 Training Loss: 0.2172624611807931 Test Loss: 0.2142763699874562\n",
      "Epoch: 2512 Training Loss: 0.21726494131858598 Test Loss: 0.21402276654753372\n",
      "Epoch: 2513 Training Loss: 0.21726372176344214 Test Loss: 0.21382194746701444\n",
      "Epoch: 2514 Training Loss: 0.2172640000260407 Test Loss: 0.21381219947136826\n",
      "Epoch: 2515 Training Loss: 0.21726303911632291 Test Loss: 0.21376264283924684\n",
      "Epoch: 2516 Training Loss: 0.21726258757168923 Test Loss: 0.21378715541872412\n",
      "Epoch: 2517 Training Loss: 0.21726498871180985 Test Loss: 0.2136811848543525\n",
      "Epoch: 2518 Training Loss: 0.21726449623829477 Test Loss: 0.2137178046518265\n",
      "Epoch: 2519 Training Loss: 0.21726630137679664 Test Loss: 0.2137885294713019\n",
      "Epoch: 2520 Training Loss: 0.21726438219330657 Test Loss: 0.21367902007340447\n",
      "Epoch: 2521 Training Loss: 0.21726730610238373 Test Loss: 0.21380683288865882\n",
      "Epoch: 2522 Training Loss: 0.21726469327073666 Test Loss: 0.21374662086767945\n",
      "Epoch: 2523 Training Loss: 0.217265827713532 Test Loss: 0.2136776071325462\n",
      "Epoch: 2524 Training Loss: 0.2172660139063205 Test Loss: 0.2136219580031459\n",
      "Epoch: 2525 Training Loss: 0.21726588894995255 Test Loss: 0.21366057406568573\n",
      "Epoch: 2526 Training Loss: 0.21726480714537463 Test Loss: 0.21378117958628676\n",
      "Epoch: 2527 Training Loss: 0.21726542595599102 Test Loss: 0.21377075752711186\n",
      "Epoch: 2528 Training Loss: 0.21726497069054992 Test Loss: 0.21372130459707178\n",
      "Epoch: 2529 Training Loss: 0.217264380418078 Test Loss: 0.2137223805061657\n",
      "Epoch: 2530 Training Loss: 0.21726447448726166 Test Loss: 0.21376935754901374\n",
      "Epoch: 2531 Training Loss: 0.21726511504891116 Test Loss: 0.2137024308182675\n",
      "Epoch: 2532 Training Loss: 0.217265023615673 Test Loss: 0.21373914135506264\n",
      "Epoch: 2533 Training Loss: 0.21726502629644748 Test Loss: 0.2137561484964028\n",
      "Epoch: 2534 Training Loss: 0.21726510245196082 Test Loss: 0.21374379498596288\n",
      "Epoch: 2535 Training Loss: 0.21726595373683028 Test Loss: 0.2137270600625863\n",
      "Epoch: 2536 Training Loss: 0.21726582457550167 Test Loss: 0.2137516374558644\n",
      "Epoch: 2537 Training Loss: 0.21726367638752347 Test Loss: 0.21368708291022884\n",
      "Epoch: 2538 Training Loss: 0.21726616464833204 Test Loss: 0.21366979058816502\n",
      "Epoch: 2539 Training Loss: 0.21726571588309662 Test Loss: 0.21381844752176915\n",
      "Epoch: 2540 Training Loss: 0.2172646693948087 Test Loss: 0.21372831745032256\n",
      "Epoch: 2541 Training Loss: 0.2172643999276609 Test Loss: 0.21380685881417916\n",
      "Epoch: 2542 Training Loss: 0.21726463082393296 Test Loss: 0.21368113300331182\n",
      "Epoch: 2543 Training Loss: 0.21726550304394768 Test Loss: 0.21381992527642826\n",
      "Epoch: 2544 Training Loss: 0.2172634017560743 Test Loss: 0.21392901986600033\n",
      "Epoch: 2545 Training Loss: 0.21726559355370834 Test Loss: 0.21420418037608183\n",
      "Epoch: 2546 Training Loss: 0.21726285427737035 Test Loss: 0.2139751024783968\n",
      "Epoch: 2547 Training Loss: 0.21726252877396654 Test Loss: 0.2139161089568732\n",
      "Epoch: 2548 Training Loss: 0.2172632902484078 Test Loss: 0.2137322062783729\n",
      "Epoch: 2549 Training Loss: 0.21726409571803434 Test Loss: 0.21363571149168392\n",
      "Epoch: 2550 Training Loss: 0.21726286829091726 Test Loss: 0.2137619298874376\n",
      "Epoch: 2551 Training Loss: 0.21726464198635517 Test Loss: 0.21376679092250053\n",
      "Epoch: 2552 Training Loss: 0.21726362447553593 Test Loss: 0.21372340456421896\n",
      "Epoch: 2553 Training Loss: 0.2172646020526777 Test Loss: 0.21909268352779548\n",
      "Epoch: 2554 Training Loss: 0.2172650126146352 Test Loss: 0.2139515232176516\n",
      "Epoch: 2555 Training Loss: 0.21726225491357623 Test Loss: 0.21377570930149598\n",
      "Epoch: 2556 Training Loss: 0.21726422661872835 Test Loss: 0.21371105105377908\n",
      "Epoch: 2557 Training Loss: 0.2172646021244041 Test Loss: 0.2137945312292596\n",
      "Epoch: 2558 Training Loss: 0.21726306600476 Test Loss: 0.21372252309652756\n",
      "Epoch: 2559 Training Loss: 0.21726461081226522 Test Loss: 0.21366898689703462\n",
      "Epoch: 2560 Training Loss: 0.217263090553123 Test Loss: 0.2140322293624562\n",
      "Epoch: 2561 Training Loss: 0.21726779976835034 Test Loss: 0.2138003515085749\n",
      "Epoch: 2562 Training Loss: 0.21726463713585684 Test Loss: 0.21376073731350217\n",
      "Epoch: 2563 Training Loss: 0.2172642879806701 Test Loss: 0.21371168622902728\n",
      "Epoch: 2564 Training Loss: 0.21726522420753785 Test Loss: 0.21377535930697145\n",
      "Epoch: 2565 Training Loss: 0.21726450137569872 Test Loss: 0.21377709631683392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2566 Training Loss: 0.21726467248801004 Test Loss: 0.21371597690264282\n",
      "Epoch: 2567 Training Loss: 0.21726450307023512 Test Loss: 0.21370253452034885\n",
      "Epoch: 2568 Training Loss: 0.21726505336420057 Test Loss: 0.213700538255283\n",
      "Epoch: 2569 Training Loss: 0.21726538791409755 Test Loss: 0.21360383606443134\n",
      "Epoch: 2570 Training Loss: 0.21726571869835815 Test Loss: 0.21365842224749787\n",
      "Epoch: 2571 Training Loss: 0.21726759865646902 Test Loss: 0.21395994901176066\n",
      "Epoch: 2572 Training Loss: 0.2172650563856755 Test Loss: 0.213960208266964\n",
      "Epoch: 2573 Training Loss: 0.2172644245656819 Test Loss: 0.21382798811325263\n",
      "Epoch: 2574 Training Loss: 0.21726330199360705 Test Loss: 0.213780038863392\n",
      "Epoch: 2575 Training Loss: 0.21726512452576277 Test Loss: 0.2137993922643225\n",
      "Epoch: 2576 Training Loss: 0.2172649152550026 Test Loss: 0.21370129009537273\n",
      "Epoch: 2577 Training Loss: 0.21726455744781792 Test Loss: 0.21373744323348068\n",
      "Epoch: 2578 Training Loss: 0.217263451005219 Test Loss: 0.21429304009703196\n",
      "Epoch: 2579 Training Loss: 0.21726564880993965 Test Loss: 0.21376421133322715\n",
      "Epoch: 2580 Training Loss: 0.21726349091199904 Test Loss: 0.21374961526527822\n",
      "Epoch: 2581 Training Loss: 0.21726568164270277 Test Loss: 0.21379294977251911\n",
      "Epoch: 2582 Training Loss: 0.21726399990051948 Test Loss: 0.2137660001941303\n",
      "Epoch: 2583 Training Loss: 0.21726428646544974 Test Loss: 0.21371528987635394\n",
      "Epoch: 2584 Training Loss: 0.2172653517012275 Test Loss: 0.21379825154142773\n",
      "Epoch: 2585 Training Loss: 0.21726265112128643 Test Loss: 0.21390988683199266\n",
      "Epoch: 2586 Training Loss: 0.21726265804288475 Test Loss: 0.21364993163958798\n",
      "Epoch: 2587 Training Loss: 0.21726400251853337 Test Loss: 0.21371919166716444\n",
      "Epoch: 2588 Training Loss: 0.21726407281937868 Test Loss: 0.21379982003540804\n",
      "Epoch: 2589 Training Loss: 0.21726310911233102 Test Loss: 0.2138249418646132\n",
      "Epoch: 2590 Training Loss: 0.21726360245552878 Test Loss: 0.21378311103755177\n",
      "Epoch: 2591 Training Loss: 0.21726453440570945 Test Loss: 0.21377602040774\n",
      "Epoch: 2592 Training Loss: 0.21726404534816454 Test Loss: 0.2138076365797892\n",
      "Epoch: 2593 Training Loss: 0.21726356666405136 Test Loss: 0.21378177587325448\n",
      "Epoch: 2594 Training Loss: 0.2172654257408118 Test Loss: 0.2137471782663667\n",
      "Epoch: 2595 Training Loss: 0.21726456260315347 Test Loss: 0.21374971896735956\n",
      "Epoch: 2596 Training Loss: 0.2172635894640832 Test Loss: 0.2137873628228868\n",
      "Epoch: 2597 Training Loss: 0.21726432396042936 Test Loss: 0.21381752716579722\n",
      "Epoch: 2598 Training Loss: 0.2172635062883477 Test Loss: 0.21365873335374191\n",
      "Epoch: 2599 Training Loss: 0.21726497855355736 Test Loss: 0.21374960230251805\n",
      "Epoch: 2600 Training Loss: 0.21726339554277424 Test Loss: 0.21366568139319184\n",
      "Epoch: 2601 Training Loss: 0.2172645975249482 Test Loss: 0.21366963503504302\n",
      "Epoch: 2602 Training Loss: 0.21726544747391333 Test Loss: 0.21370582706143146\n",
      "Epoch: 2603 Training Loss: 0.21726504562671434 Test Loss: 0.21369270874814167\n",
      "Epoch: 2604 Training Loss: 0.21726516100760687 Test Loss: 0.21367518309639483\n",
      "Epoch: 2605 Training Loss: 0.21726658653409617 Test Loss: 0.21374360054456037\n",
      "Epoch: 2606 Training Loss: 0.21726277975363278 Test Loss: 0.2137984330200701\n",
      "Epoch: 2607 Training Loss: 0.21726327194920803 Test Loss: 0.21372766931231416\n",
      "Epoch: 2608 Training Loss: 0.2172661091589899 Test Loss: 0.21370784925201763\n",
      "Epoch: 2609 Training Loss: 0.2172653313219619 Test Loss: 0.21376400392906444\n",
      "Epoch: 2610 Training Loss: 0.21726549426642852 Test Loss: 0.21377257231353536\n",
      "Epoch: 2611 Training Loss: 0.21726471726322003 Test Loss: 0.21374233019406394\n",
      "Epoch: 2612 Training Loss: 0.21726471934328584 Test Loss: 0.21373600436710205\n",
      "Epoch: 2613 Training Loss: 0.2172644309224348 Test Loss: 0.21367143685870632\n",
      "Epoch: 2614 Training Loss: 0.2172632875317701 Test Loss: 0.21379987188644872\n",
      "Epoch: 2615 Training Loss: 0.21726405113110617 Test Loss: 0.2137591947450422\n",
      "Epoch: 2616 Training Loss: 0.21726440375605788 Test Loss: 0.2137031956211174\n",
      "Epoch: 2617 Training Loss: 0.21726512335124284 Test Loss: 0.21377333711638527\n",
      "Epoch: 2618 Training Loss: 0.21726502792822325 Test Loss: 0.2136366059221355\n",
      "Epoch: 2619 Training Loss: 0.21726470933745198 Test Loss: 0.21395891199094721\n",
      "Epoch: 2620 Training Loss: 0.2172613765699195 Test Loss: 0.2143289987937374\n",
      "Epoch: 2621 Training Loss: 0.21726340582654793 Test Loss: 0.21404024034823987\n",
      "Epoch: 2622 Training Loss: 0.21726239829466518 Test Loss: 0.21399061890231763\n",
      "Epoch: 2623 Training Loss: 0.21726484467621746 Test Loss: 0.21382881772990336\n",
      "Epoch: 2624 Training Loss: 0.21726344863824754 Test Loss: 0.21372755264747265\n",
      "Epoch: 2625 Training Loss: 0.21726330796483048 Test Loss: 0.2137180379815095\n",
      "Epoch: 2626 Training Loss: 0.21726350553522042 Test Loss: 0.21390894055050041\n",
      "Epoch: 2627 Training Loss: 0.21726358617363423 Test Loss: 0.2137916664592625\n",
      "Epoch: 2628 Training Loss: 0.21726377327196864 Test Loss: 0.21376992791046112\n",
      "Epoch: 2629 Training Loss: 0.21726477924380205 Test Loss: 0.21374452090053228\n",
      "Epoch: 2630 Training Loss: 0.21726608999010744 Test Loss: 0.21370681223120422\n",
      "Epoch: 2631 Training Loss: 0.21726421548320354 Test Loss: 0.21373724879207814\n",
      "Epoch: 2632 Training Loss: 0.21726603280622894 Test Loss: 0.2136567241259159\n",
      "Epoch: 2633 Training Loss: 0.2172646478679206 Test Loss: 0.21378738874840714\n",
      "Epoch: 2634 Training Loss: 0.21726385863535957 Test Loss: 0.21365519452021609\n",
      "Epoch: 2635 Training Loss: 0.2172664196895061 Test Loss: 0.21367966821141288\n",
      "Epoch: 2636 Training Loss: 0.21726605430621962 Test Loss: 0.2137612558239089\n",
      "Epoch: 2637 Training Loss: 0.2172644001518059 Test Loss: 0.21382809181533396\n",
      "Epoch: 2638 Training Loss: 0.2172647238082547 Test Loss: 0.21377888517773708\n",
      "Epoch: 2639 Training Loss: 0.21726429871273384 Test Loss: 0.21746883856157748\n",
      "Epoch: 2640 Training Loss: 0.2172663324343312 Test Loss: 0.21391330900067695\n",
      "Epoch: 2641 Training Loss: 0.21726225716399228 Test Loss: 0.21437580732070324\n",
      "Epoch: 2642 Training Loss: 0.21726677586491502 Test Loss: 0.21382382706723876\n",
      "Epoch: 2643 Training Loss: 0.21726415501784188 Test Loss: 0.21373313959710497\n",
      "Epoch: 2644 Training Loss: 0.21726567194170612 Test Loss: 0.21383239545170968\n",
      "Epoch: 2645 Training Loss: 0.21726446585319534 Test Loss: 0.2138258233323046\n",
      "Epoch: 2646 Training Loss: 0.21726577508428035 Test Loss: 0.21381932898946054\n",
      "Epoch: 2647 Training Loss: 0.21726400540552127 Test Loss: 0.2137927553311166\n",
      "Epoch: 2648 Training Loss: 0.21726452052664957 Test Loss: 0.2137796370178268\n",
      "Epoch: 2649 Training Loss: 0.2172634904457774 Test Loss: 0.21373307478330414\n",
      "Epoch: 2650 Training Loss: 0.21726369529639772 Test Loss: 0.21374659494215914\n",
      "Epoch: 2651 Training Loss: 0.21726403842656622 Test Loss: 0.21374738567052937\n",
      "Epoch: 2652 Training Loss: 0.21726448409860027 Test Loss: 0.21374087836492514\n",
      "Epoch: 2653 Training Loss: 0.21726554945093343 Test Loss: 0.2137694223628146\n",
      "Epoch: 2654 Training Loss: 0.21726447432587723 Test Loss: 0.21376616871001247\n",
      "Epoch: 2655 Training Loss: 0.21726491972893727 Test Loss: 0.213760646574181\n",
      "Epoch: 2656 Training Loss: 0.21726566919817103 Test Loss: 0.213687095872989\n",
      "Epoch: 2657 Training Loss: 0.21726528456530989 Test Loss: 0.2137204231293804\n",
      "Epoch: 2658 Training Loss: 0.21726527456844183 Test Loss: 0.21375377631129208\n",
      "Epoch: 2659 Training Loss: 0.2172643215665605 Test Loss: 0.21368535886312653\n",
      "Epoch: 2660 Training Loss: 0.21726508795426067 Test Loss: 0.21373789693008655\n",
      "Epoch: 2661 Training Loss: 0.21726412214024976 Test Loss: 0.21380261999160427\n",
      "Epoch: 2662 Training Loss: 0.21726434490454039 Test Loss: 0.21385580619657268\n",
      "Epoch: 2663 Training Loss: 0.21726417888480404 Test Loss: 0.21371665096617154\n",
      "Epoch: 2664 Training Loss: 0.21726565895922634 Test Loss: 0.21369282541298318\n",
      "Epoch: 2665 Training Loss: 0.2172635699903635 Test Loss: 0.21365999074147818\n",
      "Epoch: 2666 Training Loss: 0.21726315522344533 Test Loss: 0.21376568908788626\n",
      "Epoch: 2667 Training Loss: 0.21726500171222124 Test Loss: 0.21369727163972072\n",
      "Epoch: 2668 Training Loss: 0.21726652586252107 Test Loss: 0.21372795449303786\n",
      "Epoch: 2669 Training Loss: 0.2172649621909706 Test Loss: 0.21379498492586546\n",
      "Epoch: 2670 Training Loss: 0.2172642541437373 Test Loss: 0.2138207160047985\n",
      "Epoch: 2671 Training Loss: 0.2172631631312818 Test Loss: 0.21378689616352076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2672 Training Loss: 0.2172658624649765 Test Loss: 0.2137289007745301\n",
      "Epoch: 2673 Training Loss: 0.21726380510056206 Test Loss: 0.2136973234907614\n",
      "Epoch: 2674 Training Loss: 0.21726500554061826 Test Loss: 0.21373273775153978\n",
      "Epoch: 2675 Training Loss: 0.21726444292764227 Test Loss: 0.21376222803092146\n",
      "Epoch: 2676 Training Loss: 0.2172645094897486 Test Loss: 0.21375935029816423\n",
      "Epoch: 2677 Training Loss: 0.21726427524026695 Test Loss: 0.21373492845800812\n",
      "Epoch: 2678 Training Loss: 0.21726369600469597 Test Loss: 0.21378429064872703\n",
      "Epoch: 2679 Training Loss: 0.21726689616803227 Test Loss: 0.21374398942736542\n",
      "Epoch: 2680 Training Loss: 0.21726521379824293 Test Loss: 0.21373459142624376\n",
      "Epoch: 2681 Training Loss: 0.21726533091850087 Test Loss: 0.21383851387450886\n",
      "Epoch: 2682 Training Loss: 0.21726410737357557 Test Loss: 0.21380766250530955\n",
      "Epoch: 2683 Training Loss: 0.21726210545367425 Test Loss: 0.21382798811325263\n",
      "Epoch: 2684 Training Loss: 0.21726407336629255 Test Loss: 0.21372038424109988\n",
      "Epoch: 2685 Training Loss: 0.21726439390264263 Test Loss: 0.2136813533702347\n",
      "Epoch: 2686 Training Loss: 0.2172642215171876 Test Loss: 0.2137202935017787\n",
      "Epoch: 2687 Training Loss: 0.2172664495994181 Test Loss: 0.21376867052272486\n",
      "Epoch: 2688 Training Loss: 0.21726570980428359 Test Loss: 0.21379105720953462\n",
      "Epoch: 2689 Training Loss: 0.2172635120712893 Test Loss: 0.21382126044072555\n",
      "Epoch: 2690 Training Loss: 0.21726323394317776 Test Loss: 0.21375070413713232\n",
      "Epoch: 2691 Training Loss: 0.21726705482684605 Test Loss: 0.21376172248327494\n",
      "Epoch: 2692 Training Loss: 0.21726443016930752 Test Loss: 0.21376325208897473\n",
      "Epoch: 2693 Training Loss: 0.21726539886134053 Test Loss: 0.21376759461363093\n",
      "Epoch: 2694 Training Loss: 0.21726457273450855 Test Loss: 0.2137368339837528\n",
      "Epoch: 2695 Training Loss: 0.21726546946702308 Test Loss: 0.21483847415661173\n",
      "Epoch: 2696 Training Loss: 0.217264920777936 Test Loss: 0.21381280872109615\n",
      "Epoch: 2697 Training Loss: 0.21726515155765266 Test Loss: 0.21405272348628146\n",
      "Epoch: 2698 Training Loss: 0.21726648911170293 Test Loss: 0.21381437721507646\n",
      "Epoch: 2699 Training Loss: 0.2172645386823965 Test Loss: 0.2137583910539118\n",
      "Epoch: 2700 Training Loss: 0.21726503176558606 Test Loss: 0.2137572632937772\n",
      "Epoch: 2701 Training Loss: 0.21726520582764589 Test Loss: 0.2136905180416733\n",
      "Epoch: 2702 Training Loss: 0.21726309372701658 Test Loss: 0.21371430470658118\n",
      "Epoch: 2703 Training Loss: 0.21726681066118853 Test Loss: 0.21380729954802485\n",
      "Epoch: 2704 Training Loss: 0.21726411073575094 Test Loss: 0.21373299700674311\n",
      "Epoch: 2705 Training Loss: 0.21726598827309557 Test Loss: 0.2137097158894818\n",
      "Epoch: 2706 Training Loss: 0.21726531378485522 Test Loss: 0.21378938501347297\n",
      "Epoch: 2707 Training Loss: 0.21726559107914728 Test Loss: 0.21374686716012264\n",
      "Epoch: 2708 Training Loss: 0.21726418897133012 Test Loss: 0.21373998393447355\n",
      "Epoch: 2709 Training Loss: 0.21726377592584573 Test Loss: 0.2137420190878199\n",
      "Epoch: 2710 Training Loss: 0.2172661109431843 Test Loss: 0.2137849128612151\n",
      "Epoch: 2711 Training Loss: 0.21726359031583428 Test Loss: 0.2137912257254168\n",
      "Epoch: 2712 Training Loss: 0.21726400425789877 Test Loss: 0.21374733381948868\n",
      "Epoch: 2713 Training Loss: 0.21726486413200552 Test Loss: 0.213725880451411\n",
      "Epoch: 2714 Training Loss: 0.21726427058701625 Test Loss: 0.21395025286715513\n",
      "Epoch: 2715 Training Loss: 0.2172633463474244 Test Loss: 0.21381435128955611\n",
      "Epoch: 2716 Training Loss: 0.21726344620851545 Test Loss: 0.21374590791587023\n",
      "Epoch: 2717 Training Loss: 0.21726574304947355 Test Loss: 0.21374490978333732\n",
      "Epoch: 2718 Training Loss: 0.21726433576838922 Test Loss: 0.21376384837594245\n",
      "Epoch: 2719 Training Loss: 0.21726509322615165 Test Loss: 0.2137035974666826\n",
      "Epoch: 2720 Training Loss: 0.2172646085797808 Test Loss: 0.21371770094974515\n",
      "Epoch: 2721 Training Loss: 0.21726548709378776 Test Loss: 0.21609144159166932\n",
      "Epoch: 2722 Training Loss: 0.21726543936882925 Test Loss: 0.21376878718756637\n",
      "Epoch: 2723 Training Loss: 0.21726530197689536 Test Loss: 0.21375159856758388\n",
      "Epoch: 2724 Training Loss: 0.2172642880613623 Test Loss: 0.21372637303629738\n",
      "Epoch: 2725 Training Loss: 0.2172645561029478 Test Loss: 0.2136804200515026\n",
      "Epoch: 2726 Training Loss: 0.21726420396214932 Test Loss: 0.21371138808554344\n",
      "Epoch: 2727 Training Loss: 0.2172631053556604 Test Loss: 0.2137320766507712\n",
      "Epoch: 2728 Training Loss: 0.21726256707586825 Test Loss: 0.2137381561852899\n",
      "Epoch: 2729 Training Loss: 0.21726461322406568 Test Loss: 0.2138370879708904\n",
      "Epoch: 2730 Training Loss: 0.21726491772059786 Test Loss: 0.2138642579162021\n",
      "Epoch: 2731 Training Loss: 0.21726399045056527 Test Loss: 0.21377844444389138\n",
      "Epoch: 2732 Training Loss: 0.21726390692516354 Test Loss: 0.21371551024327679\n",
      "Epoch: 2733 Training Loss: 0.21726430341977934 Test Loss: 0.21374778751609455\n",
      "Epoch: 2734 Training Loss: 0.2172660325551865 Test Loss: 0.2138865409009305\n",
      "Epoch: 2735 Training Loss: 0.2172653518357145 Test Loss: 0.21375975214372941\n",
      "Epoch: 2736 Training Loss: 0.2172647048724831 Test Loss: 0.21372646377561857\n",
      "Epoch: 2737 Training Loss: 0.2172653947819011 Test Loss: 0.2138064828941343\n",
      "Epoch: 2738 Training Loss: 0.21726459357999578 Test Loss: 0.2137564596026468\n",
      "Epoch: 2739 Training Loss: 0.21726381682782972 Test Loss: 0.21383076214392854\n",
      "Epoch: 2740 Training Loss: 0.2172644357639673 Test Loss: 0.21378215179329935\n",
      "Epoch: 2741 Training Loss: 0.21726410719425956 Test Loss: 0.21382612147578847\n",
      "Epoch: 2742 Training Loss: 0.21726248526293448 Test Loss: 0.21390572578597883\n",
      "Epoch: 2743 Training Loss: 0.21726325709287583 Test Loss: 0.21370598261455348\n",
      "Epoch: 2744 Training Loss: 0.21726635369224526 Test Loss: 0.21376037435621748\n",
      "Epoch: 2745 Training Loss: 0.21726451314779538 Test Loss: 0.21371898426300176\n",
      "Epoch: 2746 Training Loss: 0.21726542842158628 Test Loss: 0.21371640467372838\n",
      "Epoch: 2747 Training Loss: 0.2172650375574935 Test Loss: 0.2136725386933206\n",
      "Epoch: 2748 Training Loss: 0.21726486067120634 Test Loss: 0.21371391582377616\n",
      "Epoch: 2749 Training Loss: 0.21726433266622208 Test Loss: 0.21368900139873367\n",
      "Epoch: 2750 Training Loss: 0.21726506065339674 Test Loss: 0.213717999093229\n",
      "Epoch: 2751 Training Loss: 0.21726459228892045 Test Loss: 0.2137498226694409\n",
      "Epoch: 2752 Training Loss: 0.2172630313698709 Test Loss: 0.21374160427949454\n",
      "Epoch: 2753 Training Loss: 0.2172622263126712 Test Loss: 0.21378123143732744\n",
      "Epoch: 2754 Training Loss: 0.2172627612033906 Test Loss: 0.21403370711711533\n",
      "Epoch: 2755 Training Loss: 0.21726467174384856 Test Loss: 0.2140104648881345\n",
      "Epoch: 2756 Training Loss: 0.21726411265443235 Test Loss: 0.21378782948225283\n",
      "Epoch: 2757 Training Loss: 0.21726438820039323 Test Loss: 0.21379397383057236\n",
      "Epoch: 2758 Training Loss: 0.21726392478503906 Test Loss: 0.21375315409880402\n",
      "Epoch: 2759 Training Loss: 0.21726425666312735 Test Loss: 0.21368430887955295\n",
      "Epoch: 2760 Training Loss: 0.2172625142314374 Test Loss: 0.2171700858279906\n",
      "Epoch: 2761 Training Loss: 0.21726622087286984 Test Loss: 0.21383615465215833\n",
      "Epoch: 2762 Training Loss: 0.2172657052675883 Test Loss: 0.21372362493114183\n",
      "Epoch: 2763 Training Loss: 0.21726420390835452 Test Loss: 0.21372584156313051\n",
      "Epoch: 2764 Training Loss: 0.2172662954055732 Test Loss: 0.21374126724773018\n",
      "Epoch: 2765 Training Loss: 0.21726584054359316 Test Loss: 0.21370228822790566\n",
      "Epoch: 2766 Training Loss: 0.2172646824938439 Test Loss: 0.21376776312951312\n",
      "Epoch: 2767 Training Loss: 0.2172647863895454 Test Loss: 0.21397668393513725\n",
      "Epoch: 2768 Training Loss: 0.21726366389816276 Test Loss: 0.2138293362403101\n",
      "Epoch: 2769 Training Loss: 0.21726427160911754 Test Loss: 0.2137642372587475\n",
      "Epoch: 2770 Training Loss: 0.21726467726678195 Test Loss: 0.2137803629323962\n",
      "Epoch: 2771 Training Loss: 0.21726600789923387 Test Loss: 0.2138131976039012\n",
      "Epoch: 2772 Training Loss: 0.21726446330690785 Test Loss: 0.21372305456969443\n",
      "Epoch: 2773 Training Loss: 0.21726551625057247 Test Loss: 0.2136866940274238\n",
      "Epoch: 2774 Training Loss: 0.21726450359921737 Test Loss: 0.21362536720907002\n",
      "Epoch: 2775 Training Loss: 0.21726662487186105 Test Loss: 0.21373965986546936\n",
      "Epoch: 2776 Training Loss: 0.2172665494425776 Test Loss: 0.21371272324984072\n",
      "Epoch: 2777 Training Loss: 0.21726436263889468 Test Loss: 0.21376119101010804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2778 Training Loss: 0.21726559421717762 Test Loss: 0.21380508291603617\n",
      "Epoch: 2779 Training Loss: 0.21726528123003194 Test Loss: 0.21399258924186312\n",
      "Epoch: 2780 Training Loss: 0.21726353939905063 Test Loss: 0.21405941227052802\n",
      "Epoch: 2781 Training Loss: 0.21726544213029594 Test Loss: 0.21382899920854573\n",
      "Epoch: 2782 Training Loss: 0.21726523337058642 Test Loss: 0.2137703945698272\n",
      "Epoch: 2783 Training Loss: 0.21726502122180413 Test Loss: 0.21375632997504512\n",
      "Epoch: 2784 Training Loss: 0.21726479321251993 Test Loss: 0.2137359006650207\n",
      "Epoch: 2785 Training Loss: 0.2172630931352737 Test Loss: 0.21673578151132925\n",
      "Epoch: 2786 Training Loss: 0.21726416209185884 Test Loss: 0.2139110794059281\n",
      "Epoch: 2787 Training Loss: 0.21726321842337631 Test Loss: 0.21372315827177576\n",
      "Epoch: 2788 Training Loss: 0.2172645309000813 Test Loss: 0.21379994966300972\n",
      "Epoch: 2789 Training Loss: 0.21726488473541614 Test Loss: 0.2137364710264681\n",
      "Epoch: 2790 Training Loss: 0.21726453778581642 Test Loss: 0.2137353432663335\n",
      "Epoch: 2791 Training Loss: 0.21726533801941522 Test Loss: 0.21378731097184614\n",
      "Epoch: 2792 Training Loss: 0.21726466891065543 Test Loss: 0.21377708335407375\n",
      "Epoch: 2793 Training Loss: 0.21726436959635623 Test Loss: 0.2137326081239381\n",
      "Epoch: 2794 Training Loss: 0.21726479525672257 Test Loss: 0.21379164053374217\n",
      "Epoch: 2795 Training Loss: 0.2172649442414371 Test Loss: 0.2139897374346262\n",
      "Epoch: 2796 Training Loss: 0.21726401657690927 Test Loss: 0.21375894845259902\n",
      "Epoch: 2797 Training Loss: 0.21726372723258072 Test Loss: 0.21374380794872305\n",
      "Epoch: 2798 Training Loss: 0.2172663614924921 Test Loss: 0.2137671279542649\n",
      "Epoch: 2799 Training Loss: 0.21726445092513674 Test Loss: 0.21437338328455188\n",
      "Epoch: 2800 Training Loss: 0.21726459567799322 Test Loss: 0.21372980816774184\n",
      "Epoch: 2801 Training Loss: 0.21726710795818252 Test Loss: 0.21384246751636005\n",
      "Epoch: 2802 Training Loss: 0.21726461219299859 Test Loss: 0.2137982645041879\n",
      "Epoch: 2803 Training Loss: 0.21726455756437335 Test Loss: 0.2137738037757513\n",
      "Epoch: 2804 Training Loss: 0.21726482194791202 Test Loss: 0.21380986617453807\n",
      "Epoch: 2805 Training Loss: 0.21726481420146 Test Loss: 0.21381264020521398\n",
      "Epoch: 2806 Training Loss: 0.21726493626187424 Test Loss: 0.21383404172225098\n",
      "Epoch: 2807 Training Loss: 0.21726377899214966 Test Loss: 0.21376019287757514\n",
      "Epoch: 2808 Training Loss: 0.21726469146861066 Test Loss: 0.2138044995918286\n",
      "Epoch: 2809 Training Loss: 0.21726441837031346 Test Loss: 0.21378102403316476\n",
      "Epoch: 2810 Training Loss: 0.21726244476441156 Test Loss: 0.21517881142481682\n",
      "Epoch: 2811 Training Loss: 0.21726553810919522 Test Loss: 0.2137802073792742\n",
      "Epoch: 2812 Training Loss: 0.21726531630424528 Test Loss: 0.2137164305992487\n",
      "Epoch: 2813 Training Loss: 0.21726430534742655 Test Loss: 0.21375642071436632\n",
      "Epoch: 2814 Training Loss: 0.21726361896156834 Test Loss: 0.21392168294374536\n",
      "Epoch: 2815 Training Loss: 0.2172625265863111 Test Loss: 0.21377611114706116\n",
      "Epoch: 2816 Training Loss: 0.21726295832549047 Test Loss: 0.21369705127279787\n",
      "Epoch: 2817 Training Loss: 0.21726515413083755 Test Loss: 0.21374292648103166\n",
      "Epoch: 2818 Training Loss: 0.21726385525525263 Test Loss: 0.21748408276753478\n",
      "Epoch: 2819 Training Loss: 0.2172655854665559 Test Loss: 0.21385333030938064\n",
      "Epoch: 2820 Training Loss: 0.21726456257625607 Test Loss: 0.21376211136607995\n",
      "Epoch: 2821 Training Loss: 0.21726526975380672 Test Loss: 0.21370543817862642\n",
      "Epoch: 2822 Training Loss: 0.21726495708942986 Test Loss: 0.21369561240641924\n",
      "Epoch: 2823 Training Loss: 0.21726704252576715 Test Loss: 0.21368410147539024\n",
      "Epoch: 2824 Training Loss: 0.21726523537892584 Test Loss: 0.2137062937207975\n",
      "Epoch: 2825 Training Loss: 0.21726483174753247 Test Loss: 0.21372122682051078\n",
      "Epoch: 2826 Training Loss: 0.2172659987361853 Test Loss: 0.2136805367163441\n",
      "Epoch: 2827 Training Loss: 0.2172642861606125 Test Loss: 0.2137105973571732\n",
      "Epoch: 2828 Training Loss: 0.21726411351514924 Test Loss: 0.21381009950422109\n",
      "Epoch: 2829 Training Loss: 0.21726531693185136 Test Loss: 0.21374750233537088\n",
      "Epoch: 2830 Training Loss: 0.2172637747513258 Test Loss: 0.21367448310734577\n",
      "Epoch: 2831 Training Loss: 0.21726577563119423 Test Loss: 0.2136787608182011\n",
      "Epoch: 2832 Training Loss: 0.21726493662947208 Test Loss: 0.21374010059931506\n",
      "Epoch: 2833 Training Loss: 0.21726485608968207 Test Loss: 0.21370626779527715\n",
      "Epoch: 2834 Training Loss: 0.2172646236692238 Test Loss: 0.21369384947103642\n",
      "Epoch: 2835 Training Loss: 0.21726715830115492 Test Loss: 0.21414518685455824\n",
      "Epoch: 2836 Training Loss: 0.2172642627509062 Test Loss: 0.2137360302926224\n",
      "Epoch: 2837 Training Loss: 0.21726506737774748 Test Loss: 0.21428028474102687\n",
      "Epoch: 2838 Training Loss: 0.2172636914680007 Test Loss: 0.21444540438004406\n",
      "Epoch: 2839 Training Loss: 0.2172650255343544 Test Loss: 0.21407033987734947\n",
      "Epoch: 2840 Training Loss: 0.21726326255304862 Test Loss: 0.21387416146497026\n",
      "Epoch: 2841 Training Loss: 0.2172663614476631 Test Loss: 0.2138306195535667\n",
      "Epoch: 2842 Training Loss: 0.21726269688273453 Test Loss: 0.2139107164486434\n",
      "Epoch: 2843 Training Loss: 0.21726316554308225 Test Loss: 0.2137754889345731\n",
      "Epoch: 2844 Training Loss: 0.2172638542779803 Test Loss: 0.21382189561597378\n",
      "Epoch: 2845 Training Loss: 0.21726436979360386 Test Loss: 0.21383855276278937\n",
      "Epoch: 2846 Training Loss: 0.21726430123212392 Test Loss: 0.21379941818984283\n",
      "Epoch: 2847 Training Loss: 0.21726400138884244 Test Loss: 0.21379849783387092\n",
      "Epoch: 2848 Training Loss: 0.21726420037582894 Test Loss: 0.2137464005007566\n",
      "Epoch: 2849 Training Loss: 0.21726414648239936 Test Loss: 0.21380002743957072\n",
      "Epoch: 2850 Training Loss: 0.2172647385838947 Test Loss: 0.21373078037475443\n",
      "Epoch: 2851 Training Loss: 0.21726710627261195 Test Loss: 0.21375853364427366\n",
      "Epoch: 2852 Training Loss: 0.21726597538027378 Test Loss: 0.2140780138313688\n",
      "Epoch: 2853 Training Loss: 0.2172643543006998 Test Loss: 0.21398536898444967\n",
      "Epoch: 2854 Training Loss: 0.21726377403406172 Test Loss: 0.21450093684460284\n",
      "Epoch: 2855 Training Loss: 0.217262043257913 Test Loss: 0.21405123276886218\n",
      "Epoch: 2856 Training Loss: 0.21726179121131645 Test Loss: 0.2151062847816781\n",
      "Epoch: 2857 Training Loss: 0.21726605526556034 Test Loss: 0.214031983070013\n",
      "Epoch: 2858 Training Loss: 0.21726403439195577 Test Loss: 0.21566212793767256\n",
      "Epoch: 2859 Training Loss: 0.21726283183597056 Test Loss: 0.21380372182621854\n",
      "Epoch: 2860 Training Loss: 0.217264623749916 Test Loss: 0.21388401316269778\n",
      "Epoch: 2861 Training Loss: 0.2172648237590038 Test Loss: 0.2137684112675215\n",
      "Epoch: 2862 Training Loss: 0.21726417782683952 Test Loss: 0.21374735974500902\n",
      "Epoch: 2863 Training Loss: 0.2172657109429403 Test Loss: 0.21384399712205984\n",
      "Epoch: 2864 Training Loss: 0.21726531894915657 Test Loss: 0.21380026076925374\n",
      "Epoch: 2865 Training Loss: 0.21726503809544154 Test Loss: 0.21376456132775168\n",
      "Epoch: 2866 Training Loss: 0.217262740510322 Test Loss: 0.2137690594055299\n",
      "Epoch: 2867 Training Loss: 0.21726402136464698 Test Loss: 0.21385637655802006\n",
      "Epoch: 2868 Training Loss: 0.21726420861540002 Test Loss: 0.21372848596620472\n",
      "Epoch: 2869 Training Loss: 0.21726558685625502 Test Loss: 0.21402999976770734\n",
      "Epoch: 2870 Training Loss: 0.21726143911534698 Test Loss: 0.21444568956076776\n",
      "Epoch: 2871 Training Loss: 0.21726265520969165 Test Loss: 0.21408503964737974\n",
      "Epoch: 2872 Training Loss: 0.2172629060459051 Test Loss: 0.21374525977786185\n",
      "Epoch: 2873 Training Loss: 0.21726372006890576 Test Loss: 0.21375817068698896\n",
      "Epoch: 2874 Training Loss: 0.21726634092494468 Test Loss: 0.21380081816794097\n",
      "Epoch: 2875 Training Loss: 0.21726427660306868 Test Loss: 0.2138207160047985\n",
      "Epoch: 2876 Training Loss: 0.2172632781266449 Test Loss: 0.21368799030344057\n",
      "Epoch: 2877 Training Loss: 0.2172628910640517 Test Loss: 0.2136513705059666\n",
      "Epoch: 2878 Training Loss: 0.2172645945034733 Test Loss: 0.21367513124535414\n",
      "Epoch: 2879 Training Loss: 0.21726457201724447 Test Loss: 0.2138106180146278\n",
      "Epoch: 2880 Training Loss: 0.2172638288509688 Test Loss: 0.2137494597121562\n",
      "Epoch: 2881 Training Loss: 0.21726337131718004 Test Loss: 0.21370869183142854\n",
      "Epoch: 2882 Training Loss: 0.21726403890175366 Test Loss: 0.21375639478884598\n",
      "Epoch: 2883 Training Loss: 0.21726519214583362 Test Loss: 0.21376174840879528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2884 Training Loss: 0.21726369600469597 Test Loss: 0.2135848974718262\n",
      "Epoch: 2885 Training Loss: 0.2172662026902255 Test Loss: 0.2139747654466324\n",
      "Epoch: 2886 Training Loss: 0.21726258585922123 Test Loss: 0.21417106052385315\n",
      "Epoch: 2887 Training Loss: 0.217263542761226 Test Loss: 0.21391724967976797\n",
      "Epoch: 2888 Training Loss: 0.21726555896364824 Test Loss: 0.2137622539564418\n",
      "Epoch: 2889 Training Loss: 0.21726307597473066 Test Loss: 0.21383095658533105\n",
      "Epoch: 2890 Training Loss: 0.21726332283909428 Test Loss: 0.21383011400592014\n",
      "Epoch: 2891 Training Loss: 0.21726426787037856 Test Loss: 0.21377125011199824\n",
      "Epoch: 2892 Training Loss: 0.21726448417929248 Test Loss: 0.21367944784449003\n",
      "Epoch: 2893 Training Loss: 0.21726555781602572 Test Loss: 0.21388083728645668\n",
      "Epoch: 2894 Training Loss: 0.21726423958327654 Test Loss: 0.21367743861666402\n",
      "Epoch: 2895 Training Loss: 0.21726492255316457 Test Loss: 0.2136588889068639\n",
      "Epoch: 2896 Training Loss: 0.21726528483428392 Test Loss: 0.21373898580194065\n",
      "Epoch: 2897 Training Loss: 0.2172640851742524 Test Loss: 0.2137245193615934\n",
      "Epoch: 2898 Training Loss: 0.21726607116192542 Test Loss: 0.21389732591739008\n",
      "Epoch: 2899 Training Loss: 0.21726556946260117 Test Loss: 0.213781970314657\n",
      "Epoch: 2900 Training Loss: 0.21726279747902127 Test Loss: 0.214487274095386\n",
      "Epoch: 2901 Training Loss: 0.21726608843902387 Test Loss: 0.2138795798987204\n",
      "Epoch: 2902 Training Loss: 0.21726591531837317 Test Loss: 0.2145166088216457\n",
      "Epoch: 2903 Training Loss: 0.21726444609257 Test Loss: 0.21378933316243232\n",
      "Epoch: 2904 Training Loss: 0.2172659990051593 Test Loss: 0.2138142864757553\n",
      "Epoch: 2905 Training Loss: 0.2172664484966246 Test Loss: 0.21378033700687588\n",
      "Epoch: 2906 Training Loss: 0.21726509537794386 Test Loss: 0.21375817068698896\n",
      "Epoch: 2907 Training Loss: 0.21726334060034597 Test Loss: 0.21356336632718753\n",
      "Epoch: 2908 Training Loss: 0.21726693271263697 Test Loss: 0.2137890479817086\n",
      "Epoch: 2909 Training Loss: 0.21726518929470892 Test Loss: 0.21380789583499257\n",
      "Epoch: 2910 Training Loss: 0.21726138081970914 Test Loss: 0.21373109148099845\n",
      "Epoch: 2911 Training Loss: 0.21726313037921088 Test Loss: 0.21375981695753027\n",
      "Epoch: 2912 Training Loss: 0.21726456038860065 Test Loss: 0.21380540698504036\n",
      "Epoch: 2913 Training Loss: 0.21726536432507523 Test Loss: 0.2137710297450754\n",
      "Epoch: 2914 Training Loss: 0.21726352384338596 Test Loss: 0.21362592460775726\n",
      "Epoch: 2915 Training Loss: 0.21726542036133123 Test Loss: 0.2137561873846833\n",
      "Epoch: 2916 Training Loss: 0.21726421467628146 Test Loss: 0.2136903236002708\n",
      "Epoch: 2917 Training Loss: 0.21726365074533274 Test Loss: 0.21372031942729902\n",
      "Epoch: 2918 Training Loss: 0.21726402749725485 Test Loss: 0.21368677180398482\n",
      "Epoch: 2919 Training Loss: 0.2172651081004154 Test Loss: 0.2136963512837488\n",
      "Epoch: 2920 Training Loss: 0.21726552351287126 Test Loss: 0.21373872654673728\n",
      "Epoch: 2921 Training Loss: 0.2172644750072781 Test Loss: 0.21380774028187055\n",
      "Epoch: 2922 Training Loss: 0.21726457112963019 Test Loss: 0.21407500647100988\n",
      "Epoch: 2923 Training Loss: 0.21726567617356418 Test Loss: 0.21382231042429914\n",
      "Epoch: 2924 Training Loss: 0.21726501282084862 Test Loss: 0.21376211136607995\n",
      "Epoch: 2925 Training Loss: 0.21726227399280068 Test Loss: 0.2137419413112589\n",
      "Epoch: 2926 Training Loss: 0.21726306434608683 Test Loss: 0.21380287924680763\n",
      "Epoch: 2927 Training Loss: 0.2172666859737946 Test Loss: 0.21391503304777928\n",
      "Epoch: 2928 Training Loss: 0.21726394151522366 Test Loss: 0.21392826802591058\n",
      "Epoch: 2929 Training Loss: 0.21726258561714462 Test Loss: 0.21373523956425214\n",
      "Epoch: 2930 Training Loss: 0.2172631830263941 Test Loss: 0.21451646623128384\n",
      "Epoch: 2931 Training Loss: 0.21726594719179557 Test Loss: 0.21369410872623978\n",
      "Epoch: 2932 Training Loss: 0.21726580826670971 Test Loss: 0.21370730481609057\n",
      "Epoch: 2933 Training Loss: 0.21726545714801257 Test Loss: 0.21371525098807342\n",
      "Epoch: 2934 Training Loss: 0.21726446658839102 Test Loss: 0.21371500469563023\n",
      "Epoch: 2935 Training Loss: 0.21726487082945883 Test Loss: 0.21374328943831633\n",
      "Epoch: 2936 Training Loss: 0.2172651293762611 Test Loss: 0.21372301568141394\n",
      "Epoch: 2937 Training Loss: 0.21726356081834913 Test Loss: 0.21372709895086678\n",
      "Epoch: 2938 Training Loss: 0.21726734647538545 Test Loss: 0.21417982334972654\n",
      "Epoch: 2939 Training Loss: 0.21726284014726804 Test Loss: 0.2144213325344125\n",
      "Epoch: 2940 Training Loss: 0.21726536705067873 Test Loss: 0.21398236162409073\n",
      "Epoch: 2941 Training Loss: 0.21726358587776282 Test Loss: 0.21387657253836148\n",
      "Epoch: 2942 Training Loss: 0.2172642506112117 Test Loss: 0.21380195889083572\n",
      "Epoch: 2943 Training Loss: 0.21726286760951638 Test Loss: 0.21514740265693028\n",
      "Epoch: 2944 Training Loss: 0.21726433619874766 Test Loss: 0.21373258219841776\n",
      "Epoch: 2945 Training Loss: 0.21726274285936184 Test Loss: 0.21377520375384942\n",
      "Epoch: 2946 Training Loss: 0.2172643814132819 Test Loss: 0.21369499019393118\n",
      "Epoch: 2947 Training Loss: 0.21726478807511598 Test Loss: 0.21367025724753105\n",
      "Epoch: 2948 Training Loss: 0.2172643607740081 Test Loss: 0.21364140214339758\n",
      "Epoch: 2949 Training Loss: 0.21726541907025587 Test Loss: 0.21371998239553466\n",
      "Epoch: 2950 Training Loss: 0.2172628397348412 Test Loss: 0.21367966821141288\n",
      "Epoch: 2951 Training Loss: 0.2172648572821336 Test Loss: 0.21374879861138765\n",
      "Epoch: 2952 Training Loss: 0.21726544736632372 Test Loss: 0.21368503479412232\n",
      "Epoch: 2953 Training Loss: 0.2172651909533821 Test Loss: 0.21377171677136428\n",
      "Epoch: 2954 Training Loss: 0.21726426427509238 Test Loss: 0.21369819199569262\n",
      "Epoch: 2955 Training Loss: 0.21726749536147616 Test Loss: 0.21376408170562547\n",
      "Epoch: 2956 Training Loss: 0.2172665363704398 Test Loss: 0.21360456197900074\n",
      "Epoch: 2957 Training Loss: 0.21726531228756646 Test Loss: 0.21368524219828502\n",
      "Epoch: 2958 Training Loss: 0.21726489023145212 Test Loss: 0.2136409095585112\n",
      "Epoch: 2959 Training Loss: 0.2172673590813016 Test Loss: 0.2138020496301569\n",
      "Epoch: 2960 Training Loss: 0.2172652077463273 Test Loss: 0.21372168051711665\n",
      "Epoch: 2961 Training Loss: 0.21726561555578391 Test Loss: 0.21373370995855234\n",
      "Epoch: 2962 Training Loss: 0.21726467849509667 Test Loss: 0.21373680805823245\n",
      "Epoch: 2963 Training Loss: 0.21726606132644177 Test Loss: 0.21373210257629155\n",
      "Epoch: 2964 Training Loss: 0.21726458872053167 Test Loss: 0.2142298596039742\n",
      "Epoch: 2965 Training Loss: 0.21726407607396445 Test Loss: 0.21371684540757407\n",
      "Epoch: 2966 Training Loss: 0.21726483988847972 Test Loss: 0.21374808565957842\n",
      "Epoch: 2967 Training Loss: 0.21726595094846618 Test Loss: 0.21368169040199905\n",
      "Epoch: 2968 Training Loss: 0.21726597052080968 Test Loss: 0.21370446597161383\n",
      "Epoch: 2969 Training Loss: 0.21726639032650796 Test Loss: 0.21373180443280768\n",
      "Epoch: 2970 Training Loss: 0.2172645113098062 Test Loss: 0.213696597576192\n",
      "Epoch: 2971 Training Loss: 0.21726407463047048 Test Loss: 0.21378375917556017\n",
      "Epoch: 2972 Training Loss: 0.2172647869633567 Test Loss: 0.21376846311856218\n",
      "Epoch: 2973 Training Loss: 0.21726339450274135 Test Loss: 0.21374743752157002\n",
      "Epoch: 2974 Training Loss: 0.21726076085250443 Test Loss: 0.21434312820232027\n",
      "Epoch: 2975 Training Loss: 0.21726621319814424 Test Loss: 0.21386998745619623\n",
      "Epoch: 2976 Training Loss: 0.21726501041801397 Test Loss: 0.21369423835384146\n",
      "Epoch: 2977 Training Loss: 0.2172669020137345 Test Loss: 0.2137904220342864\n",
      "Epoch: 2978 Training Loss: 0.21726535303713182 Test Loss: 0.2137267748818626\n",
      "Epoch: 2979 Training Loss: 0.2172649106555467 Test Loss: 0.2137400098599939\n",
      "Epoch: 2980 Training Loss: 0.2172679257109564 Test Loss: 0.21378352584587715\n",
      "Epoch: 2981 Training Loss: 0.21726561205912154 Test Loss: 0.21375137820066104\n",
      "Epoch: 2982 Training Loss: 0.21726479879821395 Test Loss: 0.21367972006245353\n",
      "Epoch: 2983 Training Loss: 0.21726492331525765 Test Loss: 0.2137291859552538\n",
      "Epoch: 2984 Training Loss: 0.21726522969460804 Test Loss: 0.21368939028153872\n",
      "Epoch: 2985 Training Loss: 0.217265057488469 Test Loss: 0.2137208120121854\n",
      "Epoch: 2986 Training Loss: 0.21726483160407964 Test Loss: 0.2137393746847457\n",
      "Epoch: 2987 Training Loss: 0.21726464993005482 Test Loss: 0.21370827702310316\n",
      "Epoch: 2988 Training Loss: 0.21726520909119743 Test Loss: 0.21367098316210045\n",
      "Epoch: 2989 Training Loss: 0.2172651154972012 Test Loss: 0.21377010938910349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2990 Training Loss: 0.21726518057098457 Test Loss: 0.21434544853639032\n",
      "Epoch: 2991 Training Loss: 0.21726561127909685 Test Loss: 0.2137524411469948\n",
      "Epoch: 2992 Training Loss: 0.2172658792579217 Test Loss: 0.2137491874941927\n",
      "Epoch: 2993 Training Loss: 0.21726443045621313 Test Loss: 0.21519251306231416\n",
      "Epoch: 2994 Training Loss: 0.217266179154998 Test Loss: 0.21390619244534487\n",
      "Epoch: 2995 Training Loss: 0.21726167103372038 Test Loss: 0.21414451279102953\n",
      "Epoch: 2996 Training Loss: 0.21726537132736579 Test Loss: 0.21395909346958958\n",
      "Epoch: 2997 Training Loss: 0.21726285825818598 Test Loss: 0.21385333030938064\n",
      "Epoch: 2998 Training Loss: 0.21726386596041897 Test Loss: 0.2138716207639774\n",
      "Epoch: 2999 Training Loss: 0.21726303085882023 Test Loss: 0.2137531152105235\n",
      "Epoch: 3000 Training Loss: 0.21726434278861137 Test Loss: 0.21375995954789212\n",
      "Epoch: 3001 Training Loss: 0.2172638963544842 Test Loss: 0.21384588968504434\n",
      "Epoch: 3002 Training Loss: 0.2172649377950262 Test Loss: 0.213671488709747\n",
      "Epoch: 3003 Training Loss: 0.21726516993754463 Test Loss: 0.21371974906585164\n",
      "Epoch: 3004 Training Loss: 0.21726575042832771 Test Loss: 0.21362850419703064\n",
      "Epoch: 3005 Training Loss: 0.21726471384724985 Test Loss: 0.21360983782238904\n",
      "Epoch: 3006 Training Loss: 0.21726553277454363 Test Loss: 0.21374751529813105\n",
      "Epoch: 3007 Training Loss: 0.21726520162268523 Test Loss: 0.21367654418621243\n",
      "Epoch: 3008 Training Loss: 0.2172651638228684 Test Loss: 0.2152485899628\n",
      "Epoch: 3009 Training Loss: 0.21726556828808125 Test Loss: 0.2137690205172494\n",
      "Epoch: 3010 Training Loss: 0.21726233817000395 Test Loss: 0.21383509170582457\n",
      "Epoch: 3011 Training Loss: 0.21726173504953925 Test Loss: 0.21377049827190853\n",
      "Epoch: 3012 Training Loss: 0.21726269075909246 Test Loss: 0.21374345795419852\n",
      "Epoch: 3013 Training Loss: 0.21726531055716689 Test Loss: 0.21379136831577863\n",
      "Epoch: 3014 Training Loss: 0.2172607661512928 Test Loss: 0.214845448121582\n",
      "Epoch: 3015 Training Loss: 0.21726724199690686 Test Loss: 0.2138756781079099\n",
      "Epoch: 3016 Training Loss: 0.2172638896749625 Test Loss: 0.21382539556121907\n",
      "Epoch: 3017 Training Loss: 0.21726320224907136 Test Loss: 0.21383510466858474\n",
      "Epoch: 3018 Training Loss: 0.2172651985563813 Test Loss: 0.2138375027792158\n",
      "Epoch: 3019 Training Loss: 0.21726350026332944 Test Loss: 0.2136977642246071\n",
      "Epoch: 3020 Training Loss: 0.21726304707795419 Test Loss: 0.21371925648096526\n",
      "Epoch: 3021 Training Loss: 0.21726461464962804 Test Loss: 0.21368754956959488\n",
      "Epoch: 3022 Training Loss: 0.21726545158025015 Test Loss: 0.21372935447113597\n",
      "Epoch: 3023 Training Loss: 0.2172635269186557 Test Loss: 0.2137535300188489\n",
      "Epoch: 3024 Training Loss: 0.21726387803735286 Test Loss: 0.21384343972337264\n",
      "Epoch: 3025 Training Loss: 0.2172645796381753 Test Loss: 0.21380920507376952\n",
      "Epoch: 3026 Training Loss: 0.2172642580528265 Test Loss: 0.21369511982153286\n",
      "Epoch: 3027 Training Loss: 0.21726387326754673 Test Loss: 0.2137175194711028\n",
      "Epoch: 3028 Training Loss: 0.21726473650382888 Test Loss: 0.213719930544494\n",
      "Epoch: 3029 Training Loss: 0.2172656361053997 Test Loss: 0.21365445564288654\n",
      "Epoch: 3030 Training Loss: 0.21726401713278892 Test Loss: 0.2137292637318148\n",
      "Epoch: 3031 Training Loss: 0.21726488512991138 Test Loss: 0.21369300689162551\n",
      "Epoch: 3032 Training Loss: 0.2172649918050112 Test Loss: 0.2136417132496416\n",
      "Epoch: 3033 Training Loss: 0.21726586637406572 Test Loss: 0.21376132063770972\n",
      "Epoch: 3034 Training Loss: 0.2172645627376405 Test Loss: 0.2151718115343262\n",
      "Epoch: 3035 Training Loss: 0.21726331834722798 Test Loss: 0.21373281552810078\n",
      "Epoch: 3036 Training Loss: 0.21726679685385505 Test Loss: 0.21377043345810767\n",
      "Epoch: 3037 Training Loss: 0.21726518235517897 Test Loss: 0.2137390376529813\n",
      "Epoch: 3038 Training Loss: 0.21726290905841422 Test Loss: 0.21374498755989832\n",
      "Epoch: 3039 Training Loss: 0.21726571963080143 Test Loss: 0.21379905523255813\n",
      "Epoch: 3040 Training Loss: 0.21726279888665204 Test Loss: 0.2137630965358527\n",
      "Epoch: 3041 Training Loss: 0.21726626486805514 Test Loss: 0.21379660527088643\n",
      "Epoch: 3042 Training Loss: 0.21726335064204305 Test Loss: 0.21430818060090792\n",
      "Epoch: 3043 Training Loss: 0.21726584513408326 Test Loss: 0.21375763921382207\n",
      "Epoch: 3044 Training Loss: 0.21726332352946096 Test Loss: 0.21372317123453594\n",
      "Epoch: 3045 Training Loss: 0.21726482454799428 Test Loss: 0.21371383804721514\n",
      "Epoch: 3046 Training Loss: 0.21726564989480157 Test Loss: 0.21368410147539024\n",
      "Epoch: 3047 Training Loss: 0.21726479444980049 Test Loss: 0.21375275225323884\n",
      "Epoch: 3048 Training Loss: 0.21726483129027663 Test Loss: 0.2137016660154176\n",
      "Epoch: 3049 Training Loss: 0.21726477355948423 Test Loss: 0.2137052048489434\n",
      "Epoch: 3050 Training Loss: 0.2172642588687144 Test Loss: 0.2136203506208851\n",
      "Epoch: 3051 Training Loss: 0.2172654976196381 Test Loss: 0.21374606346899225\n",
      "Epoch: 3052 Training Loss: 0.2172628836851975 Test Loss: 0.21365820188057502\n",
      "Epoch: 3053 Training Loss: 0.21726550184253035 Test Loss: 0.21365665931211505\n",
      "Epoch: 3054 Training Loss: 0.21726422216272526 Test Loss: 0.2136490372091364\n",
      "Epoch: 3055 Training Loss: 0.21726703178473758 Test Loss: 0.21375526702871137\n",
      "Epoch: 3056 Training Loss: 0.21726328681450602 Test Loss: 0.21375202633866944\n",
      "Epoch: 3057 Training Loss: 0.21726414164983265 Test Loss: 0.21380309961373048\n",
      "Epoch: 3058 Training Loss: 0.21726325137269484 Test Loss: 0.21381336611978338\n",
      "Epoch: 3059 Training Loss: 0.2172638689998255 Test Loss: 0.2137279415302777\n",
      "Epoch: 3060 Training Loss: 0.21726529227589872 Test Loss: 0.2137285378172454\n",
      "Epoch: 3061 Training Loss: 0.2172643833050659 Test Loss: 0.21365827965713602\n",
      "Epoch: 3062 Training Loss: 0.2172659371859617 Test Loss: 0.2137658835292888\n",
      "Epoch: 3063 Training Loss: 0.21726432376318172 Test Loss: 0.21371105105377908\n",
      "Epoch: 3064 Training Loss: 0.21726451961213789 Test Loss: 0.2137801555282335\n",
      "Epoch: 3065 Training Loss: 0.2172653303626212 Test Loss: 0.21383686760396756\n",
      "Epoch: 3066 Training Loss: 0.21726364867423273 Test Loss: 0.21386000613086706\n",
      "Epoch: 3067 Training Loss: 0.21726454214319568 Test Loss: 0.2136914902486859\n",
      "Epoch: 3068 Training Loss: 0.21726327251405347 Test Loss: 0.2137057622476306\n",
      "Epoch: 3069 Training Loss: 0.2172645682785055 Test Loss: 0.21367572753232186\n",
      "Epoch: 3070 Training Loss: 0.21726502812547088 Test Loss: 0.21370328636043856\n",
      "Epoch: 3071 Training Loss: 0.21726458288379524 Test Loss: 0.2137641076311458\n",
      "Epoch: 3072 Training Loss: 0.21726300158548012 Test Loss: 0.213722017548881\n",
      "Epoch: 3073 Training Loss: 0.21726555270551917 Test Loss: 0.21379679971228893\n",
      "Epoch: 3074 Training Loss: 0.21726472812080497 Test Loss: 0.21370981959156313\n",
      "Epoch: 3075 Training Loss: 0.21726475171879311 Test Loss: 0.21388411686477912\n",
      "Epoch: 3076 Training Loss: 0.21726227618045613 Test Loss: 0.2142473982184812\n",
      "Epoch: 3077 Training Loss: 0.21726413860146032 Test Loss: 0.21400045763728495\n",
      "Epoch: 3078 Training Loss: 0.21726438885489668 Test Loss: 0.21380198481635607\n",
      "Epoch: 3079 Training Loss: 0.21726332168250595 Test Loss: 0.2136702053964904\n",
      "Epoch: 3080 Training Loss: 0.21726190539975745 Test Loss: 0.21443801560674844\n",
      "Epoch: 3081 Training Loss: 0.2172651199442385 Test Loss: 0.21396461560542104\n",
      "Epoch: 3082 Training Loss: 0.21726439674480152 Test Loss: 0.21387771326125624\n",
      "Epoch: 3083 Training Loss: 0.2172665194430076 Test Loss: 0.21379609972323987\n",
      "Epoch: 3084 Training Loss: 0.21726527582365396 Test Loss: 0.21383887683179356\n",
      "Epoch: 3085 Training Loss: 0.2172640811486078 Test Loss: 0.21373805248320857\n",
      "Epoch: 3086 Training Loss: 0.21726489755651152 Test Loss: 0.21376440577462966\n",
      "Epoch: 3087 Training Loss: 0.21726487628963162 Test Loss: 0.21378521100469894\n",
      "Epoch: 3088 Training Loss: 0.21726551680645215 Test Loss: 0.21375822253802962\n",
      "Epoch: 3089 Training Loss: 0.2172641273493801 Test Loss: 0.21373395625099553\n",
      "Epoch: 3090 Training Loss: 0.21726620048463846 Test Loss: 0.21388735755482108\n",
      "Epoch: 3091 Training Loss: 0.21726373393899984 Test Loss: 0.21386478938936895\n",
      "Epoch: 3092 Training Loss: 0.2172638858286339 Test Loss: 0.21377707039131358\n",
      "Epoch: 3093 Training Loss: 0.21726140532324317 Test Loss: 0.21433056728771768\n",
      "Epoch: 3094 Training Loss: 0.21726623382845223 Test Loss: 0.213778107412127\n",
      "Epoch: 3095 Training Loss: 0.21726379855552735 Test Loss: 0.21378942390175348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3096 Training Loss: 0.21726307984795667 Test Loss: 0.21375845586771264\n",
      "Epoch: 3097 Training Loss: 0.21726569118231498 Test Loss: 0.2138163734801423\n",
      "Epoch: 3098 Training Loss: 0.21726316174158264 Test Loss: 0.213839991629168\n",
      "Epoch: 3099 Training Loss: 0.21726562522988313 Test Loss: 0.21376968161801796\n",
      "Epoch: 3100 Training Loss: 0.21726502716613016 Test Loss: 0.2136829607524955\n",
      "Epoch: 3101 Training Loss: 0.21726565335560075 Test Loss: 0.2137009789891287\n",
      "Epoch: 3102 Training Loss: 0.21726675176484203 Test Loss: 0.21389245191956702\n",
      "Epoch: 3103 Training Loss: 0.21726360478663703 Test Loss: 0.21379799228622437\n",
      "Epoch: 3104 Training Loss: 0.21726523582721588 Test Loss: 0.21373592659054105\n",
      "Epoch: 3105 Training Loss: 0.2172635354899614 Test Loss: 0.2137114010483036\n",
      "Epoch: 3106 Training Loss: 0.21726683529024376 Test Loss: 0.21382980289967612\n",
      "Epoch: 3107 Training Loss: 0.21726444077585005 Test Loss: 0.2137170787372571\n",
      "Epoch: 3108 Training Loss: 0.21726488573958583 Test Loss: 0.21377968886886747\n",
      "Epoch: 3109 Training Loss: 0.21726478787786838 Test Loss: 0.2137577947669441\n",
      "Epoch: 3110 Training Loss: 0.217264988120067 Test Loss: 0.21379184793790484\n",
      "Epoch: 3111 Training Loss: 0.21726373438728988 Test Loss: 0.2137656372368456\n",
      "Epoch: 3112 Training Loss: 0.21726425249402992 Test Loss: 0.21373027482710788\n",
      "Epoch: 3113 Training Loss: 0.21726534740660883 Test Loss: 0.21390340545190878\n",
      "Epoch: 3114 Training Loss: 0.21726497316511098 Test Loss: 0.2138256677791826\n",
      "Epoch: 3115 Training Loss: 0.21726417335290485 Test Loss: 0.21374912268039184\n",
      "Epoch: 3116 Training Loss: 0.21726354865175723 Test Loss: 0.2137155491315573\n",
      "Epoch: 3117 Training Loss: 0.21726335633532665 Test Loss: 0.2136831163056175\n",
      "Epoch: 3118 Training Loss: 0.21726384845917548 Test Loss: 0.2138069625162605\n",
      "Epoch: 3119 Training Loss: 0.21726441908757751 Test Loss: 0.21377144455340077\n",
      "Epoch: 3120 Training Loss: 0.2172643757468957 Test Loss: 0.21370806961894048\n",
      "Epoch: 3121 Training Loss: 0.21726578900816926 Test Loss: 0.21385409511223055\n",
      "Epoch: 3122 Training Loss: 0.2172642991520581 Test Loss: 0.213712126962873\n",
      "Epoch: 3123 Training Loss: 0.2172628285455216 Test Loss: 0.2136994493834289\n",
      "Epoch: 3124 Training Loss: 0.217265098345624 Test Loss: 0.21378090736832325\n",
      "Epoch: 3125 Training Loss: 0.21726445188447743 Test Loss: 0.21374043763107944\n",
      "Epoch: 3126 Training Loss: 0.21726394829336917 Test Loss: 0.21366245366591005\n",
      "Epoch: 3127 Training Loss: 0.21726639370661494 Test Loss: 0.21377854814597272\n",
      "Epoch: 3128 Training Loss: 0.21726463351367326 Test Loss: 0.2138117068864819\n",
      "Epoch: 3129 Training Loss: 0.21726428925381383 Test Loss: 0.2137378710045662\n",
      "Epoch: 3130 Training Loss: 0.2172640747290943 Test Loss: 0.21369863272953835\n",
      "Epoch: 3131 Training Loss: 0.21726557148887218 Test Loss: 0.21370699370984655\n",
      "Epoch: 3132 Training Loss: 0.2172644411882769 Test Loss: 0.2139236273577705\n",
      "Epoch: 3133 Training Loss: 0.21726512968109835 Test Loss: 0.21474166826367871\n",
      "Epoch: 3134 Training Loss: 0.2172665559696807 Test Loss: 0.2137950108513858\n",
      "Epoch: 3135 Training Loss: 0.21726489232944954 Test Loss: 0.21372726746674897\n",
      "Epoch: 3136 Training Loss: 0.21726418757266516 Test Loss: 0.2137419413112589\n",
      "Epoch: 3137 Training Loss: 0.21726522373235038 Test Loss: 0.213781970314657\n",
      "Epoch: 3138 Training Loss: 0.21726466968171432 Test Loss: 0.2137717556596448\n",
      "Epoch: 3139 Training Loss: 0.21726399778459046 Test Loss: 0.21370385672188594\n",
      "Epoch: 3140 Training Loss: 0.21726517209830268 Test Loss: 0.21378274808026707\n",
      "Epoch: 3141 Training Loss: 0.2172628594954665 Test Loss: 0.21381656792154483\n",
      "Epoch: 3142 Training Loss: 0.21726444949957438 Test Loss: 0.2137321285018119\n",
      "Epoch: 3143 Training Loss: 0.21726423164854267 Test Loss: 0.21386261164566078\n",
      "Epoch: 3144 Training Loss: 0.21726429336911648 Test Loss: 0.21386329867194967\n",
      "Epoch: 3145 Training Loss: 0.21726403379124712 Test Loss: 0.2137577169903831\n",
      "Epoch: 3146 Training Loss: 0.2172665466811109 Test Loss: 0.21380991802557875\n",
      "Epoch: 3147 Training Loss: 0.21726414319195042 Test Loss: 0.21372770820059467\n",
      "Epoch: 3148 Training Loss: 0.21726389875731886 Test Loss: 0.21374948563767654\n",
      "Epoch: 3149 Training Loss: 0.21726560460854094 Test Loss: 0.21379928856224115\n",
      "Epoch: 3150 Training Loss: 0.21726422677114696 Test Loss: 0.21406078632310582\n",
      "Epoch: 3151 Training Loss: 0.21726436916599778 Test Loss: 0.2136781645312334\n",
      "Epoch: 3152 Training Loss: 0.2172658437443841 Test Loss: 0.21374321166175533\n",
      "Epoch: 3153 Training Loss: 0.21726484167267412 Test Loss: 0.2136525371543817\n",
      "Epoch: 3154 Training Loss: 0.2172652173845633 Test Loss: 0.21368201447100324\n",
      "Epoch: 3155 Training Loss: 0.2172658446409642 Test Loss: 0.2137313507362018\n",
      "Epoch: 3156 Training Loss: 0.21726436870874194 Test Loss: 0.21375696515029335\n",
      "Epoch: 3157 Training Loss: 0.2172670573103729 Test Loss: 0.21367493680395164\n",
      "Epoch: 3158 Training Loss: 0.2172633968876444 Test Loss: 0.21371527691359377\n",
      "Epoch: 3159 Training Loss: 0.21726527440705742 Test Loss: 0.21379003315148137\n",
      "Epoch: 3160 Training Loss: 0.21726500575579746 Test Loss: 0.213708186283782\n",
      "Epoch: 3161 Training Loss: 0.21726331111182662 Test Loss: 0.21381146059403872\n",
      "Epoch: 3162 Training Loss: 0.21726322835748377 Test Loss: 0.21371748058282228\n",
      "Epoch: 3163 Training Loss: 0.2172658333530208 Test Loss: 0.21388663164025168\n",
      "Epoch: 3164 Training Loss: 0.2172637616074616 Test Loss: 0.21377325933982425\n",
      "Epoch: 3165 Training Loss: 0.2172643057508876 Test Loss: 0.21371006588400632\n",
      "Epoch: 3166 Training Loss: 0.21726631608967603 Test Loss: 0.21370421967917064\n",
      "Epoch: 3167 Training Loss: 0.21726495597767057 Test Loss: 0.2137456486606669\n",
      "Epoch: 3168 Training Loss: 0.21726526430259974 Test Loss: 0.21366653693536292\n",
      "Epoch: 3169 Training Loss: 0.2172678239401497 Test Loss: 0.21378992944940003\n",
      "Epoch: 3170 Training Loss: 0.21726490896101033 Test Loss: 0.21372709895086678\n",
      "Epoch: 3171 Training Loss: 0.21726441188803935 Test Loss: 0.2137553707307927\n",
      "Epoch: 3172 Training Loss: 0.21726408950473428 Test Loss: 0.21382183080217293\n",
      "Epoch: 3173 Training Loss: 0.21726396550770702 Test Loss: 0.21689031057528937\n",
      "Epoch: 3174 Training Loss: 0.21725783187774975 Test Loss: 0.21575386539137995\n",
      "Epoch: 3175 Training Loss: 0.2172631416492227 Test Loss: 0.21473952940825103\n",
      "Epoch: 3176 Training Loss: 0.21726306250809763 Test Loss: 0.21440256245768954\n",
      "Epoch: 3177 Training Loss: 0.21726118339277203 Test Loss: 0.21404644951036025\n",
      "Epoch: 3178 Training Loss: 0.21726512819277538 Test Loss: 0.21380753287770787\n",
      "Epoch: 3179 Training Loss: 0.2172649957230662 Test Loss: 0.21388180949346927\n",
      "Epoch: 3180 Training Loss: 0.21726240240100203 Test Loss: 0.2138690800629845\n",
      "Epoch: 3181 Training Loss: 0.21726512875762083 Test Loss: 0.21370314377007674\n",
      "Epoch: 3182 Training Loss: 0.21726445111341855 Test Loss: 0.21370169194093794\n",
      "Epoch: 3183 Training Loss: 0.21726362644801214 Test Loss: 0.2136963123954683\n",
      "Epoch: 3184 Training Loss: 0.21726491044933327 Test Loss: 0.2137911479488558\n",
      "Epoch: 3185 Training Loss: 0.2172641719452741 Test Loss: 0.213747048638765\n",
      "Epoch: 3186 Training Loss: 0.2172640400852394 Test Loss: 0.21374917453143252\n",
      "Epoch: 3187 Training Loss: 0.21726348718222585 Test Loss: 0.21371595097712248\n",
      "Epoch: 3188 Training Loss: 0.21726506628391976 Test Loss: 0.21383818980550467\n",
      "Epoch: 3189 Training Loss: 0.21726394131797602 Test Loss: 0.21393303832165234\n",
      "Epoch: 3190 Training Loss: 0.21726279018085928 Test Loss: 0.2138344694933365\n",
      "Epoch: 3191 Training Loss: 0.21726375577969098 Test Loss: 0.21473554984087953\n",
      "Epoch: 3192 Training Loss: 0.21726409394280574 Test Loss: 0.21454739537704418\n",
      "Epoch: 3193 Training Loss: 0.2172649423586189 Test Loss: 0.21407142874920357\n",
      "Epoch: 3194 Training Loss: 0.21726453904999435 Test Loss: 0.21381982157434692\n",
      "Epoch: 3195 Training Loss: 0.2172665540061703 Test Loss: 0.2137707056760712\n",
      "Epoch: 3196 Training Loss: 0.2172638673142549 Test Loss: 0.21372476565403659\n",
      "Epoch: 3197 Training Loss: 0.21726580668872875 Test Loss: 0.21383281026003503\n",
      "Epoch: 3198 Training Loss: 0.21726436002088081 Test Loss: 0.2137354988194555\n",
      "Epoch: 3199 Training Loss: 0.21726513792963523 Test Loss: 0.21362913937227887\n",
      "Epoch: 3200 Training Loss: 0.21726425486996717 Test Loss: 0.21369014212162843\n",
      "Epoch: 3201 Training Loss: 0.21726311576495533 Test Loss: 0.21371001403296563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3202 Training Loss: 0.21726543536111623 Test Loss: 0.21376409466838564\n",
      "Epoch: 3203 Training Loss: 0.21726558700867366 Test Loss: 0.2137322451666534\n",
      "Epoch: 3204 Training Loss: 0.21726465887792418 Test Loss: 0.2136788126692418\n",
      "Epoch: 3205 Training Loss: 0.21726494927125142 Test Loss: 0.21367187759255205\n",
      "Epoch: 3206 Training Loss: 0.21726757845651945 Test Loss: 0.2137683334909605\n",
      "Epoch: 3207 Training Loss: 0.21726473193127038 Test Loss: 0.2137842258349262\n",
      "Epoch: 3208 Training Loss: 0.2172642468007463 Test Loss: 0.21444483401859668\n",
      "Epoch: 3209 Training Loss: 0.21726067038757277 Test Loss: 0.21447345579304714\n",
      "Epoch: 3210 Training Loss: 0.21726495116303543 Test Loss: 0.2142112710058936\n",
      "Epoch: 3211 Training Loss: 0.21726410243341926 Test Loss: 0.21395782311909312\n",
      "Epoch: 3212 Training Loss: 0.21726468796298248 Test Loss: 0.21375130042410004\n",
      "Epoch: 3213 Training Loss: 0.2172655025059996 Test Loss: 0.2137148232169879\n",
      "Epoch: 3214 Training Loss: 0.21726346762781396 Test Loss: 0.21379301458631994\n",
      "Epoch: 3215 Training Loss: 0.2172658481286608 Test Loss: 0.21370059010632367\n",
      "Epoch: 3216 Training Loss: 0.21726446673184382 Test Loss: 0.21367107390142162\n",
      "Epoch: 3217 Training Loss: 0.21726451821347292 Test Loss: 0.21372315827177576\n",
      "Epoch: 3218 Training Loss: 0.2172646430443197 Test Loss: 0.21377153529272194\n",
      "Epoch: 3219 Training Loss: 0.21726453024557782 Test Loss: 0.2137454412565042\n",
      "Epoch: 3220 Training Loss: 0.21726244198501327 Test Loss: 0.21376860570892403\n",
      "Epoch: 3221 Training Loss: 0.21726387014744802 Test Loss: 0.213766440927976\n",
      "Epoch: 3222 Training Loss: 0.21726468765814524 Test Loss: 0.2138459804243655\n",
      "Epoch: 3223 Training Loss: 0.21726747965339288 Test Loss: 0.21371934722028646\n",
      "Epoch: 3224 Training Loss: 0.21726496306065332 Test Loss: 0.2137865591317564\n",
      "Epoch: 3225 Training Loss: 0.21726549596096492 Test Loss: 0.21380718288318334\n",
      "Epoch: 3226 Training Loss: 0.21726403582648393 Test Loss: 0.21376623352381333\n",
      "Epoch: 3227 Training Loss: 0.2172670479949057 Test Loss: 0.21385344697422215\n",
      "Epoch: 3228 Training Loss: 0.21726538144975507 Test Loss: 0.21377061493675004\n",
      "Epoch: 3229 Training Loss: 0.21726553571532636 Test Loss: 0.2137444690494916\n",
      "Epoch: 3230 Training Loss: 0.21726377034911754 Test Loss: 0.21377447783928002\n",
      "Epoch: 3231 Training Loss: 0.2172626360766724 Test Loss: 0.2136944587207643\n",
      "Epoch: 3232 Training Loss: 0.2172655218362665 Test Loss: 0.21365528525953728\n",
      "Epoch: 3233 Training Loss: 0.21726481576150936 Test Loss: 0.21367791823879023\n",
      "Epoch: 3234 Training Loss: 0.21726559532893694 Test Loss: 0.21375731514481788\n",
      "Epoch: 3235 Training Loss: 0.21726478211285835 Test Loss: 0.21371479729146756\n",
      "Epoch: 3236 Training Loss: 0.2172651266147944 Test Loss: 0.21369913827718487\n",
      "Epoch: 3237 Training Loss: 0.21726505430560966 Test Loss: 0.21372625637145587\n",
      "Epoch: 3238 Training Loss: 0.21726410898741977 Test Loss: 0.21369034952579113\n",
      "Epoch: 3239 Training Loss: 0.21726642351790312 Test Loss: 0.21368406258710976\n",
      "Epoch: 3240 Training Loss: 0.21726517183829444 Test Loss: 0.21373548585669533\n",
      "Epoch: 3241 Training Loss: 0.21726358818197364 Test Loss: 0.21663647380568388\n",
      "Epoch: 3242 Training Loss: 0.21726469980680554 Test Loss: 0.2138962759338165\n",
      "Epoch: 3243 Training Loss: 0.21726439612616127 Test Loss: 0.2137608928666242\n",
      "Epoch: 3244 Training Loss: 0.21726376340958758 Test Loss: 0.21378215179329935\n",
      "Epoch: 3245 Training Loss: 0.21726388658176116 Test Loss: 0.21378513322813794\n",
      "Epoch: 3246 Training Loss: 0.21726661006035788 Test Loss: 0.2137728445314989\n",
      "Epoch: 3247 Training Loss: 0.21726461169091374 Test Loss: 0.21397043588473638\n",
      "Epoch: 3248 Training Loss: 0.21726368263668674 Test Loss: 0.21386797822837023\n",
      "Epoch: 3249 Training Loss: 0.21726284641436291 Test Loss: 0.21381124022711587\n",
      "Epoch: 3250 Training Loss: 0.2172641905313795 Test Loss: 0.21380615882513007\n",
      "Epoch: 3251 Training Loss: 0.21726392465055205 Test Loss: 0.2138036959006982\n",
      "Epoch: 3252 Training Loss: 0.217264077580219 Test Loss: 0.213787920221574\n",
      "Epoch: 3253 Training Loss: 0.21726387053297747 Test Loss: 0.21373612103194356\n",
      "Epoch: 3254 Training Loss: 0.2172646749446395 Test Loss: 0.2137396469027092\n",
      "Epoch: 3255 Training Loss: 0.21726523277884355 Test Loss: 0.21382241412638048\n",
      "Epoch: 3256 Training Loss: 0.2172649375619154 Test Loss: 0.21368228668896677\n",
      "Epoch: 3257 Training Loss: 0.217263155734496 Test Loss: 0.21379305347460045\n",
      "Epoch: 3258 Training Loss: 0.217264810552379 Test Loss: 0.21372195273508018\n",
      "Epoch: 3259 Training Loss: 0.2172641588103757 Test Loss: 0.21374424868256875\n",
      "Epoch: 3260 Training Loss: 0.21726320202492636 Test Loss: 0.21379844598283027\n",
      "Epoch: 3261 Training Loss: 0.21726461503515748 Test Loss: 0.21390221287797334\n",
      "Epoch: 3262 Training Loss: 0.21726338189682518 Test Loss: 0.21380552364988187\n",
      "Epoch: 3263 Training Loss: 0.21726390010218902 Test Loss: 0.2138206382282375\n",
      "Epoch: 3264 Training Loss: 0.21726447988467382 Test Loss: 0.21376282431788918\n",
      "Epoch: 3265 Training Loss: 0.21726345698540822 Test Loss: 0.21382170117457125\n",
      "Epoch: 3266 Training Loss: 0.21726536724792633 Test Loss: 0.21377084826643306\n",
      "Epoch: 3267 Training Loss: 0.21726494306691718 Test Loss: 0.21381833085692764\n",
      "Epoch: 3268 Training Loss: 0.21726261714090078 Test Loss: 0.21375189671106776\n",
      "Epoch: 3269 Training Loss: 0.2172651323260096 Test Loss: 0.21379374050088934\n",
      "Epoch: 3270 Training Loss: 0.21726256314884743 Test Loss: 0.21379791450966337\n",
      "Epoch: 3271 Training Loss: 0.21726491841993034 Test Loss: 0.2138294010541109\n",
      "Epoch: 3272 Training Loss: 0.2172652715380011 Test Loss: 0.21374235611958425\n",
      "Epoch: 3273 Training Loss: 0.21726578266934798 Test Loss: 0.21375973918096927\n",
      "Epoch: 3274 Training Loss: 0.21726523008910328 Test Loss: 0.2138622357256159\n",
      "Epoch: 3275 Training Loss: 0.21726442409049446 Test Loss: 0.2137716260320431\n",
      "Epoch: 3276 Training Loss: 0.21726414289607898 Test Loss: 0.21369097173827917\n",
      "Epoch: 3277 Training Loss: 0.21726500152393943 Test Loss: 0.21378907390722895\n",
      "Epoch: 3278 Training Loss: 0.2172663635725579 Test Loss: 0.21380509587879631\n",
      "Epoch: 3279 Training Loss: 0.21726488045872908 Test Loss: 0.21380237369916108\n",
      "Epoch: 3280 Training Loss: 0.217263817715444 Test Loss: 0.21375897437811936\n",
      "Epoch: 3281 Training Loss: 0.21726447099956508 Test Loss: 0.21457938746913827\n",
      "Epoch: 3282 Training Loss: 0.21726500098599136 Test Loss: 0.21376405578010513\n",
      "Epoch: 3283 Training Loss: 0.21726641568179308 Test Loss: 0.21390449432376288\n",
      "Epoch: 3284 Training Loss: 0.21726577619603968 Test Loss: 0.21406092891346767\n",
      "Epoch: 3285 Training Loss: 0.217264962038552 Test Loss: 0.2139114812514933\n",
      "Epoch: 3286 Training Loss: 0.2172625854467944 Test Loss: 0.21379683860056944\n",
      "Epoch: 3287 Training Loss: 0.21726650163692687 Test Loss: 0.21373997097171338\n",
      "Epoch: 3288 Training Loss: 0.21726548642135268 Test Loss: 0.2137511967220187\n",
      "Epoch: 3289 Training Loss: 0.21726570975048878 Test Loss: 0.21376575390168712\n",
      "Epoch: 3290 Training Loss: 0.21726445009131723 Test Loss: 0.21370712333744823\n",
      "Epoch: 3291 Training Loss: 0.2172653537812933 Test Loss: 0.2137393746847457\n",
      "Epoch: 3292 Training Loss: 0.21726684411259192 Test Loss: 0.21372454528711374\n",
      "Epoch: 3293 Training Loss: 0.2172661895732587 Test Loss: 0.21389477225363704\n",
      "Epoch: 3294 Training Loss: 0.2172647797638185 Test Loss: 0.21383252507931136\n",
      "Epoch: 3295 Training Loss: 0.21726552801370333 Test Loss: 0.21371318990920676\n",
      "Epoch: 3296 Training Loss: 0.21726525285327192 Test Loss: 0.21372864151932675\n",
      "Epoch: 3297 Training Loss: 0.21726495394243373 Test Loss: 0.21370766777337527\n",
      "Epoch: 3298 Training Loss: 0.21726548227915266 Test Loss: 0.21394731032059705\n",
      "Epoch: 3299 Training Loss: 0.2172645403141723 Test Loss: 0.21381121430159553\n",
      "Epoch: 3300 Training Loss: 0.2172648190788557 Test Loss: 0.213712126962873\n",
      "Epoch: 3301 Training Loss: 0.2172662513745247 Test Loss: 0.2139030813829046\n",
      "Epoch: 3302 Training Loss: 0.21726379402779786 Test Loss: 0.21376224099368163\n",
      "Epoch: 3303 Training Loss: 0.2172652171962815 Test Loss: 0.2137574318096594\n",
      "Epoch: 3304 Training Loss: 0.2172652135651321 Test Loss: 0.21373959505166854\n",
      "Epoch: 3305 Training Loss: 0.2172661344425486 Test Loss: 0.2136910754403605\n",
      "Epoch: 3306 Training Loss: 0.21726546828353735 Test Loss: 0.2137187638960789\n",
      "Epoch: 3307 Training Loss: 0.2172654324024019 Test Loss: 0.21376283728064935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3308 Training Loss: 0.2172635400894173 Test Loss: 0.2137176361359443\n",
      "Epoch: 3309 Training Loss: 0.21726675585324726 Test Loss: 0.213857608020236\n",
      "Epoch: 3310 Training Loss: 0.21726494119306478 Test Loss: 0.2137576651393424\n",
      "Epoch: 3311 Training Loss: 0.21726370405598525 Test Loss: 0.21370567150830944\n",
      "Epoch: 3312 Training Loss: 0.21726556609146 Test Loss: 0.2139101590499562\n",
      "Epoch: 3313 Training Loss: 0.21726430705092875 Test Loss: 0.21367006280612855\n",
      "Epoch: 3314 Training Loss: 0.21726518858641064 Test Loss: 0.2136578518860505\n",
      "Epoch: 3315 Training Loss: 0.2172654928319004 Test Loss: 0.21381525868276788\n",
      "Epoch: 3316 Training Loss: 0.2172636437878712 Test Loss: 0.21373352847991\n",
      "Epoch: 3317 Training Loss: 0.2172646870305392 Test Loss: 0.2137536077954099\n",
      "Epoch: 3318 Training Loss: 0.2172660980055335 Test Loss: 0.21370867886866837\n",
      "Epoch: 3319 Training Loss: 0.2172655778994199 Test Loss: 0.21369123099348253\n",
      "Epoch: 3320 Training Loss: 0.21726534511136378 Test Loss: 0.21371711762553758\n",
      "Epoch: 3321 Training Loss: 0.21726563863375556 Test Loss: 0.21369335688615004\n",
      "Epoch: 3322 Training Loss: 0.21726536351815315 Test Loss: 0.21371318990920676\n",
      "Epoch: 3323 Training Loss: 0.2172656043037037 Test Loss: 0.21375049673296964\n",
      "Epoch: 3324 Training Loss: 0.21726372306348327 Test Loss: 0.21372389714910534\n",
      "Epoch: 3325 Training Loss: 0.21726480236660273 Test Loss: 0.21373382662339385\n",
      "Epoch: 3326 Training Loss: 0.21726359209106286 Test Loss: 0.21420341557323191\n",
      "Epoch: 3327 Training Loss: 0.21726569991500513 Test Loss: 0.21380291813508814\n",
      "Epoch: 3328 Training Loss: 0.2172639373909552 Test Loss: 0.21372763042403367\n",
      "Epoch: 3329 Training Loss: 0.2172653976061284 Test Loss: 0.21396977478396784\n",
      "Epoch: 3330 Training Loss: 0.21726562886999834 Test Loss: 0.21372096756530742\n",
      "Epoch: 3331 Training Loss: 0.21726370619881166 Test Loss: 0.21372921188077412\n",
      "Epoch: 3332 Training Loss: 0.21726286160242975 Test Loss: 0.21399990023859775\n",
      "Epoch: 3333 Training Loss: 0.21726270538231382 Test Loss: 0.21386640973438995\n",
      "Epoch: 3334 Training Loss: 0.21726196529130787 Test Loss: 0.21371584727504114\n",
      "Epoch: 3335 Training Loss: 0.2172645525524906 Test Loss: 0.21377744631135845\n",
      "Epoch: 3336 Training Loss: 0.2172647723849643 Test Loss: 0.21372479157955693\n",
      "Epoch: 3337 Training Loss: 0.21726349086717003 Test Loss: 0.21377561856217478\n",
      "Epoch: 3338 Training Loss: 0.21726571299610872 Test Loss: 0.21379905523255813\n",
      "Epoch: 3339 Training Loss: 0.2172634960852662 Test Loss: 0.21373717101551715\n",
      "Epoch: 3340 Training Loss: 0.21726718561995043 Test Loss: 0.21415404041975283\n",
      "Epoch: 3341 Training Loss: 0.21726565057620245 Test Loss: 0.2137381561852899\n",
      "Epoch: 3342 Training Loss: 0.21726440154150506 Test Loss: 0.21382595295990628\n",
      "Epoch: 3343 Training Loss: 0.21726307021868646 Test Loss: 0.21372314530901562\n",
      "Epoch: 3344 Training Loss: 0.21726441256047443 Test Loss: 0.21377156121824228\n",
      "Epoch: 3345 Training Loss: 0.21726514074489672 Test Loss: 0.21367021835925057\n",
      "Epoch: 3346 Training Loss: 0.21726657932559218 Test Loss: 0.21484285556954844\n",
      "Epoch: 3347 Training Loss: 0.2172660044025715 Test Loss: 0.2136947050132075\n",
      "Epoch: 3348 Training Loss: 0.2172635606838621 Test Loss: 0.21370455671093502\n",
      "Epoch: 3349 Training Loss: 0.21726495134235146 Test Loss: 0.21389560187028778\n",
      "Epoch: 3350 Training Loss: 0.2172645654004834 Test Loss: 0.21376750387430976\n",
      "Epoch: 3351 Training Loss: 0.21726463631100315 Test Loss: 0.21370670852912285\n",
      "Epoch: 3352 Training Loss: 0.21726688635944602 Test Loss: 0.21364683353990788\n",
      "Epoch: 3353 Training Loss: 0.21726519601905964 Test Loss: 0.2147177001201285\n",
      "Epoch: 3354 Training Loss: 0.2172611989394709 Test Loss: 0.21482390401418314\n",
      "Epoch: 3355 Training Loss: 0.21726523258159594 Test Loss: 0.2141981526926038\n",
      "Epoch: 3356 Training Loss: 0.21726419849301074 Test Loss: 0.21397343028233515\n",
      "Epoch: 3357 Training Loss: 0.21726411269029555 Test Loss: 0.21384467118558856\n",
      "Epoch: 3358 Training Loss: 0.21726563756682526 Test Loss: 0.21384515080771477\n",
      "Epoch: 3359 Training Loss: 0.2172647748057306 Test Loss: 0.2138396286718833\n",
      "Epoch: 3360 Training Loss: 0.2172621412451517 Test Loss: 0.21379314421392162\n",
      "Epoch: 3361 Training Loss: 0.21726406053623137 Test Loss: 0.21369693460795636\n",
      "Epoch: 3362 Training Loss: 0.21726422875258897 Test Loss: 0.2137538151995726\n",
      "Epoch: 3363 Training Loss: 0.21726364228161663 Test Loss: 0.2137362636223054\n",
      "Epoch: 3364 Training Loss: 0.21726517069067192 Test Loss: 0.21373643213818758\n",
      "Epoch: 3365 Training Loss: 0.21726384585012742 Test Loss: 0.21373745619624085\n",
      "Epoch: 3366 Training Loss: 0.2172642877834225 Test Loss: 0.213760413244498\n",
      "Epoch: 3367 Training Loss: 0.21726473142021974 Test Loss: 0.2137461671710736\n",
      "Epoch: 3368 Training Loss: 0.21726430229008845 Test Loss: 0.21380753287770787\n",
      "Epoch: 3369 Training Loss: 0.21726432586117916 Test Loss: 0.21375258373735664\n",
      "Epoch: 3370 Training Loss: 0.21726418222008198 Test Loss: 0.2137283044875624\n",
      "Epoch: 3371 Training Loss: 0.21726499339195796 Test Loss: 0.21366574620699266\n",
      "Epoch: 3372 Training Loss: 0.2172640543946577 Test Loss: 0.21372485639335775\n",
      "Epoch: 3373 Training Loss: 0.2172636362117694 Test Loss: 0.21373673028167145\n",
      "Epoch: 3374 Training Loss: 0.21726408567633726 Test Loss: 0.21373885617433896\n",
      "Epoch: 3375 Training Loss: 0.2172661230649472 Test Loss: 0.21374856528170463\n",
      "Epoch: 3376 Training Loss: 0.21726508782873946 Test Loss: 0.2137327247887796\n",
      "Epoch: 3377 Training Loss: 0.21726456485356951 Test Loss: 0.21695027630382552\n",
      "Epoch: 3378 Training Loss: 0.21726481498148467 Test Loss: 0.21390931647054529\n",
      "Epoch: 3379 Training Loss: 0.21726626597981447 Test Loss: 0.21368439961887412\n",
      "Epoch: 3380 Training Loss: 0.21726623614162888 Test Loss: 0.2138280010760128\n",
      "Epoch: 3381 Training Loss: 0.21726433112410432 Test Loss: 0.21382560296538175\n",
      "Epoch: 3382 Training Loss: 0.21726500388194508 Test Loss: 0.21373827285013142\n",
      "Epoch: 3383 Training Loss: 0.2172665852250892 Test Loss: 0.21373972467927022\n",
      "Epoch: 3384 Training Loss: 0.21726570681867186 Test Loss: 0.21378894427962727\n",
      "Epoch: 3385 Training Loss: 0.21726513081078924 Test Loss: 0.21381337908254353\n",
      "Epoch: 3386 Training Loss: 0.21726356227080887 Test Loss: 0.21379937930156234\n",
      "Epoch: 3387 Training Loss: 0.2172650172589201 Test Loss: 0.2138249807528937\n",
      "Epoch: 3388 Training Loss: 0.21726433832364248 Test Loss: 0.2139928355343063\n",
      "Epoch: 3389 Training Loss: 0.21726100596853684 Test Loss: 0.2143674593031552\n",
      "Epoch: 3390 Training Loss: 0.21726450900559535 Test Loss: 0.21410484674491612\n",
      "Epoch: 3391 Training Loss: 0.21726276149926202 Test Loss: 0.2138566876642641\n",
      "Epoch: 3392 Training Loss: 0.21726213085378837 Test Loss: 0.2137002530745593\n",
      "Epoch: 3393 Training Loss: 0.21726517257349012 Test Loss: 0.21375945400024557\n",
      "Epoch: 3394 Training Loss: 0.21726241969603208 Test Loss: 0.21368167743923888\n",
      "Epoch: 3395 Training Loss: 0.21726514882308337 Test Loss: 0.21385992835430603\n",
      "Epoch: 3396 Training Loss: 0.21726427637892365 Test Loss: 0.21373468216556493\n",
      "Epoch: 3397 Training Loss: 0.21726348267242795 Test Loss: 0.21377761482724064\n",
      "Epoch: 3398 Training Loss: 0.2172622509058632 Test Loss: 0.21380605512304873\n",
      "Epoch: 3399 Training Loss: 0.21726424331304972 Test Loss: 0.21387431701809229\n",
      "Epoch: 3400 Training Loss: 0.21726413081914508 Test Loss: 0.2138281955174153\n",
      "Epoch: 3401 Training Loss: 0.2172631220051528 Test Loss: 0.21369098470103934\n",
      "Epoch: 3402 Training Loss: 0.21726520077989994 Test Loss: 0.21381752716579722\n",
      "Epoch: 3403 Training Loss: 0.21726637967513643 Test Loss: 0.21368574774593155\n",
      "Epoch: 3404 Training Loss: 0.21726471222443988 Test Loss: 0.21375075598817297\n",
      "Epoch: 3405 Training Loss: 0.21726532801358134 Test Loss: 0.21372518046236194\n",
      "Epoch: 3406 Training Loss: 0.2172634269679066 Test Loss: 0.21422074678357625\n",
      "Epoch: 3407 Training Loss: 0.21726638226625292 Test Loss: 0.2137558244273986\n",
      "Epoch: 3408 Training Loss: 0.21726405585608327 Test Loss: 0.21376520946576005\n",
      "Epoch: 3409 Training Loss: 0.21726406249974178 Test Loss: 0.2138229326367872\n",
      "Epoch: 3410 Training Loss: 0.21726521309891045 Test Loss: 0.21373172665624668\n",
      "Epoch: 3411 Training Loss: 0.2172688531065102 Test Loss: 0.21372991186982318\n",
      "Epoch: 3412 Training Loss: 0.21726548602685744 Test Loss: 0.2137520393014296\n",
      "Epoch: 3413 Training Loss: 0.21726394994307655 Test Loss: 0.21364579651909446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3414 Training Loss: 0.21726491093348654 Test Loss: 0.21365413157388233\n",
      "Epoch: 3415 Training Loss: 0.2172674591127429 Test Loss: 0.21380508291603617\n",
      "Epoch: 3416 Training Loss: 0.21726493910403316 Test Loss: 0.21369773829908675\n",
      "Epoch: 3417 Training Loss: 0.2172672667873465 Test Loss: 0.21375306335948285\n",
      "Epoch: 3418 Training Loss: 0.2172654631909624 Test Loss: 0.21372573786104918\n",
      "Epoch: 3419 Training Loss: 0.21726590511529167 Test Loss: 0.21371513432323191\n",
      "Epoch: 3420 Training Loss: 0.2172609092365103 Test Loss: 0.21637685564504353\n",
      "Epoch: 3421 Training Loss: 0.21726882738362724 Test Loss: 0.21378847762026124\n",
      "Epoch: 3422 Training Loss: 0.2172660017486944 Test Loss: 0.21373145443828315\n",
      "Epoch: 3423 Training Loss: 0.21726539655712968 Test Loss: 0.2137329710812228\n",
      "Epoch: 3424 Training Loss: 0.2172668729914368 Test Loss: 0.21374678938356165\n",
      "Epoch: 3425 Training Loss: 0.21726467549155334 Test Loss: 0.21375421704513778\n",
      "Epoch: 3426 Training Loss: 0.21726496377791737 Test Loss: 0.21378515915365828\n",
      "Epoch: 3427 Training Loss: 0.21726473349131975 Test Loss: 0.2137528041042795\n",
      "Epoch: 3428 Training Loss: 0.2172649031601371 Test Loss: 0.21379011092804237\n",
      "Epoch: 3429 Training Loss: 0.21726542596495682 Test Loss: 0.21372969150290033\n",
      "Epoch: 3430 Training Loss: 0.2172664017937674 Test Loss: 0.21382950475619225\n",
      "Epoch: 3431 Training Loss: 0.21726341502545973 Test Loss: 0.21369416057728044\n",
      "Epoch: 3432 Training Loss: 0.2172654116734701 Test Loss: 0.21379313125116145\n",
      "Epoch: 3433 Training Loss: 0.2172647312140063 Test Loss: 0.21367624604272858\n",
      "Epoch: 3434 Training Loss: 0.21726480563912007 Test Loss: 0.21370932700667675\n",
      "Epoch: 3435 Training Loss: 0.2172631174415601 Test Loss: 0.2137590780802007\n",
      "Epoch: 3436 Training Loss: 0.21726265434897477 Test Loss: 0.21372772116335484\n",
      "Epoch: 3437 Training Loss: 0.21726530268519365 Test Loss: 0.21375979103200993\n",
      "Epoch: 3438 Training Loss: 0.21726351761215432 Test Loss: 0.213766440927976\n",
      "Epoch: 3439 Training Loss: 0.21726270284499216 Test Loss: 0.21377305193566157\n",
      "Epoch: 3440 Training Loss: 0.21726472797735216 Test Loss: 0.213735382154614\n",
      "Epoch: 3441 Training Loss: 0.21726435320687207 Test Loss: 0.21413336481728523\n",
      "Epoch: 3442 Training Loss: 0.21726489622957296 Test Loss: 0.21378465360601173\n",
      "Epoch: 3443 Training Loss: 0.21726478877444846 Test Loss: 0.21381391055571042\n",
      "Epoch: 3444 Training Loss: 0.21726538244495897 Test Loss: 0.21375977806924976\n",
      "Epoch: 3445 Training Loss: 0.21726565450322327 Test Loss: 0.21380742917562653\n",
      "Epoch: 3446 Training Loss: 0.21726560502993358 Test Loss: 0.21630849004791866\n",
      "Epoch: 3447 Training Loss: 0.21726627694498904 Test Loss: 0.2138320195316648\n",
      "Epoch: 3448 Training Loss: 0.2172651198904437 Test Loss: 0.21378077774072157\n",
      "Epoch: 3449 Training Loss: 0.21726407314214752 Test Loss: 0.21374655605387863\n",
      "Epoch: 3450 Training Loss: 0.217264341407878 Test Loss: 0.21373805248320857\n",
      "Epoch: 3451 Training Loss: 0.21726420410560213 Test Loss: 0.2138003515085749\n",
      "Epoch: 3452 Training Loss: 0.21726309552017675 Test Loss: 0.2137590780802007\n",
      "Epoch: 3453 Training Loss: 0.2172648911459638 Test Loss: 0.2137047641150977\n",
      "Epoch: 3454 Training Loss: 0.2172656220649554 Test Loss: 0.21374083947664463\n",
      "Epoch: 3455 Training Loss: 0.21726439210948245 Test Loss: 0.21489614547659816\n",
      "Epoch: 3456 Training Loss: 0.21726491651021473 Test Loss: 0.21374860416998515\n",
      "Epoch: 3457 Training Loss: 0.21726389964493317 Test Loss: 0.21377336304190558\n",
      "Epoch: 3458 Training Loss: 0.21726496712216115 Test Loss: 0.21381611422493893\n",
      "Epoch: 3459 Training Loss: 0.21726463601513174 Test Loss: 0.2138618338800507\n",
      "Epoch: 3460 Training Loss: 0.2172647140803607 Test Loss: 0.21379241829935222\n",
      "Epoch: 3461 Training Loss: 0.21726464748239116 Test Loss: 0.213789696119717\n",
      "Epoch: 3462 Training Loss: 0.21726418260561142 Test Loss: 0.21371888056092042\n",
      "Epoch: 3463 Training Loss: 0.21726450776831482 Test Loss: 0.21379007203976186\n",
      "Epoch: 3464 Training Loss: 0.2172651321108304 Test Loss: 0.21368852177660747\n",
      "Epoch: 3465 Training Loss: 0.21726442484362174 Test Loss: 0.2136331059768902\n",
      "Epoch: 3466 Training Loss: 0.2172667363974592 Test Loss: 0.21374481904401615\n",
      "Epoch: 3467 Training Loss: 0.2172653471914296 Test Loss: 0.2137652742795609\n",
      "Epoch: 3468 Training Loss: 0.21726479541810698 Test Loss: 0.21388318354604705\n",
      "Epoch: 3469 Training Loss: 0.2172649965120567 Test Loss: 0.21371762317318413\n",
      "Epoch: 3470 Training Loss: 0.21726643748662103 Test Loss: 0.21365006126718966\n",
      "Epoch: 3471 Training Loss: 0.2172659483035549 Test Loss: 0.21365859076338006\n",
      "Epoch: 3472 Training Loss: 0.21726435950983014 Test Loss: 0.21372183607023867\n",
      "Epoch: 3473 Training Loss: 0.21726421278449745 Test Loss: 0.2137250767602806\n",
      "Epoch: 3474 Training Loss: 0.2172640841252537 Test Loss: 0.21370843257622518\n",
      "Epoch: 3475 Training Loss: 0.21726605927327336 Test Loss: 0.21405462901202613\n",
      "Epoch: 3476 Training Loss: 0.21726345927168747 Test Loss: 0.21438574975775193\n",
      "Epoch: 3477 Training Loss: 0.21726435981466738 Test Loss: 0.21410527451600164\n",
      "Epoch: 3478 Training Loss: 0.21726284751715644 Test Loss: 0.21391031460307822\n",
      "Epoch: 3479 Training Loss: 0.21726334267144598 Test Loss: 0.21380566624024372\n",
      "Epoch: 3480 Training Loss: 0.21726469868608042 Test Loss: 0.21383205841994532\n",
      "Epoch: 3481 Training Loss: 0.21726489034800753 Test Loss: 0.21371181585662896\n",
      "Epoch: 3482 Training Loss: 0.21726492151313168 Test Loss: 0.21381400129503159\n",
      "Epoch: 3483 Training Loss: 0.21726568777531063 Test Loss: 0.21369553462985824\n",
      "Epoch: 3484 Training Loss: 0.21726349677563286 Test Loss: 0.21372184903299885\n",
      "Epoch: 3485 Training Loss: 0.2172652900882433 Test Loss: 0.21383392505740947\n",
      "Epoch: 3486 Training Loss: 0.21726485387512923 Test Loss: 0.21371041587853085\n",
      "Epoch: 3487 Training Loss: 0.21726552841716437 Test Loss: 0.21373849321705427\n",
      "Epoch: 3488 Training Loss: 0.21726529518978405 Test Loss: 0.2137464523517973\n",
      "Epoch: 3489 Training Loss: 0.21726450827039967 Test Loss: 0.2136671461850908\n",
      "Epoch: 3490 Training Loss: 0.2172660287447211 Test Loss: 0.2137167935565334\n",
      "Epoch: 3491 Training Loss: 0.21726586830171293 Test Loss: 0.21368994768022592\n",
      "Epoch: 3492 Training Loss: 0.21726482040579426 Test Loss: 0.21371084364961637\n",
      "Epoch: 3493 Training Loss: 0.21726467157349832 Test Loss: 0.21387545774098704\n",
      "Epoch: 3494 Training Loss: 0.21726393345496858 Test Loss: 0.21380364404965754\n",
      "Epoch: 3495 Training Loss: 0.21726514365878202 Test Loss: 0.21369268282262133\n",
      "Epoch: 3496 Training Loss: 0.2172636766564975 Test Loss: 0.2137036752432436\n",
      "Epoch: 3497 Training Loss: 0.21726592637320574 Test Loss: 0.21374825417546062\n",
      "Epoch: 3498 Training Loss: 0.21726434563973607 Test Loss: 0.2136669387809281\n",
      "Epoch: 3499 Training Loss: 0.2172641777371815 Test Loss: 0.21378299437271026\n",
      "Epoch: 3500 Training Loss: 0.21726465973864106 Test Loss: 0.21374214871542158\n",
      "Epoch: 3501 Training Loss: 0.21726565462874448 Test Loss: 0.21374782640437506\n",
      "Epoch: 3502 Training Loss: 0.21726559270195725 Test Loss: 0.2137610484197462\n",
      "Epoch: 3503 Training Loss: 0.21726482331967956 Test Loss: 0.21370681223120422\n",
      "Epoch: 3504 Training Loss: 0.2172638830850988 Test Loss: 0.2136825459441701\n",
      "Epoch: 3505 Training Loss: 0.21726517985372049 Test Loss: 0.2137400098599939\n",
      "Epoch: 3506 Training Loss: 0.21726600217905284 Test Loss: 0.2137134491644101\n",
      "Epoch: 3507 Training Loss: 0.217266052217188 Test Loss: 0.21375190967382793\n",
      "Epoch: 3508 Training Loss: 0.21726432416664276 Test Loss: 0.21374190242297839\n",
      "Epoch: 3509 Training Loss: 0.21726463614961875 Test Loss: 0.2137540096409751\n",
      "Epoch: 3510 Training Loss: 0.21726588134695332 Test Loss: 0.21373431920828023\n",
      "Epoch: 3511 Training Loss: 0.2172646039444617 Test Loss: 0.21373890802537962\n",
      "Epoch: 3512 Training Loss: 0.21726444357317995 Test Loss: 0.21377677224782973\n",
      "Epoch: 3513 Training Loss: 0.21726368810582533 Test Loss: 0.2139065683653897\n",
      "Epoch: 3514 Training Loss: 0.21726307931897443 Test Loss: 0.21374098206700648\n",
      "Epoch: 3515 Training Loss: 0.21726594030606045 Test Loss: 0.21371656022685037\n",
      "Epoch: 3516 Training Loss: 0.21726478742061253 Test Loss: 0.21369753089492408\n",
      "Epoch: 3517 Training Loss: 0.21726535893662885 Test Loss: 0.21370024011179914\n",
      "Epoch: 3518 Training Loss: 0.21726414272572878 Test Loss: 0.21406201778532175\n",
      "Epoch: 3519 Training Loss: 0.2172682304405994 Test Loss: 0.213743107959674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3520 Training Loss: 0.21726404364466237 Test Loss: 0.21373146740104332\n",
      "Epoch: 3521 Training Loss: 0.2172634026078254 Test Loss: 0.21371929536924578\n",
      "Epoch: 3522 Training Loss: 0.21726479827819747 Test Loss: 0.2137025085948285\n",
      "Epoch: 3523 Training Loss: 0.21726484380653474 Test Loss: 0.213719930544494\n",
      "Epoch: 3524 Training Loss: 0.2172644366426158 Test Loss: 0.2137103121764495\n",
      "Epoch: 3525 Training Loss: 0.2172636092067769 Test Loss: 0.2138667856544348\n",
      "Epoch: 3526 Training Loss: 0.2172644664449382 Test Loss: 0.21379196460274635\n",
      "Epoch: 3527 Training Loss: 0.21726456594739724 Test Loss: 0.21374264130030796\n",
      "Epoch: 3528 Training Loss: 0.2172633605940821 Test Loss: 0.21375342631676755\n",
      "Epoch: 3529 Training Loss: 0.2172633988152916 Test Loss: 0.2137062159442365\n",
      "Epoch: 3530 Training Loss: 0.21726489913449248 Test Loss: 0.21376960384145693\n",
      "Epoch: 3531 Training Loss: 0.21726532228443454 Test Loss: 0.21376775016675295\n",
      "Epoch: 3532 Training Loss: 0.21726403121806223 Test Loss: 0.21369680498035468\n",
      "Epoch: 3533 Training Loss: 0.21726425794523688 Test Loss: 0.21380517365535734\n",
      "Epoch: 3534 Training Loss: 0.2172654389653682 Test Loss: 0.21373893395089996\n",
      "Epoch: 3535 Training Loss: 0.2172649368625829 Test Loss: 0.21373535622909365\n",
      "Epoch: 3536 Training Loss: 0.21726543779981408 Test Loss: 0.21375314113604385\n",
      "Epoch: 3537 Training Loss: 0.21726441674750346 Test Loss: 0.21378378510108048\n",
      "Epoch: 3538 Training Loss: 0.2172643553048695 Test Loss: 0.21372041016662022\n",
      "Epoch: 3539 Training Loss: 0.21726509492068802 Test Loss: 0.2137211749694701\n",
      "Epoch: 3540 Training Loss: 0.217264743075761 Test Loss: 0.21378300733547043\n",
      "Epoch: 3541 Training Loss: 0.21726594897598997 Test Loss: 0.21382619925234947\n",
      "Epoch: 3542 Training Loss: 0.21726521103677623 Test Loss: 0.21379919782291998\n",
      "Epoch: 3543 Training Loss: 0.21726394527189424 Test Loss: 0.21376864459720452\n",
      "Epoch: 3544 Training Loss: 0.21726543767429288 Test Loss: 0.21376117804734787\n",
      "Epoch: 3545 Training Loss: 0.21726561234602715 Test Loss: 0.2137896572314365\n",
      "Epoch: 3546 Training Loss: 0.21726632288575315 Test Loss: 0.2137721963934905\n",
      "Epoch: 3547 Training Loss: 0.21726524303571987 Test Loss: 0.21373032667814856\n",
      "Epoch: 3548 Training Loss: 0.21726490624437264 Test Loss: 0.2137247138029959\n",
      "Epoch: 3549 Training Loss: 0.2172673573867652 Test Loss: 0.21373211553905172\n",
      "Epoch: 3550 Training Loss: 0.21726548961317782 Test Loss: 0.21384471007386907\n",
      "Epoch: 3551 Training Loss: 0.21726417480536458 Test Loss: 0.21378344806931612\n",
      "Epoch: 3552 Training Loss: 0.21726404879999792 Test Loss: 0.2137474893726107\n",
      "Epoch: 3553 Training Loss: 0.2172647134975836 Test Loss: 0.21391201272466018\n",
      "Epoch: 3554 Training Loss: 0.21726563633851054 Test Loss: 0.2137903442577254\n",
      "Epoch: 3555 Training Loss: 0.21726355802101924 Test Loss: 0.2138210789620832\n",
      "Epoch: 3556 Training Loss: 0.21726515982412115 Test Loss: 0.2138687559939803\n",
      "Epoch: 3557 Training Loss: 0.21726313478141915 Test Loss: 0.21376460021603216\n",
      "Epoch: 3558 Training Loss: 0.21726499307815492 Test Loss: 0.21377841851837104\n",
      "Epoch: 3559 Training Loss: 0.21726461846905926 Test Loss: 0.2137795981295463\n",
      "Epoch: 3560 Training Loss: 0.2172649200337745 Test Loss: 0.21378001293787166\n",
      "Epoch: 3561 Training Loss: 0.2172628350636589 Test Loss: 0.21379449234097908\n",
      "Epoch: 3562 Training Loss: 0.217263761679188 Test Loss: 0.21377688891267124\n",
      "Epoch: 3563 Training Loss: 0.2172640103098144 Test Loss: 0.21373768952592387\n",
      "Epoch: 3564 Training Loss: 0.21726352760902237 Test Loss: 0.2138127050190148\n",
      "Epoch: 3565 Training Loss: 0.21726478976965238 Test Loss: 0.213665461026269\n",
      "Epoch: 3566 Training Loss: 0.21726808395734332 Test Loss: 0.2137193342575263\n",
      "Epoch: 3567 Training Loss: 0.2172644469263895 Test Loss: 0.21372362493114183\n",
      "Epoch: 3568 Training Loss: 0.21726463211500832 Test Loss: 0.21367220166155623\n",
      "Epoch: 3569 Training Loss: 0.21726563761165427 Test Loss: 0.21374051540764044\n",
      "Epoch: 3570 Training Loss: 0.21726412738524334 Test Loss: 0.21437825728237497\n",
      "Epoch: 3571 Training Loss: 0.21726570787663638 Test Loss: 0.21380991802557875\n",
      "Epoch: 3572 Training Loss: 0.21726529705467063 Test Loss: 0.21378552211094298\n",
      "Epoch: 3573 Training Loss: 0.2172636016306751 Test Loss: 0.21386600788882473\n",
      "Epoch: 3574 Training Loss: 0.21726247631506512 Test Loss: 0.21408353596720028\n",
      "Epoch: 3575 Training Loss: 0.2172641317336568 Test Loss: 0.21393620123513327\n",
      "Epoch: 3576 Training Loss: 0.21726415050804398 Test Loss: 0.21375535776803256\n",
      "Epoch: 3577 Training Loss: 0.2172627809191869 Test Loss: 0.21370691593328556\n",
      "Epoch: 3578 Training Loss: 0.21726451170430144 Test Loss: 0.21432892101717638\n",
      "Epoch: 3579 Training Loss: 0.2172663156055228 Test Loss: 0.21379376642640968\n",
      "Epoch: 3580 Training Loss: 0.2172638410175607 Test Loss: 0.2137072659278101\n",
      "Epoch: 3581 Training Loss: 0.21726620106741554 Test Loss: 0.21376920199589175\n",
      "Epoch: 3582 Training Loss: 0.2172659157308 Test Loss: 0.2137093529321971\n",
      "Epoch: 3583 Training Loss: 0.2172652668399214 Test Loss: 0.21380950321725337\n",
      "Epoch: 3584 Training Loss: 0.21726417968276032 Test Loss: 0.21373526548977248\n",
      "Epoch: 3585 Training Loss: 0.21726501125183345 Test Loss: 0.21381304205077917\n",
      "Epoch: 3586 Training Loss: 0.21726540395391547 Test Loss: 0.21380109038590447\n",
      "Epoch: 3587 Training Loss: 0.2172638671349389 Test Loss: 0.21373600436710205\n",
      "Epoch: 3588 Training Loss: 0.21726447601144783 Test Loss: 0.21372336567593847\n",
      "Epoch: 3589 Training Loss: 0.21726530909574132 Test Loss: 0.21371910092784327\n",
      "Epoch: 3590 Training Loss: 0.21726381009451318 Test Loss: 0.21382241412638048\n",
      "Epoch: 3591 Training Loss: 0.21726492773539754 Test Loss: 0.21375295965740151\n",
      "Epoch: 3592 Training Loss: 0.2172655024611706 Test Loss: 0.2137557855391181\n",
      "Epoch: 3593 Training Loss: 0.21726544057921238 Test Loss: 0.21372917299249364\n",
      "Epoch: 3594 Training Loss: 0.21726461853181986 Test Loss: 0.21380302183716948\n",
      "Epoch: 3595 Training Loss: 0.21726385159720582 Test Loss: 0.21373381366063368\n",
      "Epoch: 3596 Training Loss: 0.21726387289098312 Test Loss: 0.2137943627133774\n",
      "Epoch: 3597 Training Loss: 0.2172643785083624 Test Loss: 0.21378370732451948\n",
      "Epoch: 3598 Training Loss: 0.21726220546718394 Test Loss: 0.21376775016675295\n",
      "Epoch: 3599 Training Loss: 0.21726583389993465 Test Loss: 0.2138794502711187\n",
      "Epoch: 3600 Training Loss: 0.2172651517907635 Test Loss: 0.2137982256159074\n",
      "Epoch: 3601 Training Loss: 0.21726444849540466 Test Loss: 0.2136721238849952\n",
      "Epoch: 3602 Training Loss: 0.21726527783199337 Test Loss: 0.2137323229432144\n",
      "Epoch: 3603 Training Loss: 0.21726439684342536 Test Loss: 0.21374271907686895\n",
      "Epoch: 3604 Training Loss: 0.21726521181680092 Test Loss: 0.21375599294328076\n",
      "Epoch: 3605 Training Loss: 0.21726484498105467 Test Loss: 0.21379765525446\n",
      "Epoch: 3606 Training Loss: 0.21726586377398346 Test Loss: 0.2138469007803374\n",
      "Epoch: 3607 Training Loss: 0.21726376310475037 Test Loss: 0.21373657472854943\n",
      "Epoch: 3608 Training Loss: 0.21726500554958406 Test Loss: 0.21370333821147924\n",
      "Epoch: 3609 Training Loss: 0.21726342265535634 Test Loss: 0.21370843257622518\n",
      "Epoch: 3610 Training Loss: 0.21726561552888649 Test Loss: 0.21382726219868323\n",
      "Epoch: 3611 Training Loss: 0.2172644094314099 Test Loss: 0.21370989736812412\n",
      "Epoch: 3612 Training Loss: 0.21726630985844436 Test Loss: 0.21377487968484524\n",
      "Epoch: 3613 Training Loss: 0.21726512972592735 Test Loss: 0.2137294581732173\n",
      "Epoch: 3614 Training Loss: 0.21726470768774458 Test Loss: 0.21377398525439364\n",
      "Epoch: 3615 Training Loss: 0.21726490938240298 Test Loss: 0.21374173390709622\n",
      "Epoch: 3616 Training Loss: 0.21726477587266088 Test Loss: 0.21376470391811353\n",
      "Epoch: 3617 Training Loss: 0.2172647140624291 Test Loss: 0.2137167157799724\n",
      "Epoch: 3618 Training Loss: 0.2172663166814189 Test Loss: 0.21371307324436525\n",
      "Epoch: 3619 Training Loss: 0.21726445197413544 Test Loss: 0.21379135535301846\n",
      "Epoch: 3620 Training Loss: 0.21726337239307617 Test Loss: 0.21371980091689233\n",
      "Epoch: 3621 Training Loss: 0.21726433311451213 Test Loss: 0.21367940895620952\n",
      "Epoch: 3622 Training Loss: 0.2172648769889641 Test Loss: 0.21374971896735956\n",
      "Epoch: 3623 Training Loss: 0.21726465045007126 Test Loss: 0.21376850200684266\n",
      "Epoch: 3624 Training Loss: 0.2172653387546109 Test Loss: 0.21376307061033237\n",
      "Epoch: 3625 Training Loss: 0.21726235399464264 Test Loss: 0.21420211929721514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3626 Training Loss: 0.21726631839388688 Test Loss: 0.2137585984580745\n",
      "Epoch: 3627 Training Loss: 0.21726713646046378 Test Loss: 0.2138165290332643\n",
      "Epoch: 3628 Training Loss: 0.21726377640103317 Test Loss: 0.21377704446579324\n",
      "Epoch: 3629 Training Loss: 0.21726435549315132 Test Loss: 0.21373863580741612\n",
      "Epoch: 3630 Training Loss: 0.21726493615428463 Test Loss: 0.21375592812947994\n",
      "Epoch: 3631 Training Loss: 0.2172641123316635 Test Loss: 0.217672379821732\n",
      "Epoch: 3632 Training Loss: 0.21726544387862715 Test Loss: 0.21384366009029548\n",
      "Epoch: 3633 Training Loss: 0.21726441757235718 Test Loss: 0.21376775016675295\n",
      "Epoch: 3634 Training Loss: 0.2172617468485333 Test Loss: 0.2145023627482213\n",
      "Epoch: 3635 Training Loss: 0.21726623017040544 Test Loss: 0.21386528197425533\n",
      "Epoch: 3636 Training Loss: 0.21726494823121853 Test Loss: 0.2138226474560635\n",
      "Epoch: 3637 Training Loss: 0.21726446409589834 Test Loss: 0.21381289946041734\n",
      "Epoch: 3638 Training Loss: 0.217266487892354 Test Loss: 0.2138892371550454\n",
      "Epoch: 3639 Training Loss: 0.21726399564176405 Test Loss: 0.21416652355779442\n",
      "Epoch: 3640 Training Loss: 0.21726505419802006 Test Loss: 0.21387912620211452\n",
      "Epoch: 3641 Training Loss: 0.21726495294722983 Test Loss: 0.21378847762026124\n",
      "Epoch: 3642 Training Loss: 0.21726409128892865 Test Loss: 0.21368117189159233\n",
      "Epoch: 3643 Training Loss: 0.21726510538377775 Test Loss: 0.2136671461850908\n",
      "Epoch: 3644 Training Loss: 0.217265741722535 Test Loss: 0.21373332107574733\n",
      "Epoch: 3645 Training Loss: 0.2172644235884096 Test Loss: 0.21376111323354705\n",
      "Epoch: 3646 Training Loss: 0.21726216200098092 Test Loss: 0.214330282106994\n",
      "Epoch: 3647 Training Loss: 0.21726510255955042 Test Loss: 0.21479978031751087\n",
      "Epoch: 3648 Training Loss: 0.21726442938031704 Test Loss: 0.2138083624943586\n",
      "Epoch: 3649 Training Loss: 0.21726405826788373 Test Loss: 0.21374411905496707\n",
      "Epoch: 3650 Training Loss: 0.21726478795856058 Test Loss: 0.21382578444402411\n",
      "Epoch: 3651 Training Loss: 0.21726433275588009 Test Loss: 0.2137701742029043\n",
      "Epoch: 3652 Training Loss: 0.21726431642915656 Test Loss: 0.21380427922490575\n",
      "Epoch: 3653 Training Loss: 0.21726503463464236 Test Loss: 0.21381173281200222\n",
      "Epoch: 3654 Training Loss: 0.21726393609091407 Test Loss: 0.2137493041590342\n",
      "Epoch: 3655 Training Loss: 0.217265545461152 Test Loss: 0.2140033353700422\n",
      "Epoch: 3656 Training Loss: 0.21726425844732175 Test Loss: 0.21375257077459647\n",
      "Epoch: 3657 Training Loss: 0.21726325739771307 Test Loss: 0.21369097173827917\n",
      "Epoch: 3658 Training Loss: 0.21726632550376704 Test Loss: 0.21367899414788413\n",
      "Epoch: 3659 Training Loss: 0.21726470255034064 Test Loss: 0.2136372929484244\n",
      "Epoch: 3660 Training Loss: 0.21726748214588557 Test Loss: 0.21370012344695763\n",
      "Epoch: 3661 Training Loss: 0.21726655821113092 Test Loss: 0.21366901282255496\n",
      "Epoch: 3662 Training Loss: 0.21726481118895086 Test Loss: 0.2136296060316449\n",
      "Epoch: 3663 Training Loss: 0.21726453525746056 Test Loss: 0.2136388225541242\n",
      "Epoch: 3664 Training Loss: 0.21726547361818893 Test Loss: 0.213679058961685\n",
      "Epoch: 3665 Training Loss: 0.21726566175655623 Test Loss: 0.21372923780629446\n",
      "Epoch: 3666 Training Loss: 0.21726568324758114 Test Loss: 0.21372610081833388\n",
      "Epoch: 3667 Training Loss: 0.21726455860440624 Test Loss: 0.21369519759809388\n",
      "Epoch: 3668 Training Loss: 0.21726500384608188 Test Loss: 0.21379161460822182\n",
      "Epoch: 3669 Training Loss: 0.2172620768886324 Test Loss: 0.21370047344148216\n",
      "Epoch: 3670 Training Loss: 0.21726515251699338 Test Loss: 0.21380605512304873\n",
      "Epoch: 3671 Training Loss: 0.21726433881676155 Test Loss: 0.21375905215468036\n",
      "Epoch: 3672 Training Loss: 0.21726557085230033 Test Loss: 0.2137612817494292\n",
      "Epoch: 3673 Training Loss: 0.21726569546796784 Test Loss: 0.21375540961907322\n",
      "Epoch: 3674 Training Loss: 0.2172656016498266 Test Loss: 0.2137724815742142\n",
      "Epoch: 3675 Training Loss: 0.21726483363931648 Test Loss: 0.21372550453136616\n",
      "Epoch: 3676 Training Loss: 0.21726411097782758 Test Loss: 0.21378001293787166\n",
      "Epoch: 3677 Training Loss: 0.21726406713506088 Test Loss: 0.21385228032580705\n",
      "Epoch: 3678 Training Loss: 0.21726537479713076 Test Loss: 0.21390192769724967\n",
      "Epoch: 3679 Training Loss: 0.21726406103831622 Test Loss: 0.21378886650306625\n",
      "Epoch: 3680 Training Loss: 0.21726372826364784 Test Loss: 0.21377725186995594\n",
      "Epoch: 3681 Training Loss: 0.2172638129097747 Test Loss: 0.2138040588579829\n",
      "Epoch: 3682 Training Loss: 0.21726245051148996 Test Loss: 0.2136720979594749\n",
      "Epoch: 3683 Training Loss: 0.21726525019939483 Test Loss: 0.21380959395657453\n",
      "Epoch: 3684 Training Loss: 0.21726486028567693 Test Loss: 0.21374980970668073\n",
      "Epoch: 3685 Training Loss: 0.21726445596391686 Test Loss: 0.21485584425523654\n",
      "Epoch: 3686 Training Loss: 0.21726499805417446 Test Loss: 0.2137451560757805\n",
      "Epoch: 3687 Training Loss: 0.21726441837031346 Test Loss: 0.21376255209992567\n",
      "Epoch: 3688 Training Loss: 0.21726424012122458 Test Loss: 0.21370271599899118\n",
      "Epoch: 3689 Training Loss: 0.2172661746452001 Test Loss: 0.21369161987628757\n",
      "Epoch: 3690 Training Loss: 0.21726305044012953 Test Loss: 0.21372976927946136\n",
      "Epoch: 3691 Training Loss: 0.21726594406273103 Test Loss: 0.21379329976704364\n",
      "Epoch: 3692 Training Loss: 0.21726514380223486 Test Loss: 0.2137239101118655\n",
      "Epoch: 3693 Training Loss: 0.2172653685479675 Test Loss: 0.21363867996376235\n",
      "Epoch: 3694 Training Loss: 0.2172665075991845 Test Loss: 0.21373836358945258\n",
      "Epoch: 3695 Training Loss: 0.2172644095389995 Test Loss: 0.21370932700667675\n",
      "Epoch: 3696 Training Loss: 0.21726514280703094 Test Loss: 0.21368350518842252\n",
      "Epoch: 3697 Training Loss: 0.2172644364812314 Test Loss: 0.21372138237363278\n",
      "Epoch: 3698 Training Loss: 0.21726599093593846 Test Loss: 0.21377145751616095\n",
      "Epoch: 3699 Training Loss: 0.21726397729773528 Test Loss: 0.213944756656844\n",
      "Epoch: 3700 Training Loss: 0.2172638496157638 Test Loss: 0.2137264897011389\n",
      "Epoch: 3701 Training Loss: 0.2172632810315644 Test Loss: 0.21371028625092917\n",
      "Epoch: 3702 Training Loss: 0.217265341498146 Test Loss: 0.21374309499691382\n",
      "Epoch: 3703 Training Loss: 0.21726402456543792 Test Loss: 0.2137161583812852\n",
      "Epoch: 3704 Training Loss: 0.21726544232754358 Test Loss: 0.2137526744766778\n",
      "Epoch: 3705 Training Loss: 0.2172627013028744 Test Loss: 0.21385518398408462\n",
      "Epoch: 3706 Training Loss: 0.21726424981325543 Test Loss: 0.21377885925221674\n",
      "Epoch: 3707 Training Loss: 0.21726186950069043 Test Loss: 0.21374212278990123\n",
      "Epoch: 3708 Training Loss: 0.21726639454940022 Test Loss: 0.21369633832098864\n",
      "Epoch: 3709 Training Loss: 0.2172645196569669 Test Loss: 0.2137747759827639\n",
      "Epoch: 3710 Training Loss: 0.2172640110898391 Test Loss: 0.2137260489672932\n",
      "Epoch: 3711 Training Loss: 0.21726489596956475 Test Loss: 0.21383758055577679\n",
      "Epoch: 3712 Training Loss: 0.2172636606704744 Test Loss: 0.2137684112675215\n",
      "Epoch: 3713 Training Loss: 0.21726614808849767 Test Loss: 0.21386082278475763\n",
      "Epoch: 3714 Training Loss: 0.21726444734778214 Test Loss: 0.21377398525439364\n",
      "Epoch: 3715 Training Loss: 0.21726323288521324 Test Loss: 0.21370362339220292\n",
      "Epoch: 3716 Training Loss: 0.21726382157970422 Test Loss: 0.21369926790478655\n",
      "Epoch: 3717 Training Loss: 0.21726496744492999 Test Loss: 0.21376160581843343\n",
      "Epoch: 3718 Training Loss: 0.21726415852347006 Test Loss: 0.21373470809108527\n",
      "Epoch: 3719 Training Loss: 0.2172651415069898 Test Loss: 0.21368421814023175\n",
      "Epoch: 3720 Training Loss: 0.2172640112153603 Test Loss: 0.21369640313478946\n",
      "Epoch: 3721 Training Loss: 0.2172663073121569 Test Loss: 0.21373487660696744\n",
      "Epoch: 3722 Training Loss: 0.217263862625141 Test Loss: 0.21377595559393917\n",
      "Epoch: 3723 Training Loss: 0.2172667019598177 Test Loss: 0.21377685002439073\n",
      "Epoch: 3724 Training Loss: 0.21726436486241332 Test Loss: 0.21377648706710603\n",
      "Epoch: 3725 Training Loss: 0.2172638466122205 Test Loss: 0.21366564250491132\n",
      "Epoch: 3726 Training Loss: 0.21726737517491432 Test Loss: 0.21369999381935595\n",
      "Epoch: 3727 Training Loss: 0.21726358060587184 Test Loss: 0.21380226999707974\n",
      "Epoch: 3728 Training Loss: 0.21726394243870115 Test Loss: 0.21370730481609057\n",
      "Epoch: 3729 Training Loss: 0.21726525270085328 Test Loss: 0.21371888056092042\n",
      "Epoch: 3730 Training Loss: 0.21726343964554917 Test Loss: 0.21371963240101013\n",
      "Epoch: 3731 Training Loss: 0.21726525613475506 Test Loss: 0.21368626625633827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3732 Training Loss: 0.21726528991789307 Test Loss: 0.2137175972476638\n",
      "Epoch: 3733 Training Loss: 0.2172648863851235 Test Loss: 0.21368844400004647\n",
      "Epoch: 3734 Training Loss: 0.21726650099138922 Test Loss: 0.21364635391778167\n",
      "Epoch: 3735 Training Loss: 0.21726474967459047 Test Loss: 0.21360377125063051\n",
      "Epoch: 3736 Training Loss: 0.2172662073793394 Test Loss: 0.21369184024321042\n",
      "Epoch: 3737 Training Loss: 0.21726522889665176 Test Loss: 0.21369674016655385\n",
      "Epoch: 3738 Training Loss: 0.21726586148770421 Test Loss: 0.21366184441618216\n",
      "Epoch: 3739 Training Loss: 0.21726476107012352 Test Loss: 0.21371898426300176\n",
      "Epoch: 3740 Training Loss: 0.21726724190724886 Test Loss: 0.21371471951490656\n",
      "Epoch: 3741 Training Loss: 0.21726533795665462 Test Loss: 0.21388655386369065\n",
      "Epoch: 3742 Training Loss: 0.2172640253633942 Test Loss: 0.21516849306772326\n",
      "Epoch: 3743 Training Loss: 0.21726524213913975 Test Loss: 0.2138002737320139\n",
      "Epoch: 3744 Training Loss: 0.21726432419354016 Test Loss: 0.21374204501334024\n",
      "Epoch: 3745 Training Loss: 0.217264438435776 Test Loss: 0.21371500469563023\n",
      "Epoch: 3746 Training Loss: 0.2172638652790181 Test Loss: 0.21375831327735081\n",
      "Epoch: 3747 Training Loss: 0.21726482616183845 Test Loss: 0.2138098921000584\n",
      "Epoch: 3748 Training Loss: 0.21726407052413366 Test Loss: 0.21375941511196506\n",
      "Epoch: 3749 Training Loss: 0.21726401905147033 Test Loss: 0.21373080630027477\n",
      "Epoch: 3750 Training Loss: 0.21726350604627107 Test Loss: 0.2138036959006982\n",
      "Epoch: 3751 Training Loss: 0.21726342106840957 Test Loss: 0.21387856880342732\n",
      "Epoch: 3752 Training Loss: 0.2172643365932429 Test Loss: 0.21385765987127667\n",
      "Epoch: 3753 Training Loss: 0.21726471079887752 Test Loss: 0.21373657472854943\n",
      "Epoch: 3754 Training Loss: 0.2172649115252294 Test Loss: 0.21373539511737416\n",
      "Epoch: 3755 Training Loss: 0.21726500641030094 Test Loss: 0.21372569897276866\n",
      "Epoch: 3756 Training Loss: 0.2172637746168388 Test Loss: 0.21381365130050706\n",
      "Epoch: 3757 Training Loss: 0.21726553312420988 Test Loss: 0.21372703413706595\n",
      "Epoch: 3758 Training Loss: 0.21726113480709663 Test Loss: 0.21372205643716152\n",
      "Epoch: 3759 Training Loss: 0.217265242443977 Test Loss: 0.21398037832178507\n",
      "Epoch: 3760 Training Loss: 0.2172649011428319 Test Loss: 0.21428149027772248\n",
      "Epoch: 3761 Training Loss: 0.2172626819457101 Test Loss: 0.2140247368870792\n",
      "Epoch: 3762 Training Loss: 0.21726268133603566 Test Loss: 0.21391789781777637\n",
      "Epoch: 3763 Training Loss: 0.2172632659959162 Test Loss: 0.2136995271599899\n",
      "Epoch: 3764 Training Loss: 0.21726621666790918 Test Loss: 0.2138233215195922\n",
      "Epoch: 3765 Training Loss: 0.21726285137245085 Test Loss: 0.21377859999701337\n",
      "Epoch: 3766 Training Loss: 0.21726487618204202 Test Loss: 0.213671488709747\n",
      "Epoch: 3767 Training Loss: 0.21726443497497683 Test Loss: 0.21369395317311776\n",
      "Epoch: 3768 Training Loss: 0.21726427803759685 Test Loss: 0.2136781645312334\n",
      "Epoch: 3769 Training Loss: 0.21726560391817426 Test Loss: 0.21371617134404533\n",
      "Epoch: 3770 Training Loss: 0.21726377038498074 Test Loss: 0.2136874199419932\n",
      "Epoch: 3771 Training Loss: 0.21726328832972638 Test Loss: 0.2171941058225815\n",
      "Epoch: 3772 Training Loss: 0.21726475966249276 Test Loss: 0.2138424934418804\n",
      "Epoch: 3773 Training Loss: 0.21726370712228918 Test Loss: 0.2137528818808405\n",
      "Epoch: 3774 Training Loss: 0.21726392384362994 Test Loss: 0.21377230009557183\n",
      "Epoch: 3775 Training Loss: 0.21726579341037752 Test Loss: 0.21376320023793405\n",
      "Epoch: 3776 Training Loss: 0.21726381562641237 Test Loss: 0.21372835633860307\n",
      "Epoch: 3777 Training Loss: 0.21726466734164027 Test Loss: 0.2137590780802007\n",
      "Epoch: 3778 Training Loss: 0.21726419385769163 Test Loss: 0.21367043872617342\n",
      "Epoch: 3779 Training Loss: 0.2172634950990281 Test Loss: 0.2137406061469616\n",
      "Epoch: 3780 Training Loss: 0.21726471533557282 Test Loss: 0.21363108378630402\n",
      "Epoch: 3781 Training Loss: 0.21726547165467852 Test Loss: 0.21373337292678798\n",
      "Epoch: 3782 Training Loss: 0.2172657464475121 Test Loss: 0.21361857472274212\n",
      "Epoch: 3783 Training Loss: 0.2172655547586876 Test Loss: 0.21367375719277637\n",
      "Epoch: 3784 Training Loss: 0.21726404147493852 Test Loss: 0.21369931975582723\n",
      "Epoch: 3785 Training Loss: 0.217263890688098 Test Loss: 0.21381354759842572\n",
      "Epoch: 3786 Training Loss: 0.2172669422970782 Test Loss: 0.2137923016345107\n",
      "Epoch: 3787 Training Loss: 0.2172651144123393 Test Loss: 0.2137133713878491\n",
      "Epoch: 3788 Training Loss: 0.21726383520772166 Test Loss: 0.21855612895893065\n",
      "Epoch: 3789 Training Loss: 0.21726537724479442 Test Loss: 0.2139540120676038\n",
      "Epoch: 3790 Training Loss: 0.21726343451711103 Test Loss: 0.21371558801983778\n",
      "Epoch: 3791 Training Loss: 0.21726476320398413 Test Loss: 0.21378534063230062\n",
      "Epoch: 3792 Training Loss: 0.21726479019104503 Test Loss: 0.21375551332115456\n",
      "Epoch: 3793 Training Loss: 0.21726427750861457 Test Loss: 0.2137242341808697\n",
      "Epoch: 3794 Training Loss: 0.2172636233727424 Test Loss: 0.2137540485292556\n",
      "Epoch: 3795 Training Loss: 0.21726649835544373 Test Loss: 0.2138302047452413\n",
      "Epoch: 3796 Training Loss: 0.21726380427570838 Test Loss: 0.2137740759937148\n",
      "Epoch: 3797 Training Loss: 0.21726357235733496 Test Loss: 0.21376931866073326\n",
      "Epoch: 3798 Training Loss: 0.21726475329677408 Test Loss: 0.21387771326125624\n",
      "Epoch: 3799 Training Loss: 0.2172642198943776 Test Loss: 0.21376187803639693\n",
      "Epoch: 3800 Training Loss: 0.2172645526780118 Test Loss: 0.21371556209431747\n",
      "Epoch: 3801 Training Loss: 0.21726500947660488 Test Loss: 0.21372923780629446\n",
      "Epoch: 3802 Training Loss: 0.21726365848281898 Test Loss: 0.21520926094845094\n",
      "Epoch: 3803 Training Loss: 0.2172665832795104 Test Loss: 0.21379169238478285\n",
      "Epoch: 3804 Training Loss: 0.21726385057510453 Test Loss: 0.2137348895697276\n",
      "Epoch: 3805 Training Loss: 0.2172652134934057 Test Loss: 0.21366057406568573\n",
      "Epoch: 3806 Training Loss: 0.21726471484245377 Test Loss: 0.2137167935565334\n",
      "Epoch: 3807 Training Loss: 0.21726442603607327 Test Loss: 0.2136627647721541\n",
      "Epoch: 3808 Training Loss: 0.21726483996020615 Test Loss: 0.21367380904381703\n",
      "Epoch: 3809 Training Loss: 0.21726450908628755 Test Loss: 0.21368671995294414\n",
      "Epoch: 3810 Training Loss: 0.2172642253455846 Test Loss: 0.21372992483258335\n",
      "Epoch: 3811 Training Loss: 0.21726430345564257 Test Loss: 0.2136998123407136\n",
      "Epoch: 3812 Training Loss: 0.21726523645482196 Test Loss: 0.2137172342903791\n",
      "Epoch: 3813 Training Loss: 0.21726584726794387 Test Loss: 0.21378171105945365\n",
      "Epoch: 3814 Training Loss: 0.21726418478430107 Test Loss: 0.21376532613060156\n",
      "Epoch: 3815 Training Loss: 0.21726437349647965 Test Loss: 0.21374186353469787\n",
      "Epoch: 3816 Training Loss: 0.21726354117427923 Test Loss: 0.2136683906100669\n",
      "Epoch: 3817 Training Loss: 0.21726589368389546 Test Loss: 0.2137260878555737\n",
      "Epoch: 3818 Training Loss: 0.21726530631634303 Test Loss: 0.21372494713267892\n",
      "Epoch: 3819 Training Loss: 0.21726515709851765 Test Loss: 0.21402745906671444\n",
      "Epoch: 3820 Training Loss: 0.21726373919295922 Test Loss: 0.21370695482156604\n",
      "Epoch: 3821 Training Loss: 0.21726535881110765 Test Loss: 0.21381956231914356\n",
      "Epoch: 3822 Training Loss: 0.21726636740992072 Test Loss: 0.21376758165087076\n",
      "Epoch: 3823 Training Loss: 0.21726564387874914 Test Loss: 0.21373942653578634\n",
      "Epoch: 3824 Training Loss: 0.21726568312205993 Test Loss: 0.21371869908227806\n",
      "Epoch: 3825 Training Loss: 0.2172651227774316 Test Loss: 0.2138059903092479\n",
      "Epoch: 3826 Training Loss: 0.21726733039970433 Test Loss: 0.213805458836081\n",
      "Epoch: 3827 Training Loss: 0.21726450907732175 Test Loss: 0.2137627854296087\n",
      "Epoch: 3828 Training Loss: 0.21726640723600857 Test Loss: 0.21369335688615004\n",
      "Epoch: 3829 Training Loss: 0.2172652442550688 Test Loss: 0.2137802333047945\n",
      "Epoch: 3830 Training Loss: 0.21726441835238186 Test Loss: 0.2137484097285826\n",
      "Epoch: 3831 Training Loss: 0.21726566095859995 Test Loss: 0.2138470304079391\n",
      "Epoch: 3832 Training Loss: 0.2172636859361015 Test Loss: 0.21375656330472814\n",
      "Epoch: 3833 Training Loss: 0.21726405627747591 Test Loss: 0.21378701282836227\n",
      "Epoch: 3834 Training Loss: 0.21726601991340716 Test Loss: 0.21382902513406604\n",
      "Epoch: 3835 Training Loss: 0.21726496708629794 Test Loss: 0.2137928331076776\n",
      "Epoch: 3836 Training Loss: 0.21726307209253884 Test Loss: 0.21379269051731575\n",
      "Epoch: 3837 Training Loss: 0.21726474251091552 Test Loss: 0.21369572907126075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3838 Training Loss: 0.21726513347363213 Test Loss: 0.21369834754881464\n",
      "Epoch: 3839 Training Loss: 0.21726500815863214 Test Loss: 0.21557570521563396\n",
      "Epoch: 3840 Training Loss: 0.21726732894724457 Test Loss: 0.21366590176011468\n",
      "Epoch: 3841 Training Loss: 0.2172658202988146 Test Loss: 0.2136640091971302\n",
      "Epoch: 3842 Training Loss: 0.21726490833340426 Test Loss: 0.21373129888516115\n",
      "Epoch: 3843 Training Loss: 0.21726543564802187 Test Loss: 0.21370104380292954\n",
      "Epoch: 3844 Training Loss: 0.21726464542025695 Test Loss: 0.21371899722576193\n",
      "Epoch: 3845 Training Loss: 0.21726532308239083 Test Loss: 0.21374098206700648\n",
      "Epoch: 3846 Training Loss: 0.21726556203891798 Test Loss: 0.21372052683146173\n",
      "Epoch: 3847 Training Loss: 0.21726503729748525 Test Loss: 0.21370409005156898\n",
      "Epoch: 3848 Training Loss: 0.2172655865603836 Test Loss: 0.21368840511176596\n",
      "Epoch: 3849 Training Loss: 0.2172660735557943 Test Loss: 0.2136930846681865\n",
      "Epoch: 3850 Training Loss: 0.21726375875633688 Test Loss: 0.21376577982720743\n",
      "Epoch: 3851 Training Loss: 0.2172663951052799 Test Loss: 0.21378368139899914\n",
      "Epoch: 3852 Training Loss: 0.2172647413005324 Test Loss: 0.21375748366070008\n",
      "Epoch: 3853 Training Loss: 0.21726451248432613 Test Loss: 0.2137192824064856\n",
      "Epoch: 3854 Training Loss: 0.21726613971443956 Test Loss: 0.21367590901096423\n",
      "Epoch: 3855 Training Loss: 0.21726474771108006 Test Loss: 0.21384084717133905\n",
      "Epoch: 3856 Training Loss: 0.21726468479805475 Test Loss: 0.21372353419182064\n",
      "Epoch: 3857 Training Loss: 0.21726546144263123 Test Loss: 0.21376838534200115\n",
      "Epoch: 3858 Training Loss: 0.2172635671123414 Test Loss: 0.22007754219430287\n",
      "Epoch: 3859 Training Loss: 0.21726842993863654 Test Loss: 0.21414386465302113\n",
      "Epoch: 3860 Training Loss: 0.21726443804128076 Test Loss: 0.21379105720953462\n",
      "Epoch: 3861 Training Loss: 0.217265453839632 Test Loss: 0.21375306335948285\n",
      "Epoch: 3862 Training Loss: 0.21726370419943805 Test Loss: 0.21371370841961346\n",
      "Epoch: 3863 Training Loss: 0.21726587847789702 Test Loss: 0.21368369962982506\n",
      "Epoch: 3864 Training Loss: 0.2172651303983624 Test Loss: 0.21377077048987203\n",
      "Epoch: 3865 Training Loss: 0.21726487019288698 Test Loss: 0.21377446487651985\n",
      "Epoch: 3866 Training Loss: 0.21726299530941945 Test Loss: 0.21379148498062014\n",
      "Epoch: 3867 Training Loss: 0.21726370597466665 Test Loss: 0.21375132634962038\n",
      "Epoch: 3868 Training Loss: 0.21726479538224377 Test Loss: 0.21381419573643412\n",
      "Epoch: 3869 Training Loss: 0.2172661231725368 Test Loss: 0.2137250767602806\n",
      "Epoch: 3870 Training Loss: 0.21726382450255532 Test Loss: 0.2136716053745885\n",
      "Epoch: 3871 Training Loss: 0.21726663398111484 Test Loss: 0.21370000678211612\n",
      "Epoch: 3872 Training Loss: 0.2172659426730319 Test Loss: 0.21369009027058777\n",
      "Epoch: 3873 Training Loss: 0.2172652171693841 Test Loss: 0.21390973127887067\n",
      "Epoch: 3874 Training Loss: 0.21726515593296353 Test Loss: 0.21378497767501592\n",
      "Epoch: 3875 Training Loss: 0.21726556172511494 Test Loss: 0.2137638224504221\n",
      "Epoch: 3876 Training Loss: 0.21726450411026801 Test Loss: 0.21384008236848917\n",
      "Epoch: 3877 Training Loss: 0.21726417639231135 Test Loss: 0.2136869792081475\n",
      "Epoch: 3878 Training Loss: 0.2172650397182515 Test Loss: 0.2138139624067511\n",
      "Epoch: 3879 Training Loss: 0.2172630558375417 Test Loss: 0.21374833195202161\n",
      "Epoch: 3880 Training Loss: 0.21726689698392015 Test Loss: 0.2137810888469656\n",
      "Epoch: 3881 Training Loss: 0.2172632877379835 Test Loss: 0.21372345641525964\n",
      "Epoch: 3882 Training Loss: 0.21726489717994787 Test Loss: 0.21379523121830865\n",
      "Epoch: 3883 Training Loss: 0.21726443553085648 Test Loss: 0.2138179419741226\n",
      "Epoch: 3884 Training Loss: 0.2172649639213702 Test Loss: 0.2138126661307343\n",
      "Epoch: 3885 Training Loss: 0.21726465372258863 Test Loss: 0.21367082760897843\n",
      "Epoch: 3886 Training Loss: 0.21726507255998043 Test Loss: 0.21363361152453675\n",
      "Epoch: 3887 Training Loss: 0.21726630143955727 Test Loss: 0.21377202787760832\n",
      "Epoch: 3888 Training Loss: 0.21726602273763446 Test Loss: 0.2138935019031406\n",
      "Epoch: 3889 Training Loss: 0.21726502033418985 Test Loss: 0.21372402677670702\n",
      "Epoch: 3890 Training Loss: 0.21726426905386428 Test Loss: 0.2200524203650977\n",
      "Epoch: 3891 Training Loss: 0.21726689135339716 Test Loss: 0.21401763329450726\n",
      "Epoch: 3892 Training Loss: 0.21726399368721944 Test Loss: 0.21387895768623236\n",
      "Epoch: 3893 Training Loss: 0.21726386025816954 Test Loss: 0.21381352167290538\n",
      "Epoch: 3894 Training Loss: 0.21726542409110441 Test Loss: 0.21377437413719869\n",
      "Epoch: 3895 Training Loss: 0.21726474947734287 Test Loss: 0.2137415783539742\n",
      "Epoch: 3896 Training Loss: 0.21726474404406748 Test Loss: 0.21369639017202932\n",
      "Epoch: 3897 Training Loss: 0.21726481083031882 Test Loss: 0.21372617859489487\n",
      "Epoch: 3898 Training Loss: 0.21726355174495857 Test Loss: 0.21375508555006903\n",
      "Epoch: 3899 Training Loss: 0.21726479522982517 Test Loss: 0.21375788550626526\n",
      "Epoch: 3900 Training Loss: 0.21726576638745343 Test Loss: 0.21375241522147445\n",
      "Epoch: 3901 Training Loss: 0.2172650983187266 Test Loss: 0.21390619244534487\n",
      "Epoch: 3902 Training Loss: 0.2172648107585924 Test Loss: 0.21380360516137703\n",
      "Epoch: 3903 Training Loss: 0.21726369378117735 Test Loss: 0.21400476127366067\n",
      "Epoch: 3904 Training Loss: 0.21726731887865008 Test Loss: 0.2137907849915711\n",
      "Epoch: 3905 Training Loss: 0.21726230137435681 Test Loss: 0.2136878995641194\n",
      "Epoch: 3906 Training Loss: 0.2172635308277449 Test Loss: 0.21374649124007777\n",
      "Epoch: 3907 Training Loss: 0.21726379920106503 Test Loss: 0.21375262262563716\n",
      "Epoch: 3908 Training Loss: 0.21726414337126643 Test Loss: 0.21563976717638314\n",
      "Epoch: 3909 Training Loss: 0.21726469856952502 Test Loss: 0.21379463493134093\n",
      "Epoch: 3910 Training Loss: 0.2172649917781138 Test Loss: 0.21381690495330918\n",
      "Epoch: 3911 Training Loss: 0.21726446479523082 Test Loss: 0.21373584881398003\n",
      "Epoch: 3912 Training Loss: 0.2172660379436329 Test Loss: 0.21381993823918843\n",
      "Epoch: 3913 Training Loss: 0.21726366481267445 Test Loss: 0.21377397229163347\n",
      "Epoch: 3914 Training Loss: 0.21726429641748882 Test Loss: 0.2137751648655689\n",
      "Epoch: 3915 Training Loss: 0.21726530378798717 Test Loss: 0.21413266482823617\n",
      "Epoch: 3916 Training Loss: 0.217266373569426 Test Loss: 0.21384471007386907\n",
      "Epoch: 3917 Training Loss: 0.21726422079992352 Test Loss: 0.2137705890112297\n",
      "Epoch: 3918 Training Loss: 0.21726566355868224 Test Loss: 0.21381238095001062\n",
      "Epoch: 3919 Training Loss: 0.2172647898951736 Test Loss: 0.2138059514209674\n",
      "Epoch: 3920 Training Loss: 0.2172650834444628 Test Loss: 0.2137554873956342\n",
      "Epoch: 3921 Training Loss: 0.21726549273327656 Test Loss: 0.21386166536416854\n",
      "Epoch: 3922 Training Loss: 0.21726489051835776 Test Loss: 0.21378339621827547\n",
      "Epoch: 3923 Training Loss: 0.21726658914314423 Test Loss: 0.2137534781678082\n",
      "Epoch: 3924 Training Loss: 0.21726508936189143 Test Loss: 0.21371352694097112\n",
      "Epoch: 3925 Training Loss: 0.21726610465815782 Test Loss: 0.21386166536416854\n",
      "Epoch: 3926 Training Loss: 0.2172647552244213 Test Loss: 0.2137240786277477\n",
      "Epoch: 3927 Training Loss: 0.21726559261229925 Test Loss: 0.21377274082941755\n",
      "Epoch: 3928 Training Loss: 0.21726410355414438 Test Loss: 0.2138234122589134\n",
      "Epoch: 3929 Training Loss: 0.21726463741379667 Test Loss: 0.21365195383017416\n",
      "Epoch: 3930 Training Loss: 0.2172665120013928 Test Loss: 0.2137455190330652\n",
      "Epoch: 3931 Training Loss: 0.21726396019098704 Test Loss: 0.2137170268862164\n",
      "Epoch: 3932 Training Loss: 0.2172662812754709 Test Loss: 0.21373128592240098\n",
      "Epoch: 3933 Training Loss: 0.2172638214721146 Test Loss: 0.21372057868250238\n",
      "Epoch: 3934 Training Loss: 0.21726572418542833 Test Loss: 0.21368171632751937\n",
      "Epoch: 3935 Training Loss: 0.21726433436075845 Test Loss: 0.21368725142611103\n",
      "Epoch: 3936 Training Loss: 0.21726419411769987 Test Loss: 0.21386332459747\n",
      "Epoch: 3937 Training Loss: 0.21726703819528526 Test Loss: 0.2140325404687002\n",
      "Epoch: 3938 Training Loss: 0.21726366638168962 Test Loss: 0.21383032141008282\n",
      "Epoch: 3939 Training Loss: 0.2172633416583105 Test Loss: 0.2138097754352169\n",
      "Epoch: 3940 Training Loss: 0.21726496252270525 Test Loss: 0.21381572534213392\n",
      "Epoch: 3941 Training Loss: 0.21726481934782974 Test Loss: 0.21377542412077227\n",
      "Epoch: 3942 Training Loss: 0.2172651527321726 Test Loss: 0.21382938809135074\n",
      "Epoch: 3943 Training Loss: 0.21726373156306258 Test Loss: 0.2137359784415817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3944 Training Loss: 0.21726350281858273 Test Loss: 0.21376056879761998\n",
      "Epoch: 3945 Training Loss: 0.2172644460118778 Test Loss: 0.21378622209999204\n",
      "Epoch: 3946 Training Loss: 0.21726347587635084 Test Loss: 0.21375909104296087\n",
      "Epoch: 3947 Training Loss: 0.21726595311819 Test Loss: 0.21370598261455348\n",
      "Epoch: 3948 Training Loss: 0.21726519428866004 Test Loss: 0.2137744908020402\n",
      "Epoch: 3949 Training Loss: 0.21726465608059428 Test Loss: 0.21380456440562945\n",
      "Epoch: 3950 Training Loss: 0.21726399273684452 Test Loss: 0.21383699723156924\n",
      "Epoch: 3951 Training Loss: 0.21726511564961984 Test Loss: 0.21371280102640172\n",
      "Epoch: 3952 Training Loss: 0.2172649816736561 Test Loss: 0.21375845586771264\n",
      "Epoch: 3953 Training Loss: 0.2172651079748942 Test Loss: 0.21381409203435275\n",
      "Epoch: 3954 Training Loss: 0.2172649087996259 Test Loss: 0.2137228730910521\n",
      "Epoch: 3955 Training Loss: 0.21726469336936047 Test Loss: 0.21371033810196985\n",
      "Epoch: 3956 Training Loss: 0.21726642176060612 Test Loss: 0.21378524989297945\n",
      "Epoch: 3957 Training Loss: 0.21726439866348293 Test Loss: 0.21479279338978044\n",
      "Epoch: 3958 Training Loss: 0.21726279052155972 Test Loss: 0.21458640032238904\n",
      "Epoch: 3959 Training Loss: 0.21726419319422235 Test Loss: 0.2142283299982744\n",
      "Epoch: 3960 Training Loss: 0.21726220570926058 Test Loss: 0.21403902184878412\n",
      "Epoch: 3961 Training Loss: 0.21726151843578806 Test Loss: 0.2138991277410534\n",
      "Epoch: 3962 Training Loss: 0.21726509273303257 Test Loss: 0.21379418123473506\n",
      "Epoch: 3963 Training Loss: 0.21726404975933863 Test Loss: 0.2141878602610306\n",
      "Epoch: 3964 Training Loss: 0.21726415933935794 Test Loss: 0.213714058414138\n",
      "Epoch: 3965 Training Loss: 0.21726435487451107 Test Loss: 0.21371234732979585\n",
      "Epoch: 3966 Training Loss: 0.2172639935348008 Test Loss: 0.21383610280111767\n",
      "Epoch: 3967 Training Loss: 0.2172637260580608 Test Loss: 0.21378041478343687\n",
      "Epoch: 3968 Training Loss: 0.2172648714391333 Test Loss: 0.21379189978894553\n",
      "Epoch: 3969 Training Loss: 0.21726428339018 Test Loss: 0.21809439544175427\n",
      "Epoch: 3970 Training Loss: 0.21726596836005163 Test Loss: 0.2139207755505336\n",
      "Epoch: 3971 Training Loss: 0.21726469346798427 Test Loss: 0.2137555781349554\n",
      "Epoch: 3972 Training Loss: 0.2172659184833009 Test Loss: 0.21373027482710788\n",
      "Epoch: 3973 Training Loss: 0.2172658213388475 Test Loss: 0.2137262822969762\n",
      "Epoch: 3974 Training Loss: 0.21726449794179695 Test Loss: 0.2137030659935157\n",
      "Epoch: 3975 Training Loss: 0.21726649753955585 Test Loss: 0.21373797470664754\n",
      "Epoch: 3976 Training Loss: 0.21726356605437688 Test Loss: 0.21370538632758576\n",
      "Epoch: 3977 Training Loss: 0.21726735380044482 Test Loss: 0.21376832052820033\n",
      "Epoch: 3978 Training Loss: 0.21726459804496467 Test Loss: 0.21370792702857863\n",
      "Epoch: 3979 Training Loss: 0.21726852453680245 Test Loss: 0.2161869641713457\n",
      "Epoch: 3980 Training Loss: 0.217264406723738 Test Loss: 0.2138203919357943\n",
      "Epoch: 3981 Training Loss: 0.2172655099476144 Test Loss: 0.21379772006826087\n",
      "Epoch: 3982 Training Loss: 0.21726475500924206 Test Loss: 0.21376654463005734\n",
      "Epoch: 3983 Training Loss: 0.21726417705578063 Test Loss: 0.21405254200763912\n",
      "Epoch: 3984 Training Loss: 0.21726538916930968 Test Loss: 0.21377287045701923\n",
      "Epoch: 3985 Training Loss: 0.21726617238581827 Test Loss: 0.21379135535301846\n",
      "Epoch: 3986 Training Loss: 0.21726451035046548 Test Loss: 0.2139042869196002\n",
      "Epoch: 3987 Training Loss: 0.21726671001110698 Test Loss: 0.21381264020521398\n",
      "Epoch: 3988 Training Loss: 0.21726512529682165 Test Loss: 0.21560316034166932\n",
      "Epoch: 3989 Training Loss: 0.217264000958484 Test Loss: 0.21382951771895242\n",
      "Epoch: 3990 Training Loss: 0.21726530268519365 Test Loss: 0.2138187845535335\n",
      "Epoch: 3991 Training Loss: 0.21726546544137845 Test Loss: 0.2137198138796525\n",
      "Epoch: 3992 Training Loss: 0.21726542274623428 Test Loss: 0.21374159131673437\n",
      "Epoch: 3993 Training Loss: 0.21726604598595634 Test Loss: 0.21376058176038015\n",
      "Epoch: 3994 Training Loss: 0.2172653686196939 Test Loss: 0.21372333975041813\n",
      "Epoch: 3995 Training Loss: 0.21726423595212713 Test Loss: 0.21409201361235\n",
      "Epoch: 3996 Training Loss: 0.2172631390939694 Test Loss: 0.21424584268726107\n",
      "Epoch: 3997 Training Loss: 0.2172656135833077 Test Loss: 0.21414698867822157\n",
      "Epoch: 3998 Training Loss: 0.21726511382956223 Test Loss: 0.21392562362283635\n",
      "Epoch: 3999 Training Loss: 0.21726430430739366 Test Loss: 0.21383554540243044\n",
      "Epoch: 4000 Training Loss: 0.21726337333448528 Test Loss: 0.2137513133868602\n",
      "Epoch: 4001 Training Loss: 0.21726426050945596 Test Loss: 0.21380399404418207\n",
      "Epoch: 4002 Training Loss: 0.2172645442053299 Test Loss: 0.2137872850463258\n",
      "Epoch: 4003 Training Loss: 0.2172654247814711 Test Loss: 0.21379985892368855\n",
      "Epoch: 4004 Training Loss: 0.2172653221589133 Test Loss: 0.2137735704460683\n",
      "Epoch: 4005 Training Loss: 0.217264896453718 Test Loss: 0.21380801249983408\n",
      "Epoch: 4006 Training Loss: 0.2172663757570814 Test Loss: 0.21374542829374402\n",
      "Epoch: 4007 Training Loss: 0.2172647254220989 Test Loss: 0.21375656330472814\n",
      "Epoch: 4008 Training Loss: 0.21726579623460482 Test Loss: 0.21374686716012264\n",
      "Epoch: 4009 Training Loss: 0.21726374720838526 Test Loss: 0.21371387693549565\n",
      "Epoch: 4010 Training Loss: 0.2172641589269311 Test Loss: 0.21372711191362695\n",
      "Epoch: 4011 Training Loss: 0.21726432744812593 Test Loss: 0.21370670852912285\n",
      "Epoch: 4012 Training Loss: 0.21726704638106153 Test Loss: 0.2138355583651906\n",
      "Epoch: 4013 Training Loss: 0.21726325715563644 Test Loss: 0.21374170798157588\n",
      "Epoch: 4014 Training Loss: 0.21726433056822467 Test Loss: 0.21382724923592306\n",
      "Epoch: 4015 Training Loss: 0.2172644581515723 Test Loss: 0.21404127736905332\n",
      "Epoch: 4016 Training Loss: 0.21726582232508562 Test Loss: 0.21379420716025538\n",
      "Epoch: 4017 Training Loss: 0.21726512914315027 Test Loss: 0.21374878564862748\n",
      "Epoch: 4018 Training Loss: 0.2172635800410264 Test Loss: 0.21371127142070193\n",
      "Epoch: 4019 Training Loss: 0.21726497633003872 Test Loss: 0.21369024582370977\n",
      "Epoch: 4020 Training Loss: 0.21726528213557783 Test Loss: 0.21383925275183843\n",
      "Epoch: 4021 Training Loss: 0.21726480728882744 Test Loss: 0.21384165086246948\n",
      "Epoch: 4022 Training Loss: 0.21726361076682627 Test Loss: 0.21396920442252043\n",
      "Epoch: 4023 Training Loss: 0.2172632482884593 Test Loss: 0.2141800566794096\n",
      "Epoch: 4024 Training Loss: 0.21726418024760577 Test Loss: 0.2139527287543472\n",
      "Epoch: 4025 Training Loss: 0.21726258606543467 Test Loss: 0.21390251102145721\n",
      "Epoch: 4026 Training Loss: 0.2172630997161716 Test Loss: 0.21377907961913958\n",
      "Epoch: 4027 Training Loss: 0.21726237471460866 Test Loss: 0.21374579125102872\n",
      "Epoch: 4028 Training Loss: 0.21726348737050766 Test Loss: 0.2136805367163441\n",
      "Epoch: 4029 Training Loss: 0.21726306096597986 Test Loss: 0.21383479356234072\n",
      "Epoch: 4030 Training Loss: 0.21726217466069186 Test Loss: 0.2147738547971753\n",
      "Epoch: 4031 Training Loss: 0.21726597484232574 Test Loss: 0.21373885617433896\n",
      "Epoch: 4032 Training Loss: 0.21726361411107004 Test Loss: 0.21371308620712542\n",
      "Epoch: 4033 Training Loss: 0.21726508412586365 Test Loss: 0.21381791604860226\n",
      "Epoch: 4034 Training Loss: 0.21726417141629184 Test Loss: 0.2137174416945418\n",
      "Epoch: 4035 Training Loss: 0.21726342327399661 Test Loss: 0.213731519252084\n",
      "Epoch: 4036 Training Loss: 0.21726387419102425 Test Loss: 0.21373837655221276\n",
      "Epoch: 4037 Training Loss: 0.21726352485652148 Test Loss: 0.2137262434086957\n",
      "Epoch: 4038 Training Loss: 0.217263866229393 Test Loss: 0.21376457429051185\n",
      "Epoch: 4039 Training Loss: 0.21726419852887394 Test Loss: 0.2137686964482452\n",
      "Epoch: 4040 Training Loss: 0.2172644300348205 Test Loss: 0.2137206564590634\n",
      "Epoch: 4041 Training Loss: 0.21726378976904243 Test Loss: 0.2138010774231443\n",
      "Epoch: 4042 Training Loss: 0.21726335347523615 Test Loss: 0.21393729010698737\n",
      "Epoch: 4043 Training Loss: 0.21726313275514814 Test Loss: 0.21387490034229983\n",
      "Epoch: 4044 Training Loss: 0.21726333035243547 Test Loss: 0.21379165349650234\n",
      "Epoch: 4045 Training Loss: 0.21726371984476073 Test Loss: 0.21374147465189286\n",
      "Epoch: 4046 Training Loss: 0.21726323117274526 Test Loss: 0.21365243345230037\n",
      "Epoch: 4047 Training Loss: 0.21726382022586826 Test Loss: 0.21369742719284274\n",
      "Epoch: 4048 Training Loss: 0.2172640273000072 Test Loss: 0.2136663295312002\n",
      "Epoch: 4049 Training Loss: 0.21726550198598316 Test Loss: 0.21376327801449507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4050 Training Loss: 0.21726487183362855 Test Loss: 0.21378981278455852\n",
      "Epoch: 4051 Training Loss: 0.21726467048863643 Test Loss: 0.21387167261501805\n",
      "Epoch: 4052 Training Loss: 0.21726392584300358 Test Loss: 0.21367549420263884\n",
      "Epoch: 4053 Training Loss: 0.21726295696268874 Test Loss: 0.21369701238451735\n",
      "Epoch: 4054 Training Loss: 0.2172656230153303 Test Loss: 0.21375480036934533\n",
      "Epoch: 4055 Training Loss: 0.2172655997670084 Test Loss: 0.21377423154683683\n",
      "Epoch: 4056 Training Loss: 0.217265285641206 Test Loss: 0.21382360670031592\n",
      "Epoch: 4057 Training Loss: 0.2172639178365433 Test Loss: 0.21375760032554159\n",
      "Epoch: 4058 Training Loss: 0.2172639570529567 Test Loss: 0.21367994042937638\n",
      "Epoch: 4059 Training Loss: 0.2172664131265398 Test Loss: 0.21377688891267124\n",
      "Epoch: 4060 Training Loss: 0.2172648180926176 Test Loss: 0.21368736809095254\n",
      "Epoch: 4061 Training Loss: 0.21726397918951929 Test Loss: 0.21462268308809865\n",
      "Epoch: 4062 Training Loss: 0.21726126678368676 Test Loss: 0.21479083601299512\n",
      "Epoch: 4063 Training Loss: 0.21726306246326862 Test Loss: 0.21422180972991\n",
      "Epoch: 4064 Training Loss: 0.2172644270581746 Test Loss: 0.21403269602182223\n",
      "Epoch: 4065 Training Loss: 0.2172627406358432 Test Loss: 0.2138643745810436\n",
      "Epoch: 4066 Training Loss: 0.21726349534110473 Test Loss: 0.21377539819525193\n",
      "Epoch: 4067 Training Loss: 0.21726508019884283 Test Loss: 0.21376878718756637\n",
      "Epoch: 4068 Training Loss: 0.21726335253382706 Test Loss: 0.2137034808018411\n",
      "Epoch: 4069 Training Loss: 0.2172654873896592 Test Loss: 0.21379223682070989\n",
      "Epoch: 4070 Training Loss: 0.21726464238981621 Test Loss: 0.21371754539662313\n",
      "Epoch: 4071 Training Loss: 0.2172646460388972 Test Loss: 0.21371024736264868\n",
      "Epoch: 4072 Training Loss: 0.21726437752212427 Test Loss: 0.21375704292685435\n",
      "Epoch: 4073 Training Loss: 0.21726506844467777 Test Loss: 0.21372330086213762\n",
      "Epoch: 4074 Training Loss: 0.21726423620316956 Test Loss: 0.21371890648644074\n",
      "Epoch: 4075 Training Loss: 0.21726520572902205 Test Loss: 0.21379182201238453\n",
      "Epoch: 4076 Training Loss: 0.2172647945304927 Test Loss: 0.21365875927926223\n",
      "Epoch: 4077 Training Loss: 0.2172640168458833 Test Loss: 0.21373068963543326\n",
      "Epoch: 4078 Training Loss: 0.21726413978494605 Test Loss: 0.21366743136581448\n",
      "Epoch: 4079 Training Loss: 0.2172649481056973 Test Loss: 0.21358616782232265\n",
      "Epoch: 4080 Training Loss: 0.21726614626844007 Test Loss: 0.21372550453136616\n",
      "Epoch: 4081 Training Loss: 0.21726626933302404 Test Loss: 0.21371492691906924\n",
      "Epoch: 4082 Training Loss: 0.21726568996296605 Test Loss: 0.21372493416991875\n",
      "Epoch: 4083 Training Loss: 0.21726580745978763 Test Loss: 0.21370490670545955\n",
      "Epoch: 4084 Training Loss: 0.21726510635208424 Test Loss: 0.21394514553964905\n",
      "Epoch: 4085 Training Loss: 0.21726439192120062 Test Loss: 0.21380286628404746\n",
      "Epoch: 4086 Training Loss: 0.21726567369003733 Test Loss: 0.2138616783269287\n",
      "Epoch: 4087 Training Loss: 0.21726439191223482 Test Loss: 0.2137520393014296\n",
      "Epoch: 4088 Training Loss: 0.2172659688621365 Test Loss: 0.21378544433438196\n",
      "Epoch: 4089 Training Loss: 0.2172650956289863 Test Loss: 0.21375410038029627\n",
      "Epoch: 4090 Training Loss: 0.21726451066426852 Test Loss: 0.21379622935084155\n",
      "Epoch: 4091 Training Loss: 0.21726495791428355 Test Loss: 0.21377993516131066\n",
      "Epoch: 4092 Training Loss: 0.21726534849147075 Test Loss: 0.213803138502011\n",
      "Epoch: 4093 Training Loss: 0.2172655397589026 Test Loss: 0.21368280519937347\n",
      "Epoch: 4094 Training Loss: 0.21726522623380887 Test Loss: 0.21366979058816502\n",
      "Epoch: 4095 Training Loss: 0.21726435200545474 Test Loss: 0.21363573741720426\n",
      "Epoch: 4096 Training Loss: 0.2172652301608297 Test Loss: 0.2137843813880482\n",
      "Epoch: 4097 Training Loss: 0.21726459109646892 Test Loss: 0.213778107412127\n",
      "Epoch: 4098 Training Loss: 0.21726455845198764 Test Loss: 0.2136712035290233\n",
      "Epoch: 4099 Training Loss: 0.21726577399045266 Test Loss: 0.213627065330652\n",
      "Epoch: 4100 Training Loss: 0.21726504860336027 Test Loss: 0.21374724308016751\n",
      "Epoch: 4101 Training Loss: 0.21726295536677617 Test Loss: 0.21391862373234577\n",
      "Epoch: 4102 Training Loss: 0.21726711856472505 Test Loss: 0.2137032345093979\n",
      "Epoch: 4103 Training Loss: 0.21726521570795854 Test Loss: 0.213780038863392\n",
      "Epoch: 4104 Training Loss: 0.21726358059690604 Test Loss: 0.2183940944568334\n",
      "Epoch: 4105 Training Loss: 0.21726608295195368 Test Loss: 0.21393669382001965\n",
      "Epoch: 4106 Training Loss: 0.21726554251140348 Test Loss: 0.21376745202326908\n",
      "Epoch: 4107 Training Loss: 0.2172658192139527 Test Loss: 0.21377091308023388\n",
      "Epoch: 4108 Training Loss: 0.21726517356869401 Test Loss: 0.2137268137701431\n",
      "Epoch: 4109 Training Loss: 0.2172629903154683 Test Loss: 0.21461919610561353\n",
      "Epoch: 4110 Training Loss: 0.21726627409386434 Test Loss: 0.2139218255341072\n",
      "Epoch: 4111 Training Loss: 0.21726507578766877 Test Loss: 0.2138020885184374\n",
      "Epoch: 4112 Training Loss: 0.21726383437390218 Test Loss: 0.21373850617981444\n",
      "Epoch: 4113 Training Loss: 0.21726676022855815 Test Loss: 0.21379713674405332\n",
      "Epoch: 4114 Training Loss: 0.21726353524788478 Test Loss: 0.21377224824453117\n",
      "Epoch: 4115 Training Loss: 0.21726412490171645 Test Loss: 0.21375964844164808\n",
      "Epoch: 4116 Training Loss: 0.21726376498756855 Test Loss: 0.21373387847443454\n",
      "Epoch: 4117 Training Loss: 0.21726302313926563 Test Loss: 0.213830334372843\n",
      "Epoch: 4118 Training Loss: 0.2172631613919164 Test Loss: 0.213743030183113\n",
      "Epoch: 4119 Training Loss: 0.21726365001013706 Test Loss: 0.2140451402715833\n",
      "Epoch: 4120 Training Loss: 0.21726573691686568 Test Loss: 0.21374061910972178\n",
      "Epoch: 4121 Training Loss: 0.2172625843619325 Test Loss: 0.21367222758707657\n",
      "Epoch: 4122 Training Loss: 0.21726565795505665 Test Loss: 0.21369496426841086\n",
      "Epoch: 4123 Training Loss: 0.2172664279470088 Test Loss: 0.2138057051285242\n",
      "Epoch: 4124 Training Loss: 0.21726426613997896 Test Loss: 0.2139890633710975\n",
      "Epoch: 4125 Training Loss: 0.2172639177648169 Test Loss: 0.21383911016147658\n",
      "Epoch: 4126 Training Loss: 0.21726393696956256 Test Loss: 0.21386996153067592\n",
      "Epoch: 4127 Training Loss: 0.21726386745770773 Test Loss: 0.2137321285018119\n",
      "Epoch: 4128 Training Loss: 0.2172641131296198 Test Loss: 0.21370114750501087\n",
      "Epoch: 4129 Training Loss: 0.21726396884298496 Test Loss: 0.21358493636010673\n",
      "Epoch: 4130 Training Loss: 0.21726564514292707 Test Loss: 0.21365774818396915\n",
      "Epoch: 4131 Training Loss: 0.2172651172544982 Test Loss: 0.21383352321184426\n",
      "Epoch: 4132 Training Loss: 0.21726655499240838 Test Loss: 0.2137048418916587\n",
      "Epoch: 4133 Training Loss: 0.21726433310554633 Test Loss: 0.21384973962481416\n",
      "Epoch: 4134 Training Loss: 0.21726430785785084 Test Loss: 0.21372324901109696\n",
      "Epoch: 4135 Training Loss: 0.2172651376516954 Test Loss: 0.21368804215448126\n",
      "Epoch: 4136 Training Loss: 0.21726486902733286 Test Loss: 0.21365882409306308\n",
      "Epoch: 4137 Training Loss: 0.21726648921032674 Test Loss: 0.21452429573842519\n",
      "Epoch: 4138 Training Loss: 0.21726291311095625 Test Loss: 0.21442449544789344\n",
      "Epoch: 4139 Training Loss: 0.21726446693805723 Test Loss: 0.2141138947515132\n",
      "Epoch: 4140 Training Loss: 0.2172639822378916 Test Loss: 0.21395953420343528\n",
      "Epoch: 4141 Training Loss: 0.21726397459902919 Test Loss: 0.21396492671166506\n",
      "Epoch: 4142 Training Loss: 0.21726578498252463 Test Loss: 0.21377476302000373\n",
      "Epoch: 4143 Training Loss: 0.21726498779729816 Test Loss: 0.21385587101037354\n",
      "Epoch: 4144 Training Loss: 0.21726453597472462 Test Loss: 0.2138006366892986\n",
      "Epoch: 4145 Training Loss: 0.21726322313042182 Test Loss: 0.21371624912060636\n",
      "Epoch: 4146 Training Loss: 0.21726444290971067 Test Loss: 0.21379819969038708\n",
      "Epoch: 4147 Training Loss: 0.2172608270380471 Test Loss: 0.21368309038009717\n",
      "Epoch: 4148 Training Loss: 0.21726293868142058 Test Loss: 0.21386813378149225\n",
      "Epoch: 4149 Training Loss: 0.21726279884182303 Test Loss: 0.21377178158516513\n",
      "Epoch: 4150 Training Loss: 0.21726463267088797 Test Loss: 0.2137274489453913\n",
      "Epoch: 4151 Training Loss: 0.21726418357391794 Test Loss: 0.21387637809695895\n",
      "Epoch: 4152 Training Loss: 0.21726431328216042 Test Loss: 0.21378500360053626\n",
      "Epoch: 4153 Training Loss: 0.2172649826150652 Test Loss: 0.2140980542585882\n",
      "Epoch: 4154 Training Loss: 0.21726234493918367 Test Loss: 0.21451984951168765\n",
      "Epoch: 4155 Training Loss: 0.21726315049846823 Test Loss: 0.21412575567706674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4156 Training Loss: 0.2172650358001965 Test Loss: 0.21385248772996973\n",
      "Epoch: 4157 Training Loss: 0.2172648582504401 Test Loss: 0.21372961372633933\n",
      "Epoch: 4158 Training Loss: 0.21726501473056423 Test Loss: 0.21375133931238052\n",
      "Epoch: 4159 Training Loss: 0.21726389487512707 Test Loss: 0.21369387539655677\n",
      "Epoch: 4160 Training Loss: 0.2172611484889089 Test Loss: 0.21453138636823696\n",
      "Epoch: 4161 Training Loss: 0.21726650054309915 Test Loss: 0.2138356361417516\n",
      "Epoch: 4162 Training Loss: 0.21726461005017214 Test Loss: 0.213681379295755\n",
      "Epoch: 4163 Training Loss: 0.2172627155316005 Test Loss: 0.21375235040767362\n",
      "Epoch: 4164 Training Loss: 0.2172639925933917 Test Loss: 0.21372433788295106\n",
      "Epoch: 4165 Training Loss: 0.21726357203456612 Test Loss: 0.2136625573679914\n",
      "Epoch: 4166 Training Loss: 0.2172647069166857 Test Loss: 0.21373350255438966\n",
      "Epoch: 4167 Training Loss: 0.21726546435651653 Test Loss: 0.2137783148162897\n",
      "Epoch: 4168 Training Loss: 0.21726362560522686 Test Loss: 0.21373068963543326\n",
      "Epoch: 4169 Training Loss: 0.21726420975405675 Test Loss: 0.21365441675460603\n",
      "Epoch: 4170 Training Loss: 0.21726523086016217 Test Loss: 0.2137912257254168\n",
      "Epoch: 4171 Training Loss: 0.2172647770023518 Test Loss: 0.21385904688661464\n",
      "Epoch: 4172 Training Loss: 0.21726405182147285 Test Loss: 0.21378457582945073\n",
      "Epoch: 4173 Training Loss: 0.21726425738935723 Test Loss: 0.21378938501347297\n",
      "Epoch: 4174 Training Loss: 0.21726377534306868 Test Loss: 0.21375744477241956\n",
      "Epoch: 4175 Training Loss: 0.21726571702175335 Test Loss: 0.2138046681077108\n",
      "Epoch: 4176 Training Loss: 0.2172632025897718 Test Loss: 0.21375052265848996\n",
      "Epoch: 4177 Training Loss: 0.2172646898458007 Test Loss: 0.21376802238471648\n",
      "Epoch: 4178 Training Loss: 0.2172625527664499 Test Loss: 0.2137683334909605\n",
      "Epoch: 4179 Training Loss: 0.21726419472737432 Test Loss: 0.21378928131139163\n",
      "Epoch: 4180 Training Loss: 0.2172649939568034 Test Loss: 0.21374465052813396\n",
      "Epoch: 4181 Training Loss: 0.21726501484711966 Test Loss: 0.2137674131349886\n",
      "Epoch: 4182 Training Loss: 0.2172640393321121 Test Loss: 0.2137377932280052\n",
      "Epoch: 4183 Training Loss: 0.21726406133418766 Test Loss: 0.21374214871542158\n",
      "Epoch: 4184 Training Loss: 0.2172661084058626 Test Loss: 0.21378300733547043\n",
      "Epoch: 4185 Training Loss: 0.21726503456291596 Test Loss: 0.21374533755442285\n",
      "Epoch: 4186 Training Loss: 0.21726509550346507 Test Loss: 0.21375897437811936\n",
      "Epoch: 4187 Training Loss: 0.2172630454282468 Test Loss: 0.2144815186298715\n",
      "Epoch: 4188 Training Loss: 0.21726657990836926 Test Loss: 0.2138686393291388\n",
      "Epoch: 4189 Training Loss: 0.2172636017382647 Test Loss: 0.21381923825013938\n",
      "Epoch: 4190 Training Loss: 0.21726611079076566 Test Loss: 0.21379091461917277\n",
      "Epoch: 4191 Training Loss: 0.21726438194226416 Test Loss: 0.21372620452041521\n",
      "Epoch: 4192 Training Loss: 0.2172654220558676 Test Loss: 0.21416229769797973\n",
      "Epoch: 4193 Training Loss: 0.21726431737953145 Test Loss: 0.213708108507221\n",
      "Epoch: 4194 Training Loss: 0.21726351342512526 Test Loss: 0.21370429745573166\n",
      "Epoch: 4195 Training Loss: 0.2172655630161903 Test Loss: 0.21375478740658516\n",
      "Epoch: 4196 Training Loss: 0.21726478199630295 Test Loss: 0.21370913256527424\n",
      "Epoch: 4197 Training Loss: 0.21726496338342213 Test Loss: 0.2136551167436551\n",
      "Epoch: 4198 Training Loss: 0.21726524593167357 Test Loss: 0.2196875705174153\n",
      "Epoch: 4199 Training Loss: 0.2172673115177275 Test Loss: 0.21436997407862773\n",
      "Epoch: 4200 Training Loss: 0.21726350034402164 Test Loss: 0.2139728469581276\n",
      "Epoch: 4201 Training Loss: 0.21726664322485562 Test Loss: 0.21378613136067087\n",
      "Epoch: 4202 Training Loss: 0.2172639839593254 Test Loss: 0.21369592351266326\n",
      "Epoch: 4203 Training Loss: 0.21726456810815525 Test Loss: 0.21369123099348253\n",
      "Epoch: 4204 Training Loss: 0.21726378353781076 Test Loss: 0.21363236709956063\n",
      "Epoch: 4205 Training Loss: 0.21726608418923424 Test Loss: 0.21378916464655012\n",
      "Epoch: 4206 Training Loss: 0.2172648425064936 Test Loss: 0.21377612410982133\n",
      "Epoch: 4207 Training Loss: 0.21726499787485842 Test Loss: 0.21377491857312572\n",
      "Epoch: 4208 Training Loss: 0.21726615472319039 Test Loss: 0.21382832514501698\n",
      "Epoch: 4209 Training Loss: 0.21726421648737326 Test Loss: 0.213863246820909\n",
      "Epoch: 4210 Training Loss: 0.21726510560792275 Test Loss: 0.2137873628228868\n",
      "Epoch: 4211 Training Loss: 0.21726623857136093 Test Loss: 0.21379607379771956\n",
      "Epoch: 4212 Training Loss: 0.21726544887257826 Test Loss: 0.2137518318972669\n",
      "Epoch: 4213 Training Loss: 0.2172635725187194 Test Loss: 0.21373172665624668\n",
      "Epoch: 4214 Training Loss: 0.21726485236887466 Test Loss: 0.21382102711104253\n",
      "Epoch: 4215 Training Loss: 0.21726386329757608 Test Loss: 0.21375841697943215\n",
      "Epoch: 4216 Training Loss: 0.2172653425740421 Test Loss: 0.21374198019953938\n",
      "Epoch: 4217 Training Loss: 0.21726403390780252 Test Loss: 0.21379800524898454\n",
      "Epoch: 4218 Training Loss: 0.21726451022494428 Test Loss: 0.2173452256806175\n",
      "Epoch: 4219 Training Loss: 0.2172658868609209 Test Loss: 0.21386840599945578\n",
      "Epoch: 4220 Training Loss: 0.2172634949914385 Test Loss: 0.21376507983815837\n",
      "Epoch: 4221 Training Loss: 0.2172652752139795 Test Loss: 0.21387251519442896\n",
      "Epoch: 4222 Training Loss: 0.21726510720383532 Test Loss: 0.21376234469576297\n",
      "Epoch: 4223 Training Loss: 0.2172630783685995 Test Loss: 0.21375470963002416\n",
      "Epoch: 4224 Training Loss: 0.21726490714095273 Test Loss: 0.2137381561852899\n",
      "Epoch: 4225 Training Loss: 0.21726506058167033 Test Loss: 0.21373774137696452\n",
      "Epoch: 4226 Training Loss: 0.2172654387322574 Test Loss: 0.21373818211081025\n",
      "Epoch: 4227 Training Loss: 0.21726459308687673 Test Loss: 0.21374713937808618\n",
      "Epoch: 4228 Training Loss: 0.21726590266762802 Test Loss: 0.2138222456104983\n",
      "Epoch: 4229 Training Loss: 0.21726449883837706 Test Loss: 0.21366648508432223\n",
      "Epoch: 4230 Training Loss: 0.21726322624155472 Test Loss: 0.2137272156157083\n",
      "Epoch: 4231 Training Loss: 0.21726607530412548 Test Loss: 0.21370634557183815\n",
      "Epoch: 4232 Training Loss: 0.21726483507384461 Test Loss: 0.2137037530198046\n",
      "Epoch: 4233 Training Loss: 0.21726601731332487 Test Loss: 0.21369576795954126\n",
      "Epoch: 4234 Training Loss: 0.21726560761208424 Test Loss: 0.21374290055551132\n",
      "Epoch: 4235 Training Loss: 0.2172649825612704 Test Loss: 0.2137234304897393\n",
      "Epoch: 4236 Training Loss: 0.21726544456899383 Test Loss: 0.213843932308259\n",
      "Epoch: 4237 Training Loss: 0.21726471489624857 Test Loss: 0.21374689308564299\n",
      "Epoch: 4238 Training Loss: 0.21726434240308193 Test Loss: 0.21377731668375677\n",
      "Epoch: 4239 Training Loss: 0.21726380035765336 Test Loss: 0.21386385607063688\n",
      "Epoch: 4240 Training Loss: 0.21726231686726086 Test Loss: 0.2140679158411981\n",
      "Epoch: 4241 Training Loss: 0.21726443878544224 Test Loss: 0.21391839040266275\n",
      "Epoch: 4242 Training Loss: 0.21726291212471813 Test Loss: 0.21386784860076855\n",
      "Epoch: 4243 Training Loss: 0.21726313638629752 Test Loss: 0.21381632162910164\n",
      "Epoch: 4244 Training Loss: 0.2172650909488382 Test Loss: 0.21386301349122597\n",
      "Epoch: 4245 Training Loss: 0.21726407145657695 Test Loss: 0.21380304776268982\n",
      "Epoch: 4246 Training Loss: 0.21726399211820427 Test Loss: 0.21376269469028752\n",
      "Epoch: 4247 Training Loss: 0.21726399554314021 Test Loss: 0.2137242341808697\n",
      "Epoch: 4248 Training Loss: 0.21726469136102106 Test Loss: 0.21434949291756264\n",
      "Epoch: 4249 Training Loss: 0.21726482503214756 Test Loss: 0.21377261120181587\n",
      "Epoch: 4250 Training Loss: 0.21726467359976934 Test Loss: 0.21376817793783848\n",
      "Epoch: 4251 Training Loss: 0.21726546016948747 Test Loss: 0.21377112048439656\n",
      "Epoch: 4252 Training Loss: 0.21726532832738438 Test Loss: 0.21369951419722974\n",
      "Epoch: 4253 Training Loss: 0.217265242399148 Test Loss: 0.21370700667260673\n",
      "Epoch: 4254 Training Loss: 0.21726628912951254 Test Loss: 0.21368640884670012\n",
      "Epoch: 4255 Training Loss: 0.21726653862982165 Test Loss: 0.21375957066508708\n",
      "Epoch: 4256 Training Loss: 0.21726577680571416 Test Loss: 0.2141840751350616\n",
      "Epoch: 4257 Training Loss: 0.21726499318574452 Test Loss: 0.21371768798698498\n",
      "Epoch: 4258 Training Loss: 0.2172658212940185 Test Loss: 0.21375503369902835\n",
      "Epoch: 4259 Training Loss: 0.21726471515625678 Test Loss: 0.21375185782278724\n",
      "Epoch: 4260 Training Loss: 0.21726405009107325 Test Loss: 0.2136897143505429\n",
      "Epoch: 4261 Training Loss: 0.21726425468168534 Test Loss: 0.21365874631650206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4262 Training Loss: 0.21726658813897454 Test Loss: 0.2137079399913388\n",
      "Epoch: 4263 Training Loss: 0.2172652975746871 Test Loss: 0.2137257637865695\n",
      "Epoch: 4264 Training Loss: 0.21726475904385248 Test Loss: 0.2136930587426662\n",
      "Epoch: 4265 Training Loss: 0.21726401006773777 Test Loss: 0.21390251102145721\n",
      "Epoch: 4266 Training Loss: 0.217263623301016 Test Loss: 0.21380634030377244\n",
      "Epoch: 4267 Training Loss: 0.21726224905890823 Test Loss: 0.21602592780178134\n",
      "Epoch: 4268 Training Loss: 0.2172632071085355 Test Loss: 0.21376867052272486\n",
      "Epoch: 4269 Training Loss: 0.21726341639722727 Test Loss: 0.2136409095585112\n",
      "Epoch: 4270 Training Loss: 0.21726459977536425 Test Loss: 0.21364185584000345\n",
      "Epoch: 4271 Training Loss: 0.21726554003684242 Test Loss: 0.21374606346899225\n",
      "Epoch: 4272 Training Loss: 0.21726546537861785 Test Loss: 0.21374860416998515\n",
      "Epoch: 4273 Training Loss: 0.21726490991138522 Test Loss: 0.21380593845820722\n",
      "Epoch: 4274 Training Loss: 0.21726343923312233 Test Loss: 0.21373093592787645\n",
      "Epoch: 4275 Training Loss: 0.21726532816599997 Test Loss: 0.2137438338742434\n",
      "Epoch: 4276 Training Loss: 0.21726477463538035 Test Loss: 0.21379341643188515\n",
      "Epoch: 4277 Training Loss: 0.21726465182183882 Test Loss: 0.21379318310220213\n",
      "Epoch: 4278 Training Loss: 0.2172651006050058 Test Loss: 0.21374226538026309\n",
      "Epoch: 4279 Training Loss: 0.2172623863342867 Test Loss: 0.21377012235186366\n",
      "Epoch: 4280 Training Loss: 0.21726441194183416 Test Loss: 0.2138049792139548\n",
      "Epoch: 4281 Training Loss: 0.21726412923219832 Test Loss: 0.21379674786124828\n",
      "Epoch: 4282 Training Loss: 0.21726399401895405 Test Loss: 0.21375365964645057\n",
      "Epoch: 4283 Training Loss: 0.21726531487868295 Test Loss: 0.2138641282886004\n",
      "Epoch: 4284 Training Loss: 0.21726414507476863 Test Loss: 0.21365413157388233\n",
      "Epoch: 4285 Training Loss: 0.2172638402733992 Test Loss: 0.21396609336008018\n",
      "Epoch: 4286 Training Loss: 0.21726180456139407 Test Loss: 0.21414360539781777\n",
      "Epoch: 4287 Training Loss: 0.21726502173285478 Test Loss: 0.21383863053935037\n",
      "Epoch: 4288 Training Loss: 0.21726519697840033 Test Loss: 0.21386228757665657\n",
      "Epoch: 4289 Training Loss: 0.21726409370072913 Test Loss: 0.2137731556377429\n",
      "Epoch: 4290 Training Loss: 0.21726268476097163 Test Loss: 0.21700667727331555\n",
      "Epoch: 4291 Training Loss: 0.2172635343154415 Test Loss: 0.21384420452622252\n",
      "Epoch: 4292 Training Loss: 0.21726509171989708 Test Loss: 0.21369515870981337\n",
      "Epoch: 4293 Training Loss: 0.217264898614476 Test Loss: 0.21367099612486062\n",
      "Epoch: 4294 Training Loss: 0.21726495891845327 Test Loss: 0.213731441475523\n",
      "Epoch: 4295 Training Loss: 0.21726581783321933 Test Loss: 0.2136996567875916\n",
      "Epoch: 4296 Training Loss: 0.21726485435928247 Test Loss: 0.2136624277403897\n",
      "Epoch: 4297 Training Loss: 0.21726379961349188 Test Loss: 0.21363757812914808\n",
      "Epoch: 4298 Training Loss: 0.2172652714304115 Test Loss: 0.21363263931752416\n",
      "Epoch: 4299 Training Loss: 0.21726319400950028 Test Loss: 0.2137245971381544\n",
      "Epoch: 4300 Training Loss: 0.2172637596170538 Test Loss: 0.2137641854077068\n",
      "Epoch: 4301 Training Loss: 0.2172647721608193 Test Loss: 0.21372983409326218\n",
      "Epoch: 4302 Training Loss: 0.21726486586240512 Test Loss: 0.21372639896181772\n",
      "Epoch: 4303 Training Loss: 0.21726400241094376 Test Loss: 0.2137081344327413\n",
      "Epoch: 4304 Training Loss: 0.2172652294973604 Test Loss: 0.2137467634580413\n",
      "Epoch: 4305 Training Loss: 0.21726444204899378 Test Loss: 0.21370937885771743\n",
      "Epoch: 4306 Training Loss: 0.21726527251527342 Test Loss: 0.2137025863713895\n",
      "Epoch: 4307 Training Loss: 0.217264251498826 Test Loss: 0.21368819770760328\n",
      "Epoch: 4308 Training Loss: 0.21726539174249457 Test Loss: 0.21375104116889668\n",
      "Epoch: 4309 Training Loss: 0.21726411894842462 Test Loss: 0.21374430053360943\n",
      "Epoch: 4310 Training Loss: 0.21726401988528982 Test Loss: 0.2138011681624655\n",
      "Epoch: 4311 Training Loss: 0.21726434186513385 Test Loss: 0.21408234339326485\n",
      "Epoch: 4312 Training Loss: 0.21726424285579388 Test Loss: 0.21382376225343794\n",
      "Epoch: 4313 Training Loss: 0.21726357346012848 Test Loss: 0.21379839413178958\n",
      "Epoch: 4314 Training Loss: 0.21726291155987268 Test Loss: 0.2137214601501938\n",
      "Epoch: 4315 Training Loss: 0.21726503804164674 Test Loss: 0.21374409312944675\n",
      "Epoch: 4316 Training Loss: 0.21726436761491422 Test Loss: 0.21360859339741292\n",
      "Epoch: 4317 Training Loss: 0.21726429874859707 Test Loss: 0.21374074873732346\n",
      "Epoch: 4318 Training Loss: 0.21726548454750028 Test Loss: 0.21374408016668658\n",
      "Epoch: 4319 Training Loss: 0.21726487780485199 Test Loss: 0.21374126724773018\n",
      "Epoch: 4320 Training Loss: 0.2172644533100398 Test Loss: 0.21370993625640464\n",
      "Epoch: 4321 Training Loss: 0.21726415298260507 Test Loss: 0.21360939708854332\n",
      "Epoch: 4322 Training Loss: 0.21726622841310844 Test Loss: 0.21373030075262822\n",
      "Epoch: 4323 Training Loss: 0.21726446157650828 Test Loss: 0.21384717299830094\n",
      "Epoch: 4324 Training Loss: 0.21726307176080423 Test Loss: 0.21409808018410853\n",
      "Epoch: 4325 Training Loss: 0.21726380703717507 Test Loss: 0.21392434030957974\n",
      "Epoch: 4326 Training Loss: 0.21726474633931253 Test Loss: 0.21386800415389057\n",
      "Epoch: 4327 Training Loss: 0.2172639401165587 Test Loss: 0.2137659094548091\n",
      "Epoch: 4328 Training Loss: 0.21726494080753533 Test Loss: 0.21371986573069315\n",
      "Epoch: 4329 Training Loss: 0.21726551018969104 Test Loss: 0.21374356165627986\n",
      "Epoch: 4330 Training Loss: 0.21726297576397335 Test Loss: 0.2137288618862496\n",
      "Epoch: 4331 Training Loss: 0.21726458274034244 Test Loss: 0.2137520004131491\n",
      "Epoch: 4332 Training Loss: 0.21726513188668536 Test Loss: 0.21375575961359775\n",
      "Epoch: 4333 Training Loss: 0.2172644589046996 Test Loss: 0.21439854400203753\n",
      "Epoch: 4334 Training Loss: 0.2172663194966804 Test Loss: 0.2137356673353377\n",
      "Epoch: 4335 Training Loss: 0.21726514104973396 Test Loss: 0.21373989319515238\n",
      "Epoch: 4336 Training Loss: 0.21726380133492565 Test Loss: 0.21376037435621748\n",
      "Epoch: 4337 Training Loss: 0.21726468786435868 Test Loss: 0.21396859517279257\n",
      "Epoch: 4338 Training Loss: 0.21726239347106427 Test Loss: 0.2143658130326139\n",
      "Epoch: 4339 Training Loss: 0.21726552556603967 Test Loss: 0.2139093553588258\n",
      "Epoch: 4340 Training Loss: 0.2172639254036793 Test Loss: 0.21538283230709754\n",
      "Epoch: 4341 Training Loss: 0.2172641029624015 Test Loss: 0.21375124857305935\n",
      "Epoch: 4342 Training Loss: 0.21726415924969994 Test Loss: 0.21369899568682305\n",
      "Epoch: 4343 Training Loss: 0.2172648964447522 Test Loss: 0.21362793383558326\n",
      "Epoch: 4344 Training Loss: 0.21726426608618415 Test Loss: 0.21365063162863704\n",
      "Epoch: 4345 Training Loss: 0.21726407316904492 Test Loss: 0.21368461998579696\n",
      "Epoch: 4346 Training Loss: 0.21726626316455297 Test Loss: 0.213813029088019\n",
      "Epoch: 4347 Training Loss: 0.21726437242058352 Test Loss: 0.21370703259812707\n",
      "Epoch: 4348 Training Loss: 0.21726639016512356 Test Loss: 0.2136991253144247\n",
      "Epoch: 4349 Training Loss: 0.21726589667847296 Test Loss: 0.21366909059911596\n",
      "Epoch: 4350 Training Loss: 0.21726411841047658 Test Loss: 0.21392120332161915\n",
      "Epoch: 4351 Training Loss: 0.21726735401562405 Test Loss: 0.2136977642246071\n",
      "Epoch: 4352 Training Loss: 0.217265055462198 Test Loss: 0.2138182271548463\n",
      "Epoch: 4353 Training Loss: 0.21726550411984377 Test Loss: 0.21372293790485292\n",
      "Epoch: 4354 Training Loss: 0.21726555241861356 Test Loss: 0.21365713893424126\n",
      "Epoch: 4355 Training Loss: 0.21726496871807371 Test Loss: 0.21377618892362218\n",
      "Epoch: 4356 Training Loss: 0.21726415815587222 Test Loss: 0.2137713149257991\n",
      "Epoch: 4357 Training Loss: 0.21726460921635266 Test Loss: 0.21396155639402145\n",
      "Epoch: 4358 Training Loss: 0.2172644561880619 Test Loss: 0.21378689616352076\n",
      "Epoch: 4359 Training Loss: 0.21726557894841858 Test Loss: 0.21372650266389906\n",
      "Epoch: 4360 Training Loss: 0.2172651837179807 Test Loss: 0.2138242159500438\n",
      "Epoch: 4361 Training Loss: 0.2172640411880329 Test Loss: 0.21402419245115217\n",
      "Epoch: 4362 Training Loss: 0.2172636523053821 Test Loss: 0.21400658902284433\n",
      "Epoch: 4363 Training Loss: 0.21726241654903594 Test Loss: 0.21380745510114685\n",
      "Epoch: 4364 Training Loss: 0.21726363249096198 Test Loss: 0.21372637303629738\n",
      "Epoch: 4365 Training Loss: 0.21726438219330657 Test Loss: 0.21375702996409418\n",
      "Epoch: 4366 Training Loss: 0.2172638299447965 Test Loss: 0.21386672084063396\n",
      "Epoch: 4367 Training Loss: 0.2172641514942821 Test Loss: 0.21378075181520123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4368 Training Loss: 0.21726214553977036 Test Loss: 0.21377137973959992\n",
      "Epoch: 4369 Training Loss: 0.21726462787418446 Test Loss: 0.2138453841373978\n",
      "Epoch: 4370 Training Loss: 0.21726505303246593 Test Loss: 0.21378334436723478\n",
      "Epoch: 4371 Training Loss: 0.21726455875682488 Test Loss: 0.213770537160189\n",
      "Epoch: 4372 Training Loss: 0.2172656995653389 Test Loss: 0.2138486377901999\n",
      "Epoch: 4373 Training Loss: 0.2172649359211738 Test Loss: 0.2136908421106775\n",
      "Epoch: 4374 Training Loss: 0.2172643684308021 Test Loss: 0.21385878763141128\n",
      "Epoch: 4375 Training Loss: 0.21726384413765942 Test Loss: 0.2140263572321002\n",
      "Epoch: 4376 Training Loss: 0.2172613168845825 Test Loss: 0.21382750849112642\n",
      "Epoch: 4377 Training Loss: 0.21726352041845 Test Loss: 0.21605072556198232\n",
      "Epoch: 4378 Training Loss: 0.217266272802789 Test Loss: 0.21389827219888233\n",
      "Epoch: 4379 Training Loss: 0.2172641393456218 Test Loss: 0.21371250288291785\n",
      "Epoch: 4380 Training Loss: 0.21726347339282398 Test Loss: 0.21374072281180312\n",
      "Epoch: 4381 Training Loss: 0.21726433732843858 Test Loss: 0.21372118793223027\n",
      "Epoch: 4382 Training Loss: 0.2172662128843412 Test Loss: 0.21371560098259795\n",
      "Epoch: 4383 Training Loss: 0.21726396682567975 Test Loss: 0.21372563415896784\n",
      "Epoch: 4384 Training Loss: 0.21726353751623242 Test Loss: 0.21374532459166268\n",
      "Epoch: 4385 Training Loss: 0.21726549095804798 Test Loss: 0.21377099085679488\n",
      "Epoch: 4386 Training Loss: 0.217266082037442 Test Loss: 0.21383426208917383\n",
      "Epoch: 4387 Training Loss: 0.21726454479707277 Test Loss: 0.2137725204624947\n",
      "Epoch: 4388 Training Loss: 0.21726509475930358 Test Loss: 0.21371558801983778\n",
      "Epoch: 4389 Training Loss: 0.21726545418033244 Test Loss: 0.21370825109758282\n",
      "Epoch: 4390 Training Loss: 0.21726322106828758 Test Loss: 0.21375136523790086\n",
      "Epoch: 4391 Training Loss: 0.21726470052406963 Test Loss: 0.21375906511744053\n",
      "Epoch: 4392 Training Loss: 0.21726546903666463 Test Loss: 0.21375784661798475\n",
      "Epoch: 4393 Training Loss: 0.21726591124789954 Test Loss: 0.21464541976943294\n",
      "Epoch: 4394 Training Loss: 0.21726232762622202 Test Loss: 0.21464230870699266\n",
      "Epoch: 4395 Training Loss: 0.21726286040101242 Test Loss: 0.21401268152012318\n",
      "Epoch: 4396 Training Loss: 0.21726633109842683 Test Loss: 0.21384157308590845\n",
      "Epoch: 4397 Training Loss: 0.2172613794031126 Test Loss: 0.21370647519943983\n",
      "Epoch: 4398 Training Loss: 0.2172635988243794 Test Loss: 0.21377355748330812\n",
      "Epoch: 4399 Training Loss: 0.2172644749265859 Test Loss: 0.21378693505180127\n",
      "Epoch: 4400 Training Loss: 0.21726362735355803 Test Loss: 0.21366740544029414\n",
      "Epoch: 4401 Training Loss: 0.21726477915414405 Test Loss: 0.21371726021589943\n",
      "Epoch: 4402 Training Loss: 0.21726509914358028 Test Loss: 0.21370823813482265\n",
      "Epoch: 4403 Training Loss: 0.21726457634772633 Test Loss: 0.21376148915359192\n",
      "Epoch: 4404 Training Loss: 0.21726438770727416 Test Loss: 0.2137695390276561\n",
      "Epoch: 4405 Training Loss: 0.21726529368352948 Test Loss: 0.21381901788321653\n",
      "Epoch: 4406 Training Loss: 0.21726410124993353 Test Loss: 0.21384704337069926\n",
      "Epoch: 4407 Training Loss: 0.21726331216979114 Test Loss: 0.21383107325017256\n",
      "Epoch: 4408 Training Loss: 0.2172648256776852 Test Loss: 0.21375435963549963\n",
      "Epoch: 4409 Training Loss: 0.21726438483821786 Test Loss: 0.2138096587703754\n",
      "Epoch: 4410 Training Loss: 0.21726491721851302 Test Loss: 0.21372963965185968\n",
      "Epoch: 4411 Training Loss: 0.217265122714671 Test Loss: 0.21379121276265664\n",
      "Epoch: 4412 Training Loss: 0.21726351888529805 Test Loss: 0.21374560977238638\n",
      "Epoch: 4413 Training Loss: 0.21726646875933475 Test Loss: 0.21374039874279893\n",
      "Epoch: 4414 Training Loss: 0.21726428752341426 Test Loss: 0.21388283355152252\n",
      "Epoch: 4415 Training Loss: 0.21726269714274274 Test Loss: 0.21570404950405517\n",
      "Epoch: 4416 Training Loss: 0.21726448890426958 Test Loss: 0.21380943840345254\n",
      "Epoch: 4417 Training Loss: 0.21726355986797422 Test Loss: 0.21372690450946427\n",
      "Epoch: 4418 Training Loss: 0.21726379175048444 Test Loss: 0.21372319716005628\n",
      "Epoch: 4419 Training Loss: 0.21726549614924673 Test Loss: 0.21375862438359483\n",
      "Epoch: 4420 Training Loss: 0.21726511321092198 Test Loss: 0.21375583739015874\n",
      "Epoch: 4421 Training Loss: 0.21726324151927956 Test Loss: 0.21380851804748063\n",
      "Epoch: 4422 Training Loss: 0.2172614036914674 Test Loss: 0.21380566624024372\n",
      "Epoch: 4423 Training Loss: 0.21726282187496568 Test Loss: 0.21366381475572768\n",
      "Epoch: 4424 Training Loss: 0.21726310631500112 Test Loss: 0.2137080566561803\n",
      "Epoch: 4425 Training Loss: 0.21726364129537853 Test Loss: 0.2136387058892827\n",
      "Epoch: 4426 Training Loss: 0.21726451052978152 Test Loss: 0.2146435012809281\n",
      "Epoch: 4427 Training Loss: 0.2172646683637416 Test Loss: 0.21370569743382978\n",
      "Epoch: 4428 Training Loss: 0.21726416329327616 Test Loss: 0.21368473665063847\n",
      "Epoch: 4429 Training Loss: 0.2172644056926709 Test Loss: 0.21367899414788413\n",
      "Epoch: 4430 Training Loss: 0.21726461681038609 Test Loss: 0.2137390376529813\n",
      "Epoch: 4431 Training Loss: 0.21726555171928108 Test Loss: 0.21374032096623793\n",
      "Epoch: 4432 Training Loss: 0.21726591423351124 Test Loss: 0.21375881882499734\n",
      "Epoch: 4433 Training Loss: 0.21726563588125467 Test Loss: 0.21376558538580492\n",
      "Epoch: 4434 Training Loss: 0.21726677904777436 Test Loss: 0.21379856264767177\n",
      "Epoch: 4435 Training Loss: 0.21726624024796573 Test Loss: 0.21377423154683683\n",
      "Epoch: 4436 Training Loss: 0.21726574196461163 Test Loss: 0.213871128179091\n",
      "Epoch: 4437 Training Loss: 0.21726341831590867 Test Loss: 0.21379051277360758\n",
      "Epoch: 4438 Training Loss: 0.21726601500911402 Test Loss: 0.21378723319528511\n",
      "Epoch: 4439 Training Loss: 0.21726464572509416 Test Loss: 0.2137222638413242\n",
      "Epoch: 4440 Training Loss: 0.2172658549516353 Test Loss: 0.2137906553639694\n",
      "Epoch: 4441 Training Loss: 0.21726600709231178 Test Loss: 0.21377607225878067\n",
      "Epoch: 4442 Training Loss: 0.21726408547908965 Test Loss: 0.21378067403864023\n",
      "Epoch: 4443 Training Loss: 0.21726676015683172 Test Loss: 0.21378445916460923\n",
      "Epoch: 4444 Training Loss: 0.21726402626894012 Test Loss: 0.21368700513366784\n",
      "Epoch: 4445 Training Loss: 0.21726390131257214 Test Loss: 0.2136415576965196\n",
      "Epoch: 4446 Training Loss: 0.21726445036029127 Test Loss: 0.21364971127266513\n",
      "Epoch: 4447 Training Loss: 0.21726441838824506 Test Loss: 0.21366833875902622\n",
      "Epoch: 4448 Training Loss: 0.21726352650622885 Test Loss: 0.21365030755963285\n",
      "Epoch: 4449 Training Loss: 0.21726299348936184 Test Loss: 0.21371132327174258\n",
      "Epoch: 4450 Training Loss: 0.21726141433387314 Test Loss: 0.2137074603692126\n",
      "Epoch: 4451 Training Loss: 0.21726209168220398 Test Loss: 0.21369286430126366\n",
      "Epoch: 4452 Training Loss: 0.2172627809819475 Test Loss: 0.2138469526313781\n",
      "Epoch: 4453 Training Loss: 0.21726224524844281 Test Loss: 0.21368272742281247\n",
      "Epoch: 4454 Training Loss: 0.21726369760060857 Test Loss: 0.21374357461904003\n",
      "Epoch: 4455 Training Loss: 0.2172635334098956 Test Loss: 0.21381192725340475\n",
      "Epoch: 4456 Training Loss: 0.2172632652158915 Test Loss: 0.21372542675480513\n",
      "Epoch: 4457 Training Loss: 0.2172632809329406 Test Loss: 0.21374357461904003\n",
      "Epoch: 4458 Training Loss: 0.21726267081018533 Test Loss: 0.21372326197385713\n",
      "Epoch: 4459 Training Loss: 0.2172654471690761 Test Loss: 0.21375595405500025\n",
      "Epoch: 4460 Training Loss: 0.21726290447688992 Test Loss: 0.2137810888469656\n",
      "Epoch: 4461 Training Loss: 0.21726493662050628 Test Loss: 0.21376138545151058\n",
      "Epoch: 4462 Training Loss: 0.21726496356273817 Test Loss: 0.2137462838359151\n",
      "Epoch: 4463 Training Loss: 0.2172641730928966 Test Loss: 0.2138623653532176\n",
      "Epoch: 4464 Training Loss: 0.21726308033210992 Test Loss: 0.21377302601014123\n",
      "Epoch: 4465 Training Loss: 0.2172647378307674 Test Loss: 0.21369746608112322\n",
      "Epoch: 4466 Training Loss: 0.21726419659226093 Test Loss: 0.2137215379267548\n",
      "Epoch: 4467 Training Loss: 0.21726470758912078 Test Loss: 0.21372716376466763\n",
      "Epoch: 4468 Training Loss: 0.21726533093643247 Test Loss: 0.21373692472307396\n",
      "Epoch: 4469 Training Loss: 0.21726399203751204 Test Loss: 0.2137178824283875\n",
      "Epoch: 4470 Training Loss: 0.217265214515507 Test Loss: 0.21376099656870554\n",
      "Epoch: 4471 Training Loss: 0.21726486904526446 Test Loss: 0.21374198019953938\n",
      "Epoch: 4472 Training Loss: 0.21726443892889505 Test Loss: 0.21379422012301555\n",
      "Epoch: 4473 Training Loss: 0.21726366076909823 Test Loss: 0.21373997097171338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4474 Training Loss: 0.21726528237765447 Test Loss: 0.2139530528233514\n",
      "Epoch: 4475 Training Loss: 0.21726127626950417 Test Loss: 0.2196018348216656\n",
      "Epoch: 4476 Training Loss: 0.21726357996033419 Test Loss: 0.21414602943396915\n",
      "Epoch: 4477 Training Loss: 0.21726549845345758 Test Loss: 0.2138707263335258\n",
      "Epoch: 4478 Training Loss: 0.21726448120264658 Test Loss: 0.21382124747796538\n",
      "Epoch: 4479 Training Loss: 0.21726461915942594 Test Loss: 0.21375897437811936\n",
      "Epoch: 4480 Training Loss: 0.21726366418506837 Test Loss: 0.21368817178208294\n",
      "Epoch: 4481 Training Loss: 0.21726570859390046 Test Loss: 0.21379249607591325\n",
      "Epoch: 4482 Training Loss: 0.21726358707918014 Test Loss: 0.21370666964084237\n",
      "Epoch: 4483 Training Loss: 0.21726249117139732 Test Loss: 0.21378782948225283\n",
      "Epoch: 4484 Training Loss: 0.21726462953285763 Test Loss: 0.21379284607043778\n",
      "Epoch: 4485 Training Loss: 0.2172645264261466 Test Loss: 0.21380278850748646\n",
      "Epoch: 4486 Training Loss: 0.21726592249101392 Test Loss: 0.2137252841644433\n",
      "Epoch: 4487 Training Loss: 0.21726556384104398 Test Loss: 0.21364912794845758\n",
      "Epoch: 4488 Training Loss: 0.21726545224371943 Test Loss: 0.21367601271304557\n",
      "Epoch: 4489 Training Loss: 0.21726346309111866 Test Loss: 0.21389145378703409\n",
      "Epoch: 4490 Training Loss: 0.21726588315804513 Test Loss: 0.21384141753278646\n",
      "Epoch: 4491 Training Loss: 0.21726360409627035 Test Loss: 0.21378751837600882\n",
      "Epoch: 4492 Training Loss: 0.2172638062930136 Test Loss: 0.2137687353365257\n",
      "Epoch: 4493 Training Loss: 0.21726510905975613 Test Loss: 0.2137338525489142\n",
      "Epoch: 4494 Training Loss: 0.21726443341492746 Test Loss: 0.2137426931513486\n",
      "Epoch: 4495 Training Loss: 0.21726392227461477 Test Loss: 0.21382741775180525\n",
      "Epoch: 4496 Training Loss: 0.21726386820186921 Test Loss: 0.21381261427969364\n",
      "Epoch: 4497 Training Loss: 0.21726571680657414 Test Loss: 0.2136933309606297\n",
      "Epoch: 4498 Training Loss: 0.21726490962447959 Test Loss: 0.21372321012281645\n",
      "Epoch: 4499 Training Loss: 0.21726538142285765 Test Loss: 0.21367478125082962\n",
      "Epoch: 4500 Training Loss: 0.21726415952763975 Test Loss: 0.21381139578023786\n",
      "Epoch: 4501 Training Loss: 0.21726432875713286 Test Loss: 0.21374231723130377\n",
      "Epoch: 4502 Training Loss: 0.21726455774368936 Test Loss: 0.21381265316797415\n",
      "Epoch: 4503 Training Loss: 0.21726439970351585 Test Loss: 0.21506030587136296\n",
      "Epoch: 4504 Training Loss: 0.2172641466168864 Test Loss: 0.21375658923024848\n",
      "Epoch: 4505 Training Loss: 0.21726398803876482 Test Loss: 0.21373911542954233\n",
      "Epoch: 4506 Training Loss: 0.21726429379947493 Test Loss: 0.21381896603217584\n",
      "Epoch: 4507 Training Loss: 0.21726520750425066 Test Loss: 0.21371026032540882\n",
      "Epoch: 4508 Training Loss: 0.21726348078064397 Test Loss: 0.21363365041281723\n",
      "Epoch: 4509 Training Loss: 0.21726475188017752 Test Loss: 0.21372577674932966\n",
      "Epoch: 4510 Training Loss: 0.21726578092998258 Test Loss: 0.21373434513380057\n",
      "Epoch: 4511 Training Loss: 0.21726512113669003 Test Loss: 0.21373361921923117\n",
      "Epoch: 4512 Training Loss: 0.21726574622336708 Test Loss: 0.21376641500245566\n",
      "Epoch: 4513 Training Loss: 0.2172656913526652 Test Loss: 0.21378356473415763\n",
      "Epoch: 4514 Training Loss: 0.21726436000294921 Test Loss: 0.2137052048489434\n",
      "Epoch: 4515 Training Loss: 0.2172645553856837 Test Loss: 0.21373133777344164\n",
      "Epoch: 4516 Training Loss: 0.2172634285996824 Test Loss: 0.21948470332078954\n",
      "Epoch: 4517 Training Loss: 0.21726377059119417 Test Loss: 0.21408561000882712\n",
      "Epoch: 4518 Training Loss: 0.21726592483108798 Test Loss: 0.21387952804767973\n",
      "Epoch: 4519 Training Loss: 0.2172632585632672 Test Loss: 0.21375559109771558\n",
      "Epoch: 4520 Training Loss: 0.2172638466391179 Test Loss: 0.2137861961744717\n",
      "Epoch: 4521 Training Loss: 0.2172644606978598 Test Loss: 0.21390873314633774\n",
      "Epoch: 4522 Training Loss: 0.21726376773110365 Test Loss: 0.21374344499143835\n",
      "Epoch: 4523 Training Loss: 0.2172634877201739 Test Loss: 0.21376085397834368\n",
      "Epoch: 4524 Training Loss: 0.21726363217715897 Test Loss: 0.2137528041042795\n",
      "Epoch: 4525 Training Loss: 0.21726645989215762 Test Loss: 0.21381906973425718\n",
      "Epoch: 4526 Training Loss: 0.21726614513874914 Test Loss: 0.2137514430144619\n",
      "Epoch: 4527 Training Loss: 0.21726466648988918 Test Loss: 0.21381873270249283\n",
      "Epoch: 4528 Training Loss: 0.21726457837399737 Test Loss: 0.21373269886325927\n",
      "Epoch: 4529 Training Loss: 0.21726644777039472 Test Loss: 0.2137184009387942\n",
      "Epoch: 4530 Training Loss: 0.21726611826824369 Test Loss: 0.21370752518301345\n",
      "Epoch: 4531 Training Loss: 0.2172644953596463 Test Loss: 0.21377972775714799\n",
      "Epoch: 4532 Training Loss: 0.21726411345238864 Test Loss: 0.21373545993117501\n",
      "Epoch: 4533 Training Loss: 0.21726494950436226 Test Loss: 0.21375776884142375\n",
      "Epoch: 4534 Training Loss: 0.21726483138890043 Test Loss: 0.21395687683760087\n",
      "Epoch: 4535 Training Loss: 0.21726386969019215 Test Loss: 0.2139877282068002\n",
      "Epoch: 4536 Training Loss: 0.21726489039283653 Test Loss: 0.2137241564043087\n",
      "Epoch: 4537 Training Loss: 0.21726645168844974 Test Loss: 0.21438499791766222\n",
      "Epoch: 4538 Training Loss: 0.21726160596890284 Test Loss: 0.21462846447913347\n",
      "Epoch: 4539 Training Loss: 0.21726382262870292 Test Loss: 0.21406466218839598\n",
      "Epoch: 4540 Training Loss: 0.21726261251454748 Test Loss: 0.21396689705121058\n",
      "Epoch: 4541 Training Loss: 0.2172619779151556 Test Loss: 0.21376892977792822\n",
      "Epoch: 4542 Training Loss: 0.21726499493407572 Test Loss: 0.21365783892329032\n",
      "Epoch: 4543 Training Loss: 0.21726548947869082 Test Loss: 0.21370761592233462\n",
      "Epoch: 4544 Training Loss: 0.21726472790562576 Test Loss: 0.2136901939726691\n",
      "Epoch: 4545 Training Loss: 0.21726487949042256 Test Loss: 0.21374207093886058\n",
      "Epoch: 4546 Training Loss: 0.21726551694990495 Test Loss: 0.21375772995314324\n",
      "Epoch: 4547 Training Loss: 0.21726581244477297 Test Loss: 0.21370502337030106\n",
      "Epoch: 4548 Training Loss: 0.2172653050790625 Test Loss: 0.21370606039111448\n",
      "Epoch: 4549 Training Loss: 0.21726380873171144 Test Loss: 0.21371378619617448\n",
      "Epoch: 4550 Training Loss: 0.2172650984173504 Test Loss: 0.21371283991468223\n",
      "Epoch: 4551 Training Loss: 0.21726345260113156 Test Loss: 0.21368844400004647\n",
      "Epoch: 4552 Training Loss: 0.21726431010826686 Test Loss: 0.21365065755415738\n",
      "Epoch: 4553 Training Loss: 0.21726515064314098 Test Loss: 0.2137626169137265\n",
      "Epoch: 4554 Training Loss: 0.2172643515123357 Test Loss: 0.21376924088417223\n",
      "Epoch: 4555 Training Loss: 0.21726427938246698 Test Loss: 0.21386290978914463\n",
      "Epoch: 4556 Training Loss: 0.21726311889401986 Test Loss: 0.21377983145922932\n",
      "Epoch: 4557 Training Loss: 0.21726612546778185 Test Loss: 0.21378715541872412\n",
      "Epoch: 4558 Training Loss: 0.21726263138755852 Test Loss: 0.21425568142222842\n",
      "Epoch: 4559 Training Loss: 0.21726678299272678 Test Loss: 0.21376475576915419\n",
      "Epoch: 4560 Training Loss: 0.21726346024895976 Test Loss: 0.2137390376529813\n",
      "Epoch: 4561 Training Loss: 0.2172655996414872 Test Loss: 0.21381179762580307\n",
      "Epoch: 4562 Training Loss: 0.2172622517845117 Test Loss: 0.2136664202705214\n",
      "Epoch: 4563 Training Loss: 0.2172657744925375 Test Loss: 0.2137836295479585\n",
      "Epoch: 4564 Training Loss: 0.21726349766324718 Test Loss: 0.213784057319044\n",
      "Epoch: 4565 Training Loss: 0.21726600583709965 Test Loss: 0.21385137293259532\n",
      "Epoch: 4566 Training Loss: 0.21726577879612197 Test Loss: 0.2138739540608076\n",
      "Epoch: 4567 Training Loss: 0.21726404645095806 Test Loss: 0.2138962759338165\n",
      "Epoch: 4568 Training Loss: 0.21726528036931506 Test Loss: 0.2138141179598731\n",
      "Epoch: 4569 Training Loss: 0.2172641887023561 Test Loss: 0.21383116398949373\n",
      "Epoch: 4570 Training Loss: 0.21726497624038071 Test Loss: 0.21371567875915898\n",
      "Epoch: 4571 Training Loss: 0.21726561761791813 Test Loss: 0.21371661207789105\n",
      "Epoch: 4572 Training Loss: 0.21726501397743694 Test Loss: 0.21380549772436153\n",
      "Epoch: 4573 Training Loss: 0.21726501005938192 Test Loss: 0.21378165920841297\n",
      "Epoch: 4574 Training Loss: 0.21726392658716503 Test Loss: 0.21367686825521662\n",
      "Epoch: 4575 Training Loss: 0.21726577852714793 Test Loss: 0.21371881574711957\n",
      "Epoch: 4576 Training Loss: 0.21726333847545115 Test Loss: 0.21381328834322236\n",
      "Epoch: 4577 Training Loss: 0.21726558179954328 Test Loss: 0.21392294033148163\n",
      "Epoch: 4578 Training Loss: 0.2172643120897089 Test Loss: 0.21373958208890836\n",
      "Epoch: 4579 Training Loss: 0.21726628538180776 Test Loss: 0.2137154324667158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4580 Training Loss: 0.21726536336573452 Test Loss: 0.21375938918644471\n",
      "Epoch: 4581 Training Loss: 0.21726481260554742 Test Loss: 0.2138080773136349\n",
      "Epoch: 4582 Training Loss: 0.21726367870966592 Test Loss: 0.21373063778439258\n",
      "Epoch: 4583 Training Loss: 0.21726477369397126 Test Loss: 0.21368968842502256\n",
      "Epoch: 4584 Training Loss: 0.21726584248020617 Test Loss: 0.2137516763441449\n",
      "Epoch: 4585 Training Loss: 0.21726569375549987 Test Loss: 0.21371886759816025\n",
      "Epoch: 4586 Training Loss: 0.21726514841962233 Test Loss: 0.21368900139873367\n",
      "Epoch: 4587 Training Loss: 0.21726618175508025 Test Loss: 0.21372441565951206\n",
      "Epoch: 4588 Training Loss: 0.21726433264829048 Test Loss: 0.21378705171664278\n",
      "Epoch: 4589 Training Loss: 0.21726532154923886 Test Loss: 0.2136475464917171\n",
      "Epoch: 4590 Training Loss: 0.2172653555834193 Test Loss: 0.21370156231333626\n",
      "Epoch: 4591 Training Loss: 0.21726504824472823 Test Loss: 0.2137493041590342\n",
      "Epoch: 4592 Training Loss: 0.2172642225661863 Test Loss: 0.2137427450023893\n",
      "Epoch: 4593 Training Loss: 0.2172637326927535 Test Loss: 0.21378846465750106\n",
      "Epoch: 4594 Training Loss: 0.21726628367830556 Test Loss: 0.2138628838636243\n",
      "Epoch: 4595 Training Loss: 0.21726495760048053 Test Loss: 0.2137538151995726\n",
      "Epoch: 4596 Training Loss: 0.21726371791711352 Test Loss: 0.21368977916434373\n",
      "Epoch: 4597 Training Loss: 0.21726628994540043 Test Loss: 0.21371391582377616\n",
      "Epoch: 4598 Training Loss: 0.2172648622402215 Test Loss: 0.2136975957087249\n",
      "Epoch: 4599 Training Loss: 0.2172647834487627 Test Loss: 0.2136707627951776\n",
      "Epoch: 4600 Training Loss: 0.21726585442265306 Test Loss: 0.21371609356748433\n",
      "Epoch: 4601 Training Loss: 0.21726146312576194 Test Loss: 0.21453734923791415\n",
      "Epoch: 4602 Training Loss: 0.21726926658235302 Test Loss: 0.21390358693055114\n",
      "Epoch: 4603 Training Loss: 0.21726554673429574 Test Loss: 0.2137909405446931\n",
      "Epoch: 4604 Training Loss: 0.21726361636148606 Test Loss: 0.21372671006806174\n",
      "Epoch: 4605 Training Loss: 0.21726600969239407 Test Loss: 0.21381336611978338\n",
      "Epoch: 4606 Training Loss: 0.2172640371982515 Test Loss: 0.21377471116896304\n",
      "Epoch: 4607 Training Loss: 0.21726482463765232 Test Loss: 0.2137614113770309\n",
      "Epoch: 4608 Training Loss: 0.2172645927372105 Test Loss: 0.21370485485441887\n",
      "Epoch: 4609 Training Loss: 0.21726493141137593 Test Loss: 0.21391670524384093\n",
      "Epoch: 4610 Training Loss: 0.2172634335308729 Test Loss: 0.21374859120722497\n",
      "Epoch: 4611 Training Loss: 0.2172620049918745 Test Loss: 0.21430458991634146\n",
      "Epoch: 4612 Training Loss: 0.21726739771493792 Test Loss: 0.21378562581302432\n",
      "Epoch: 4613 Training Loss: 0.21726457080686135 Test Loss: 0.2138716466894977\n",
      "Epoch: 4614 Training Loss: 0.21726177421215784 Test Loss: 0.21379161460822182\n",
      "Epoch: 4615 Training Loss: 0.21726427220086042 Test Loss: 0.21381442906611714\n",
      "Epoch: 4616 Training Loss: 0.2172648582414743 Test Loss: 0.21384443785590554\n",
      "Epoch: 4617 Training Loss: 0.2172639233863741 Test Loss: 0.21376641500245566\n",
      "Epoch: 4618 Training Loss: 0.21726382165143063 Test Loss: 0.2137887628009849\n",
      "Epoch: 4619 Training Loss: 0.21726402779312629 Test Loss: 0.2137981478393464\n",
      "Epoch: 4620 Training Loss: 0.21726432006927174 Test Loss: 0.21386104315168047\n",
      "Epoch: 4621 Training Loss: 0.2172648087323214 Test Loss: 0.21384380268065734\n",
      "Epoch: 4622 Training Loss: 0.2172637821212142 Test Loss: 0.2137663242631345\n",
      "Epoch: 4623 Training Loss: 0.21726632167537002 Test Loss: 0.21375270040219815\n",
      "Epoch: 4624 Training Loss: 0.21726392571748235 Test Loss: 0.21382736590076457\n",
      "Epoch: 4625 Training Loss: 0.2172640364630558 Test Loss: 0.21363108378630402\n",
      "Epoch: 4626 Training Loss: 0.2172662232219097 Test Loss: 0.21370659186428134\n",
      "Epoch: 4627 Training Loss: 0.2172644160571368 Test Loss: 0.21365393713247982\n",
      "Epoch: 4628 Training Loss: 0.21726622836827944 Test Loss: 0.21370650112496017\n",
      "Epoch: 4629 Training Loss: 0.21726365430475572 Test Loss: 0.21372461010091456\n",
      "Epoch: 4630 Training Loss: 0.21726439220810626 Test Loss: 0.213755007773508\n",
      "Epoch: 4631 Training Loss: 0.217264419652423 Test Loss: 0.21387598921415393\n",
      "Epoch: 4632 Training Loss: 0.21726470914917015 Test Loss: 0.2137827740057874\n",
      "Epoch: 4633 Training Loss: 0.21726432600463197 Test Loss: 0.213838293507586\n",
      "Epoch: 4634 Training Loss: 0.21726492408631654 Test Loss: 0.21358190307422745\n",
      "Epoch: 4635 Training Loss: 0.21726383336076668 Test Loss: 0.21372096756530742\n",
      "Epoch: 4636 Training Loss: 0.21726552569156088 Test Loss: 0.21379244422487256\n",
      "Epoch: 4637 Training Loss: 0.2172652030123844 Test Loss: 0.21372433788295106\n",
      "Epoch: 4638 Training Loss: 0.21726498980563758 Test Loss: 0.21376497613607703\n",
      "Epoch: 4639 Training Loss: 0.21726417122801 Test Loss: 0.21373606918090288\n",
      "Epoch: 4640 Training Loss: 0.21726542099790308 Test Loss: 0.2137712630747584\n",
      "Epoch: 4641 Training Loss: 0.21726110405439936 Test Loss: 0.21475625136886747\n",
      "Epoch: 4642 Training Loss: 0.21726833916886762 Test Loss: 0.21385686914290644\n",
      "Epoch: 4643 Training Loss: 0.21726532272375879 Test Loss: 0.21374973193011973\n",
      "Epoch: 4644 Training Loss: 0.21726405598160448 Test Loss: 0.2137277730143955\n",
      "Epoch: 4645 Training Loss: 0.2172647973905832 Test Loss: 0.21379366272432834\n",
      "Epoch: 4646 Training Loss: 0.2172659857716371 Test Loss: 0.2137525189235558\n",
      "Epoch: 4647 Training Loss: 0.21726490228148862 Test Loss: 0.2137486560210258\n",
      "Epoch: 4648 Training Loss: 0.2172645505351854 Test Loss: 0.21384157308590845\n",
      "Epoch: 4649 Training Loss: 0.21726529554841606 Test Loss: 0.2137521948545516\n",
      "Epoch: 4650 Training Loss: 0.21726684431880533 Test Loss: 0.21381567349109323\n",
      "Epoch: 4651 Training Loss: 0.21726543259068373 Test Loss: 0.21375443741206063\n",
      "Epoch: 4652 Training Loss: 0.2172658109026552 Test Loss: 0.21364770204483913\n",
      "Epoch: 4653 Training Loss: 0.21726616862018186 Test Loss: 0.2136940568751991\n",
      "Epoch: 4654 Training Loss: 0.2172650572374266 Test Loss: 0.21376605204517096\n",
      "Epoch: 4655 Training Loss: 0.21726515705368865 Test Loss: 0.21367281091128412\n",
      "Epoch: 4656 Training Loss: 0.21726463340608365 Test Loss: 0.21366622582911887\n",
      "Epoch: 4657 Training Loss: 0.21726592466073777 Test Loss: 0.21374767085125304\n",
      "Epoch: 4658 Training Loss: 0.2172640450164299 Test Loss: 0.21383647872116252\n",
      "Epoch: 4659 Training Loss: 0.2172652463351346 Test Loss: 0.213853589564584\n",
      "Epoch: 4660 Training Loss: 0.21726311707396226 Test Loss: 0.21403235899005787\n",
      "Epoch: 4661 Training Loss: 0.21726505410836205 Test Loss: 0.21389884256032973\n",
      "Epoch: 4662 Training Loss: 0.21726511936146142 Test Loss: 0.2138439711965395\n",
      "Epoch: 4663 Training Loss: 0.21726317295779965 Test Loss: 0.21378234623470188\n",
      "Epoch: 4664 Training Loss: 0.21726343684821928 Test Loss: 0.21377517782832908\n",
      "Epoch: 4665 Training Loss: 0.21726579913952435 Test Loss: 0.21376755572535042\n",
      "Epoch: 4666 Training Loss: 0.21726546726143603 Test Loss: 0.21371586023780131\n",
      "Epoch: 4667 Training Loss: 0.21726435251650542 Test Loss: 0.21364001512805963\n",
      "Epoch: 4668 Training Loss: 0.21726586211531027 Test Loss: 0.2137847443453329\n",
      "Epoch: 4669 Training Loss: 0.21726545818804546 Test Loss: 0.21363875774032334\n",
      "Epoch: 4670 Training Loss: 0.2172673685043584 Test Loss: 0.21368601996389508\n",
      "Epoch: 4671 Training Loss: 0.21726469784329513 Test Loss: 0.2137167935565334\n",
      "Epoch: 4672 Training Loss: 0.21726414580099848 Test Loss: 0.21366727581269246\n",
      "Epoch: 4673 Training Loss: 0.2172651414442292 Test Loss: 0.21376649277901666\n",
      "Epoch: 4674 Training Loss: 0.21726528432323328 Test Loss: 0.21372879707244877\n",
      "Epoch: 4675 Training Loss: 0.2172655147443179 Test Loss: 0.2145122662969895\n",
      "Epoch: 4676 Training Loss: 0.21726485116745733 Test Loss: 0.21368622736805776\n",
      "Epoch: 4677 Training Loss: 0.21726560032288808 Test Loss: 0.21377510005176809\n",
      "Epoch: 4678 Training Loss: 0.21726388651003475 Test Loss: 0.2136547537863704\n",
      "Epoch: 4679 Training Loss: 0.21726508060230387 Test Loss: 0.21372702117430578\n",
      "Epoch: 4680 Training Loss: 0.21726584490097242 Test Loss: 0.21378329251619413\n",
      "Epoch: 4681 Training Loss: 0.21726058454002858 Test Loss: 0.21474667188910349\n",
      "Epoch: 4682 Training Loss: 0.21726637344390476 Test Loss: 0.21390872018357757\n",
      "Epoch: 4683 Training Loss: 0.21726478777027877 Test Loss: 0.21390052771915152\n",
      "Epoch: 4684 Training Loss: 0.2172651105749765 Test Loss: 0.21380028669477408\n",
      "Epoch: 4685 Training Loss: 0.21726378061495963 Test Loss: 0.21381541423588987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4686 Training Loss: 0.21726431909199942 Test Loss: 0.21379459604306042\n",
      "Epoch: 4687 Training Loss: 0.21726456177829978 Test Loss: 0.2137037530198046\n",
      "Epoch: 4688 Training Loss: 0.21726670909659526 Test Loss: 0.2137814518042503\n",
      "Epoch: 4689 Training Loss: 0.2172636969192077 Test Loss: 0.21389111675526973\n",
      "Epoch: 4690 Training Loss: 0.21726480636534995 Test Loss: 0.2138291677244279\n",
      "Epoch: 4691 Training Loss: 0.21726197570956857 Test Loss: 0.2144037939199055\n",
      "Epoch: 4692 Training Loss: 0.21726601791403355 Test Loss: 0.21387671512872333\n",
      "Epoch: 4693 Training Loss: 0.21726330282742654 Test Loss: 0.2137862739510327\n",
      "Epoch: 4694 Training Loss: 0.2172651666739931 Test Loss: 0.2137886850244239\n",
      "Epoch: 4695 Training Loss: 0.21726551527330018 Test Loss: 0.2137751648655689\n",
      "Epoch: 4696 Training Loss: 0.21726742662068022 Test Loss: 0.2137931960649623\n",
      "Epoch: 4697 Training Loss: 0.21726600961170184 Test Loss: 0.21373124703412047\n",
      "Epoch: 4698 Training Loss: 0.2172647591962711 Test Loss: 0.21374010059931506\n",
      "Epoch: 4699 Training Loss: 0.21726567178032172 Test Loss: 0.21381155133335988\n",
      "Epoch: 4700 Training Loss: 0.2172649714347114 Test Loss: 0.213754696667264\n",
      "Epoch: 4701 Training Loss: 0.21726502789236005 Test Loss: 0.21370148453677526\n",
      "Epoch: 4702 Training Loss: 0.21726496126749312 Test Loss: 0.21373544696841484\n",
      "Epoch: 4703 Training Loss: 0.21726388254715073 Test Loss: 0.21370497151926038\n",
      "Epoch: 4704 Training Loss: 0.21726349682046187 Test Loss: 0.21373981541859138\n",
      "Epoch: 4705 Training Loss: 0.21726420538771166 Test Loss: 0.21395231394602182\n",
      "Epoch: 4706 Training Loss: 0.2172658892996188 Test Loss: 0.21387729845293088\n",
      "Epoch: 4707 Training Loss: 0.21726417430327974 Test Loss: 0.21377478894552404\n",
      "Epoch: 4708 Training Loss: 0.21726429977966416 Test Loss: 0.21373447476140225\n",
      "Epoch: 4709 Training Loss: 0.2172626068212639 Test Loss: 0.21416259584146358\n",
      "Epoch: 4710 Training Loss: 0.21726211744095014 Test Loss: 0.21379262570351493\n",
      "Epoch: 4711 Training Loss: 0.21726119264547863 Test Loss: 0.2137748019082842\n",
      "Epoch: 4712 Training Loss: 0.2172637692552898 Test Loss: 0.21365516859469577\n",
      "Epoch: 4713 Training Loss: 0.21726357718093586 Test Loss: 0.2135931936383336\n",
      "Epoch: 4714 Training Loss: 0.21726237370147314 Test Loss: 0.21380329405513301\n",
      "Epoch: 4715 Training Loss: 0.21726375945566936 Test Loss: 0.21374181168365722\n",
      "Epoch: 4716 Training Loss: 0.21726419781160985 Test Loss: 0.21373704138791547\n",
      "Epoch: 4717 Training Loss: 0.2172633624769003 Test Loss: 0.21376195581295795\n",
      "Epoch: 4718 Training Loss: 0.21726354593511954 Test Loss: 0.21370034381388048\n",
      "Epoch: 4719 Training Loss: 0.21726534682383175 Test Loss: 0.2137266063659804\n",
      "Epoch: 4720 Training Loss: 0.21726403990592336 Test Loss: 0.21373507104836997\n",
      "Epoch: 4721 Training Loss: 0.21726397297621922 Test Loss: 0.21367966821141288\n",
      "Epoch: 4722 Training Loss: 0.21726547644241623 Test Loss: 0.21373455253796325\n",
      "Epoch: 4723 Training Loss: 0.21726494307588298 Test Loss: 0.2137216545915963\n",
      "Epoch: 4724 Training Loss: 0.21726460092298677 Test Loss: 0.2137263600735372\n",
      "Epoch: 4725 Training Loss: 0.2172652125968256 Test Loss: 0.21448532968136083\n",
      "Epoch: 4726 Training Loss: 0.2172646994123103 Test Loss: 0.21366757395617633\n",
      "Epoch: 4727 Training Loss: 0.21726612162145323 Test Loss: 0.2137478134416149\n",
      "Epoch: 4728 Training Loss: 0.2172643580663362 Test Loss: 0.21392615509600324\n",
      "Epoch: 4729 Training Loss: 0.21726471733494643 Test Loss: 0.21376163174395377\n",
      "Epoch: 4730 Training Loss: 0.2172639372475024 Test Loss: 0.21373358033095066\n",
      "Epoch: 4731 Training Loss: 0.21726436795561466 Test Loss: 0.2137192046299246\n",
      "Epoch: 4732 Training Loss: 0.2172645394982844 Test Loss: 0.21381144763127855\n",
      "Epoch: 4733 Training Loss: 0.21726345167765404 Test Loss: 0.21373963393994902\n",
      "Epoch: 4734 Training Loss: 0.21726690308066482 Test Loss: 0.21379091461917277\n",
      "Epoch: 4735 Training Loss: 0.21726467251490744 Test Loss: 0.21375417815685727\n",
      "Epoch: 4736 Training Loss: 0.21726557164129082 Test Loss: 0.21415021640550336\n",
      "Epoch: 4737 Training Loss: 0.21726554138171256 Test Loss: 0.21373868765845677\n",
      "Epoch: 4738 Training Loss: 0.21726547298161705 Test Loss: 0.2137594799257659\n",
      "Epoch: 4739 Training Loss: 0.21726653767048093 Test Loss: 0.21376570205064643\n",
      "Epoch: 4740 Training Loss: 0.21726552518947603 Test Loss: 0.21375017266396543\n",
      "Epoch: 4741 Training Loss: 0.21726426860557424 Test Loss: 0.21367262943264176\n",
      "Epoch: 4742 Training Loss: 0.2172659840860665 Test Loss: 0.21374428757084926\n",
      "Epoch: 4743 Training Loss: 0.2172654883848631 Test Loss: 0.2137943238250969\n",
      "Epoch: 4744 Training Loss: 0.2172649723671547 Test Loss: 0.21375040599364845\n",
      "Epoch: 4745 Training Loss: 0.21726515427429036 Test Loss: 0.2137756055994146\n",
      "Epoch: 4746 Training Loss: 0.21726429393396193 Test Loss: 0.2137279415302777\n",
      "Epoch: 4747 Training Loss: 0.21726476514956294 Test Loss: 0.21369504204497186\n",
      "Epoch: 4748 Training Loss: 0.21726384660325468 Test Loss: 0.21386640973438995\n",
      "Epoch: 4749 Training Loss: 0.21726581206820933 Test Loss: 0.21682083018079007\n",
      "Epoch: 4750 Training Loss: 0.2172651451740024 Test Loss: 0.21374619309659393\n",
      "Epoch: 4751 Training Loss: 0.21726571122984595 Test Loss: 0.2137070196353669\n",
      "Epoch: 4752 Training Loss: 0.21726425252989312 Test Loss: 0.21368257186969045\n",
      "Epoch: 4753 Training Loss: 0.21726515981515535 Test Loss: 0.21371127142070193\n",
      "Epoch: 4754 Training Loss: 0.2172652152148395 Test Loss: 0.21375044488192896\n",
      "Epoch: 4755 Training Loss: 0.21726351586382311 Test Loss: 0.2137874017111673\n",
      "Epoch: 4756 Training Loss: 0.21726416176909 Test Loss: 0.21375224670559229\n",
      "Epoch: 4757 Training Loss: 0.21726387411929785 Test Loss: 0.21378915168378995\n",
      "Epoch: 4758 Training Loss: 0.21726304129501256 Test Loss: 0.21378318881411276\n",
      "Epoch: 4759 Training Loss: 0.2172645218177249 Test Loss: 0.2137662464865735\n",
      "Epoch: 4760 Training Loss: 0.21726525890518755 Test Loss: 0.21373466920280476\n",
      "Epoch: 4761 Training Loss: 0.21726732805963028 Test Loss: 0.21471027245855234\n",
      "Epoch: 4762 Training Loss: 0.21726429868583644 Test Loss: 0.213729587800819\n",
      "Epoch: 4763 Training Loss: 0.2172647552064897 Test Loss: 0.21378661098279708\n",
      "Epoch: 4764 Training Loss: 0.21726440345122067 Test Loss: 0.21385277291069343\n",
      "Epoch: 4765 Training Loss: 0.21726565062999725 Test Loss: 0.21369557351813873\n",
      "Epoch: 4766 Training Loss: 0.21726469345005267 Test Loss: 0.21364868721461186\n",
      "Epoch: 4767 Training Loss: 0.21726501679269847 Test Loss: 0.2137274489453913\n",
      "Epoch: 4768 Training Loss: 0.21726593467553743 Test Loss: 0.21375784661798475\n",
      "Epoch: 4769 Training Loss: 0.21726573535681631 Test Loss: 0.21404411621353006\n",
      "Epoch: 4770 Training Loss: 0.2172622712402998 Test Loss: 0.21378847762026124\n",
      "Epoch: 4771 Training Loss: 0.21726582728317353 Test Loss: 0.21373190813488904\n",
      "Epoch: 4772 Training Loss: 0.21726394805129254 Test Loss: 0.21381432536403577\n",
      "Epoch: 4773 Training Loss: 0.2172635240944284 Test Loss: 0.2137126454732797\n",
      "Epoch: 4774 Training Loss: 0.21726566738707925 Test Loss: 0.21377913147018027\n",
      "Epoch: 4775 Training Loss: 0.21726473369753319 Test Loss: 0.21369125691900287\n",
      "Epoch: 4776 Training Loss: 0.21726455604018718 Test Loss: 0.21372807115787937\n",
      "Epoch: 4777 Training Loss: 0.21726510448719763 Test Loss: 0.21372438973399172\n",
      "Epoch: 4778 Training Loss: 0.2172638502613015 Test Loss: 0.21368402369882925\n",
      "Epoch: 4779 Training Loss: 0.21726527234492318 Test Loss: 0.21372848596620472\n",
      "Epoch: 4780 Training Loss: 0.2172661035195011 Test Loss: 0.21368630514461878\n",
      "Epoch: 4781 Training Loss: 0.21726440130839422 Test Loss: 0.21411744654779918\n",
      "Epoch: 4782 Training Loss: 0.21726478836202162 Test Loss: 0.21425492958213868\n",
      "Epoch: 4783 Training Loss: 0.21726625152694332 Test Loss: 0.2141031615860943\n",
      "Epoch: 4784 Training Loss: 0.21726405062902132 Test Loss: 0.2138200289785096\n",
      "Epoch: 4785 Training Loss: 0.217263785958577 Test Loss: 0.21380194592807555\n",
      "Epoch: 4786 Training Loss: 0.21726498985943238 Test Loss: 0.21380284035852712\n",
      "Epoch: 4787 Training Loss: 0.21726403147807047 Test Loss: 0.2136835570394632\n",
      "Epoch: 4788 Training Loss: 0.21726654935291959 Test Loss: 0.21374875972310714\n",
      "Epoch: 4789 Training Loss: 0.21726463650825079 Test Loss: 0.21371024736264868\n",
      "Epoch: 4790 Training Loss: 0.21726493996475005 Test Loss: 0.2137396080144287\n",
      "Epoch: 4791 Training Loss: 0.2172654801363262 Test Loss: 0.21376842423028167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4792 Training Loss: 0.21726368290566078 Test Loss: 0.21369465316216682\n",
      "Epoch: 4793 Training Loss: 0.21726605534625254 Test Loss: 0.213762655802007\n",
      "Epoch: 4794 Training Loss: 0.21726417558538927 Test Loss: 0.21387421331601095\n",
      "Epoch: 4795 Training Loss: 0.21726424150195794 Test Loss: 0.2137960089839187\n",
      "Epoch: 4796 Training Loss: 0.21726426420336595 Test Loss: 0.21379847190835058\n",
      "Epoch: 4797 Training Loss: 0.21726514004556424 Test Loss: 0.21377543708353244\n",
      "Epoch: 4798 Training Loss: 0.2172647983947529 Test Loss: 0.21375490407142667\n",
      "Epoch: 4799 Training Loss: 0.21726433731050698 Test Loss: 0.21395862681022354\n",
      "Epoch: 4800 Training Loss: 0.2172632419137748 Test Loss: 0.21375216892903126\n",
      "Epoch: 4801 Training Loss: 0.2172635156934729 Test Loss: 0.21373170073072634\n",
      "Epoch: 4802 Training Loss: 0.21726490843202806 Test Loss: 0.2138032551668525\n",
      "Epoch: 4803 Training Loss: 0.21726373772256785 Test Loss: 0.21374299129483249\n",
      "Epoch: 4804 Training Loss: 0.2172631829905309 Test Loss: 0.21384346564889295\n",
      "Epoch: 4805 Training Loss: 0.21726453217322503 Test Loss: 0.21383211027098598\n",
      "Epoch: 4806 Training Loss: 0.21726319088043575 Test Loss: 0.21390812389660985\n",
      "Epoch: 4807 Training Loss: 0.21726501363673653 Test Loss: 0.21384919518888712\n",
      "Epoch: 4808 Training Loss: 0.2172633249460575 Test Loss: 0.21376892977792822\n",
      "Epoch: 4809 Training Loss: 0.21726584192432652 Test Loss: 0.21370208082374295\n",
      "Epoch: 4810 Training Loss: 0.21726586442848692 Test Loss: 0.2137180379815095\n",
      "Epoch: 4811 Training Loss: 0.21726549631063113 Test Loss: 0.21371736391798077\n",
      "Epoch: 4812 Training Loss: 0.21726511764899345 Test Loss: 0.21370310488179622\n",
      "Epoch: 4813 Training Loss: 0.21726590924852593 Test Loss: 0.21377101678231522\n",
      "Epoch: 4814 Training Loss: 0.21726480054654512 Test Loss: 0.2137567447833705\n",
      "Epoch: 4815 Training Loss: 0.21726362033333588 Test Loss: 0.21382282893470586\n",
      "Epoch: 4816 Training Loss: 0.21726475222984376 Test Loss: 0.21375731514481788\n",
      "Epoch: 4817 Training Loss: 0.21726406746679552 Test Loss: 0.21382229746153897\n",
      "Epoch: 4818 Training Loss: 0.21726340941286834 Test Loss: 0.21379900338151747\n",
      "Epoch: 4819 Training Loss: 0.21726597381125862 Test Loss: 0.213789696119717\n",
      "Epoch: 4820 Training Loss: 0.2172656164613298 Test Loss: 0.21374214871542158\n",
      "Epoch: 4821 Training Loss: 0.2172637813680869 Test Loss: 0.21377545004629261\n",
      "Epoch: 4822 Training Loss: 0.21726510107122748 Test Loss: 0.2137169491096554\n",
      "Epoch: 4823 Training Loss: 0.21726540411529988 Test Loss: 0.21371366953133297\n",
      "Epoch: 4824 Training Loss: 0.21726514401741406 Test Loss: 0.21373727471759849\n",
      "Epoch: 4825 Training Loss: 0.21726583916285983 Test Loss: 0.2138172549478337\n",
      "Epoch: 4826 Training Loss: 0.21726530914057032 Test Loss: 0.2138510488635911\n",
      "Epoch: 4827 Training Loss: 0.2172641337330304 Test Loss: 0.21457710602334873\n",
      "Epoch: 4828 Training Loss: 0.2172644778584028 Test Loss: 0.21375337446572687\n",
      "Epoch: 4829 Training Loss: 0.21726555042820572 Test Loss: 0.21374019133863625\n",
      "Epoch: 4830 Training Loss: 0.21726493271141706 Test Loss: 0.21368122374263301\n",
      "Epoch: 4831 Training Loss: 0.2172647974623096 Test Loss: 0.21370175675473876\n",
      "Epoch: 4832 Training Loss: 0.21726449297474323 Test Loss: 0.21369947530894923\n",
      "Epoch: 4833 Training Loss: 0.2172643319399922 Test Loss: 0.21369578092230143\n",
      "Epoch: 4834 Training Loss: 0.21726313773116768 Test Loss: 0.21766807618535627\n",
      "Epoch: 4835 Training Loss: 0.2172671789493945 Test Loss: 0.2137924312621124\n",
      "Epoch: 4836 Training Loss: 0.21726485405444526 Test Loss: 0.21373592659054105\n",
      "Epoch: 4837 Training Loss: 0.21726475864039144 Test Loss: 0.21368478850167916\n",
      "Epoch: 4838 Training Loss: 0.21726546470618277 Test Loss: 0.21368936435601837\n",
      "Epoch: 4839 Training Loss: 0.21726554558667321 Test Loss: 0.21362875048947383\n",
      "Epoch: 4840 Training Loss: 0.2172649797729063 Test Loss: 0.21394225484413162\n",
      "Epoch: 4841 Training Loss: 0.21726377702863925 Test Loss: 0.21439254224407986\n",
      "Epoch: 4842 Training Loss: 0.21726509176472608 Test Loss: 0.21385006369381837\n",
      "Epoch: 4843 Training Loss: 0.21726524369918912 Test Loss: 0.2138133920453037\n",
      "Epoch: 4844 Training Loss: 0.217262879202297 Test Loss: 0.21396212675546883\n",
      "Epoch: 4845 Training Loss: 0.217264412085287 Test Loss: 0.21373470809108527\n",
      "Epoch: 4846 Training Loss: 0.21726381680989812 Test Loss: 0.21379379235193002\n",
      "Epoch: 4847 Training Loss: 0.21726457453663456 Test Loss: 0.21369574203402092\n",
      "Epoch: 4848 Training Loss: 0.21726342625064254 Test Loss: 0.21374287462999098\n",
      "Epoch: 4849 Training Loss: 0.2172633587740245 Test Loss: 0.2137177139125053\n",
      "Epoch: 4850 Training Loss: 0.2172635325940077 Test Loss: 0.21367135908214532\n",
      "Epoch: 4851 Training Loss: 0.2172646748729131 Test Loss: 0.21372039720386005\n",
      "Epoch: 4852 Training Loss: 0.21726447740114696 Test Loss: 0.21373613399470373\n",
      "Epoch: 4853 Training Loss: 0.2172656016229292 Test Loss: 0.21379810895106588\n",
      "Epoch: 4854 Training Loss: 0.21726498932148433 Test Loss: 0.21373793581836706\n",
      "Epoch: 4855 Training Loss: 0.21726375461413686 Test Loss: 0.21367762009530636\n",
      "Epoch: 4856 Training Loss: 0.21726487871039787 Test Loss: 0.2146267663575515\n",
      "Epoch: 4857 Training Loss: 0.2172604427458864 Test Loss: 0.21492895422258282\n",
      "Epoch: 4858 Training Loss: 0.21726377230366214 Test Loss: 0.21434699110485028\n",
      "Epoch: 4859 Training Loss: 0.21726425976529448 Test Loss: 0.21390872018357757\n",
      "Epoch: 4860 Training Loss: 0.21726706408851845 Test Loss: 0.21377302601014123\n",
      "Epoch: 4861 Training Loss: 0.21726630634385038 Test Loss: 0.21372712487638712\n",
      "Epoch: 4862 Training Loss: 0.21726585522957514 Test Loss: 0.2138066773355368\n",
      "Epoch: 4863 Training Loss: 0.21726448949601246 Test Loss: 0.21380102557210365\n",
      "Epoch: 4864 Training Loss: 0.2172644974935069 Test Loss: 0.21373984134411173\n",
      "Epoch: 4865 Training Loss: 0.21726470971401562 Test Loss: 0.21371563987087847\n",
      "Epoch: 4866 Training Loss: 0.21726499033461982 Test Loss: 0.2137635113441781\n",
      "Epoch: 4867 Training Loss: 0.21726544467658343 Test Loss: 0.21373820803633056\n",
      "Epoch: 4868 Training Loss: 0.21726534852733395 Test Loss: 0.213669479481921\n",
      "Epoch: 4869 Training Loss: 0.21726404573369398 Test Loss: 0.21369701238451735\n",
      "Epoch: 4870 Training Loss: 0.21726591595494504 Test Loss: 0.21380234777364077\n",
      "Epoch: 4871 Training Loss: 0.21726518597736255 Test Loss: 0.21365698338111924\n",
      "Epoch: 4872 Training Loss: 0.21726495569076493 Test Loss: 0.21377458154136136\n",
      "Epoch: 4873 Training Loss: 0.21726441393224197 Test Loss: 0.21376143730255123\n",
      "Epoch: 4874 Training Loss: 0.2172642299540063 Test Loss: 0.21384086013409923\n",
      "Epoch: 4875 Training Loss: 0.21726447424518502 Test Loss: 0.21373866173293646\n",
      "Epoch: 4876 Training Loss: 0.21726435201442054 Test Loss: 0.21375728921929754\n",
      "Epoch: 4877 Training Loss: 0.2172657118215888 Test Loss: 0.21371509543495143\n",
      "Epoch: 4878 Training Loss: 0.21726587964345115 Test Loss: 0.21373037852918922\n",
      "Epoch: 4879 Training Loss: 0.2172644273002512 Test Loss: 0.21372188792127933\n",
      "Epoch: 4880 Training Loss: 0.217265479965976 Test Loss: 0.21370998810744532\n",
      "Epoch: 4881 Training Loss: 0.21726717235056503 Test Loss: 0.2136328207961665\n",
      "Epoch: 4882 Training Loss: 0.21726598714340464 Test Loss: 0.2136623499638287\n",
      "Epoch: 4883 Training Loss: 0.21726517779158627 Test Loss: 0.21369138654660455\n",
      "Epoch: 4884 Training Loss: 0.21726618027572311 Test Loss: 0.21386386903339705\n",
      "Epoch: 4885 Training Loss: 0.21726346851542824 Test Loss: 0.21378129625112827\n",
      "Epoch: 4886 Training Loss: 0.21726573961557177 Test Loss: 0.21371770094974515\n",
      "Epoch: 4887 Training Loss: 0.21726448599935008 Test Loss: 0.213704401157813\n",
      "Epoch: 4888 Training Loss: 0.21726614790918164 Test Loss: 0.2137727926804582\n",
      "Epoch: 4889 Training Loss: 0.2172637035180372 Test Loss: 0.2137796370178268\n",
      "Epoch: 4890 Training Loss: 0.2172656658628931 Test Loss: 0.21429795298313556\n",
      "Epoch: 4891 Training Loss: 0.21726454497638878 Test Loss: 0.2138132753804622\n",
      "Epoch: 4892 Training Loss: 0.2172649619309624 Test Loss: 0.2137450782992195\n",
      "Epoch: 4893 Training Loss: 0.21726409944780753 Test Loss: 0.21378150365529097\n",
      "Epoch: 4894 Training Loss: 0.2172656771956655 Test Loss: 0.21382814366637465\n",
      "Epoch: 4895 Training Loss: 0.21726393057694648 Test Loss: 0.21367632381928958\n",
      "Epoch: 4896 Training Loss: 0.21726431581948208 Test Loss: 0.21368980508986407\n",
      "Epoch: 4897 Training Loss: 0.2172651304431914 Test Loss: 0.21380652178241477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4898 Training Loss: 0.21726545965843683 Test Loss: 0.2137950886279468\n",
      "Epoch: 4899 Training Loss: 0.21726613085622823 Test Loss: 0.21374799492025726\n",
      "Epoch: 4900 Training Loss: 0.2172647264710976 Test Loss: 0.213812873534897\n",
      "Epoch: 4901 Training Loss: 0.21726508959500224 Test Loss: 0.21374163020501485\n",
      "Epoch: 4902 Training Loss: 0.2172657894116303 Test Loss: 0.21370165305265743\n",
      "Epoch: 4903 Training Loss: 0.21726734869890407 Test Loss: 0.21375804105938728\n",
      "Epoch: 4904 Training Loss: 0.21726533572417017 Test Loss: 0.21371833612499336\n",
      "Epoch: 4905 Training Loss: 0.2172660670735202 Test Loss: 0.213853511788023\n",
      "Epoch: 4906 Training Loss: 0.21726504958063256 Test Loss: 0.21376478169467453\n",
      "Epoch: 4907 Training Loss: 0.21726518144066728 Test Loss: 0.21385107478911144\n",
      "Epoch: 4908 Training Loss: 0.2172638456259824 Test Loss: 0.2138264325820325\n",
      "Epoch: 4909 Training Loss: 0.2172630836404905 Test Loss: 0.21373141555000266\n",
      "Epoch: 4910 Training Loss: 0.21726549344157484 Test Loss: 0.21374618013383376\n",
      "Epoch: 4911 Training Loss: 0.217265173389378 Test Loss: 0.21376281135512903\n",
      "Epoch: 4912 Training Loss: 0.21726512058081035 Test Loss: 0.2137089899749124\n",
      "Epoch: 4913 Training Loss: 0.2172664736456963 Test Loss: 0.21367979783901456\n",
      "Epoch: 4914 Training Loss: 0.21726521882805727 Test Loss: 0.21369261800882047\n",
      "Epoch: 4915 Training Loss: 0.21726576586743698 Test Loss: 0.2136505668148362\n",
      "Epoch: 4916 Training Loss: 0.21726686404356743 Test Loss: 0.21374641346351678\n",
      "Epoch: 4917 Training Loss: 0.21726492688364643 Test Loss: 0.2137334118150685\n",
      "Epoch: 4918 Training Loss: 0.21726606197197945 Test Loss: 0.2137115954897061\n",
      "Epoch: 4919 Training Loss: 0.2172665927294646 Test Loss: 0.21377944257642428\n",
      "Epoch: 4920 Training Loss: 0.21726446171996108 Test Loss: 0.21370743444369225\n",
      "Epoch: 4921 Training Loss: 0.2172663653388207 Test Loss: 0.21384284343640492\n",
      "Epoch: 4922 Training Loss: 0.21726606932393625 Test Loss: 0.21380274961920595\n",
      "Epoch: 4923 Training Loss: 0.21726544776081896 Test Loss: 0.21373877839777797\n",
      "Epoch: 4924 Training Loss: 0.21726549790654373 Test Loss: 0.2137022234141048\n",
      "Epoch: 4925 Training Loss: 0.21726615370108907 Test Loss: 0.2136658239835537\n",
      "Epoch: 4926 Training Loss: 0.2172658092170846 Test Loss: 0.21363590593308643\n",
      "Epoch: 4927 Training Loss: 0.21726443780816992 Test Loss: 0.21373667843063077\n",
      "Epoch: 4928 Training Loss: 0.21726423412310375 Test Loss: 0.21365115013904376\n",
      "Epoch: 4929 Training Loss: 0.21726698268801153 Test Loss: 0.21380986617453807\n",
      "Epoch: 4930 Training Loss: 0.21726565209142282 Test Loss: 0.21363963920801476\n",
      "Epoch: 4931 Training Loss: 0.21726508724596238 Test Loss: 0.21364442246651666\n",
      "Epoch: 4932 Training Loss: 0.21726705658414303 Test Loss: 0.21376501502435755\n",
      "Epoch: 4933 Training Loss: 0.2172638017294209 Test Loss: 0.2138564543345811\n",
      "Epoch: 4934 Training Loss: 0.21726571115811952 Test Loss: 0.2138040199697024\n",
      "Epoch: 4935 Training Loss: 0.21726554460940092 Test Loss: 0.21377583892909766\n",
      "Epoch: 4936 Training Loss: 0.21726326655179584 Test Loss: 0.21375695218753318\n",
      "Epoch: 4937 Training Loss: 0.21726544977812418 Test Loss: 0.21380672918657748\n",
      "Epoch: 4938 Training Loss: 0.21726456221762402 Test Loss: 0.21373030075262822\n",
      "Epoch: 4939 Training Loss: 0.21726516651260866 Test Loss: 0.21383654353496337\n",
      "Epoch: 4940 Training Loss: 0.21726659122321007 Test Loss: 0.21381310686458002\n",
      "Epoch: 4941 Training Loss: 0.21726500023286408 Test Loss: 0.21377904073085907\n",
      "Epoch: 4942 Training Loss: 0.21726484962533957 Test Loss: 0.21372967854014016\n",
      "Epoch: 4943 Training Loss: 0.21726522072880708 Test Loss: 0.21372232865512503\n",
      "Epoch: 4944 Training Loss: 0.21726435286617166 Test Loss: 0.21375622627296378\n",
      "Epoch: 4945 Training Loss: 0.2172650247812271 Test Loss: 0.21377115937267707\n",
      "Epoch: 4946 Training Loss: 0.21726521970670576 Test Loss: 0.21376425022150763\n",
      "Epoch: 4947 Training Loss: 0.2172647947636035 Test Loss: 0.2137581577242288\n",
      "Epoch: 4948 Training Loss: 0.2172650182182608 Test Loss: 0.213708264060343\n",
      "Epoch: 4949 Training Loss: 0.2172654031738908 Test Loss: 0.21381463647027982\n",
      "Epoch: 4950 Training Loss: 0.21726398742012457 Test Loss: 0.21371334546232876\n",
      "Epoch: 4951 Training Loss: 0.2172665111855049 Test Loss: 0.2138441397124217\n",
      "Epoch: 4952 Training Loss: 0.21726548022598421 Test Loss: 0.2137454412565042\n",
      "Epoch: 4953 Training Loss: 0.21726502493364572 Test Loss: 0.21374910971763167\n",
      "Epoch: 4954 Training Loss: 0.2172654998252251 Test Loss: 0.2137361080691834\n",
      "Epoch: 4955 Training Loss: 0.2172658839111724 Test Loss: 0.2137440672039264\n",
      "Epoch: 4956 Training Loss: 0.21726501182564473 Test Loss: 0.21370707148640755\n",
      "Epoch: 4957 Training Loss: 0.21726425999840532 Test Loss: 0.21380094779554262\n",
      "Epoch: 4958 Training Loss: 0.21726276651114476 Test Loss: 0.2146775026008482\n",
      "Epoch: 4959 Training Loss: 0.217267452666332 Test Loss: 0.2138286103257407\n",
      "Epoch: 4960 Training Loss: 0.21726553376078175 Test Loss: 0.21385816541892322\n",
      "Epoch: 4961 Training Loss: 0.2172660886900663 Test Loss: 0.21380029965753425\n",
      "Epoch: 4962 Training Loss: 0.21726630604797895 Test Loss: 0.21374148761465303\n",
      "Epoch: 4963 Training Loss: 0.21726626369353522 Test Loss: 0.21377253342525485\n",
      "Epoch: 4964 Training Loss: 0.21726442416222086 Test Loss: 0.21375552628391473\n",
      "Epoch: 4965 Training Loss: 0.21726417479639878 Test Loss: 0.21373247849633642\n",
      "Epoch: 4966 Training Loss: 0.21726564033725776 Test Loss: 0.21380731251078502\n",
      "Epoch: 4967 Training Loss: 0.21726430144730313 Test Loss: 0.21375162449310423\n",
      "Epoch: 4968 Training Loss: 0.21726301659423092 Test Loss: 0.2144401674249363\n",
      "Epoch: 4969 Training Loss: 0.21726648247701022 Test Loss: 0.21381575126765423\n",
      "Epoch: 4970 Training Loss: 0.2172647853136493 Test Loss: 0.2190781133853669\n",
      "Epoch: 4971 Training Loss: 0.21726517459976113 Test Loss: 0.21417681598936764\n",
      "Epoch: 4972 Training Loss: 0.21726621191603468 Test Loss: 0.21390446839824254\n",
      "Epoch: 4973 Training Loss: 0.2172646283852351 Test Loss: 0.2138181364155251\n",
      "Epoch: 4974 Training Loss: 0.2172635876171282 Test Loss: 0.21376574093892695\n",
      "Epoch: 4975 Training Loss: 0.2172627911133026 Test Loss: 0.2137395691261482\n",
      "Epoch: 4976 Training Loss: 0.21726603612357528 Test Loss: 0.213731441475523\n",
      "Epoch: 4977 Training Loss: 0.21726494408901847 Test Loss: 0.21367222758707657\n",
      "Epoch: 4978 Training Loss: 0.21726482126651114 Test Loss: 0.21361862657378278\n",
      "Epoch: 4979 Training Loss: 0.21726590219244057 Test Loss: 0.21371233436703568\n",
      "Epoch: 4980 Training Loss: 0.21726412168299392 Test Loss: 0.2137303137153884\n",
      "Epoch: 4981 Training Loss: 0.2172636129724133 Test Loss: 0.21367969413693322\n",
      "Epoch: 4982 Training Loss: 0.21726563199009705 Test Loss: 0.2137618909991571\n",
      "Epoch: 4983 Training Loss: 0.2172666636041212 Test Loss: 0.21402945533178028\n",
      "Epoch: 4984 Training Loss: 0.21726437253713896 Test Loss: 0.21369834754881464\n",
      "Epoch: 4985 Training Loss: 0.21726374919879307 Test Loss: 0.21377800371004566\n",
      "Epoch: 4986 Training Loss: 0.2172659819342743 Test Loss: 0.21388110950442019\n",
      "Epoch: 4987 Training Loss: 0.21726445231483588 Test Loss: 0.2137925220014336\n",
      "Epoch: 4988 Training Loss: 0.21726508100576492 Test Loss: 0.2139750506273561\n",
      "Epoch: 4989 Training Loss: 0.21726238149275418 Test Loss: 0.21418964912193375\n",
      "Epoch: 4990 Training Loss: 0.21726229409412642 Test Loss: 0.21386918376506583\n",
      "Epoch: 4991 Training Loss: 0.21726523545961804 Test Loss: 0.2137810888469656\n",
      "Epoch: 4992 Training Loss: 0.21726451370367506 Test Loss: 0.21370008455867712\n",
      "Epoch: 4993 Training Loss: 0.21726383714433467 Test Loss: 0.21379957374296485\n",
      "Epoch: 4994 Training Loss: 0.21726215181583103 Test Loss: 0.21369079025963683\n",
      "Epoch: 4995 Training Loss: 0.21726461511584969 Test Loss: 0.2136744960701059\n",
      "Epoch: 4996 Training Loss: 0.21726603608771208 Test Loss: 0.2137339303254752\n",
      "Epoch: 4997 Training Loss: 0.21726645747139137 Test Loss: 0.21372047498042104\n",
      "Epoch: 4998 Training Loss: 0.21726551044969927 Test Loss: 0.2137474115960497\n",
      "Epoch: 4999 Training Loss: 0.2172641477196799 Test Loss: 0.21363508927919586\n",
      "Epoch: 5000 Training Loss: 0.21726441570747057 Test Loss: 0.21372313234625545\n",
      "Epoch: 5001 Training Loss: 0.21726443554878808 Test Loss: 0.21371395471205665\n",
      "Epoch: 5002 Training Loss: 0.21726447057817244 Test Loss: 0.21367860526507912\n",
      "Epoch: 5003 Training Loss: 0.2172651574840471 Test Loss: 0.21405921782912551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5004 Training Loss: 0.2172637233862521 Test Loss: 0.21435432802710525\n",
      "Epoch: 5005 Training Loss: 0.21726350192200264 Test Loss: 0.21400734086293405\n",
      "Epoch: 5006 Training Loss: 0.21726506532457904 Test Loss: 0.21400705568221037\n",
      "Epoch: 5007 Training Loss: 0.21726500717239403 Test Loss: 0.21379222385794971\n",
      "Epoch: 5008 Training Loss: 0.2172634557570935 Test Loss: 0.21371298250504406\n",
      "Epoch: 5009 Training Loss: 0.21726463927868328 Test Loss: 0.21378944982727383\n",
      "Epoch: 5010 Training Loss: 0.21726396661050054 Test Loss: 0.21379368864984868\n",
      "Epoch: 5011 Training Loss: 0.21726398947329298 Test Loss: 0.2136739386714187\n",
      "Epoch: 5012 Training Loss: 0.21726531131925997 Test Loss: 0.21368949398362005\n",
      "Epoch: 5013 Training Loss: 0.21726496890635555 Test Loss: 0.21375717255445603\n",
      "Epoch: 5014 Training Loss: 0.21726459572282222 Test Loss: 0.21380697547902067\n",
      "Epoch: 5015 Training Loss: 0.21726528187556962 Test Loss: 0.21373381366063368\n",
      "Epoch: 5016 Training Loss: 0.21726415213085398 Test Loss: 0.21372936743389614\n",
      "Epoch: 5017 Training Loss: 0.2172640394576333 Test Loss: 0.21399593363398642\n",
      "Epoch: 5018 Training Loss: 0.21726520255512852 Test Loss: 0.21375143005170172\n",
      "Epoch: 5019 Training Loss: 0.21726550645991782 Test Loss: 0.2138450211801131\n",
      "Epoch: 5020 Training Loss: 0.2172659362804158 Test Loss: 0.21385991539154586\n",
      "Epoch: 5021 Training Loss: 0.21726403976247055 Test Loss: 0.21371973610309147\n",
      "Epoch: 5022 Training Loss: 0.21726556474658987 Test Loss: 0.21381383277914942\n",
      "Epoch: 5023 Training Loss: 0.2172654492222445 Test Loss: 0.2137607762017827\n",
      "Epoch: 5024 Training Loss: 0.2172642124258654 Test Loss: 0.21368382925742674\n",
      "Epoch: 5025 Training Loss: 0.2172667045061052 Test Loss: 0.2137649113222762\n",
      "Epoch: 5026 Training Loss: 0.21726572813934655 Test Loss: 0.21376312246137305\n",
      "Epoch: 5027 Training Loss: 0.21726487590410218 Test Loss: 0.214337930135493\n",
      "Epoch: 5028 Training Loss: 0.21726605384896377 Test Loss: 0.2136891569518557\n",
      "Epoch: 5029 Training Loss: 0.21726656764315355 Test Loss: 0.21375111894545767\n",
      "Epoch: 5030 Training Loss: 0.21726470812706883 Test Loss: 0.2137279415302777\n",
      "Epoch: 5031 Training Loss: 0.2172655707716081 Test Loss: 0.21360199535248753\n",
      "Epoch: 5032 Training Loss: 0.21726610784998296 Test Loss: 0.21370949552255894\n",
      "Epoch: 5033 Training Loss: 0.21726553850369046 Test Loss: 0.21372707302534644\n",
      "Epoch: 5034 Training Loss: 0.2172643225707302 Test Loss: 0.21724981976578261\n",
      "Epoch: 5035 Training Loss: 0.2172654268346395 Test Loss: 0.21393012170061457\n",
      "Epoch: 5036 Training Loss: 0.217264511542917 Test Loss: 0.21377913147018027\n",
      "Epoch: 5037 Training Loss: 0.21726391537991385 Test Loss: 0.2137506652488518\n",
      "Epoch: 5038 Training Loss: 0.21726327196713963 Test Loss: 0.21370245674378782\n",
      "Epoch: 5039 Training Loss: 0.21726546952978368 Test Loss: 0.21370096602636854\n",
      "Epoch: 5040 Training Loss: 0.21726431767540288 Test Loss: 0.21369890494750185\n",
      "Epoch: 5041 Training Loss: 0.21726519748945097 Test Loss: 0.2137775889017203\n",
      "Epoch: 5042 Training Loss: 0.2172640226467565 Test Loss: 0.21374875972310714\n",
      "Epoch: 5043 Training Loss: 0.21726259036005333 Test Loss: 0.21374411905496707\n",
      "Epoch: 5044 Training Loss: 0.21726597506647075 Test Loss: 0.21380702733006132\n",
      "Epoch: 5045 Training Loss: 0.21726612805889833 Test Loss: 0.21371510839771157\n",
      "Epoch: 5046 Training Loss: 0.21726611029764661 Test Loss: 0.21372665821702108\n",
      "Epoch: 5047 Training Loss: 0.21726456399285263 Test Loss: 0.21374710048980566\n",
      "Epoch: 5048 Training Loss: 0.21726355092010485 Test Loss: 0.21377717409339492\n",
      "Epoch: 5049 Training Loss: 0.21726518785121496 Test Loss: 0.21374170798157588\n",
      "Epoch: 5050 Training Loss: 0.21726521666729923 Test Loss: 0.21384786002458983\n",
      "Epoch: 5051 Training Loss: 0.21726555172824688 Test Loss: 0.21369998085659578\n",
      "Epoch: 5052 Training Loss: 0.2172655686736107 Test Loss: 0.2137464005007566\n",
      "Epoch: 5053 Training Loss: 0.21726524804760258 Test Loss: 0.2137568225599315\n",
      "Epoch: 5054 Training Loss: 0.21726502496950892 Test Loss: 0.21430376029969073\n",
      "Epoch: 5055 Training Loss: 0.21726528106864754 Test Loss: 0.2138546784364381\n",
      "Epoch: 5056 Training Loss: 0.21726414146155082 Test Loss: 0.21379555528731284\n",
      "Epoch: 5057 Training Loss: 0.2172648963819916 Test Loss: 0.21371749354558245\n",
      "Epoch: 5058 Training Loss: 0.21726447056024084 Test Loss: 0.21371575653571998\n",
      "Epoch: 5059 Training Loss: 0.21726261521325357 Test Loss: 0.21416209029381703\n",
      "Epoch: 5060 Training Loss: 0.21726652345072062 Test Loss: 0.2138564024835404\n",
      "Epoch: 5061 Training Loss: 0.21726489512677943 Test Loss: 0.21378688320076059\n",
      "Epoch: 5062 Training Loss: 0.21726553396699516 Test Loss: 0.21374886342518848\n",
      "Epoch: 5063 Training Loss: 0.21726481987681198 Test Loss: 0.2137972922971753\n",
      "Epoch: 5064 Training Loss: 0.2172662614700166 Test Loss: 0.2137462838359151\n",
      "Epoch: 5065 Training Loss: 0.21726512106496362 Test Loss: 0.21379047388532707\n",
      "Epoch: 5066 Training Loss: 0.21726446772704772 Test Loss: 0.21384460637178773\n",
      "Epoch: 5067 Training Loss: 0.21726535948354272 Test Loss: 0.21374470237917464\n",
      "Epoch: 5068 Training Loss: 0.21726449698245626 Test Loss: 0.213743419065918\n",
      "Epoch: 5069 Training Loss: 0.21726460146990062 Test Loss: 0.21381957528190373\n",
      "Epoch: 5070 Training Loss: 0.21726476222671182 Test Loss: 0.21373661361682994\n",
      "Epoch: 5071 Training Loss: 0.21726499717552597 Test Loss: 0.2138004422478961\n",
      "Epoch: 5072 Training Loss: 0.2172639428869912 Test Loss: 0.21373374884683286\n",
      "Epoch: 5073 Training Loss: 0.21726756238083833 Test Loss: 0.2138659430750239\n",
      "Epoch: 5074 Training Loss: 0.21726404897931395 Test Loss: 0.21376340764209675\n",
      "Epoch: 5075 Training Loss: 0.21726794651161463 Test Loss: 0.2139306402110213\n",
      "Epoch: 5076 Training Loss: 0.2172655680101414 Test Loss: 0.21374734678224885\n",
      "Epoch: 5077 Training Loss: 0.21726429364705632 Test Loss: 0.21375170226966522\n",
      "Epoch: 5078 Training Loss: 0.21726479146418876 Test Loss: 0.21380649585689446\n",
      "Epoch: 5079 Training Loss: 0.21726333488913077 Test Loss: 0.21365337973379261\n",
      "Epoch: 5080 Training Loss: 0.21726513106183168 Test Loss: 0.21375995954789212\n",
      "Epoch: 5081 Training Loss: 0.21726604776118494 Test Loss: 0.2138877075493456\n",
      "Epoch: 5082 Training Loss: 0.2172643927460543 Test Loss: 0.2139752580315188\n",
      "Epoch: 5083 Training Loss: 0.21726517004513424 Test Loss: 0.21381509016688569\n",
      "Epoch: 5084 Training Loss: 0.21726399274581032 Test Loss: 0.21375788550626526\n",
      "Epoch: 5085 Training Loss: 0.2172640391707277 Test Loss: 0.21372687858394393\n",
      "Epoch: 5086 Training Loss: 0.21726466797821214 Test Loss: 0.21397209511803786\n",
      "Epoch: 5087 Training Loss: 0.21726543909088944 Test Loss: 0.2142999881364819\n",
      "Epoch: 5088 Training Loss: 0.21726561983247095 Test Loss: 0.2140248276264004\n",
      "Epoch: 5089 Training Loss: 0.21726384986680625 Test Loss: 0.21391476082981575\n",
      "Epoch: 5090 Training Loss: 0.21725981551638293 Test Loss: 0.21497275538918975\n",
      "Epoch: 5091 Training Loss: 0.21726632931423245 Test Loss: 0.21405742896822236\n",
      "Epoch: 5092 Training Loss: 0.21726593577833095 Test Loss: 0.21387006523275726\n",
      "Epoch: 5093 Training Loss: 0.21726526829238116 Test Loss: 0.2138675893455652\n",
      "Epoch: 5094 Training Loss: 0.21726356923723622 Test Loss: 0.21375596701776042\n",
      "Epoch: 5095 Training Loss: 0.21726581611178555 Test Loss: 0.21382912883614738\n",
      "Epoch: 5096 Training Loss: 0.21726327436100848 Test Loss: 0.2137280581951192\n",
      "Epoch: 5097 Training Loss: 0.21726365555100205 Test Loss: 0.21382347707271424\n",
      "Epoch: 5098 Training Loss: 0.21726427150152794 Test Loss: 0.21379445345269857\n",
      "Epoch: 5099 Training Loss: 0.21726465318464055 Test Loss: 0.21380821990399676\n",
      "Epoch: 5100 Training Loss: 0.21726335717811193 Test Loss: 0.21403426451580254\n",
      "Epoch: 5101 Training Loss: 0.21726466858788662 Test Loss: 0.21377559263665447\n",
      "Epoch: 5102 Training Loss: 0.2172642151245715 Test Loss: 0.21381642533118297\n",
      "Epoch: 5103 Training Loss: 0.21726290494311157 Test Loss: 0.21370953441083945\n",
      "Epoch: 5104 Training Loss: 0.21726339438618592 Test Loss: 0.21377743334859828\n",
      "Epoch: 5105 Training Loss: 0.2172642607604984 Test Loss: 0.21369545685329722\n",
      "Epoch: 5106 Training Loss: 0.21726514945068945 Test Loss: 0.21371363064305246\n",
      "Epoch: 5107 Training Loss: 0.21726452484816564 Test Loss: 0.2137675686881106\n",
      "Epoch: 5108 Training Loss: 0.21726557759458265 Test Loss: 0.21386033019987125\n",
      "Epoch: 5109 Training Loss: 0.21726303811215322 Test Loss: 0.2141004653319794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5110 Training Loss: 0.2172653706280333 Test Loss: 0.21393117168418818\n",
      "Epoch: 5111 Training Loss: 0.2172615449028325 Test Loss: 0.2137557336880774\n",
      "Epoch: 5112 Training Loss: 0.21726572719793744 Test Loss: 0.21385565064345066\n",
      "Epoch: 5113 Training Loss: 0.21726682545476012 Test Loss: 0.21376597426860997\n",
      "Epoch: 5114 Training Loss: 0.21726568898569376 Test Loss: 0.21376842423028167\n",
      "Epoch: 5115 Training Loss: 0.21726615466042976 Test Loss: 0.21374634864971595\n",
      "Epoch: 5116 Training Loss: 0.21726385004612225 Test Loss: 0.21365698338111924\n",
      "Epoch: 5117 Training Loss: 0.21726517776468887 Test Loss: 0.21371588616332166\n",
      "Epoch: 5118 Training Loss: 0.21726403582648393 Test Loss: 0.2137993922643225\n",
      "Epoch: 5119 Training Loss: 0.2172642703001106 Test Loss: 0.21477255852115854\n",
      "Epoch: 5120 Training Loss: 0.21726521863977544 Test Loss: 0.21369548277881756\n",
      "Epoch: 5121 Training Loss: 0.21726681377232146 Test Loss: 0.21371114179310025\n",
      "Epoch: 5122 Training Loss: 0.2172648470880179 Test Loss: 0.2136938624337966\n",
      "Epoch: 5123 Training Loss: 0.21726450178812556 Test Loss: 0.21395773237977195\n",
      "Epoch: 5124 Training Loss: 0.21726443222247593 Test Loss: 0.21374418386876792\n",
      "Epoch: 5125 Training Loss: 0.21726475173672471 Test Loss: 0.21380184222599422\n",
      "Epoch: 5126 Training Loss: 0.21726609912625863 Test Loss: 0.21379475159618244\n",
      "Epoch: 5127 Training Loss: 0.21726394245663275 Test Loss: 0.2138139624067511\n",
      "Epoch: 5128 Training Loss: 0.217264575191138 Test Loss: 0.21362882826603483\n",
      "Epoch: 5129 Training Loss: 0.21726497735214004 Test Loss: 0.21372529712720345\n",
      "Epoch: 5130 Training Loss: 0.21726169763525183 Test Loss: 0.21415505151504594\n",
      "Epoch: 5131 Training Loss: 0.21726642358066373 Test Loss: 0.21368719957507035\n",
      "Epoch: 5132 Training Loss: 0.2172645677226258 Test Loss: 0.2136584481730182\n",
      "Epoch: 5133 Training Loss: 0.21726388646520575 Test Loss: 0.21629928648819954\n",
      "Epoch: 5134 Training Loss: 0.21726652657081935 Test Loss: 0.2137369895368748\n",
      "Epoch: 5135 Training Loss: 0.21726495206858135 Test Loss: 0.21373543400565467\n",
      "Epoch: 5136 Training Loss: 0.21726496892428715 Test Loss: 0.21472998881676755\n",
      "Epoch: 5137 Training Loss: 0.2172617318218509 Test Loss: 0.21477082151129606\n",
      "Epoch: 5138 Training Loss: 0.2172640646336024 Test Loss: 0.21421191914390197\n",
      "Epoch: 5139 Training Loss: 0.217262570420112 Test Loss: 0.21433050247391686\n",
      "Epoch: 5140 Training Loss: 0.21726558671280222 Test Loss: 0.21390245917041653\n",
      "Epoch: 5141 Training Loss: 0.21726283109180908 Test Loss: 0.2138404712512942\n",
      "Epoch: 5142 Training Loss: 0.21726332335014492 Test Loss: 0.21717462279404934\n",
      "Epoch: 5143 Training Loss: 0.21726311510148605 Test Loss: 0.21392516992623048\n",
      "Epoch: 5144 Training Loss: 0.2172623236902354 Test Loss: 0.21378303326099077\n",
      "Epoch: 5145 Training Loss: 0.21726385874294918 Test Loss: 0.21379538677143065\n",
      "Epoch: 5146 Training Loss: 0.21726453077456007 Test Loss: 0.21375727625653737\n",
      "Epoch: 5147 Training Loss: 0.2172635839680472 Test Loss: 0.21376379652490177\n",
      "Epoch: 5148 Training Loss: 0.21726507999262942 Test Loss: 0.21376466502983302\n",
      "Epoch: 5149 Training Loss: 0.21726413583999363 Test Loss: 0.21378199624017735\n",
      "Epoch: 5150 Training Loss: 0.21726450623516286 Test Loss: 0.21378886650306625\n",
      "Epoch: 5151 Training Loss: 0.21726389852420805 Test Loss: 0.21375837809115164\n",
      "Epoch: 5152 Training Loss: 0.21726313655664775 Test Loss: 0.213815038315845\n",
      "Epoch: 5153 Training Loss: 0.21726466919756107 Test Loss: 0.21533882373632793\n",
      "Epoch: 5154 Training Loss: 0.21725972348243608 Test Loss: 0.21523254206571227\n",
      "Epoch: 5155 Training Loss: 0.21726548214466562 Test Loss: 0.21435720575986247\n",
      "Epoch: 5156 Training Loss: 0.2172638841968581 Test Loss: 0.21399220035905808\n",
      "Epoch: 5157 Training Loss: 0.217266347622398 Test Loss: 0.21382950475619225\n",
      "Epoch: 5158 Training Loss: 0.21726436207404923 Test Loss: 0.21466138988995964\n",
      "Epoch: 5159 Training Loss: 0.21726375838873907 Test Loss: 0.21370870479418871\n",
      "Epoch: 5160 Training Loss: 0.21726625111451647 Test Loss: 0.21376095768042502\n",
      "Epoch: 5161 Training Loss: 0.21726398647871545 Test Loss: 0.21372973039118084\n",
      "Epoch: 5162 Training Loss: 0.217265307804666 Test Loss: 0.2136314597063489\n",
      "Epoch: 5163 Training Loss: 0.21726564672090803 Test Loss: 0.21363028009517362\n",
      "Epoch: 5164 Training Loss: 0.21726466039314454 Test Loss: 0.21370136787193375\n",
      "Epoch: 5165 Training Loss: 0.2172658838304802 Test Loss: 0.21489425291361366\n",
      "Epoch: 5166 Training Loss: 0.21726614044963524 Test Loss: 0.2137457264372279\n",
      "Epoch: 5167 Training Loss: 0.21726430118729492 Test Loss: 0.21378075181520123\n",
      "Epoch: 5168 Training Loss: 0.21726441548332553 Test Loss: 0.2136720979594749\n",
      "Epoch: 5169 Training Loss: 0.21726604799429575 Test Loss: 0.2137423820451046\n",
      "Epoch: 5170 Training Loss: 0.21726561430953756 Test Loss: 0.21369439390696346\n",
      "Epoch: 5171 Training Loss: 0.21726544197787734 Test Loss: 0.2137040382005283\n",
      "Epoch: 5172 Training Loss: 0.21726549738652726 Test Loss: 0.21376708906598438\n",
      "Epoch: 5173 Training Loss: 0.21726582845769346 Test Loss: 0.2138543154791534\n",
      "Epoch: 5174 Training Loss: 0.21726564105452184 Test Loss: 0.21369154209972654\n",
      "Epoch: 5175 Training Loss: 0.21726478228320859 Test Loss: 0.2136559074720253\n",
      "Epoch: 5176 Training Loss: 0.21726663231347584 Test Loss: 0.2136623110755482\n",
      "Epoch: 5177 Training Loss: 0.2172656623303675 Test Loss: 0.21372687858394393\n",
      "Epoch: 5178 Training Loss: 0.21726410963295742 Test Loss: 0.2136917235783689\n",
      "Epoch: 5179 Training Loss: 0.21726374930638268 Test Loss: 0.2136323022857598\n",
      "Epoch: 5180 Training Loss: 0.21726580269894732 Test Loss: 0.21391504601053946\n",
      "Epoch: 5181 Training Loss: 0.21726357159524187 Test Loss: 0.21708610010486354\n",
      "Epoch: 5182 Training Loss: 0.21725989880867383 Test Loss: 0.21530169839120739\n",
      "Epoch: 5183 Training Loss: 0.21726411189233927 Test Loss: 0.21451383479096978\n",
      "Epoch: 5184 Training Loss: 0.217263920687668 Test Loss: 0.21411192441196772\n",
      "Epoch: 5185 Training Loss: 0.21726349946537316 Test Loss: 0.21396780444442232\n",
      "Epoch: 5186 Training Loss: 0.21726258977727625 Test Loss: 0.21375705588961452\n",
      "Epoch: 5187 Training Loss: 0.21726369065211282 Test Loss: 0.21435811315307424\n",
      "Epoch: 5188 Training Loss: 0.21726336753361206 Test Loss: 0.21382662702343502\n",
      "Epoch: 5189 Training Loss: 0.21725945472358654 Test Loss: 0.21469405604558245\n",
      "Epoch: 5190 Training Loss: 0.2172657091139169 Test Loss: 0.2137565114536875\n",
      "Epoch: 5191 Training Loss: 0.21726589077001013 Test Loss: 0.21373424143171924\n",
      "Epoch: 5192 Training Loss: 0.2172651378310114 Test Loss: 0.21373701546239512\n",
      "Epoch: 5193 Training Loss: 0.21726624361014107 Test Loss: 0.21369999381935595\n",
      "Epoch: 5194 Training Loss: 0.21726477583679768 Test Loss: 0.21377629262570352\n",
      "Epoch: 5195 Training Loss: 0.21726444487322108 Test Loss: 0.2137245582498739\n",
      "Epoch: 5196 Training Loss: 0.21726361321448992 Test Loss: 0.2137386746956966\n",
      "Epoch: 5197 Training Loss: 0.21726371041273812 Test Loss: 0.21371108994205956\n",
      "Epoch: 5198 Training Loss: 0.21726388258301393 Test Loss: 0.21368113300331182\n",
      "Epoch: 5199 Training Loss: 0.21726427046149502 Test Loss: 0.21381388463019008\n",
      "Epoch: 5200 Training Loss: 0.21726556639629724 Test Loss: 0.21397104513446427\n",
      "Epoch: 5201 Training Loss: 0.21726420636498397 Test Loss: 0.2138908834255867\n",
      "Epoch: 5202 Training Loss: 0.21726515017691933 Test Loss: 0.21388165394034725\n",
      "Epoch: 5203 Training Loss: 0.21726383405113334 Test Loss: 0.2137202027624575\n",
      "Epoch: 5204 Training Loss: 0.2172656623482991 Test Loss: 0.21374710048980566\n",
      "Epoch: 5205 Training Loss: 0.21726453028144102 Test Loss: 0.21377009642634331\n",
      "Epoch: 5206 Training Loss: 0.21726542268347368 Test Loss: 0.21391929779587449\n",
      "Epoch: 5207 Training Loss: 0.21726512778034854 Test Loss: 0.21367524791019565\n",
      "Epoch: 5208 Training Loss: 0.21726518313520365 Test Loss: 0.21381772160719975\n",
      "Epoch: 5209 Training Loss: 0.21726544200477474 Test Loss: 0.21381756605407773\n",
      "Epoch: 5210 Training Loss: 0.2172644235884096 Test Loss: 0.21419918971341723\n",
      "Epoch: 5211 Training Loss: 0.21726356846617734 Test Loss: 0.213747204191887\n",
      "Epoch: 5212 Training Loss: 0.21726372721464912 Test Loss: 0.2138016477845917\n",
      "Epoch: 5213 Training Loss: 0.21726524338538608 Test Loss: 0.21374690604840316\n",
      "Epoch: 5214 Training Loss: 0.21726426059911397 Test Loss: 0.21389294450445337\n",
      "Epoch: 5215 Training Loss: 0.2172646583220445 Test Loss: 0.2138196660212249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5216 Training Loss: 0.2172638596484951 Test Loss: 0.21380221814603909\n",
      "Epoch: 5217 Training Loss: 0.2172634979680844 Test Loss: 0.21374095614148614\n",
      "Epoch: 5218 Training Loss: 0.21726627692705744 Test Loss: 0.21374489682057715\n",
      "Epoch: 5219 Training Loss: 0.2172644590481524 Test Loss: 0.213713980637577\n",
      "Epoch: 5220 Training Loss: 0.21726461204954575 Test Loss: 0.2137972922971753\n",
      "Epoch: 5221 Training Loss: 0.21726484577004515 Test Loss: 0.21369709016107838\n",
      "Epoch: 5222 Training Loss: 0.2172644121211502 Test Loss: 0.21370572335935012\n",
      "Epoch: 5223 Training Loss: 0.21726502625161848 Test Loss: 0.2137245971381544\n",
      "Epoch: 5224 Training Loss: 0.21726620517375236 Test Loss: 0.21379381827745036\n",
      "Epoch: 5225 Training Loss: 0.21726463164878665 Test Loss: 0.2138771299370487\n",
      "Epoch: 5226 Training Loss: 0.21726336916538783 Test Loss: 0.21401240930215965\n",
      "Epoch: 5227 Training Loss: 0.21726640877812634 Test Loss: 0.21377957220402596\n",
      "Epoch: 5228 Training Loss: 0.21726534466307373 Test Loss: 0.2137125676967187\n",
      "Epoch: 5229 Training Loss: 0.2172675171842357 Test Loss: 0.21378489989845492\n",
      "Epoch: 5230 Training Loss: 0.21726458165548052 Test Loss: 0.21379976818436736\n",
      "Epoch: 5231 Training Loss: 0.21726546002603467 Test Loss: 0.21369090692447834\n",
      "Epoch: 5232 Training Loss: 0.2172658473665677 Test Loss: 0.2136970383100377\n",
      "Epoch: 5233 Training Loss: 0.21726579246000263 Test Loss: 0.2136844773954351\n",
      "Epoch: 5234 Training Loss: 0.2172646993943787 Test Loss: 0.2137017437919786\n",
      "Epoch: 5235 Training Loss: 0.2172672274633435 Test Loss: 0.21367733491458266\n",
      "Epoch: 5236 Training Loss: 0.21726433293519612 Test Loss: 0.21367323868236965\n",
      "Epoch: 5237 Training Loss: 0.21726627956300293 Test Loss: 0.2138140790715926\n",
      "Epoch: 5238 Training Loss: 0.2172616505378994 Test Loss: 0.21465369001041998\n",
      "Epoch: 5239 Training Loss: 0.21726775052817146 Test Loss: 0.21402513873264442\n",
      "Epoch: 5240 Training Loss: 0.21726444248831803 Test Loss: 0.21403674040299459\n",
      "Epoch: 5241 Training Loss: 0.21726541372663852 Test Loss: 0.21389192044640012\n",
      "Epoch: 5242 Training Loss: 0.21726309673952568 Test Loss: 0.21390051475639138\n",
      "Epoch: 5243 Training Loss: 0.21726368062834733 Test Loss: 0.21383931756563926\n",
      "Epoch: 5244 Training Loss: 0.21726381140352014 Test Loss: 0.21387558736858872\n",
      "Epoch: 5245 Training Loss: 0.21726571957700663 Test Loss: 0.21378794614709434\n",
      "Epoch: 5246 Training Loss: 0.21726463355850226 Test Loss: 0.2137282267110014\n",
      "Epoch: 5247 Training Loss: 0.2172660520647694 Test Loss: 0.21379957374296485\n",
      "Epoch: 5248 Training Loss: 0.21726479970375984 Test Loss: 0.21372661932874057\n",
      "Epoch: 5249 Training Loss: 0.21726594420618386 Test Loss: 0.21367279794852395\n",
      "Epoch: 5250 Training Loss: 0.21726502921033278 Test Loss: 0.21373265997497876\n",
      "Epoch: 5251 Training Loss: 0.21726394081589118 Test Loss: 0.21715373978741903\n",
      "Epoch: 5252 Training Loss: 0.2172660335324588 Test Loss: 0.21376900755448922\n",
      "Epoch: 5253 Training Loss: 0.2172642710980669 Test Loss: 0.2136990864261442\n",
      "Epoch: 5254 Training Loss: 0.21726542419869405 Test Loss: 0.21372913410421313\n",
      "Epoch: 5255 Training Loss: 0.217264165418171 Test Loss: 0.2137950497396663\n",
      "Epoch: 5256 Training Loss: 0.2172667338601375 Test Loss: 0.21376616871001247\n",
      "Epoch: 5257 Training Loss: 0.21726438685552307 Test Loss: 0.21373074148647392\n",
      "Epoch: 5258 Training Loss: 0.21726633500751605 Test Loss: 0.213747126415326\n",
      "Epoch: 5259 Training Loss: 0.2172657314477271 Test Loss: 0.21378935908795263\n",
      "Epoch: 5260 Training Loss: 0.2172648077909123 Test Loss: 0.21366157219821866\n",
      "Epoch: 5261 Training Loss: 0.21726625803611482 Test Loss: 0.2137159380143623\n",
      "Epoch: 5262 Training Loss: 0.21726455943822573 Test Loss: 0.21372884892348942\n",
      "Epoch: 5263 Training Loss: 0.21726500434816673 Test Loss: 0.21370068084564484\n",
      "Epoch: 5264 Training Loss: 0.21726393201147462 Test Loss: 0.21368599403837474\n",
      "Epoch: 5265 Training Loss: 0.2172642795348856 Test Loss: 0.21369348651375172\n",
      "Epoch: 5266 Training Loss: 0.2172661577625969 Test Loss: 0.21472425927677338\n",
      "Epoch: 5267 Training Loss: 0.2172613094788309 Test Loss: 0.2146867968998885\n",
      "Epoch: 5268 Training Loss: 0.21726242539828147 Test Loss: 0.2141198187329099\n",
      "Epoch: 5269 Training Loss: 0.21726397806879416 Test Loss: 0.21384704337069926\n",
      "Epoch: 5270 Training Loss: 0.21726555354830446 Test Loss: 0.21401732218826325\n",
      "Epoch: 5271 Training Loss: 0.21726462139191036 Test Loss: 0.21375246707251513\n",
      "Epoch: 5272 Training Loss: 0.21726450095430608 Test Loss: 0.2137402043013964\n",
      "Epoch: 5273 Training Loss: 0.2172639306307413 Test Loss: 0.21372619155765504\n",
      "Epoch: 5274 Training Loss: 0.21726502137422274 Test Loss: 0.21365627042931\n",
      "Epoch: 5275 Training Loss: 0.21726345830338095 Test Loss: 0.2136944198324838\n",
      "Epoch: 5276 Training Loss: 0.21726489700959764 Test Loss: 0.21364824648076616\n",
      "Epoch: 5277 Training Loss: 0.21726671903070274 Test Loss: 0.21369694757071653\n",
      "Epoch: 5278 Training Loss: 0.217264335642868 Test Loss: 0.21371388989825582\n",
      "Epoch: 5279 Training Loss: 0.21726347035341745 Test Loss: 0.2137098714426038\n",
      "Epoch: 5280 Training Loss: 0.21726498561860852 Test Loss: 0.2147135909251553\n",
      "Epoch: 5281 Training Loss: 0.21726513648614126 Test Loss: 0.21369791977772912\n",
      "Epoch: 5282 Training Loss: 0.21726554211690824 Test Loss: 0.21373820803633056\n",
      "Epoch: 5283 Training Loss: 0.21726336672668997 Test Loss: 0.2137705890112297\n",
      "Epoch: 5284 Training Loss: 0.21726381626298424 Test Loss: 0.2137285896682861\n",
      "Epoch: 5285 Training Loss: 0.21726346623811482 Test Loss: 0.21371561394535812\n",
      "Epoch: 5286 Training Loss: 0.21726432074170682 Test Loss: 0.21367649233517177\n",
      "Epoch: 5287 Training Loss: 0.21726579915745595 Test Loss: 0.2136823903910481\n",
      "Epoch: 5288 Training Loss: 0.2172657633121837 Test Loss: 0.21374575236274823\n",
      "Epoch: 5289 Training Loss: 0.21726412417548657 Test Loss: 0.21374758011193187\n",
      "Epoch: 5290 Training Loss: 0.21726582949772638 Test Loss: 0.2137732074887836\n",
      "Epoch: 5291 Training Loss: 0.21726491230525408 Test Loss: 0.2138035014592957\n",
      "Epoch: 5292 Training Loss: 0.21726345371289088 Test Loss: 0.2137764222533052\n",
      "Epoch: 5293 Training Loss: 0.2172637131383416 Test Loss: 0.21378220364434003\n",
      "Epoch: 5294 Training Loss: 0.21726565659225489 Test Loss: 0.21386812081873208\n",
      "Epoch: 5295 Training Loss: 0.21726521289269704 Test Loss: 0.21364124659027556\n",
      "Epoch: 5296 Training Loss: 0.21726609766483307 Test Loss: 0.21374583013930923\n",
      "Epoch: 5297 Training Loss: 0.2172648722550212 Test Loss: 0.21366302402735743\n",
      "Epoch: 5298 Training Loss: 0.2172673441353114 Test Loss: 0.21374605050623208\n",
      "Epoch: 5299 Training Loss: 0.21726491858131475 Test Loss: 0.2139510435955254\n",
      "Epoch: 5300 Training Loss: 0.21726235020210882 Test Loss: 0.21424455937400447\n",
      "Epoch: 5301 Training Loss: 0.21726335531322535 Test Loss: 0.21905561003371563\n",
      "Epoch: 5302 Training Loss: 0.21726604523282905 Test Loss: 0.21419528792260673\n",
      "Epoch: 5303 Training Loss: 0.21726383306489525 Test Loss: 0.21380980136073724\n",
      "Epoch: 5304 Training Loss: 0.21726444259590763 Test Loss: 0.21384255825568121\n",
      "Epoch: 5305 Training Loss: 0.2172627473960571 Test Loss: 0.21380772731911038\n",
      "Epoch: 5306 Training Loss: 0.21726229300029873 Test Loss: 0.21378184068705533\n",
      "Epoch: 5307 Training Loss: 0.21726261094553231 Test Loss: 0.21377637040226452\n",
      "Epoch: 5308 Training Loss: 0.21726428861724198 Test Loss: 0.21365597228582617\n",
      "Epoch: 5309 Training Loss: 0.21726477156907642 Test Loss: 0.21376344653037724\n",
      "Epoch: 5310 Training Loss: 0.2172647675972266 Test Loss: 0.21371829723671287\n",
      "Epoch: 5311 Training Loss: 0.2172655630430877 Test Loss: 0.21367450903286608\n",
      "Epoch: 5312 Training Loss: 0.21726470235309303 Test Loss: 0.21369033656303096\n",
      "Epoch: 5313 Training Loss: 0.21726414745070588 Test Loss: 0.2137870387538826\n",
      "Epoch: 5314 Training Loss: 0.21726511396404924 Test Loss: 0.21376532613060156\n",
      "Epoch: 5315 Training Loss: 0.2172664776354777 Test Loss: 0.21371265843603987\n",
      "Epoch: 5316 Training Loss: 0.21726473414582323 Test Loss: 0.21368975323882342\n",
      "Epoch: 5317 Training Loss: 0.21726358793989703 Test Loss: 0.2137106751337342\n",
      "Epoch: 5318 Training Loss: 0.2172657210205006 Test Loss: 0.21374306907139348\n",
      "Epoch: 5319 Training Loss: 0.21726565046861285 Test Loss: 0.21373198591145004\n",
      "Epoch: 5320 Training Loss: 0.2172648554889734 Test Loss: 0.21379905523255813\n",
      "Epoch: 5321 Training Loss: 0.21726443982547514 Test Loss: 0.21378190550085616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5322 Training Loss: 0.21726312068718004 Test Loss: 0.21374796899473691\n",
      "Epoch: 5323 Training Loss: 0.2172656949748488 Test Loss: 0.21370450485989434\n",
      "Epoch: 5324 Training Loss: 0.21726409927745732 Test Loss: 0.21370967700120128\n",
      "Epoch: 5325 Training Loss: 0.21726434004507628 Test Loss: 0.21382070304203835\n",
      "Epoch: 5326 Training Loss: 0.21726383164829868 Test Loss: 0.21378042774619704\n",
      "Epoch: 5327 Training Loss: 0.21726510570654656 Test Loss: 0.2138162957035813\n",
      "Epoch: 5328 Training Loss: 0.21726314574659372 Test Loss: 0.21377312971222256\n",
      "Epoch: 5329 Training Loss: 0.21726389920560893 Test Loss: 0.21381280872109615\n",
      "Epoch: 5330 Training Loss: 0.21726242043122776 Test Loss: 0.21403860704045874\n",
      "Epoch: 5331 Training Loss: 0.21726553182416874 Test Loss: 0.21386421902792158\n",
      "Epoch: 5332 Training Loss: 0.2172640684082046 Test Loss: 0.21373250442185676\n",
      "Epoch: 5333 Training Loss: 0.21726278680971814 Test Loss: 0.2137032733976784\n",
      "Epoch: 5334 Training Loss: 0.2172630344630722 Test Loss: 0.21422186158095066\n",
      "Epoch: 5335 Training Loss: 0.2172659158114922 Test Loss: 0.21381179762580307\n",
      "Epoch: 5336 Training Loss: 0.21726509353098886 Test Loss: 0.21377988331026998\n",
      "Epoch: 5337 Training Loss: 0.21726424643314846 Test Loss: 0.21422352081425214\n",
      "Epoch: 5338 Training Loss: 0.21726535013221232 Test Loss: 0.2137437820232027\n",
      "Epoch: 5339 Training Loss: 0.2172629931576272 Test Loss: 0.21370743444369225\n",
      "Epoch: 5340 Training Loss: 0.21726509504620922 Test Loss: 0.21371422693002018\n",
      "Epoch: 5341 Training Loss: 0.21726570085641422 Test Loss: 0.2136965586879115\n",
      "Epoch: 5342 Training Loss: 0.21726571626862606 Test Loss: 0.21368761438339573\n",
      "Epoch: 5343 Training Loss: 0.21726465885999258 Test Loss: 0.21369934568134757\n",
      "Epoch: 5344 Training Loss: 0.21726688201999836 Test Loss: 0.21434849478502974\n",
      "Epoch: 5345 Training Loss: 0.21726408677913078 Test Loss: 0.21438992376652596\n",
      "Epoch: 5346 Training Loss: 0.21726524967937838 Test Loss: 0.21406874545784885\n",
      "Epoch: 5347 Training Loss: 0.21726327212852403 Test Loss: 0.21391010719891554\n",
      "Epoch: 5348 Training Loss: 0.21726301386862742 Test Loss: 0.21380573105404455\n",
      "Epoch: 5349 Training Loss: 0.2172655987000781 Test Loss: 0.21377117233543724\n",
      "Epoch: 5350 Training Loss: 0.2172636654133831 Test Loss: 0.21372138237363278\n",
      "Epoch: 5351 Training Loss: 0.21726374812289695 Test Loss: 0.2137151861742726\n",
      "Epoch: 5352 Training Loss: 0.21726486603275533 Test Loss: 0.21380481069807264\n",
      "Epoch: 5353 Training Loss: 0.2172647143045057 Test Loss: 0.2137257637865695\n",
      "Epoch: 5354 Training Loss: 0.21726438476649146 Test Loss: 0.21368692735710682\n",
      "Epoch: 5355 Training Loss: 0.21726605245926464 Test Loss: 0.21363458373154934\n",
      "Epoch: 5356 Training Loss: 0.21726513797446423 Test Loss: 0.2137408524394048\n",
      "Epoch: 5357 Training Loss: 0.21726500651789055 Test Loss: 0.2137639520780238\n",
      "Epoch: 5358 Training Loss: 0.2172647162052555 Test Loss: 0.2137248823188781\n",
      "Epoch: 5359 Training Loss: 0.21726412214024976 Test Loss: 0.21370454374817485\n",
      "Epoch: 5360 Training Loss: 0.21726510651346864 Test Loss: 0.2138105791263473\n",
      "Epoch: 5361 Training Loss: 0.21726622150944172 Test Loss: 0.21370655297600086\n",
      "Epoch: 5362 Training Loss: 0.21726546219575849 Test Loss: 0.2137260489672932\n",
      "Epoch: 5363 Training Loss: 0.21726556383207818 Test Loss: 0.21378709060492326\n",
      "Epoch: 5364 Training Loss: 0.21726502657438732 Test Loss: 0.21400494275230303\n",
      "Epoch: 5365 Training Loss: 0.21726406132522186 Test Loss: 0.21378029811859536\n",
      "Epoch: 5366 Training Loss: 0.21726418284768806 Test Loss: 0.21377392044059282\n",
      "Epoch: 5367 Training Loss: 0.21726513917588156 Test Loss: 0.21392436623510008\n",
      "Epoch: 5368 Training Loss: 0.2172650994304859 Test Loss: 0.21375428185893863\n",
      "Epoch: 5369 Training Loss: 0.2172649760162357 Test Loss: 0.21381300316249868\n",
      "Epoch: 5370 Training Loss: 0.21726601506290882 Test Loss: 0.21373726175483831\n",
      "Epoch: 5371 Training Loss: 0.21726611952345581 Test Loss: 0.21369607906578528\n",
      "Epoch: 5372 Training Loss: 0.21726587854065763 Test Loss: 0.21365153902184877\n",
      "Epoch: 5373 Training Loss: 0.21726361494488952 Test Loss: 0.21372297679313343\n",
      "Epoch: 5374 Training Loss: 0.2172661982342224 Test Loss: 0.2184523491010274\n",
      "Epoch: 5375 Training Loss: 0.21726636391325835 Test Loss: 0.21399733361208453\n",
      "Epoch: 5376 Training Loss: 0.2172645554215469 Test Loss: 0.21372028053901854\n",
      "Epoch: 5377 Training Loss: 0.21726528017206742 Test Loss: 0.21505789479797174\n",
      "Epoch: 5378 Training Loss: 0.21726353287194752 Test Loss: 0.21499974385585907\n",
      "Epoch: 5379 Training Loss: 0.2172635848915247 Test Loss: 0.21425487773109803\n",
      "Epoch: 5380 Training Loss: 0.21726409758292095 Test Loss: 0.2139495139898256\n",
      "Epoch: 5381 Training Loss: 0.21726256654688597 Test Loss: 0.2138281955174153\n",
      "Epoch: 5382 Training Loss: 0.2172639111211584 Test Loss: 0.21379626823912207\n",
      "Epoch: 5383 Training Loss: 0.21726289260616946 Test Loss: 0.21372083793770574\n",
      "Epoch: 5384 Training Loss: 0.21726354522682126 Test Loss: 0.21368357000222338\n",
      "Epoch: 5385 Training Loss: 0.21726515387082931 Test Loss: 0.2137048807799392\n",
      "Epoch: 5386 Training Loss: 0.21726406249974178 Test Loss: 0.21371407137689816\n",
      "Epoch: 5387 Training Loss: 0.2172651405476491 Test Loss: 0.21375210411523043\n",
      "Epoch: 5388 Training Loss: 0.21726506632874876 Test Loss: 0.21360902116849845\n",
      "Epoch: 5389 Training Loss: 0.2172660176091963 Test Loss: 0.2136455761521716\n",
      "Epoch: 5390 Training Loss: 0.21726408270865716 Test Loss: 0.21378115366076644\n",
      "Epoch: 5391 Training Loss: 0.217265319245028 Test Loss: 0.2137946867823816\n",
      "Epoch: 5392 Training Loss: 0.2172644918719497 Test Loss: 0.21373131184792132\n",
      "Epoch: 5393 Training Loss: 0.21726575470501477 Test Loss: 0.2137827740057874\n",
      "Epoch: 5394 Training Loss: 0.21726445986404028 Test Loss: 0.21380742917562653\n",
      "Epoch: 5395 Training Loss: 0.21726453750787658 Test Loss: 0.2138300880803998\n",
      "Epoch: 5396 Training Loss: 0.21726374506555884 Test Loss: 0.2137341377296379\n",
      "Epoch: 5397 Training Loss: 0.21726505216278325 Test Loss: 0.21368961064846156\n",
      "Epoch: 5398 Training Loss: 0.2172656949300198 Test Loss: 0.21508879801821174\n",
      "Epoch: 5399 Training Loss: 0.21726611884205493 Test Loss: 0.2137021456375438\n",
      "Epoch: 5400 Training Loss: 0.21726642861944387 Test Loss: 0.2136575278170463\n",
      "Epoch: 5401 Training Loss: 0.21726377406095912 Test Loss: 0.21374445608673145\n",
      "Epoch: 5402 Training Loss: 0.21726509223991353 Test Loss: 0.21372464898919508\n",
      "Epoch: 5403 Training Loss: 0.217266397848815 Test Loss: 0.21370691593328556\n",
      "Epoch: 5404 Training Loss: 0.21726385866225698 Test Loss: 0.21369021989818945\n",
      "Epoch: 5405 Training Loss: 0.21726569755699948 Test Loss: 0.21366864986527026\n",
      "Epoch: 5406 Training Loss: 0.21726684806651014 Test Loss: 0.21377976664542847\n",
      "Epoch: 5407 Training Loss: 0.21726556828808125 Test Loss: 0.2137032345093979\n",
      "Epoch: 5408 Training Loss: 0.21726554941507023 Test Loss: 0.2136390947720877\n",
      "Epoch: 5409 Training Loss: 0.21726569049194833 Test Loss: 0.2137411376201285\n",
      "Epoch: 5410 Training Loss: 0.21726612361186104 Test Loss: 0.21393959747829722\n",
      "Epoch: 5411 Training Loss: 0.21726469055409897 Test Loss: 0.2142038303815573\n",
      "Epoch: 5412 Training Loss: 0.21726264368863743 Test Loss: 0.21387525033682436\n",
      "Epoch: 5413 Training Loss: 0.217265665235287 Test Loss: 0.21380716992042317\n",
      "Epoch: 5414 Training Loss: 0.21726392127044508 Test Loss: 0.21383102139913188\n",
      "Epoch: 5415 Training Loss: 0.21726413878974216 Test Loss: 0.21377670743402888\n",
      "Epoch: 5416 Training Loss: 0.21726424333098132 Test Loss: 0.2137403728172786\n",
      "Epoch: 5417 Training Loss: 0.217265455596929 Test Loss: 0.21375402260373527\n",
      "Epoch: 5418 Training Loss: 0.21726411445655835 Test Loss: 0.2137400487482744\n",
      "Epoch: 5419 Training Loss: 0.21726450045222123 Test Loss: 0.21375848179323298\n",
      "Epoch: 5420 Training Loss: 0.21726393344600278 Test Loss: 0.21365923890138844\n",
      "Epoch: 5421 Training Loss: 0.2172651284527836 Test Loss: 0.21372276938897075\n",
      "Epoch: 5422 Training Loss: 0.21726417025970352 Test Loss: 0.21378750541324865\n",
      "Epoch: 5423 Training Loss: 0.217263531589838 Test Loss: 0.2136856181183299\n",
      "Epoch: 5424 Training Loss: 0.21726279645691995 Test Loss: 0.2136516427239301\n",
      "Epoch: 5425 Training Loss: 0.21726451102290056 Test Loss: 0.21370043455320165\n",
      "Epoch: 5426 Training Loss: 0.21726581342204526 Test Loss: 0.2137171954020986\n",
      "Epoch: 5427 Training Loss: 0.21726483879465203 Test Loss: 0.21378330547895427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5428 Training Loss: 0.21726430347357417 Test Loss: 0.2137946478941011\n",
      "Epoch: 5429 Training Loss: 0.21726403977143635 Test Loss: 0.21380993098833892\n",
      "Epoch: 5430 Training Loss: 0.2172640010122788 Test Loss: 0.21379270348007592\n",
      "Epoch: 5431 Training Loss: 0.21726585159842574 Test Loss: 0.2136885477021278\n",
      "Epoch: 5432 Training Loss: 0.21726599724786233 Test Loss: 0.213667625807217\n",
      "Epoch: 5433 Training Loss: 0.21726458093821643 Test Loss: 0.21370919737907507\n",
      "Epoch: 5434 Training Loss: 0.21726449437340817 Test Loss: 0.21389508335988106\n",
      "Epoch: 5435 Training Loss: 0.2172638178588968 Test Loss: 0.21376272061580784\n",
      "Epoch: 5436 Training Loss: 0.217263410013577 Test Loss: 0.21376185211087662\n",
      "Epoch: 5437 Training Loss: 0.21726275610184986 Test Loss: 0.21378299437271026\n",
      "Epoch: 5438 Training Loss: 0.21726524035494538 Test Loss: 0.2137965663826059\n",
      "Epoch: 5439 Training Loss: 0.21726481022064437 Test Loss: 0.21371834908775353\n",
      "Epoch: 5440 Training Loss: 0.21726325529074986 Test Loss: 0.2136854107141672\n",
      "Epoch: 5441 Training Loss: 0.2172654706325772 Test Loss: 0.21375275225323884\n",
      "Epoch: 5442 Training Loss: 0.21726357250078776 Test Loss: 0.21371523802531325\n",
      "Epoch: 5443 Training Loss: 0.21726454006312987 Test Loss: 0.21374557088410587\n",
      "Epoch: 5444 Training Loss: 0.21726395949165456 Test Loss: 0.21368748475579405\n",
      "Epoch: 5445 Training Loss: 0.2172647059663108 Test Loss: 0.21368989582918524\n",
      "Epoch: 5446 Training Loss: 0.21726520485037357 Test Loss: 0.2148578794085829\n",
      "Epoch: 5447 Training Loss: 0.21726194328923232 Test Loss: 0.21451907174607757\n",
      "Epoch: 5448 Training Loss: 0.21726247861031017 Test Loss: 0.21402789980056017\n",
      "Epoch: 5449 Training Loss: 0.21726399011883063 Test Loss: 0.213853589564584\n",
      "Epoch: 5450 Training Loss: 0.21726042164039094 Test Loss: 0.21372223791580386\n",
      "Epoch: 5451 Training Loss: 0.21726443446392618 Test Loss: 0.21378751837600882\n",
      "Epoch: 5452 Training Loss: 0.21726168857979286 Test Loss: 0.21371661207789105\n",
      "Epoch: 5453 Training Loss: 0.2172644898905077 Test Loss: 0.21376056879761998\n",
      "Epoch: 5454 Training Loss: 0.21726576949858636 Test Loss: 0.21373400810203622\n",
      "Epoch: 5455 Training Loss: 0.2172655680191072 Test Loss: 0.2136785274885181\n",
      "Epoch: 5456 Training Loss: 0.2172612091963472 Test Loss: 0.21440642536021956\n",
      "Epoch: 5457 Training Loss: 0.2172660521185642 Test Loss: 0.2138114994823192\n",
      "Epoch: 5458 Training Loss: 0.21726363653453823 Test Loss: 0.21382520111981657\n",
      "Epoch: 5459 Training Loss: 0.2172667355636397 Test Loss: 0.21375683552269167\n",
      "Epoch: 5460 Training Loss: 0.2172649470566986 Test Loss: 0.21383043807492433\n",
      "Epoch: 5461 Training Loss: 0.2172641831166621 Test Loss: 0.21382724923592306\n",
      "Epoch: 5462 Training Loss: 0.21726310771366605 Test Loss: 0.21381929010118006\n",
      "Epoch: 5463 Training Loss: 0.21726460821218296 Test Loss: 0.2138156475655729\n",
      "Epoch: 5464 Training Loss: 0.2172648012638092 Test Loss: 0.21373374884683286\n",
      "Epoch: 5465 Training Loss: 0.217263929877614 Test Loss: 0.21375709477789503\n",
      "Epoch: 5466 Training Loss: 0.2172663924245054 Test Loss: 0.2137705890112297\n",
      "Epoch: 5467 Training Loss: 0.21726426133430965 Test Loss: 0.21391740523289\n",
      "Epoch: 5468 Training Loss: 0.21726428961244587 Test Loss: 0.2137075770340541\n",
      "Epoch: 5469 Training Loss: 0.2172651491189548 Test Loss: 0.21374598569243125\n",
      "Epoch: 5470 Training Loss: 0.21726395157485232 Test Loss: 0.21372812300892005\n",
      "Epoch: 5471 Training Loss: 0.21726538582506594 Test Loss: 0.21376771127847244\n",
      "Epoch: 5472 Training Loss: 0.21726554718258578 Test Loss: 0.21382316596647022\n",
      "Epoch: 5473 Training Loss: 0.21726560402576386 Test Loss: 0.21376307061033237\n",
      "Epoch: 5474 Training Loss: 0.21726501464987202 Test Loss: 0.21370307895627588\n",
      "Epoch: 5475 Training Loss: 0.2172650865286983 Test Loss: 0.2137256860100085\n",
      "Epoch: 5476 Training Loss: 0.2172647524808862 Test Loss: 0.21374388572528405\n",
      "Epoch: 5477 Training Loss: 0.217265326856993 Test Loss: 0.21371156956418577\n",
      "Epoch: 5478 Training Loss: 0.21726479958720443 Test Loss: 0.2136645277075369\n",
      "Epoch: 5479 Training Loss: 0.2172637944043615 Test Loss: 0.213621348753418\n",
      "Epoch: 5480 Training Loss: 0.21726660590919203 Test Loss: 0.21364789648624163\n",
      "Epoch: 5481 Training Loss: 0.21726459687941055 Test Loss: 0.21371889352368056\n",
      "Epoch: 5482 Training Loss: 0.21726542557942738 Test Loss: 0.21366268699559307\n",
      "Epoch: 5483 Training Loss: 0.21726556341068554 Test Loss: 0.21365795558813183\n",
      "Epoch: 5484 Training Loss: 0.2172650638541877 Test Loss: 0.21371536765291493\n",
      "Epoch: 5485 Training Loss: 0.21726581029298073 Test Loss: 0.21370416782812998\n",
      "Epoch: 5486 Training Loss: 0.21726603501181596 Test Loss: 0.21366398327160985\n",
      "Epoch: 5487 Training Loss: 0.21726484104506805 Test Loss: 0.21369106247760036\n",
      "Epoch: 5488 Training Loss: 0.217264382444349 Test Loss: 0.2136953142629354\n",
      "Epoch: 5489 Training Loss: 0.21726567212102216 Test Loss: 0.21642906964299935\n",
      "Epoch: 5490 Training Loss: 0.21726539159904174 Test Loss: 0.213851658113319\n",
      "Epoch: 5491 Training Loss: 0.21726536696998652 Test Loss: 0.213766518704537\n",
      "Epoch: 5492 Training Loss: 0.21726496627937586 Test Loss: 0.21384780817354918\n",
      "Epoch: 5493 Training Loss: 0.21726471980950748 Test Loss: 0.21379869227527346\n",
      "Epoch: 5494 Training Loss: 0.2172630679055098 Test Loss: 0.2138316176860996\n",
      "Epoch: 5495 Training Loss: 0.21726595343199304 Test Loss: 0.21386576159638154\n",
      "Epoch: 5496 Training Loss: 0.21726369564606396 Test Loss: 0.21376794460815546\n",
      "Epoch: 5497 Training Loss: 0.21726356370533703 Test Loss: 0.21380978839797707\n",
      "Epoch: 5498 Training Loss: 0.21726477663475396 Test Loss: 0.21722404979856907\n",
      "Epoch: 5499 Training Loss: 0.21726422323862138 Test Loss: 0.21381380685362908\n",
      "Epoch: 5500 Training Loss: 0.21726488972040148 Test Loss: 0.21371547135499627\n",
      "Epoch: 5501 Training Loss: 0.21726531648356132 Test Loss: 0.21370861405486752\n",
      "Epoch: 5502 Training Loss: 0.21726448008192145 Test Loss: 0.21367513124535414\n",
      "Epoch: 5503 Training Loss: 0.21726347690741796 Test Loss: 0.21364622429018\n",
      "Epoch: 5504 Training Loss: 0.21726436137471675 Test Loss: 0.21368342741186153\n",
      "Epoch: 5505 Training Loss: 0.2172643532606669 Test Loss: 0.21378649431795557\n",
      "Epoch: 5506 Training Loss: 0.21726392647060963 Test Loss: 0.21365859076338006\n",
      "Epoch: 5507 Training Loss: 0.21726506744050808 Test Loss: 0.21374295240655197\n",
      "Epoch: 5508 Training Loss: 0.21726435574419375 Test Loss: 0.21389525187576325\n",
      "Epoch: 5509 Training Loss: 0.21726423906326006 Test Loss: 0.21375023747776628\n",
      "Epoch: 5510 Training Loss: 0.217264382578836 Test Loss: 0.21367299238992646\n",
      "Epoch: 5511 Training Loss: 0.21726305451060318 Test Loss: 0.2136590574227461\n",
      "Epoch: 5512 Training Loss: 0.21726616477385324 Test Loss: 0.21376275950408835\n",
      "Epoch: 5513 Training Loss: 0.217263957895742 Test Loss: 0.213727578572993\n",
      "Epoch: 5514 Training Loss: 0.21726415809311161 Test Loss: 0.21376745202326908\n",
      "Epoch: 5515 Training Loss: 0.21726391225084932 Test Loss: 0.21373337292678798\n",
      "Epoch: 5516 Training Loss: 0.21726649552225064 Test Loss: 0.21437169812573006\n",
      "Epoch: 5517 Training Loss: 0.21726312745635978 Test Loss: 0.21380396811866173\n",
      "Epoch: 5518 Training Loss: 0.21726318327743654 Test Loss: 0.21381748827751673\n",
      "Epoch: 5519 Training Loss: 0.21726445749706882 Test Loss: 0.21375622627296378\n",
      "Epoch: 5520 Training Loss: 0.2172646263320667 Test Loss: 0.2137128139891619\n",
      "Epoch: 5521 Training Loss: 0.21726430070314168 Test Loss: 0.21414111654786555\n",
      "Epoch: 5522 Training Loss: 0.2172653361007338 Test Loss: 0.21377782223140332\n",
      "Epoch: 5523 Training Loss: 0.21726624546606188 Test Loss: 0.21663298682319873\n",
      "Epoch: 5524 Training Loss: 0.21727021522685508 Test Loss: 0.21383579169487363\n",
      "Epoch: 5525 Training Loss: 0.21726575887411223 Test Loss: 0.21389469447707604\n",
      "Epoch: 5526 Training Loss: 0.217265188209847 Test Loss: 0.21377796482176517\n",
      "Epoch: 5527 Training Loss: 0.21726577293248814 Test Loss: 0.2137497060045994\n",
      "Epoch: 5528 Training Loss: 0.2172642300257327 Test Loss: 0.21369817903293245\n",
      "Epoch: 5529 Training Loss: 0.21726439105151793 Test Loss: 0.21370117343053122\n",
      "Epoch: 5530 Training Loss: 0.21726119369447733 Test Loss: 0.2148258873164888\n",
      "Epoch: 5531 Training Loss: 0.2172665001486039 Test Loss: 0.21384889704540325\n",
      "Epoch: 5532 Training Loss: 0.217264638391069 Test Loss: 0.2138387083159114\n",
      "Epoch: 5533 Training Loss: 0.21726347193139842 Test Loss: 0.2138526692086121\n",
      "Epoch: 5534 Training Loss: 0.21726290221750807 Test Loss: 0.21372283420277158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5535 Training Loss: 0.217265466902804 Test Loss: 0.21368591626181374\n",
      "Epoch: 5536 Training Loss: 0.21726419404597347 Test Loss: 0.21374529866614234\n",
      "Epoch: 5537 Training Loss: 0.21726552364735827 Test Loss: 0.21375653737920783\n",
      "Epoch: 5538 Training Loss: 0.21726537006318786 Test Loss: 0.2138738762842466\n",
      "Epoch: 5539 Training Loss: 0.21726590575186355 Test Loss: 0.21382109192484336\n",
      "Epoch: 5540 Training Loss: 0.21726424429032204 Test Loss: 0.2137528041042795\n",
      "Epoch: 5541 Training Loss: 0.2172641562013276 Test Loss: 0.21375559109771558\n",
      "Epoch: 5542 Training Loss: 0.21726426299298282 Test Loss: 0.21368719957507035\n",
      "Epoch: 5543 Training Loss: 0.21726581876566264 Test Loss: 0.21363922439968938\n",
      "Epoch: 5544 Training Loss: 0.21726429858721263 Test Loss: 0.21369618276786662\n",
      "Epoch: 5545 Training Loss: 0.21726309555603995 Test Loss: 0.21447659278100775\n",
      "Epoch: 5546 Training Loss: 0.21726752395341542 Test Loss: 0.21387558736858872\n",
      "Epoch: 5547 Training Loss: 0.21726376140124817 Test Loss: 0.21386331163470984\n",
      "Epoch: 5548 Training Loss: 0.21726419681640596 Test Loss: 0.21375785958074492\n",
      "Epoch: 5549 Training Loss: 0.2172631389953456 Test Loss: 0.2138182271548463\n",
      "Epoch: 5550 Training Loss: 0.21726406645366 Test Loss: 0.21385606545177605\n",
      "Epoch: 5551 Training Loss: 0.21726450361714897 Test Loss: 0.21378928131139163\n",
      "Epoch: 5552 Training Loss: 0.21726385007301965 Test Loss: 0.2137468412346023\n",
      "Epoch: 5553 Training Loss: 0.2172651323798044 Test Loss: 0.2137345266124429\n",
      "Epoch: 5554 Training Loss: 0.21726466959205631 Test Loss: 0.21374837084030213\n",
      "Epoch: 5555 Training Loss: 0.2172636260445511 Test Loss: 0.21630692155393835\n",
      "Epoch: 5556 Training Loss: 0.2172655219169587 Test Loss: 0.21382114377588404\n",
      "Epoch: 5557 Training Loss: 0.21726542051374984 Test Loss: 0.21370217156306415\n",
      "Epoch: 5558 Training Loss: 0.21726419857370294 Test Loss: 0.21368994768022592\n",
      "Epoch: 5559 Training Loss: 0.21726535224814134 Test Loss: 0.2137621372916003\n",
      "Epoch: 5560 Training Loss: 0.2172652469537749 Test Loss: 0.21381229021068945\n",
      "Epoch: 5561 Training Loss: 0.21726320665127966 Test Loss: 0.21407127319608155\n",
      "Epoch: 5562 Training Loss: 0.2172631418554361 Test Loss: 0.21388818717147182\n",
      "Epoch: 5563 Training Loss: 0.21726299486112938 Test Loss: 0.21376348541865775\n",
      "Epoch: 5564 Training Loss: 0.21726426814831837 Test Loss: 0.21526636190699003\n",
      "Epoch: 5565 Training Loss: 0.21726492973477116 Test Loss: 0.21369250134397896\n",
      "Epoch: 5566 Training Loss: 0.2172641878685366 Test Loss: 0.21375145597722203\n",
      "Epoch: 5567 Training Loss: 0.21726420259934756 Test Loss: 0.21372330086213762\n",
      "Epoch: 5568 Training Loss: 0.21726476228050665 Test Loss: 0.2136885477021278\n",
      "Epoch: 5569 Training Loss: 0.21726316818799352 Test Loss: 0.2137610095314657\n",
      "Epoch: 5570 Training Loss: 0.21726386197063755 Test Loss: 0.2137865980200369\n",
      "Epoch: 5571 Training Loss: 0.2172644656649135 Test Loss: 0.21378518507917862\n",
      "Epoch: 5572 Training Loss: 0.21726268147052266 Test Loss: 0.21479348041606935\n",
      "Epoch: 5573 Training Loss: 0.21726511963940126 Test Loss: 0.21377399821715382\n",
      "Epoch: 5574 Training Loss: 0.21726401640655907 Test Loss: 0.21407859715557634\n",
      "Epoch: 5575 Training Loss: 0.21726448484276176 Test Loss: 0.21376281135512903\n",
      "Epoch: 5576 Training Loss: 0.21726363181852693 Test Loss: 0.21381865492593183\n",
      "Epoch: 5577 Training Loss: 0.21726420528908785 Test Loss: 0.21373999689723372\n",
      "Epoch: 5578 Training Loss: 0.21726440074354877 Test Loss: 0.2137760592960205\n",
      "Epoch: 5579 Training Loss: 0.21726564956306693 Test Loss: 0.21387970952632207\n",
      "Epoch: 5580 Training Loss: 0.21726468962165565 Test Loss: 0.21378513322813794\n",
      "Epoch: 5581 Training Loss: 0.21726386506383888 Test Loss: 0.2137520393014296\n",
      "Epoch: 5582 Training Loss: 0.21726459313170574 Test Loss: 0.21371856945467638\n",
      "Epoch: 5583 Training Loss: 0.21726488029734467 Test Loss: 0.21372271753793007\n",
      "Epoch: 5584 Training Loss: 0.21726510016568157 Test Loss: 0.2138178641975616\n",
      "Epoch: 5585 Training Loss: 0.21726501370846293 Test Loss: 0.2138098921000584\n",
      "Epoch: 5586 Training Loss: 0.21726644286610158 Test Loss: 0.2136866551391433\n",
      "Epoch: 5587 Training Loss: 0.21726500227706672 Test Loss: 0.21374605050623208\n",
      "Epoch: 5588 Training Loss: 0.2172649200786035 Test Loss: 0.21378010367719286\n",
      "Epoch: 5589 Training Loss: 0.21726428429572592 Test Loss: 0.21372585452589068\n",
      "Epoch: 5590 Training Loss: 0.2172654558748688 Test Loss: 0.21370770666165578\n",
      "Epoch: 5591 Training Loss: 0.21726698767299688 Test Loss: 0.21514604156711267\n",
      "Epoch: 5592 Training Loss: 0.21726546924287804 Test Loss: 0.21375761328830173\n",
      "Epoch: 5593 Training Loss: 0.21726579090891907 Test Loss: 0.21364212805796698\n",
      "Epoch: 5594 Training Loss: 0.2172668815268793 Test Loss: 0.2139962188147101\n",
      "Epoch: 5595 Training Loss: 0.21726388823146855 Test Loss: 0.21426593496552113\n",
      "Epoch: 5596 Training Loss: 0.21726271576471134 Test Loss: 0.21389552409372678\n",
      "Epoch: 5597 Training Loss: 0.21726453019178302 Test Loss: 0.21383944719324094\n",
      "Epoch: 5598 Training Loss: 0.21726398572558817 Test Loss: 0.21369895679854253\n",
      "Epoch: 5599 Training Loss: 0.21726490014762798 Test Loss: 0.21497148503869332\n",
      "Epoch: 5600 Training Loss: 0.21726519936330338 Test Loss: 0.2136999678938356\n",
      "Epoch: 5601 Training Loss: 0.21726494592700768 Test Loss: 0.21368764030891604\n",
      "Epoch: 5602 Training Loss: 0.21726547892594308 Test Loss: 0.21370747333197276\n",
      "Epoch: 5603 Training Loss: 0.21726504963442736 Test Loss: 0.21369986419175427\n",
      "Epoch: 5604 Training Loss: 0.21726482246792847 Test Loss: 0.21369243653017814\n",
      "Epoch: 5605 Training Loss: 0.21726586170288342 Test Loss: 0.21369291615230435\n",
      "Epoch: 5606 Training Loss: 0.21726487746415155 Test Loss: 0.21362492647522432\n",
      "Epoch: 5607 Training Loss: 0.21726669493959555 Test Loss: 0.21373872654673728\n",
      "Epoch: 5608 Training Loss: 0.21726679019226494 Test Loss: 0.2137365876913096\n",
      "Epoch: 5609 Training Loss: 0.21726497754042187 Test Loss: 0.21376951310213577\n",
      "Epoch: 5610 Training Loss: 0.21726449316302504 Test Loss: 0.21374896712726982\n",
      "Epoch: 5611 Training Loss: 0.2172640980581084 Test Loss: 0.21372086386322608\n",
      "Epoch: 5612 Training Loss: 0.21726433118686492 Test Loss: 0.21376234469576297\n",
      "Epoch: 5613 Training Loss: 0.21726478193354234 Test Loss: 0.2137758907801383\n",
      "Epoch: 5614 Training Loss: 0.2172649125293991 Test Loss: 0.21387272259859164\n",
      "Epoch: 5615 Training Loss: 0.21726549072493714 Test Loss: 0.21381304205077917\n",
      "Epoch: 5616 Training Loss: 0.21726406200662274 Test Loss: 0.2137348117931666\n",
      "Epoch: 5617 Training Loss: 0.21726354894762864 Test Loss: 0.21377538523249176\n",
      "Epoch: 5618 Training Loss: 0.2172635362430887 Test Loss: 0.21374516903854068\n",
      "Epoch: 5619 Training Loss: 0.21726605082748887 Test Loss: 0.21380567920300386\n",
      "Epoch: 5620 Training Loss: 0.21726450896973215 Test Loss: 0.21380131075282732\n",
      "Epoch: 5621 Training Loss: 0.2172659576100563 Test Loss: 0.21382186969045344\n",
      "Epoch: 5622 Training Loss: 0.217265042399026 Test Loss: 0.21375281706703966\n",
      "Epoch: 5623 Training Loss: 0.21726344471122672 Test Loss: 0.21370816035826165\n",
      "Epoch: 5624 Training Loss: 0.21726528199212503 Test Loss: 0.2136768034414158\n",
      "Epoch: 5625 Training Loss: 0.21726318316984694 Test Loss: 0.21378552211094298\n",
      "Epoch: 5626 Training Loss: 0.21726447162717114 Test Loss: 0.21370572335935012\n",
      "Epoch: 5627 Training Loss: 0.2172627780590964 Test Loss: 0.21387029856244028\n",
      "Epoch: 5628 Training Loss: 0.21726083316168918 Test Loss: 0.21376335579105607\n",
      "Epoch: 5629 Training Loss: 0.21726281216500326 Test Loss: 0.21392764581342252\n",
      "Epoch: 5630 Training Loss: 0.21726378547442377 Test Loss: 0.2137455579213457\n",
      "Epoch: 5631 Training Loss: 0.21726588912030276 Test Loss: 0.21379205534206755\n",
      "Epoch: 5632 Training Loss: 0.2172650936744417 Test Loss: 0.21375407445477593\n",
      "Epoch: 5633 Training Loss: 0.21726541086654802 Test Loss: 0.2137216157033158\n",
      "Epoch: 5634 Training Loss: 0.21726457387316528 Test Loss: 0.2137417727953767\n",
      "Epoch: 5635 Training Loss: 0.21726356583023185 Test Loss: 0.2137696168042171\n",
      "Epoch: 5636 Training Loss: 0.2172656679698563 Test Loss: 0.2136994882717094\n",
      "Epoch: 5637 Training Loss: 0.2172647433985298 Test Loss: 0.21376658351833785\n",
      "Epoch: 5638 Training Loss: 0.21726494268138774 Test Loss: 0.2137223027296047\n",
      "Epoch: 5639 Training Loss: 0.21726492604086114 Test Loss: 0.21372096756530742\n",
      "Epoch: 5640 Training Loss: 0.21726358718676975 Test Loss: 0.21382223264773814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5641 Training Loss: 0.2172642458324398 Test Loss: 0.21373800063216788\n",
      "Epoch: 5642 Training Loss: 0.21726364905079637 Test Loss: 0.21378567766406498\n",
      "Epoch: 5643 Training Loss: 0.21726508108645715 Test Loss: 0.21385045257662338\n",
      "Epoch: 5644 Training Loss: 0.21726525803550487 Test Loss: 0.2137921071931082\n",
      "Epoch: 5645 Training Loss: 0.21726511408957047 Test Loss: 0.2137256082334475\n",
      "Epoch: 5646 Training Loss: 0.2172656286010243 Test Loss: 0.21375793735730594\n",
      "Epoch: 5647 Training Loss: 0.2172646096736085 Test Loss: 0.21366170182582034\n",
      "Epoch: 5648 Training Loss: 0.2172641289990875 Test Loss: 0.21374264130030796\n",
      "Epoch: 5649 Training Loss: 0.21726386971708955 Test Loss: 0.21376218914264097\n",
      "Epoch: 5650 Training Loss: 0.2172658596407492 Test Loss: 0.2137854702599023\n",
      "Epoch: 5651 Training Loss: 0.21726541905232427 Test Loss: 0.2136782552705546\n",
      "Epoch: 5652 Training Loss: 0.21726453047868863 Test Loss: 0.21373688583479347\n",
      "Epoch: 5653 Training Loss: 0.21726551503122354 Test Loss: 0.2137103121764495\n",
      "Epoch: 5654 Training Loss: 0.2172652957366979 Test Loss: 0.2136642425268132\n",
      "Epoch: 5655 Training Loss: 0.2172665578524989 Test Loss: 0.21361275444342678\n",
      "Epoch: 5656 Training Loss: 0.21726540739678304 Test Loss: 0.21369602721474462\n",
      "Epoch: 5657 Training Loss: 0.2172638765759273 Test Loss: 0.21368013487077891\n",
      "Epoch: 5658 Training Loss: 0.21726541672121605 Test Loss: 0.21372055275698204\n",
      "Epoch: 5659 Training Loss: 0.21726555711669326 Test Loss: 0.21373124703412047\n",
      "Epoch: 5660 Training Loss: 0.21726572917041365 Test Loss: 0.213791627570982\n",
      "Epoch: 5661 Training Loss: 0.21726551392843002 Test Loss: 0.21370839368794467\n",
      "Epoch: 5662 Training Loss: 0.21726539198457118 Test Loss: 0.21372454528711374\n",
      "Epoch: 5663 Training Loss: 0.2172653707894177 Test Loss: 0.21372406566498753\n",
      "Epoch: 5664 Training Loss: 0.21726469726051809 Test Loss: 0.21372687858394393\n",
      "Epoch: 5665 Training Loss: 0.21726421603011742 Test Loss: 0.2137543725982598\n",
      "Epoch: 5666 Training Loss: 0.2172642029490138 Test Loss: 0.2137114399365841\n",
      "Epoch: 5667 Training Loss: 0.21726493092722268 Test Loss: 0.21372442862227223\n",
      "Epoch: 5668 Training Loss: 0.21726559193089837 Test Loss: 0.21384960999721248\n",
      "Epoch: 5669 Training Loss: 0.2172656016946556 Test Loss: 0.21376864459720452\n",
      "Epoch: 5670 Training Loss: 0.21726501135045728 Test Loss: 0.21381256242865296\n",
      "Epoch: 5671 Training Loss: 0.21726642069367583 Test Loss: 0.21375503369902835\n",
      "Epoch: 5672 Training Loss: 0.21726287182344284 Test Loss: 0.2137202935017787\n",
      "Epoch: 5673 Training Loss: 0.21726520736079785 Test Loss: 0.2136588111303029\n",
      "Epoch: 5674 Training Loss: 0.21726364238024046 Test Loss: 0.2135649088956475\n",
      "Epoch: 5675 Training Loss: 0.21726541782400954 Test Loss: 0.21370831591138367\n",
      "Epoch: 5676 Training Loss: 0.2172645638135366 Test Loss: 0.21364805203936366\n",
      "Epoch: 5677 Training Loss: 0.21726343768203876 Test Loss: 0.21368773104823724\n",
      "Epoch: 5678 Training Loss: 0.21726496228062864 Test Loss: 0.21360723230759532\n",
      "Epoch: 5679 Training Loss: 0.21726355713340492 Test Loss: 0.21369016804714877\n",
      "Epoch: 5680 Training Loss: 0.217263735597673 Test Loss: 0.2137756963387358\n",
      "Epoch: 5681 Training Loss: 0.2172647676958504 Test Loss: 0.2137424598216656\n",
      "Epoch: 5682 Training Loss: 0.2172634457691912 Test Loss: 0.21376558538580492\n",
      "Epoch: 5683 Training Loss: 0.21726545597349264 Test Loss: 0.2137122565904747\n",
      "Epoch: 5684 Training Loss: 0.217265375415771 Test Loss: 0.2137451171875\n",
      "Epoch: 5685 Training Loss: 0.2172656360874681 Test Loss: 0.21379635897844323\n",
      "Epoch: 5686 Training Loss: 0.2172652226564543 Test Loss: 0.2138870205230567\n",
      "Epoch: 5687 Training Loss: 0.21726272730369717 Test Loss: 0.21373190813488904\n",
      "Epoch: 5688 Training Loss: 0.2172654583852931 Test Loss: 0.21370647519943983\n",
      "Epoch: 5689 Training Loss: 0.2172651975522116 Test Loss: 0.2136968697941555\n",
      "Epoch: 5690 Training Loss: 0.21726508742527842 Test Loss: 0.21419718048559122\n",
      "Epoch: 5691 Training Loss: 0.217264002805439 Test Loss: 0.2136603925870434\n",
      "Epoch: 5692 Training Loss: 0.2172657192542378 Test Loss: 0.2137453634799432\n",
      "Epoch: 5693 Training Loss: 0.21726375813769663 Test Loss: 0.21372015091141686\n",
      "Epoch: 5694 Training Loss: 0.21726376205575165 Test Loss: 0.21370380487084528\n",
      "Epoch: 5695 Training Loss: 0.21726460654454396 Test Loss: 0.2137118417821493\n",
      "Epoch: 5696 Training Loss: 0.21726356574057384 Test Loss: 0.21376747794878942\n",
      "Epoch: 5697 Training Loss: 0.21726384819916728 Test Loss: 0.213745194964061\n",
      "Epoch: 5698 Training Loss: 0.21726478969792598 Test Loss: 0.2136343115135858\n",
      "Epoch: 5699 Training Loss: 0.21726565721089516 Test Loss: 0.2137678927571148\n",
      "Epoch: 5700 Training Loss: 0.2172646544488185 Test Loss: 0.21377688891267124\n",
      "Epoch: 5701 Training Loss: 0.21726396093514852 Test Loss: 0.21377389451507248\n",
      "Epoch: 5702 Training Loss: 0.21726288588181875 Test Loss: 0.21374054133316078\n",
      "Epoch: 5703 Training Loss: 0.21726463814002656 Test Loss: 0.21386430976724274\n",
      "Epoch: 5704 Training Loss: 0.21726330220878626 Test Loss: 0.21381645125670332\n",
      "Epoch: 5705 Training Loss: 0.21726267771385208 Test Loss: 0.21381904380873687\n",
      "Epoch: 5706 Training Loss: 0.21726171909937933 Test Loss: 0.21374977081840024\n",
      "Epoch: 5707 Training Loss: 0.21726165364903233 Test Loss: 0.2137769018754314\n",
      "Epoch: 5708 Training Loss: 0.21726360337900627 Test Loss: 0.21377840555561087\n",
      "Epoch: 5709 Training Loss: 0.21726527959825617 Test Loss: 0.21377980553370898\n",
      "Epoch: 5710 Training Loss: 0.21726498294679983 Test Loss: 0.2136650462179436\n",
      "Epoch: 5711 Training Loss: 0.21726497922599244 Test Loss: 0.21378794614709434\n",
      "Epoch: 5712 Training Loss: 0.21726452327915047 Test Loss: 0.21374323758727568\n",
      "Epoch: 5713 Training Loss: 0.217264752669168 Test Loss: 0.21364241323869065\n",
      "Epoch: 5714 Training Loss: 0.21726444513322932 Test Loss: 0.21372138237363278\n",
      "Epoch: 5715 Training Loss: 0.2172644776163262 Test Loss: 0.21367552012815919\n",
      "Epoch: 5716 Training Loss: 0.21726475662308622 Test Loss: 0.21365226493641817\n",
      "Epoch: 5717 Training Loss: 0.21726636950791814 Test Loss: 0.21367940895620952\n",
      "Epoch: 5718 Training Loss: 0.21726434551421486 Test Loss: 0.21372120089499044\n",
      "Epoch: 5719 Training Loss: 0.21726587865721306 Test Loss: 0.21370366228048343\n",
      "Epoch: 5720 Training Loss: 0.21726494191929466 Test Loss: 0.21373758582384253\n",
      "Epoch: 5721 Training Loss: 0.21726459355309838 Test Loss: 0.21376251321164516\n",
      "Epoch: 5722 Training Loss: 0.21726396229795028 Test Loss: 0.21371383804721514\n",
      "Epoch: 5723 Training Loss: 0.2172638131339197 Test Loss: 0.2136457317052936\n",
      "Epoch: 5724 Training Loss: 0.21726566377386147 Test Loss: 0.21373278960258044\n",
      "Epoch: 5725 Training Loss: 0.21726444954440338 Test Loss: 0.2136790978499655\n",
      "Epoch: 5726 Training Loss: 0.2172664046359263 Test Loss: 0.2136263394160826\n",
      "Epoch: 5727 Training Loss: 0.21726479730989098 Test Loss: 0.21364937424090077\n",
      "Epoch: 5728 Training Loss: 0.2172657259068621 Test Loss: 0.21373813025976957\n",
      "Epoch: 5729 Training Loss: 0.21726405026142348 Test Loss: 0.2136813533702347\n",
      "Epoch: 5730 Training Loss: 0.21726461677452288 Test Loss: 0.2150993497049883\n",
      "Epoch: 5731 Training Loss: 0.21726383483115802 Test Loss: 0.21382463075836916\n",
      "Epoch: 5732 Training Loss: 0.2172640129995547 Test Loss: 0.21370613816767547\n",
      "Epoch: 5733 Training Loss: 0.21726402852832194 Test Loss: 0.21374108576908782\n",
      "Epoch: 5734 Training Loss: 0.21726547278436942 Test Loss: 0.21376014102653446\n",
      "Epoch: 5735 Training Loss: 0.2172617812861748 Test Loss: 0.21427099044198655\n",
      "Epoch: 5736 Training Loss: 0.21726451087048193 Test Loss: 0.213808932855806\n",
      "Epoch: 5737 Training Loss: 0.2172651079121336 Test Loss: 0.21377182047344562\n",
      "Epoch: 5738 Training Loss: 0.2172643806332572 Test Loss: 0.21377525560489008\n",
      "Epoch: 5739 Training Loss: 0.2172651781233209 Test Loss: 0.21378427768596686\n",
      "Epoch: 5740 Training Loss: 0.21726449509067225 Test Loss: 0.2137598428830506\n",
      "Epoch: 5741 Training Loss: 0.21726295267703588 Test Loss: 0.21379616453704073\n",
      "Epoch: 5742 Training Loss: 0.21726512702722126 Test Loss: 0.21393194944979824\n",
      "Epoch: 5743 Training Loss: 0.21726167301516242 Test Loss: 0.21402932570417862\n",
      "Epoch: 5744 Training Loss: 0.21726313925535384 Test Loss: 0.2139710710599846\n",
      "Epoch: 5745 Training Loss: 0.2172632884462818 Test Loss: 0.21375719847997637\n",
      "Epoch: 5746 Training Loss: 0.21726365093361458 Test Loss: 0.21374939489835537\n",
      "Epoch: 5747 Training Loss: 0.2172646938266163 Test Loss: 0.2137008234360067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5748 Training Loss: 0.21726356639507732 Test Loss: 0.21370683815672453\n",
      "Epoch: 5749 Training Loss: 0.21726502652955829 Test Loss: 0.21378561285026415\n",
      "Epoch: 5750 Training Loss: 0.2172642469262675 Test Loss: 0.2136821570613651\n",
      "Epoch: 5751 Training Loss: 0.2172631005051621 Test Loss: 0.213801051497624\n",
      "Epoch: 5752 Training Loss: 0.21726421189688316 Test Loss: 0.21380483662359298\n",
      "Epoch: 5753 Training Loss: 0.21726403595200514 Test Loss: 0.21376102249422588\n",
      "Epoch: 5754 Training Loss: 0.21726454430395373 Test Loss: 0.21369651979963097\n",
      "Epoch: 5755 Training Loss: 0.21726622108804908 Test Loss: 0.21457513568380324\n",
      "Epoch: 5756 Training Loss: 0.2172622122811927 Test Loss: 0.2145415491722085\n",
      "Epoch: 5757 Training Loss: 0.21726445975645067 Test Loss: 0.21410772447767334\n",
      "Epoch: 5758 Training Loss: 0.2172634026436886 Test Loss: 0.21379329976704364\n",
      "Epoch: 5759 Training Loss: 0.21726656762522192 Test Loss: 0.2137724815742142\n",
      "Epoch: 5760 Training Loss: 0.21726186704406095 Test Loss: 0.21374892823898933\n",
      "Epoch: 5761 Training Loss: 0.21726484950878416 Test Loss: 0.21361399886840288\n",
      "Epoch: 5762 Training Loss: 0.21726508885980655 Test Loss: 0.21375920770780238\n",
      "Epoch: 5763 Training Loss: 0.21726317464337022 Test Loss: 0.2136272338465342\n",
      "Epoch: 5764 Training Loss: 0.2172658529522617 Test Loss: 0.21382463075836916\n",
      "Epoch: 5765 Training Loss: 0.21726380250047977 Test Loss: 0.21364784463520098\n",
      "Epoch: 5766 Training Loss: 0.21726627309866042 Test Loss: 0.2137601669520548\n",
      "Epoch: 5767 Training Loss: 0.21726421638874943 Test Loss: 0.21363782442159127\n",
      "Epoch: 5768 Training Loss: 0.2172659549920424 Test Loss: 0.21370167897817777\n",
      "Epoch: 5769 Training Loss: 0.21726481050755 Test Loss: 0.21374667271872014\n",
      "Epoch: 5770 Training Loss: 0.21726496176061216 Test Loss: 0.21378119254904693\n",
      "Epoch: 5771 Training Loss: 0.2172654481822116 Test Loss: 0.21371680651929356\n",
      "Epoch: 5772 Training Loss: 0.21726435085783222 Test Loss: 0.21374067096076246\n",
      "Epoch: 5773 Training Loss: 0.2172641301108468 Test Loss: 0.21372986001878252\n",
      "Epoch: 5774 Training Loss: 0.21726506374659807 Test Loss: 0.2137086270176277\n",
      "Epoch: 5775 Training Loss: 0.21726517023341607 Test Loss: 0.2138176179051184\n",
      "Epoch: 5776 Training Loss: 0.2172620225558786 Test Loss: 0.21427143117583228\n",
      "Epoch: 5777 Training Loss: 0.2172655705115999 Test Loss: 0.2138065477079351\n",
      "Epoch: 5778 Training Loss: 0.2172636065618656 Test Loss: 0.21372799338131837\n",
      "Epoch: 5779 Training Loss: 0.21726717442166504 Test Loss: 0.21388190023279044\n",
      "Epoch: 5780 Training Loss: 0.2172643291964571 Test Loss: 0.21380823286675693\n",
      "Epoch: 5781 Training Loss: 0.21726392944725556 Test Loss: 0.21391048311896038\n",
      "Epoch: 5782 Training Loss: 0.21726226705327073 Test Loss: 0.21399177258797256\n",
      "Epoch: 5783 Training Loss: 0.21726453632439086 Test Loss: 0.21385147663467666\n",
      "Epoch: 5784 Training Loss: 0.21726268041255817 Test Loss: 0.21376478169467453\n",
      "Epoch: 5785 Training Loss: 0.21726475876591267 Test Loss: 0.21378738874840714\n",
      "Epoch: 5786 Training Loss: 0.21726470836914547 Test Loss: 0.21371364360581263\n",
      "Epoch: 5787 Training Loss: 0.21726559340128973 Test Loss: 0.21372682673290325\n",
      "Epoch: 5788 Training Loss: 0.2172651592772073 Test Loss: 0.21370427153021132\n",
      "Epoch: 5789 Training Loss: 0.21726492240971176 Test Loss: 0.2137100269957258\n",
      "Epoch: 5790 Training Loss: 0.2172642758409756 Test Loss: 0.2137543337099793\n",
      "Epoch: 5791 Training Loss: 0.21726482394728563 Test Loss: 0.213696675352753\n",
      "Epoch: 5792 Training Loss: 0.2172651443850119 Test Loss: 0.2138134827846249\n",
      "Epoch: 5793 Training Loss: 0.21726422614354088 Test Loss: 0.21370778443821678\n",
      "Epoch: 5794 Training Loss: 0.21726456064860886 Test Loss: 0.21462528860289237\n",
      "Epoch: 5795 Training Loss: 0.21726250716638623 Test Loss: 0.2141685457483806\n",
      "Epoch: 5796 Training Loss: 0.21726492316283905 Test Loss: 0.21383942126772063\n",
      "Epoch: 5797 Training Loss: 0.21726363513587327 Test Loss: 0.2138232567057914\n",
      "Epoch: 5798 Training Loss: 0.21726136424194317 Test Loss: 0.21371089550065706\n",
      "Epoch: 5799 Training Loss: 0.21726415918693934 Test Loss: 0.21371363064305246\n",
      "Epoch: 5800 Training Loss: 0.21726519039750242 Test Loss: 0.21381480498616198\n",
      "Epoch: 5801 Training Loss: 0.2172643944854197 Test Loss: 0.21376371874834077\n",
      "Epoch: 5802 Training Loss: 0.21726382850130255 Test Loss: 0.2138219604297746\n",
      "Epoch: 5803 Training Loss: 0.2172633491537201 Test Loss: 0.21375858549531432\n",
      "Epoch: 5804 Training Loss: 0.21726542900436335 Test Loss: 0.21371943795960763\n",
      "Epoch: 5805 Training Loss: 0.21726375460517106 Test Loss: 0.2136591351993071\n",
      "Epoch: 5806 Training Loss: 0.21726500962005768 Test Loss: 0.21379917189739964\n",
      "Epoch: 5807 Training Loss: 0.21726580678735255 Test Loss: 0.21375507258730886\n",
      "Epoch: 5808 Training Loss: 0.2172651470299232 Test Loss: 0.21366433326613438\n",
      "Epoch: 5809 Training Loss: 0.21726509415859493 Test Loss: 0.21373679509547228\n",
      "Epoch: 5810 Training Loss: 0.2172654894159302 Test Loss: 0.21378676653591908\n",
      "Epoch: 5811 Training Loss: 0.21726489749375089 Test Loss: 0.21375516332663003\n",
      "Epoch: 5812 Training Loss: 0.21726457609668393 Test Loss: 0.21371898426300176\n",
      "Epoch: 5813 Training Loss: 0.21726600314735936 Test Loss: 0.21366986836472604\n",
      "Epoch: 5814 Training Loss: 0.21726431554154224 Test Loss: 0.213702314153426\n",
      "Epoch: 5815 Training Loss: 0.21726528234179127 Test Loss: 0.21400693901736886\n",
      "Epoch: 5816 Training Loss: 0.2172666072540622 Test Loss: 0.21387750585709356\n",
      "Epoch: 5817 Training Loss: 0.21726499823349046 Test Loss: 0.21379768117998035\n",
      "Epoch: 5818 Training Loss: 0.21726379232429568 Test Loss: 0.21381043653598544\n",
      "Epoch: 5819 Training Loss: 0.21726335198691318 Test Loss: 0.21378155550633163\n",
      "Epoch: 5820 Training Loss: 0.21726491419703808 Test Loss: 0.21377861295977355\n",
      "Epoch: 5821 Training Loss: 0.21726443633777856 Test Loss: 0.21386109500272113\n",
      "Epoch: 5822 Training Loss: 0.2172638578284375 Test Loss: 0.21370538632758576\n",
      "Epoch: 5823 Training Loss: 0.217265758658933 Test Loss: 0.2137229119793326\n",
      "Epoch: 5824 Training Loss: 0.21726461293716007 Test Loss: 0.21375111894545767\n",
      "Epoch: 5825 Training Loss: 0.21726396040616627 Test Loss: 0.21372936743389614\n",
      "Epoch: 5826 Training Loss: 0.2172640718421064 Test Loss: 0.21373941357302617\n",
      "Epoch: 5827 Training Loss: 0.2172637746616678 Test Loss: 0.21380566624024372\n",
      "Epoch: 5828 Training Loss: 0.21726364696176473 Test Loss: 0.2138105013497863\n",
      "Epoch: 5829 Training Loss: 0.217264000868826 Test Loss: 0.2138978055395163\n",
      "Epoch: 5830 Training Loss: 0.21726436758801682 Test Loss: 0.21373623769678507\n",
      "Epoch: 5831 Training Loss: 0.2172642123720706 Test Loss: 0.21373604325538256\n",
      "Epoch: 5832 Training Loss: 0.21726511196467563 Test Loss: 0.21375502073626818\n",
      "Epoch: 5833 Training Loss: 0.21726368661750237 Test Loss: 0.21357639390115615\n",
      "Epoch: 5834 Training Loss: 0.21726309674849148 Test Loss: 0.2136762719682489\n",
      "Epoch: 5835 Training Loss: 0.21726417324531522 Test Loss: 0.21474069605666612\n",
      "Epoch: 5836 Training Loss: 0.2172596806169417 Test Loss: 0.2172495864360996\n",
      "Epoch: 5837 Training Loss: 0.21726227746256566 Test Loss: 0.2143818609297016\n",
      "Epoch: 5838 Training Loss: 0.2172628249771328 Test Loss: 0.21393210500292026\n",
      "Epoch: 5839 Training Loss: 0.21726415595925097 Test Loss: 0.21379427197405623\n",
      "Epoch: 5840 Training Loss: 0.21726557035918126 Test Loss: 0.21364431876443532\n",
      "Epoch: 5841 Training Loss: 0.2172644505127099 Test Loss: 0.213720008321055\n",
      "Epoch: 5842 Training Loss: 0.21726255118846893 Test Loss: 0.21367673862761496\n",
      "Epoch: 5843 Training Loss: 0.21726431396356127 Test Loss: 0.2137580280966271\n",
      "Epoch: 5844 Training Loss: 0.21726328820420515 Test Loss: 0.21370087528704737\n",
      "Epoch: 5845 Training Loss: 0.21726486442787696 Test Loss: 0.21376291505721037\n",
      "Epoch: 5846 Training Loss: 0.21726470777740262 Test Loss: 0.21371930833200595\n",
      "Epoch: 5847 Training Loss: 0.21726308330875585 Test Loss: 0.21374619309659393\n",
      "Epoch: 5848 Training Loss: 0.21726360800535957 Test Loss: 0.21368822363312362\n",
      "Epoch: 5849 Training Loss: 0.21726443505566903 Test Loss: 0.21369488649184984\n",
      "Epoch: 5850 Training Loss: 0.21726399020848866 Test Loss: 0.21373011927398589\n",
      "Epoch: 5851 Training Loss: 0.21726504074035283 Test Loss: 0.21375977806924976\n",
      "Epoch: 5852 Training Loss: 0.21726515800406357 Test Loss: 0.2136802644983806\n",
      "Epoch: 5853 Training Loss: 0.2172637252332071 Test Loss: 0.21373205072525087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5854 Training Loss: 0.21726508675284334 Test Loss: 0.21380256814056361\n",
      "Epoch: 5855 Training Loss: 0.21726360756603533 Test Loss: 0.21376432799806866\n",
      "Epoch: 5856 Training Loss: 0.2172633830713451 Test Loss: 0.21376010213825394\n",
      "Epoch: 5857 Training Loss: 0.21726351766594912 Test Loss: 0.21389472040259636\n",
      "Epoch: 5858 Training Loss: 0.21726419022654225 Test Loss: 0.21379148498062014\n",
      "Epoch: 5859 Training Loss: 0.21726444758985877 Test Loss: 0.2137997163333267\n",
      "Epoch: 5860 Training Loss: 0.2172651881919154 Test Loss: 0.21372607489281353\n",
      "Epoch: 5861 Training Loss: 0.2172662915054498 Test Loss: 0.2137521948545516\n",
      "Epoch: 5862 Training Loss: 0.21726409869468025 Test Loss: 0.2137535689071294\n",
      "Epoch: 5863 Training Loss: 0.2172636672603381 Test Loss: 0.21371596393988265\n",
      "Epoch: 5864 Training Loss: 0.21726599419949 Test Loss: 0.2137008234360067\n",
      "Epoch: 5865 Training Loss: 0.21726536636031205 Test Loss: 0.21371347508993044\n",
      "Epoch: 5866 Training Loss: 0.21726528955926103 Test Loss: 0.21372996372086386\n",
      "Epoch: 5867 Training Loss: 0.2172642289588024 Test Loss: 0.21368035523770176\n",
      "Epoch: 5868 Training Loss: 0.21726413327577457 Test Loss: 0.21374811158509877\n",
      "Epoch: 5869 Training Loss: 0.2172646534894778 Test Loss: 0.2137711075216364\n",
      "Epoch: 5870 Training Loss: 0.21726383147794848 Test Loss: 0.2137216157033158\n",
      "Epoch: 5871 Training Loss: 0.2172660941592049 Test Loss: 0.21373866173293646\n",
      "Epoch: 5872 Training Loss: 0.21726485696833056 Test Loss: 0.21369610499130562\n",
      "Epoch: 5873 Training Loss: 0.2172647356700094 Test Loss: 0.2138096587703754\n",
      "Epoch: 5874 Training Loss: 0.21726444230003622 Test Loss: 0.21378230734642137\n",
      "Epoch: 5875 Training Loss: 0.21726439782966345 Test Loss: 0.2137228212400114\n",
      "Epoch: 5876 Training Loss: 0.2172645704482293 Test Loss: 0.21367186462979187\n",
      "Epoch: 5877 Training Loss: 0.21726386596938477 Test Loss: 0.2137204231293804\n",
      "Epoch: 5878 Training Loss: 0.21726562445882425 Test Loss: 0.21374191538573856\n",
      "Epoch: 5879 Training Loss: 0.21726290033468987 Test Loss: 0.2137752167166096\n",
      "Epoch: 5880 Training Loss: 0.21726565596464884 Test Loss: 0.21379630712740258\n",
      "Epoch: 5881 Training Loss: 0.21726358364527837 Test Loss: 0.2138615875876075\n",
      "Epoch: 5882 Training Loss: 0.21726511292401635 Test Loss: 0.21370703259812707\n",
      "Epoch: 5883 Training Loss: 0.21726388642037675 Test Loss: 0.2137077714754566\n",
      "Epoch: 5884 Training Loss: 0.21726359954164345 Test Loss: 0.21373776730248487\n",
      "Epoch: 5885 Training Loss: 0.21726575259805156 Test Loss: 0.21367769787186736\n",
      "Epoch: 5886 Training Loss: 0.21726495287550343 Test Loss: 0.21370485485441887\n",
      "Epoch: 5887 Training Loss: 0.21726429227528876 Test Loss: 0.21364230953660932\n",
      "Epoch: 5888 Training Loss: 0.21726430875443092 Test Loss: 0.2137070974119279\n",
      "Epoch: 5889 Training Loss: 0.2172667926399286 Test Loss: 0.21367373126725603\n",
      "Epoch: 5890 Training Loss: 0.21726445126583716 Test Loss: 0.21444195628583945\n",
      "Epoch: 5891 Training Loss: 0.2172663445650599 Test Loss: 0.2138335102490841\n",
      "Epoch: 5892 Training Loss: 0.21726551142697156 Test Loss: 0.2137253230527238\n",
      "Epoch: 5893 Training Loss: 0.21726641726873985 Test Loss: 0.2137077714754566\n",
      "Epoch: 5894 Training Loss: 0.21726683474332992 Test Loss: 0.21416991980095837\n",
      "Epoch: 5895 Training Loss: 0.21726254503792947 Test Loss: 0.2143895478464811\n",
      "Epoch: 5896 Training Loss: 0.21726549105667178 Test Loss: 0.21389831108716284\n",
      "Epoch: 5897 Training Loss: 0.21726522360682918 Test Loss: 0.21557761074137863\n",
      "Epoch: 5898 Training Loss: 0.2172654174115827 Test Loss: 0.21375306335948285\n",
      "Epoch: 5899 Training Loss: 0.2172639569543329 Test Loss: 0.213774322286158\n",
      "Epoch: 5900 Training Loss: 0.21726268302160623 Test Loss: 0.2136794608072502\n",
      "Epoch: 5901 Training Loss: 0.21726505905748417 Test Loss: 0.2138397064484443\n",
      "Epoch: 5902 Training Loss: 0.21726378525924453 Test Loss: 0.21371560098259795\n",
      "Epoch: 5903 Training Loss: 0.2172613597859401 Test Loss: 0.21437922948938753\n",
      "Epoch: 5904 Training Loss: 0.21726566214208567 Test Loss: 0.2137927942193971\n",
      "Epoch: 5905 Training Loss: 0.2172654286277997 Test Loss: 0.2147475144685144\n",
      "Epoch: 5906 Training Loss: 0.21726391147082463 Test Loss: 0.21376021880309545\n",
      "Epoch: 5907 Training Loss: 0.21726229818253168 Test Loss: 0.21376234469576297\n",
      "Epoch: 5908 Training Loss: 0.21726437362200085 Test Loss: 0.21376938347453409\n",
      "Epoch: 5909 Training Loss: 0.21726358139486232 Test Loss: 0.21374375609768237\n",
      "Epoch: 5910 Training Loss: 0.2172647872054333 Test Loss: 0.21383498800374323\n",
      "Epoch: 5911 Training Loss: 0.2172605764528761 Test Loss: 0.21385095812426994\n",
      "Epoch: 5912 Training Loss: 0.21726307582231205 Test Loss: 0.2137493430473147\n",
      "Epoch: 5913 Training Loss: 0.21726075374262427 Test Loss: 0.2137961386115204\n",
      "Epoch: 5914 Training Loss: 0.2172647674627396 Test Loss: 0.21365010015547017\n",
      "Epoch: 5915 Training Loss: 0.21726356762339205 Test Loss: 0.2137056066945086\n",
      "Epoch: 5916 Training Loss: 0.2172644506471969 Test Loss: 0.21451835879426834\n",
      "Epoch: 5917 Training Loss: 0.21726510967839638 Test Loss: 0.21370997514468515\n",
      "Epoch: 5918 Training Loss: 0.21726418837062145 Test Loss: 0.2137180379815095\n",
      "Epoch: 5919 Training Loss: 0.2172641205264056 Test Loss: 0.21362758384105873\n",
      "Epoch: 5920 Training Loss: 0.2172668626538683 Test Loss: 0.21372461010091456\n",
      "Epoch: 5921 Training Loss: 0.21726431547878164 Test Loss: 0.21376318727517388\n",
      "Epoch: 5922 Training Loss: 0.2172643756931009 Test Loss: 0.21373902469022113\n",
      "Epoch: 5923 Training Loss: 0.21726587067765019 Test Loss: 0.21384022495885102\n",
      "Epoch: 5924 Training Loss: 0.21726454487776498 Test Loss: 0.21377626670018318\n",
      "Epoch: 5925 Training Loss: 0.21726475226570696 Test Loss: 0.21374313388519434\n",
      "Epoch: 5926 Training Loss: 0.2172648328413602 Test Loss: 0.21369934568134757\n",
      "Epoch: 5927 Training Loss: 0.2172675507970235 Test Loss: 0.21375837809115164\n",
      "Epoch: 5928 Training Loss: 0.21726423103886822 Test Loss: 0.2138009218700223\n",
      "Epoch: 5929 Training Loss: 0.21726456507771455 Test Loss: 0.2138924648823272\n",
      "Epoch: 5930 Training Loss: 0.21726375006847576 Test Loss: 0.21379389605401136\n",
      "Epoch: 5931 Training Loss: 0.21726540670641636 Test Loss: 0.2138390971987164\n",
      "Epoch: 5932 Training Loss: 0.2172651032230197 Test Loss: 0.21373929690818466\n",
      "Epoch: 5933 Training Loss: 0.21726625350838533 Test Loss: 0.21379832931798876\n",
      "Epoch: 5934 Training Loss: 0.21726484017538536 Test Loss: 0.21375744477241956\n",
      "Epoch: 5935 Training Loss: 0.21726288667080923 Test Loss: 0.2146061296433644\n",
      "Epoch: 5936 Training Loss: 0.21726608452993468 Test Loss: 0.2138325121165512\n",
      "Epoch: 5937 Training Loss: 0.2172658407677382 Test Loss: 0.2139634230314856\n",
      "Epoch: 5938 Training Loss: 0.21726405790925168 Test Loss: 0.21377220935625066\n",
      "Epoch: 5939 Training Loss: 0.21726421141272992 Test Loss: 0.2138236196630761\n",
      "Epoch: 5940 Training Loss: 0.21726607872009565 Test Loss: 0.2137686186716842\n",
      "Epoch: 5941 Training Loss: 0.21726483548627146 Test Loss: 0.21374935601007486\n",
      "Epoch: 5942 Training Loss: 0.21726617730804298 Test Loss: 0.21365379454211797\n",
      "Epoch: 5943 Training Loss: 0.21726562067525626 Test Loss: 0.21387532811338536\n",
      "Epoch: 5944 Training Loss: 0.2172649328548699 Test Loss: 0.21374437831017043\n",
      "Epoch: 5945 Training Loss: 0.21726612439188572 Test Loss: 0.21381971787226559\n",
      "Epoch: 5946 Training Loss: 0.21726417031349832 Test Loss: 0.21368403666158942\n",
      "Epoch: 5947 Training Loss: 0.2172646133495869 Test Loss: 0.21368434776783343\n",
      "Epoch: 5948 Training Loss: 0.21726636266701202 Test Loss: 0.21373986726963204\n",
      "Epoch: 5949 Training Loss: 0.21726369668609685 Test Loss: 0.21371495284458958\n",
      "Epoch: 5950 Training Loss: 0.2172663596993319 Test Loss: 0.21372954891253848\n",
      "Epoch: 5951 Training Loss: 0.21726387966912863 Test Loss: 0.21383711389641075\n",
      "Epoch: 5952 Training Loss: 0.21726437456340997 Test Loss: 0.21384476192490973\n",
      "Epoch: 5953 Training Loss: 0.21726294416849076 Test Loss: 0.21377583892909766\n",
      "Epoch: 5954 Training Loss: 0.2172653537006011 Test Loss: 0.21372271753793007\n",
      "Epoch: 5955 Training Loss: 0.21726396694223515 Test Loss: 0.21365153902184877\n",
      "Epoch: 5956 Training Loss: 0.21726442837614732 Test Loss: 0.2136181728771769\n",
      "Epoch: 5957 Training Loss: 0.2172657145471923 Test Loss: 0.21369990308003478\n",
      "Epoch: 5958 Training Loss: 0.21726443636467596 Test Loss: 0.21374396350184507\n",
      "Epoch: 5959 Training Loss: 0.21726428496816097 Test Loss: 0.2138111365250345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5960 Training Loss: 0.21726417034039572 Test Loss: 0.2137787036990947\n",
      "Epoch: 5961 Training Loss: 0.21726445754189783 Test Loss: 0.21396033789456567\n",
      "Epoch: 5962 Training Loss: 0.21726186158388816 Test Loss: 0.2141233575664357\n",
      "Epoch: 5963 Training Loss: 0.21726532027609513 Test Loss: 0.21377419265855632\n",
      "Epoch: 5964 Training Loss: 0.21726627268623358 Test Loss: 0.21372261383584873\n",
      "Epoch: 5965 Training Loss: 0.21726375759974859 Test Loss: 0.21727897301339996\n",
      "Epoch: 5966 Training Loss: 0.21726683994349447 Test Loss: 0.2139775524400685\n",
      "Epoch: 5967 Training Loss: 0.21726497584588547 Test Loss: 0.213708108507221\n",
      "Epoch: 5968 Training Loss: 0.21726550163631692 Test Loss: 0.213781736984974\n",
      "Epoch: 5969 Training Loss: 0.21726346376355374 Test Loss: 0.21367749046770468\n",
      "Epoch: 5970 Training Loss: 0.2172632866889848 Test Loss: 0.21372021572521768\n",
      "Epoch: 5971 Training Loss: 0.2172627572943014 Test Loss: 0.21368582552249257\n",
      "Epoch: 5972 Training Loss: 0.2172642289856998 Test Loss: 0.21368463294855713\n",
      "Epoch: 5973 Training Loss: 0.2172654238579936 Test Loss: 0.21374227834302326\n",
      "Epoch: 5974 Training Loss: 0.21726473873631333 Test Loss: 0.21371770094974515\n",
      "Epoch: 5975 Training Loss: 0.2172637494408697 Test Loss: 0.21372437677123154\n",
      "Epoch: 5976 Training Loss: 0.2172648620340081 Test Loss: 0.21512854184088617\n",
      "Epoch: 5977 Training Loss: 0.21726255688175256 Test Loss: 0.21461704428742567\n",
      "Epoch: 5978 Training Loss: 0.21726466076970818 Test Loss: 0.2139744024893477\n",
      "Epoch: 5979 Training Loss: 0.21726519047819465 Test Loss: 0.21382360670031592\n",
      "Epoch: 5980 Training Loss: 0.21726415938418694 Test Loss: 0.213814882762723\n",
      "Epoch: 5981 Training Loss: 0.21726184010182908 Test Loss: 0.21374476719297547\n",
      "Epoch: 5982 Training Loss: 0.2172649599764178 Test Loss: 0.21381216058308777\n",
      "Epoch: 5983 Training Loss: 0.21726424446067225 Test Loss: 0.21369511982153286\n",
      "Epoch: 5984 Training Loss: 0.21726366552097273 Test Loss: 0.21368618847977727\n",
      "Epoch: 5985 Training Loss: 0.21726293726482404 Test Loss: 0.21378768689189098\n",
      "Epoch: 5986 Training Loss: 0.21726463277847757 Test Loss: 0.21371383804721514\n",
      "Epoch: 5987 Training Loss: 0.2172634388117297 Test Loss: 0.21371230844151534\n",
      "Epoch: 5988 Training Loss: 0.2172662232846703 Test Loss: 0.21373857099361526\n",
      "Epoch: 5989 Training Loss: 0.21726477460848295 Test Loss: 0.21376577982720743\n",
      "Epoch: 5990 Training Loss: 0.2172648693052727 Test Loss: 0.21367526087295582\n",
      "Epoch: 5991 Training Loss: 0.21726465951449606 Test Loss: 0.21372467491471542\n",
      "Epoch: 5992 Training Loss: 0.21726406052726557 Test Loss: 0.2136302930579338\n",
      "Epoch: 5993 Training Loss: 0.2172659912497415 Test Loss: 0.2137392839454245\n",
      "Epoch: 5994 Training Loss: 0.21726492639949319 Test Loss: 0.21372114904394976\n",
      "Epoch: 5995 Training Loss: 0.21726152420079808 Test Loss: 0.2162869070522393\n",
      "Epoch: 5996 Training Loss: 0.21726602484459767 Test Loss: 0.21386109500272113\n",
      "Epoch: 5997 Training Loss: 0.217264847850111 Test Loss: 0.21372992483258335\n",
      "Epoch: 5998 Training Loss: 0.2172644066609774 Test Loss: 0.21390913499190295\n",
      "Epoch: 5999 Training Loss: 0.21726378581512418 Test Loss: 0.21379976818436736\n",
      "Epoch: 6000 Training Loss: 0.21726532832738438 Test Loss: 0.2138182660431268\n",
      "Epoch: 6001 Training Loss: 0.21726538910654908 Test Loss: 0.21381897899493602\n",
      "Epoch: 6002 Training Loss: 0.2172649038594696 Test Loss: 0.2137942460485359\n",
      "Epoch: 6003 Training Loss: 0.2172650760566428 Test Loss: 0.2137766944712687\n",
      "Epoch: 6004 Training Loss: 0.21726424045295922 Test Loss: 0.2164931834547892\n",
      "Epoch: 6005 Training Loss: 0.2172647900834554 Test Loss: 0.21375809291042794\n",
      "Epoch: 6006 Training Loss: 0.21726593684526127 Test Loss: 0.2137841869466457\n",
      "Epoch: 6007 Training Loss: 0.21726494115720157 Test Loss: 0.2137808814428029\n",
      "Epoch: 6008 Training Loss: 0.21726423715354445 Test Loss: 0.21375749662346022\n",
      "Epoch: 6009 Training Loss: 0.2172665094282079 Test Loss: 0.21376327801449507\n",
      "Epoch: 6010 Training Loss: 0.21726515041003014 Test Loss: 0.21372453232435357\n",
      "Epoch: 6011 Training Loss: 0.21726485680694615 Test Loss: 0.21378378510108048\n",
      "Epoch: 6012 Training Loss: 0.21726434367622566 Test Loss: 0.21374733381948868\n",
      "Epoch: 6013 Training Loss: 0.21726526293083218 Test Loss: 0.21387955397320008\n",
      "Epoch: 6014 Training Loss: 0.21726475660515462 Test Loss: 0.21380436996422694\n",
      "Epoch: 6015 Training Loss: 0.21726486709071985 Test Loss: 0.21376650574177683\n",
      "Epoch: 6016 Training Loss: 0.21726551804373267 Test Loss: 0.21376908533105024\n",
      "Epoch: 6017 Training Loss: 0.2172663204111921 Test Loss: 0.21403548301525832\n",
      "Epoch: 6018 Training Loss: 0.2172616440018305 Test Loss: 0.2144111826932011\n",
      "Epoch: 6019 Training Loss: 0.21726231466167384 Test Loss: 0.21403338304811112\n",
      "Epoch: 6020 Training Loss: 0.21726170516652465 Test Loss: 0.21384480081319024\n",
      "Epoch: 6021 Training Loss: 0.2172627473781255 Test Loss: 0.21368515145896383\n",
      "Epoch: 6022 Training Loss: 0.21726544759943453 Test Loss: 0.21364986682578713\n",
      "Epoch: 6023 Training Loss: 0.21726504937441915 Test Loss: 0.2136642425268132\n",
      "Epoch: 6024 Training Loss: 0.2172645636700838 Test Loss: 0.21369399206139827\n",
      "Epoch: 6025 Training Loss: 0.21726439562407643 Test Loss: 0.21379285903319795\n",
      "Epoch: 6026 Training Loss: 0.21726459903120277 Test Loss: 0.21374039874279893\n",
      "Epoch: 6027 Training Loss: 0.2172632932250537 Test Loss: 0.21576542817344962\n",
      "Epoch: 6028 Training Loss: 0.21726383145105108 Test Loss: 0.21375105413165685\n",
      "Epoch: 6029 Training Loss: 0.2172650740931324 Test Loss: 0.21390035920326936\n",
      "Epoch: 6030 Training Loss: 0.21726427524923275 Test Loss: 0.21375210411523043\n",
      "Epoch: 6031 Training Loss: 0.21726434227756072 Test Loss: 0.21373004149742486\n",
      "Epoch: 6032 Training Loss: 0.21726481516976648 Test Loss: 0.21372327493661727\n",
      "Epoch: 6033 Training Loss: 0.2172641224988818 Test Loss: 0.21401154079722842\n",
      "Epoch: 6034 Training Loss: 0.2172646751687845 Test Loss: 0.21431940635121324\n",
      "Epoch: 6035 Training Loss: 0.2172617205159759 Test Loss: 0.21398548564929118\n",
      "Epoch: 6036 Training Loss: 0.2172648424795962 Test Loss: 0.21396471930750238\n",
      "Epoch: 6037 Training Loss: 0.2172645460702165 Test Loss: 0.2138045903311498\n",
      "Epoch: 6038 Training Loss: 0.21726504270386324 Test Loss: 0.2137453116289025\n",
      "Epoch: 6039 Training Loss: 0.21726454747784726 Test Loss: 0.2138161660759796\n",
      "Epoch: 6040 Training Loss: 0.2172632463877095 Test Loss: 0.21386494494249098\n",
      "Epoch: 6041 Training Loss: 0.21726377146984266 Test Loss: 0.21380229592260008\n",
      "Epoch: 6042 Training Loss: 0.2172650566277521 Test Loss: 0.21385344697422215\n",
      "Epoch: 6043 Training Loss: 0.2172638456170166 Test Loss: 0.213710195511608\n",
      "Epoch: 6044 Training Loss: 0.21726465178597562 Test Loss: 0.21378409620732453\n",
      "Epoch: 6045 Training Loss: 0.2172648572821336 Test Loss: 0.21373426735723958\n",
      "Epoch: 6046 Training Loss: 0.21726454373910825 Test Loss: 0.2140252683602461\n",
      "Epoch: 6047 Training Loss: 0.21726566279658915 Test Loss: 0.2136886773297295\n",
      "Epoch: 6048 Training Loss: 0.2172633165540678 Test Loss: 0.21419916378789688\n",
      "Epoch: 6049 Training Loss: 0.21726398526833232 Test Loss: 0.21378098514488425\n",
      "Epoch: 6050 Training Loss: 0.21726388439410574 Test Loss: 0.21379922374844032\n",
      "Epoch: 6051 Training Loss: 0.21726499428853804 Test Loss: 0.21368594218733408\n",
      "Epoch: 6052 Training Loss: 0.21726506170239546 Test Loss: 0.2137283044875624\n",
      "Epoch: 6053 Training Loss: 0.21726498472202843 Test Loss: 0.21369909938890438\n",
      "Epoch: 6054 Training Loss: 0.21726564999342537 Test Loss: 0.21371722132761894\n",
      "Epoch: 6055 Training Loss: 0.21726474671587617 Test Loss: 0.21375145597722203\n",
      "Epoch: 6056 Training Loss: 0.21726404235358704 Test Loss: 0.213727500796432\n",
      "Epoch: 6057 Training Loss: 0.21726592424831093 Test Loss: 0.21364730019927392\n",
      "Epoch: 6058 Training Loss: 0.21726503436566832 Test Loss: 0.2148542109474554\n",
      "Epoch: 6059 Training Loss: 0.21726655995049632 Test Loss: 0.21384351749993363\n",
      "Epoch: 6060 Training Loss: 0.21726482775775105 Test Loss: 0.21382565481642243\n",
      "Epoch: 6061 Training Loss: 0.2172664832211717 Test Loss: 0.2137361080691834\n",
      "Epoch: 6062 Training Loss: 0.2172649386288457 Test Loss: 0.21369668831551317\n",
      "Epoch: 6063 Training Loss: 0.2172657511276602 Test Loss: 0.21377325933982425\n",
      "Epoch: 6064 Training Loss: 0.21726394806025834 Test Loss: 0.21368939028153872\n",
      "Epoch: 6065 Training Loss: 0.21726360501078204 Test Loss: 0.2136807830087873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6066 Training Loss: 0.21726435408552058 Test Loss: 0.21376592241756928\n",
      "Epoch: 6067 Training Loss: 0.21726453216425923 Test Loss: 0.2136758312344032\n",
      "Epoch: 6068 Training Loss: 0.21726340107467343 Test Loss: 0.21368445146991477\n",
      "Epoch: 6069 Training Loss: 0.21726518866710284 Test Loss: 0.21369681794311485\n",
      "Epoch: 6070 Training Loss: 0.21726488556026982 Test Loss: 0.2138590987376553\n",
      "Epoch: 6071 Training Loss: 0.21726391930693467 Test Loss: 0.21376480762019487\n",
      "Epoch: 6072 Training Loss: 0.21726367430745766 Test Loss: 0.21372400085118667\n",
      "Epoch: 6073 Training Loss: 0.21726596602894338 Test Loss: 0.2137384802542941\n",
      "Epoch: 6074 Training Loss: 0.21726579823397843 Test Loss: 0.21374980970668073\n",
      "Epoch: 6075 Training Loss: 0.21726447068576205 Test Loss: 0.21369448464628465\n",
      "Epoch: 6076 Training Loss: 0.21726500460817497 Test Loss: 0.2137176750242248\n",
      "Epoch: 6077 Training Loss: 0.21726458162858311 Test Loss: 0.2140333182343103\n",
      "Epoch: 6078 Training Loss: 0.21726557034124966 Test Loss: 0.213704478934374\n",
      "Epoch: 6079 Training Loss: 0.2172632585094724 Test Loss: 0.2137149009935489\n",
      "Epoch: 6080 Training Loss: 0.21726520207097527 Test Loss: 0.2137383117384119\n",
      "Epoch: 6081 Training Loss: 0.21726528962202163 Test Loss: 0.2137657668644473\n",
      "Epoch: 6082 Training Loss: 0.21726484631695903 Test Loss: 0.213739322833705\n",
      "Epoch: 6083 Training Loss: 0.2172640764953571 Test Loss: 0.2169766425580068\n",
      "Epoch: 6084 Training Loss: 0.2172660361863359 Test Loss: 0.21389042972898084\n",
      "Epoch: 6085 Training Loss: 0.2172649452545726 Test Loss: 0.21381324945494187\n",
      "Epoch: 6086 Training Loss: 0.21726588268285768 Test Loss: 0.21374577828826855\n",
      "Epoch: 6087 Training Loss: 0.21726476684409932 Test Loss: 0.21372686562118376\n",
      "Epoch: 6088 Training Loss: 0.21726460800596953 Test Loss: 0.21367806082915206\n",
      "Epoch: 6089 Training Loss: 0.21726694630479124 Test Loss: 0.21371768798698498\n",
      "Epoch: 6090 Training Loss: 0.21726424803802682 Test Loss: 0.2136685072749084\n",
      "Epoch: 6091 Training Loss: 0.21726518232828157 Test Loss: 0.21370305303075554\n",
      "Epoch: 6092 Training Loss: 0.2172649519251285 Test Loss: 0.21373965986546936\n",
      "Epoch: 6093 Training Loss: 0.21726415929452894 Test Loss: 0.21375201337590927\n",
      "Epoch: 6094 Training Loss: 0.21726456640465308 Test Loss: 0.21376404281734496\n",
      "Epoch: 6095 Training Loss: 0.21726442112281436 Test Loss: 0.21385777653611818\n",
      "Epoch: 6096 Training Loss: 0.2172625452889719 Test Loss: 0.21406000855749574\n",
      "Epoch: 6097 Training Loss: 0.2172634465581817 Test Loss: 0.21386913191402518\n",
      "Epoch: 6098 Training Loss: 0.2172658874526638 Test Loss: 0.21387444664569397\n",
      "Epoch: 6099 Training Loss: 0.21726461852285406 Test Loss: 0.21374023022691674\n",
      "Epoch: 6100 Training Loss: 0.21726466535123246 Test Loss: 0.21375078191369332\n",
      "Epoch: 6101 Training Loss: 0.21726496254063685 Test Loss: 0.21381100689743282\n",
      "Epoch: 6102 Training Loss: 0.21726511168673582 Test Loss: 0.21373930987094483\n",
      "Epoch: 6103 Training Loss: 0.21726485783801325 Test Loss: 0.21368859955316846\n",
      "Epoch: 6104 Training Loss: 0.21726376097088973 Test Loss: 0.21427054970814086\n",
      "Epoch: 6105 Training Loss: 0.21726770811096713 Test Loss: 0.21371242510635685\n",
      "Epoch: 6106 Training Loss: 0.21726506309209462 Test Loss: 0.21379397383057236\n",
      "Epoch: 6107 Training Loss: 0.2172642486118381 Test Loss: 0.21367214981051555\n",
      "Epoch: 6108 Training Loss: 0.2172651938134726 Test Loss: 0.21374677642080148\n",
      "Epoch: 6109 Training Loss: 0.21726524026528737 Test Loss: 0.21379704600473212\n",
      "Epoch: 6110 Training Loss: 0.21726647140424604 Test Loss: 0.2136905569299538\n",
      "Epoch: 6111 Training Loss: 0.21726580535282441 Test Loss: 0.21363052638761681\n",
      "Epoch: 6112 Training Loss: 0.2172657922896524 Test Loss: 0.2136718905553122\n",
      "Epoch: 6113 Training Loss: 0.21726624131489602 Test Loss: 0.21361442663948843\n",
      "Epoch: 6114 Training Loss: 0.21726457810502334 Test Loss: 0.21368298667801583\n",
      "Epoch: 6115 Training Loss: 0.21726599784857098 Test Loss: 0.21366115738989327\n",
      "Epoch: 6116 Training Loss: 0.21726368724510844 Test Loss: 0.21475228476425615\n",
      "Epoch: 6117 Training Loss: 0.21726663328178236 Test Loss: 0.2137400876365549\n",
      "Epoch: 6118 Training Loss: 0.2172652435198731 Test Loss: 0.21368729031439151\n",
      "Epoch: 6119 Training Loss: 0.21726431641122496 Test Loss: 0.21428699945079377\n",
      "Epoch: 6120 Training Loss: 0.21726143625525648 Test Loss: 0.21466057323606907\n",
      "Epoch: 6121 Training Loss: 0.21726189239038027 Test Loss: 0.21413923694764123\n",
      "Epoch: 6122 Training Loss: 0.21726175539294162 Test Loss: 0.21386451717140545\n",
      "Epoch: 6123 Training Loss: 0.21726171356748014 Test Loss: 0.21492308209222682\n",
      "Epoch: 6124 Training Loss: 0.21726550829790703 Test Loss: 0.2138936185679821\n",
      "Epoch: 6125 Training Loss: 0.21726285724505046 Test Loss: 0.21374128021049033\n",
      "Epoch: 6126 Training Loss: 0.21726407983063503 Test Loss: 0.21370608631663482\n",
      "Epoch: 6127 Training Loss: 0.2172638698874398 Test Loss: 0.21374627087315493\n",
      "Epoch: 6128 Training Loss: 0.21726467647779146 Test Loss: 0.21374800788301743\n",
      "Epoch: 6129 Training Loss: 0.21726484175336633 Test Loss: 0.21370213267478363\n",
      "Epoch: 6130 Training Loss: 0.21726461996634802 Test Loss: 0.21375583739015874\n",
      "Epoch: 6131 Training Loss: 0.2172640598458647 Test Loss: 0.21375378927405225\n",
      "Epoch: 6132 Training Loss: 0.21726335836159766 Test Loss: 0.21379245718763273\n",
      "Epoch: 6133 Training Loss: 0.21726241279236533 Test Loss: 0.21471811492845386\n",
      "Epoch: 6134 Training Loss: 0.2172673909726556 Test Loss: 0.2138953944661251\n",
      "Epoch: 6135 Training Loss: 0.2172643026756179 Test Loss: 0.2139194533489965\n",
      "Epoch: 6136 Training Loss: 0.21726521487413905 Test Loss: 0.2139153830423038\n",
      "Epoch: 6137 Training Loss: 0.2172640225122695 Test Loss: 0.21408494890805857\n",
      "Epoch: 6138 Training Loss: 0.2172646162455406 Test Loss: 0.21396180268646464\n",
      "Epoch: 6139 Training Loss: 0.2172635306753263 Test Loss: 0.21380510884155648\n",
      "Epoch: 6140 Training Loss: 0.217263698075796 Test Loss: 0.21377245564869385\n",
      "Epoch: 6141 Training Loss: 0.21726464747342536 Test Loss: 0.21378637765311406\n",
      "Epoch: 6142 Training Loss: 0.21726404326809873 Test Loss: 0.21377427043511735\n",
      "Epoch: 6143 Training Loss: 0.2172610981011075 Test Loss: 0.21477347887713047\n",
      "Epoch: 6144 Training Loss: 0.2172690345474242 Test Loss: 0.21395119914864738\n",
      "Epoch: 6145 Training Loss: 0.21726645068428002 Test Loss: 0.21385038776282256\n",
      "Epoch: 6146 Training Loss: 0.2172652816872878 Test Loss: 0.21387124484393252\n",
      "Epoch: 6147 Training Loss: 0.2172667393472077 Test Loss: 0.2137991589346395\n",
      "Epoch: 6148 Training Loss: 0.21726531364140242 Test Loss: 0.21379027944392456\n",
      "Epoch: 6149 Training Loss: 0.21726405296012957 Test Loss: 0.21372884892348942\n",
      "Epoch: 6150 Training Loss: 0.21726538009591911 Test Loss: 0.21370336413699959\n",
      "Epoch: 6151 Training Loss: 0.21726595443616276 Test Loss: 0.2136572944873633\n",
      "Epoch: 6152 Training Loss: 0.21726592478625897 Test Loss: 0.2137797536826683\n",
      "Epoch: 6153 Training Loss: 0.2172649275291841 Test Loss: 0.21366740544029414\n",
      "Epoch: 6154 Training Loss: 0.21726543801499332 Test Loss: 0.2144477636023946\n",
      "Epoch: 6155 Training Loss: 0.21726551894031276 Test Loss: 0.21373228405493389\n",
      "Epoch: 6156 Training Loss: 0.217264797489207 Test Loss: 0.2137581966125093\n",
      "Epoch: 6157 Training Loss: 0.21726286709846573 Test Loss: 0.21367264239540193\n",
      "Epoch: 6158 Training Loss: 0.21726675159449182 Test Loss: 0.21375283002979983\n",
      "Epoch: 6159 Training Loss: 0.2172647771099414 Test Loss: 0.21367746454218434\n",
      "Epoch: 6160 Training Loss: 0.21726508469070913 Test Loss: 0.2146714230663295\n",
      "Epoch: 6161 Training Loss: 0.2172626705412113 Test Loss: 0.21542060579022645\n",
      "Epoch: 6162 Training Loss: 0.21726534393684385 Test Loss: 0.21402669426386456\n",
      "Epoch: 6163 Training Loss: 0.2172659914738865 Test Loss: 0.2138524618044494\n",
      "Epoch: 6164 Training Loss: 0.21726427372504659 Test Loss: 0.21381059208910747\n",
      "Epoch: 6165 Training Loss: 0.2172629023878583 Test Loss: 0.21374308203415365\n",
      "Epoch: 6166 Training Loss: 0.2172649572597801 Test Loss: 0.2137158213495208\n",
      "Epoch: 6167 Training Loss: 0.2172638306172316 Test Loss: 0.21367529976123634\n",
      "Epoch: 6168 Training Loss: 0.2172672928061009 Test Loss: 0.21380993098833892\n",
      "Epoch: 6169 Training Loss: 0.21726494688634837 Test Loss: 0.21373058593335192\n",
      "Epoch: 6170 Training Loss: 0.21726459693320535 Test Loss: 0.21374277092790964\n",
      "Epoch: 6171 Training Loss: 0.21726416957830263 Test Loss: 0.213884726114507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6172 Training Loss: 0.21726311234898515 Test Loss: 0.21382968623483462\n",
      "Epoch: 6173 Training Loss: 0.2172650153133413 Test Loss: 0.21367733491458266\n",
      "Epoch: 6174 Training Loss: 0.21726447387758718 Test Loss: 0.213659977778718\n",
      "Epoch: 6175 Training Loss: 0.21726523439268774 Test Loss: 0.2147352387346355\n",
      "Epoch: 6176 Training Loss: 0.2172668139875007 Test Loss: 0.213764509476711\n",
      "Epoch: 6177 Training Loss: 0.2172649947816571 Test Loss: 0.21372236754340554\n",
      "Epoch: 6178 Training Loss: 0.21726284569709883 Test Loss: 0.21373890802537962\n",
      "Epoch: 6179 Training Loss: 0.21726587612885717 Test Loss: 0.21371431766934135\n",
      "Epoch: 6180 Training Loss: 0.2172653100371504 Test Loss: 0.21375544850735373\n",
      "Epoch: 6181 Training Loss: 0.21726449214092375 Test Loss: 0.21380147926870952\n",
      "Epoch: 6182 Training Loss: 0.21726190965851291 Test Loss: 0.21381808456448445\n",
      "Epoch: 6183 Training Loss: 0.21726527177111193 Test Loss: 0.21392804765898774\n",
      "Epoch: 6184 Training Loss: 0.21726514288772314 Test Loss: 0.2137447931184958\n",
      "Epoch: 6185 Training Loss: 0.21726465488814276 Test Loss: 0.2138427267715634\n",
      "Epoch: 6186 Training Loss: 0.2172636407574305 Test Loss: 0.2137591558567617\n",
      "Epoch: 6187 Training Loss: 0.2172634661215594 Test Loss: 0.21372240643168605\n",
      "Epoch: 6188 Training Loss: 0.21726523609618992 Test Loss: 0.21373008038570537\n",
      "Epoch: 6189 Training Loss: 0.21726534653692614 Test Loss: 0.2138439711965395\n",
      "Epoch: 6190 Training Loss: 0.21726318729411537 Test Loss: 0.21391367195796165\n",
      "Epoch: 6191 Training Loss: 0.21726267181435505 Test Loss: 0.21381908269701735\n",
      "Epoch: 6192 Training Loss: 0.21726654689629013 Test Loss: 0.21392321254944516\n",
      "Epoch: 6193 Training Loss: 0.2172633634900358 Test Loss: 0.21381453276819848\n",
      "Epoch: 6194 Training Loss: 0.2172650527545261 Test Loss: 0.21384048421405438\n",
      "Epoch: 6195 Training Loss: 0.2172660156456859 Test Loss: 0.21373105259271796\n",
      "Epoch: 6196 Training Loss: 0.21726323206035955 Test Loss: 0.21487330509318253\n",
      "Epoch: 6197 Training Loss: 0.21726580824877811 Test Loss: 0.21377490561036555\n",
      "Epoch: 6198 Training Loss: 0.21726482797293026 Test Loss: 0.21381144763127855\n",
      "Epoch: 6199 Training Loss: 0.21726409916090192 Test Loss: 0.2137906294384491\n",
      "Epoch: 6200 Training Loss: 0.21726465564127004 Test Loss: 0.21378744059944782\n",
      "Epoch: 6201 Training Loss: 0.21726429289392904 Test Loss: 0.21372127867155144\n",
      "Epoch: 6202 Training Loss: 0.21726537694892298 Test Loss: 0.21385412103775087\n",
      "Epoch: 6203 Training Loss: 0.21726367402055202 Test Loss: 0.21373947838682703\n",
      "Epoch: 6204 Training Loss: 0.2172652052896978 Test Loss: 0.21380011817889188\n",
      "Epoch: 6205 Training Loss: 0.21726438655068583 Test Loss: 0.21375391890165393\n",
      "Epoch: 6206 Training Loss: 0.2172652790782397 Test Loss: 0.21372173236815734\n",
      "Epoch: 6207 Training Loss: 0.21726603646427572 Test Loss: 0.2137391283923025\n",
      "Epoch: 6208 Training Loss: 0.21726581229235434 Test Loss: 0.2137052437372239\n",
      "Epoch: 6209 Training Loss: 0.21726754794589878 Test Loss: 0.21370748629473293\n",
      "Epoch: 6210 Training Loss: 0.21726520901947102 Test Loss: 0.21373794878112723\n",
      "Epoch: 6211 Training Loss: 0.2172655369526069 Test Loss: 0.21370628075803733\n",
      "Epoch: 6212 Training Loss: 0.2172648040073443 Test Loss: 0.21508274440921338\n",
      "Epoch: 6213 Training Loss: 0.2172657943428208 Test Loss: 0.21380573105404455\n",
      "Epoch: 6214 Training Loss: 0.21726474828489134 Test Loss: 0.21374896712726982\n",
      "Epoch: 6215 Training Loss: 0.21726363934083393 Test Loss: 0.21369690868243602\n",
      "Epoch: 6216 Training Loss: 0.21726454364945025 Test Loss: 0.21376776312951312\n",
      "Epoch: 6217 Training Loss: 0.21726492490220442 Test Loss: 0.21377080937815254\n",
      "Epoch: 6218 Training Loss: 0.21726581678422063 Test Loss: 0.21377964998058696\n",
      "Epoch: 6219 Training Loss: 0.21726516809058963 Test Loss: 0.2137887628009849\n",
      "Epoch: 6220 Training Loss: 0.21726421188791736 Test Loss: 0.21376806127299697\n",
      "Epoch: 6221 Training Loss: 0.217264528210341 Test Loss: 0.21374655605387863\n",
      "Epoch: 6222 Training Loss: 0.21726307546368 Test Loss: 0.214124848283855\n",
      "Epoch: 6223 Training Loss: 0.21726888607376033 Test Loss: 0.21370674741740336\n",
      "Epoch: 6224 Training Loss: 0.2172662100601139 Test Loss: 0.21403585893530316\n",
      "Epoch: 6225 Training Loss: 0.21726235958930243 Test Loss: 0.21442264177318945\n",
      "Epoch: 6226 Training Loss: 0.21726375750112475 Test Loss: 0.21397880982780476\n",
      "Epoch: 6227 Training Loss: 0.21726416677200694 Test Loss: 0.21387457627329565\n",
      "Epoch: 6228 Training Loss: 0.2172656415745383 Test Loss: 0.21381253650313264\n",
      "Epoch: 6229 Training Loss: 0.2172637970941018 Test Loss: 0.21374248574718593\n",
      "Epoch: 6230 Training Loss: 0.21726485834906392 Test Loss: 0.21374802084577757\n",
      "Epoch: 6231 Training Loss: 0.21726470744566798 Test Loss: 0.21374292648103166\n",
      "Epoch: 6232 Training Loss: 0.21726349540386533 Test Loss: 0.2137719889893278\n",
      "Epoch: 6233 Training Loss: 0.2172641926831717 Test Loss: 0.2137829684471899\n",
      "Epoch: 6234 Training Loss: 0.21726483918914727 Test Loss: 0.21370651408772035\n",
      "Epoch: 6235 Training Loss: 0.21726540848164494 Test Loss: 0.21367944784449003\n",
      "Epoch: 6236 Training Loss: 0.21726588417118065 Test Loss: 0.21371425285554052\n",
      "Epoch: 6237 Training Loss: 0.2172636165676995 Test Loss: 0.21376838534200115\n",
      "Epoch: 6238 Training Loss: 0.21726174619402983 Test Loss: 0.21466867496117395\n",
      "Epoch: 6239 Training Loss: 0.21726732215116745 Test Loss: 0.21383388616912896\n",
      "Epoch: 6240 Training Loss: 0.21726412777973858 Test Loss: 0.21476864376758786\n",
      "Epoch: 6241 Training Loss: 0.21726293767725088 Test Loss: 0.21489570474275246\n",
      "Epoch: 6242 Training Loss: 0.21726412959083036 Test Loss: 0.21426732198085907\n",
      "Epoch: 6243 Training Loss: 0.21726271389085894 Test Loss: 0.21399349663507486\n",
      "Epoch: 6244 Training Loss: 0.21726221104391216 Test Loss: 0.21976464708937293\n",
      "Epoch: 6245 Training Loss: 0.21726617495900313 Test Loss: 0.21416229769797973\n",
      "Epoch: 6246 Training Loss: 0.2172636456258604 Test Loss: 0.21380775324463072\n",
      "Epoch: 6247 Training Loss: 0.2172625920814871 Test Loss: 0.21371057143165287\n",
      "Epoch: 6248 Training Loss: 0.21726444433527303 Test Loss: 0.21378795910985451\n",
      "Epoch: 6249 Training Loss: 0.21726526994208853 Test Loss: 0.213677360840103\n",
      "Epoch: 6250 Training Loss: 0.21726507756289737 Test Loss: 0.21372913410421313\n",
      "Epoch: 6251 Training Loss: 0.2172652273903972 Test Loss: 0.21372556934516698\n",
      "Epoch: 6252 Training Loss: 0.21726562235186103 Test Loss: 0.2137430561086333\n",
      "Epoch: 6253 Training Loss: 0.2172653706638965 Test Loss: 0.21366962207228285\n",
      "Epoch: 6254 Training Loss: 0.2172654174026169 Test Loss: 0.2168482075302644\n",
      "Epoch: 6255 Training Loss: 0.217264689065776 Test Loss: 0.21375113190821785\n",
      "Epoch: 6256 Training Loss: 0.21726398080336345 Test Loss: 0.21368398481054873\n",
      "Epoch: 6257 Training Loss: 0.21726207453959254 Test Loss: 0.21364652243366386\n",
      "Epoch: 6258 Training Loss: 0.21726494126479118 Test Loss: 0.213715989865403\n",
      "Epoch: 6259 Training Loss: 0.21726467817232784 Test Loss: 0.2137129306540034\n",
      "Epoch: 6260 Training Loss: 0.21726337121855624 Test Loss: 0.21373604325538256\n",
      "Epoch: 6261 Training Loss: 0.21726517091481695 Test Loss: 0.2137538151995726\n",
      "Epoch: 6262 Training Loss: 0.21726309611191963 Test Loss: 0.21366608323875705\n",
      "Epoch: 6263 Training Loss: 0.21726505321178197 Test Loss: 0.2138003515085749\n",
      "Epoch: 6264 Training Loss: 0.217266074380648 Test Loss: 0.21380251628952293\n",
      "Epoch: 6265 Training Loss: 0.21726493001271097 Test Loss: 0.21370318265835722\n",
      "Epoch: 6266 Training Loss: 0.2172661784377339 Test Loss: 0.21369569018298024\n",
      "Epoch: 6267 Training Loss: 0.21726519748945097 Test Loss: 0.21375056154677047\n",
      "Epoch: 6268 Training Loss: 0.21726521934807372 Test Loss: 0.21389924440589492\n",
      "Epoch: 6269 Training Loss: 0.21726406273285262 Test Loss: 0.21370344191356058\n",
      "Epoch: 6270 Training Loss: 0.21726638196141568 Test Loss: 0.2137637446738611\n",
      "Epoch: 6271 Training Loss: 0.21726412777973858 Test Loss: 0.21365466304704922\n",
      "Epoch: 6272 Training Loss: 0.21726596553582433 Test Loss: 0.2136557908071838\n",
      "Epoch: 6273 Training Loss: 0.21726495776186494 Test Loss: 0.2137268915467041\n",
      "Epoch: 6274 Training Loss: 0.21726460279683918 Test Loss: 0.21384901371024476\n",
      "Epoch: 6275 Training Loss: 0.21726372356556814 Test Loss: 0.21372152496399463\n",
      "Epoch: 6276 Training Loss: 0.217264268524882 Test Loss: 0.2137612039728682\n",
      "Epoch: 6277 Training Loss: 0.2172656622496753 Test Loss: 0.21387165965225788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6278 Training Loss: 0.21726336756947526 Test Loss: 0.2137519226365881\n",
      "Epoch: 6279 Training Loss: 0.21726314860668422 Test Loss: 0.21377274082941755\n",
      "Epoch: 6280 Training Loss: 0.2172645748683692 Test Loss: 0.21396320266456276\n",
      "Epoch: 6281 Training Loss: 0.21726148120978248 Test Loss: 0.21376995383598146\n",
      "Epoch: 6282 Training Loss: 0.2172620685683691 Test Loss: 0.21369322725854836\n",
      "Epoch: 6283 Training Loss: 0.21726399382170644 Test Loss: 0.21521291644681825\n",
      "Epoch: 6284 Training Loss: 0.2172643438734733 Test Loss: 0.2138028144330068\n",
      "Epoch: 6285 Training Loss: 0.21726490162698514 Test Loss: 0.21369322725854836\n",
      "Epoch: 6286 Training Loss: 0.21726590386904535 Test Loss: 0.21372070831010406\n",
      "Epoch: 6287 Training Loss: 0.21726523365749206 Test Loss: 0.2136620388575847\n",
      "Epoch: 6288 Training Loss: 0.21726579662013426 Test Loss: 0.21360869709949426\n",
      "Epoch: 6289 Training Loss: 0.2172666953340908 Test Loss: 0.21371881574711957\n",
      "Epoch: 6290 Training Loss: 0.2172643150932522 Test Loss: 0.213684775538919\n",
      "Epoch: 6291 Training Loss: 0.21726354338883205 Test Loss: 0.2152350179529043\n",
      "Epoch: 6292 Training Loss: 0.21726450649517107 Test Loss: 0.2137435098052392\n",
      "Epoch: 6293 Training Loss: 0.2172652414936021 Test Loss: 0.21374838380306227\n",
      "Epoch: 6294 Training Loss: 0.21726618844356776 Test Loss: 0.2137907849915711\n",
      "Epoch: 6295 Training Loss: 0.21726337180133332 Test Loss: 0.213797266371655\n",
      "Epoch: 6296 Training Loss: 0.21726742575996333 Test Loss: 0.21375052265848996\n",
      "Epoch: 6297 Training Loss: 0.2172650787463831 Test Loss: 0.21376973346905861\n",
      "Epoch: 6298 Training Loss: 0.21726535982424314 Test Loss: 0.21381201799272592\n",
      "Epoch: 6299 Training Loss: 0.21726507384208996 Test Loss: 0.21368848288832695\n",
      "Epoch: 6300 Training Loss: 0.2172662710723894 Test Loss: 0.21373027482710788\n",
      "Epoch: 6301 Training Loss: 0.2172663484382859 Test Loss: 0.21375937622368457\n",
      "Epoch: 6302 Training Loss: 0.21726458683771346 Test Loss: 0.21375264855115747\n",
      "Epoch: 6303 Training Loss: 0.2172661212359238 Test Loss: 0.21383386024360862\n",
      "Epoch: 6304 Training Loss: 0.21726501230083217 Test Loss: 0.2137229119793326\n",
      "Epoch: 6305 Training Loss: 0.2172648629036908 Test Loss: 0.21367207203395455\n",
      "Epoch: 6306 Training Loss: 0.21726549103874018 Test Loss: 0.21376194285019778\n",
      "Epoch: 6307 Training Loss: 0.2172648593173704 Test Loss: 0.21379896449323696\n",
      "Epoch: 6308 Training Loss: 0.21726419641294492 Test Loss: 0.2138007144658596\n",
      "Epoch: 6309 Training Loss: 0.21726429498296065 Test Loss: 0.21379451826649942\n",
      "Epoch: 6310 Training Loss: 0.21726447925706777 Test Loss: 0.21370659186428134\n",
      "Epoch: 6311 Training Loss: 0.217262647221163 Test Loss: 0.21371131030898244\n",
      "Epoch: 6312 Training Loss: 0.21726636196767954 Test Loss: 0.21381466239580016\n",
      "Epoch: 6313 Training Loss: 0.21726439153567118 Test Loss: 0.21389435744531168\n",
      "Epoch: 6314 Training Loss: 0.21726228169442371 Test Loss: 0.2138151549806865\n",
      "Epoch: 6315 Training Loss: 0.2172620582935612 Test Loss: 0.21370981959156313\n",
      "Epoch: 6316 Training Loss: 0.21726443529774567 Test Loss: 0.21370888627283105\n",
      "Epoch: 6317 Training Loss: 0.2172637598143014 Test Loss: 0.2137482930637411\n",
      "Epoch: 6318 Training Loss: 0.21726586842723414 Test Loss: 0.21377993516131066\n",
      "Epoch: 6319 Training Loss: 0.21726326634558243 Test Loss: 0.21377905369361924\n",
      "Epoch: 6320 Training Loss: 0.2172643469308114 Test Loss: 0.21375845586771264\n",
      "Epoch: 6321 Training Loss: 0.21726388442100314 Test Loss: 0.21375432074721912\n",
      "Epoch: 6322 Training Loss: 0.2172648797952598 Test Loss: 0.21373875247225763\n",
      "Epoch: 6323 Training Loss: 0.2172648666065666 Test Loss: 0.21381634755462195\n",
      "Epoch: 6324 Training Loss: 0.21726424019295099 Test Loss: 0.21638649993860837\n",
      "Epoch: 6325 Training Loss: 0.2172663494424556 Test Loss: 0.2137906294384491\n",
      "Epoch: 6326 Training Loss: 0.21726556917569553 Test Loss: 0.21378439435080837\n",
      "Epoch: 6327 Training Loss: 0.21726486600585793 Test Loss: 0.21375531887975205\n",
      "Epoch: 6328 Training Loss: 0.21726663277073172 Test Loss: 0.21369811421913162\n",
      "Epoch: 6329 Training Loss: 0.21726411262753495 Test Loss: 0.21374689308564299\n",
      "Epoch: 6330 Training Loss: 0.2172639515210575 Test Loss: 0.21374336721487736\n",
      "Epoch: 6331 Training Loss: 0.21726493251416945 Test Loss: 0.2137545670396623\n",
      "Epoch: 6332 Training Loss: 0.21726166845156972 Test Loss: 0.21443853411715513\n",
      "Epoch: 6333 Training Loss: 0.2172663196311674 Test Loss: 0.21699552929957125\n",
      "Epoch: 6334 Training Loss: 0.2172669010095648 Test Loss: 0.21398350234698552\n",
      "Epoch: 6335 Training Loss: 0.21726425461892473 Test Loss: 0.21379857561043192\n",
      "Epoch: 6336 Training Loss: 0.21726520342481123 Test Loss: 0.2137531152105235\n",
      "Epoch: 6337 Training Loss: 0.21726581694560504 Test Loss: 0.21374160427949454\n",
      "Epoch: 6338 Training Loss: 0.21726461759041077 Test Loss: 0.2136785274885181\n",
      "Epoch: 6339 Training Loss: 0.217265132128762 Test Loss: 0.2136819885454829\n",
      "Epoch: 6340 Training Loss: 0.2172665534054616 Test Loss: 0.21373636732438675\n",
      "Epoch: 6341 Training Loss: 0.21726409468696722 Test Loss: 0.21365537599885845\n",
      "Epoch: 6342 Training Loss: 0.2172649310527439 Test Loss: 0.21375910400572104\n",
      "Epoch: 6343 Training Loss: 0.2172651873670617 Test Loss: 0.21380879026544414\n",
      "Epoch: 6344 Training Loss: 0.21726508967569444 Test Loss: 0.2137594410374854\n",
      "Epoch: 6345 Training Loss: 0.21726447927499937 Test Loss: 0.2137719501010473\n",
      "Epoch: 6346 Training Loss: 0.21726524481094844 Test Loss: 0.2138404712512942\n",
      "Epoch: 6347 Training Loss: 0.2172635427970892 Test Loss: 0.21390752760964213\n",
      "Epoch: 6348 Training Loss: 0.21726368442088115 Test Loss: 0.21382711960832138\n",
      "Epoch: 6349 Training Loss: 0.21726271612334339 Test Loss: 0.2138181882665658\n",
      "Epoch: 6350 Training Loss: 0.21726400215093553 Test Loss: 0.2149475557834236\n",
      "Epoch: 6351 Training Loss: 0.21726395111759647 Test Loss: 0.2137279415302777\n",
      "Epoch: 6352 Training Loss: 0.21726554228725847 Test Loss: 0.21377634447674418\n",
      "Epoch: 6353 Training Loss: 0.2172639598233892 Test Loss: 0.2137427838906698\n",
      "Epoch: 6354 Training Loss: 0.21726489766410112 Test Loss: 0.21378588506822768\n",
      "Epoch: 6355 Training Loss: 0.2172641447071708 Test Loss: 0.2136816644764787\n",
      "Epoch: 6356 Training Loss: 0.21726549644511817 Test Loss: 0.2137178046518265\n",
      "Epoch: 6357 Training Loss: 0.21726445226104107 Test Loss: 0.21370961218740045\n",
      "Epoch: 6358 Training Loss: 0.21726628052234362 Test Loss: 0.213708264060343\n",
      "Epoch: 6359 Training Loss: 0.2172652770788661 Test Loss: 0.2137582743890703\n",
      "Epoch: 6360 Training Loss: 0.21726560765691325 Test Loss: 0.21374715234084635\n",
      "Epoch: 6361 Training Loss: 0.21726439759655264 Test Loss: 0.2138851279600722\n",
      "Epoch: 6362 Training Loss: 0.21726561780619993 Test Loss: 0.21374711345256583\n",
      "Epoch: 6363 Training Loss: 0.21726484688180447 Test Loss: 0.21369452353456514\n",
      "Epoch: 6364 Training Loss: 0.21726596107085547 Test Loss: 0.2136512538411251\n",
      "Epoch: 6365 Training Loss: 0.2172638764504061 Test Loss: 0.2136991253144247\n",
      "Epoch: 6366 Training Loss: 0.21726404629853946 Test Loss: 0.2137336581075117\n",
      "Epoch: 6367 Training Loss: 0.21726469615772456 Test Loss: 0.21380024780649357\n",
      "Epoch: 6368 Training Loss: 0.2172650450439373 Test Loss: 0.21378767392913084\n",
      "Epoch: 6369 Training Loss: 0.21726620104051814 Test Loss: 0.21382202524357544\n",
      "Epoch: 6370 Training Loss: 0.21726500849933256 Test Loss: 0.21370590483799246\n",
      "Epoch: 6371 Training Loss: 0.21726357703748306 Test Loss: 0.2137416561305352\n",
      "Epoch: 6372 Training Loss: 0.21726385674357557 Test Loss: 0.21379236644831157\n",
      "Epoch: 6373 Training Loss: 0.2172645358043744 Test Loss: 0.21379864042423277\n",
      "Epoch: 6374 Training Loss: 0.21726359770365428 Test Loss: 0.21375831327735081\n",
      "Epoch: 6375 Training Loss: 0.2172650936654759 Test Loss: 0.21372559527068732\n",
      "Epoch: 6376 Training Loss: 0.21726388072709316 Test Loss: 0.21371668985445205\n",
      "Epoch: 6377 Training Loss: 0.2172654192585377 Test Loss: 0.21380456440562945\n",
      "Epoch: 6378 Training Loss: 0.21726512514440305 Test Loss: 0.2138063143782521\n",
      "Epoch: 6379 Training Loss: 0.2172636073777535 Test Loss: 0.21378186661257567\n",
      "Epoch: 6380 Training Loss: 0.21726378345711853 Test Loss: 0.2138073513990655\n",
      "Epoch: 6381 Training Loss: 0.21726305063737716 Test Loss: 0.2137125288084382\n",
      "Epoch: 6382 Training Loss: 0.21726299985508052 Test Loss: 0.2137730000846209\n",
      "Epoch: 6383 Training Loss: 0.2172638261881259 Test Loss: 0.21380488847463364\n",
      "Epoch: 6384 Training Loss: 0.21726381180698118 Test Loss: 0.2137656761251261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6385 Training Loss: 0.21726527830718084 Test Loss: 0.21379612564876022\n",
      "Epoch: 6386 Training Loss: 0.21726415463231244 Test Loss: 0.21377281860597855\n",
      "Epoch: 6387 Training Loss: 0.21726548685171113 Test Loss: 0.21379328680428347\n",
      "Epoch: 6388 Training Loss: 0.21726527412911759 Test Loss: 0.21402009621893917\n",
      "Epoch: 6389 Training Loss: 0.21726418276699586 Test Loss: 0.21377298712186074\n",
      "Epoch: 6390 Training Loss: 0.21726487553650434 Test Loss: 0.2137172342903791\n",
      "Epoch: 6391 Training Loss: 0.21726701362002485 Test Loss: 0.2137356673353377\n",
      "Epoch: 6392 Training Loss: 0.21726449379959692 Test Loss: 0.2137720408403685\n",
      "Epoch: 6393 Training Loss: 0.21726372652428244 Test Loss: 0.2137447542302153\n",
      "Epoch: 6394 Training Loss: 0.21726490259529163 Test Loss: 0.2137034808018411\n",
      "Epoch: 6395 Training Loss: 0.21726373677219293 Test Loss: 0.21372261383584873\n",
      "Epoch: 6396 Training Loss: 0.21726387155507876 Test Loss: 0.21379208126758786\n",
      "Epoch: 6397 Training Loss: 0.2172643217638081 Test Loss: 0.21376671314593954\n",
      "Epoch: 6398 Training Loss: 0.2172626427023993 Test Loss: 0.21373367107027186\n",
      "Epoch: 6399 Training Loss: 0.2172623059379495 Test Loss: 0.21683147260688782\n",
      "Epoch: 6400 Training Loss: 0.21726441263220084 Test Loss: 0.21383454726989753\n",
      "Epoch: 6401 Training Loss: 0.21726279053052552 Test Loss: 0.21372686562118376\n",
      "Epoch: 6402 Training Loss: 0.2172643243280272 Test Loss: 0.2137697853200993\n",
      "Epoch: 6403 Training Loss: 0.21726582766870298 Test Loss: 0.2137799221985505\n",
      "Epoch: 6404 Training Loss: 0.21726426200674473 Test Loss: 0.21377637040226452\n",
      "Epoch: 6405 Training Loss: 0.2172653424933499 Test Loss: 0.21377318156326325\n",
      "Epoch: 6406 Training Loss: 0.21726551127455296 Test Loss: 0.21368289593869466\n",
      "Epoch: 6407 Training Loss: 0.21726522496963094 Test Loss: 0.21376277246684852\n",
      "Epoch: 6408 Training Loss: 0.21726418669401668 Test Loss: 0.21374478015573564\n",
      "Epoch: 6409 Training Loss: 0.21726378232742763 Test Loss: 0.21382018453163162\n",
      "Epoch: 6410 Training Loss: 0.21726411390067868 Test Loss: 0.21395560648710443\n",
      "Epoch: 6411 Training Loss: 0.21726299976542252 Test Loss: 0.21381555682625172\n",
      "Epoch: 6412 Training Loss: 0.21726402091635694 Test Loss: 0.21369382354551608\n",
      "Epoch: 6413 Training Loss: 0.21726377488581283 Test Loss: 0.21375120968477887\n",
      "Epoch: 6414 Training Loss: 0.21726492588844254 Test Loss: 0.21374476719297547\n",
      "Epoch: 6415 Training Loss: 0.2172635325491787 Test Loss: 0.21903896584966018\n",
      "Epoch: 6416 Training Loss: 0.21726639179689933 Test Loss: 0.214128944516068\n",
      "Epoch: 6417 Training Loss: 0.2172636482976691 Test Loss: 0.21373818211081025\n",
      "Epoch: 6418 Training Loss: 0.21726449044638735 Test Loss: 0.21372848596620472\n",
      "Epoch: 6419 Training Loss: 0.2172632408916735 Test Loss: 0.21406114928039052\n",
      "Epoch: 6420 Training Loss: 0.2172643635085774 Test Loss: 0.21369340873719073\n",
      "Epoch: 6421 Training Loss: 0.21726379824172432 Test Loss: 0.21375184486002707\n",
      "Epoch: 6422 Training Loss: 0.21726573371607474 Test Loss: 0.21379488122378412\n",
      "Epoch: 6423 Training Loss: 0.21726529809470355 Test Loss: 0.21372801930683868\n",
      "Epoch: 6424 Training Loss: 0.21726563253701092 Test Loss: 0.2137282267110014\n",
      "Epoch: 6425 Training Loss: 0.2172649050160579 Test Loss: 0.21377463339240205\n",
      "Epoch: 6426 Training Loss: 0.21726552518947603 Test Loss: 0.21388813532043113\n",
      "Epoch: 6427 Training Loss: 0.2172650489709581 Test Loss: 0.2137721963934905\n",
      "Epoch: 6428 Training Loss: 0.21726522319440233 Test Loss: 0.21384564339260115\n",
      "Epoch: 6429 Training Loss: 0.21726428414330728 Test Loss: 0.2138507377573471\n",
      "Epoch: 6430 Training Loss: 0.21726214722534093 Test Loss: 0.21375190967382793\n",
      "Epoch: 6431 Training Loss: 0.21726548786484665 Test Loss: 0.2136452909714479\n",
      "Epoch: 6432 Training Loss: 0.21726566074342074 Test Loss: 0.21402254618061087\n",
      "Epoch: 6433 Training Loss: 0.21726692944011963 Test Loss: 0.21380439588974726\n",
      "Epoch: 6434 Training Loss: 0.21726605182269276 Test Loss: 0.21375141708894155\n",
      "Epoch: 6435 Training Loss: 0.217266115686093 Test Loss: 0.2137563040495248\n",
      "Epoch: 6436 Training Loss: 0.2172650198948656 Test Loss: 0.21377615003534167\n",
      "Epoch: 6437 Training Loss: 0.21726447616386643 Test Loss: 0.2137542559334183\n",
      "Epoch: 6438 Training Loss: 0.2172646301604637 Test Loss: 0.2137475671491717\n",
      "Epoch: 6439 Training Loss: 0.21726496784839103 Test Loss: 0.2137263600735372\n",
      "Epoch: 6440 Training Loss: 0.21726583660760654 Test Loss: 0.21433471537097137\n",
      "Epoch: 6441 Training Loss: 0.21726473233473143 Test Loss: 0.21382343818443375\n",
      "Epoch: 6442 Training Loss: 0.21726430899650756 Test Loss: 0.2138153753476094\n",
      "Epoch: 6443 Training Loss: 0.2172653124668825 Test Loss: 0.213816658660866\n",
      "Epoch: 6444 Training Loss: 0.21726455494635946 Test Loss: 0.21370196415890144\n",
      "Epoch: 6445 Training Loss: 0.21726457329038823 Test Loss: 0.21368366074154455\n",
      "Epoch: 6446 Training Loss: 0.21726540398977867 Test Loss: 0.21371580838676066\n",
      "Epoch: 6447 Training Loss: 0.21726489879379204 Test Loss: 0.21377141862788043\n",
      "Epoch: 6448 Training Loss: 0.2172641121971765 Test Loss: 0.21371254177119836\n",
      "Epoch: 6449 Training Loss: 0.217263574742238 Test Loss: 0.21373294515570246\n",
      "Epoch: 6450 Training Loss: 0.2172633997029059 Test Loss: 0.2137289007745301\n",
      "Epoch: 6451 Training Loss: 0.21726491881442558 Test Loss: 0.2137434709169587\n",
      "Epoch: 6452 Training Loss: 0.21726465217150506 Test Loss: 0.21376483354571518\n",
      "Epoch: 6453 Training Loss: 0.21726453954311342 Test Loss: 0.21373955616338802\n",
      "Epoch: 6454 Training Loss: 0.21726348570286869 Test Loss: 0.21374798195749709\n",
      "Epoch: 6455 Training Loss: 0.21726492053585936 Test Loss: 0.21493049679104279\n",
      "Epoch: 6456 Training Loss: 0.21726517105826976 Test Loss: 0.21369719386315972\n",
      "Epoch: 6457 Training Loss: 0.21726453628852765 Test Loss: 0.21375454111414197\n",
      "Epoch: 6458 Training Loss: 0.21726434019749488 Test Loss: 0.21377551486009344\n",
      "Epoch: 6459 Training Loss: 0.2172637139721611 Test Loss: 0.2137941682719749\n",
      "Epoch: 6460 Training Loss: 0.21726557111230854 Test Loss: 0.21376860570892403\n",
      "Epoch: 6461 Training Loss: 0.21726535878421024 Test Loss: 0.2136847236878783\n",
      "Epoch: 6462 Training Loss: 0.21726501776100496 Test Loss: 0.21364403358371165\n",
      "Epoch: 6463 Training Loss: 0.21726625582156198 Test Loss: 0.2152893448807675\n",
      "Epoch: 6464 Training Loss: 0.21726561982350515 Test Loss: 0.21372112311842945\n",
      "Epoch: 6465 Training Loss: 0.21726498245368078 Test Loss: 0.21370834183690401\n",
      "Epoch: 6466 Training Loss: 0.21726492406838493 Test Loss: 0.21361862657378278\n",
      "Epoch: 6467 Training Loss: 0.21726531235929286 Test Loss: 0.21366397030884968\n",
      "Epoch: 6468 Training Loss: 0.21726647264152657 Test Loss: 0.21369810125637145\n",
      "Epoch: 6469 Training Loss: 0.21726390407403884 Test Loss: 0.2137563040495248\n",
      "Epoch: 6470 Training Loss: 0.21726261324077736 Test Loss: 0.21441654927591058\n",
      "Epoch: 6471 Training Loss: 0.2172655509930512 Test Loss: 0.21384102864998142\n",
      "Epoch: 6472 Training Loss: 0.21726346354837453 Test Loss: 0.21381671051190665\n",
      "Epoch: 6473 Training Loss: 0.21726537697582038 Test Loss: 0.21383367876496628\n",
      "Epoch: 6474 Training Loss: 0.21726482792810126 Test Loss: 0.21368270149729213\n",
      "Epoch: 6475 Training Loss: 0.21726640660840252 Test Loss: 0.2137874017111673\n",
      "Epoch: 6476 Training Loss: 0.2172650939882447 Test Loss: 0.21370676038016354\n",
      "Epoch: 6477 Training Loss: 0.21726588338219016 Test Loss: 0.2136702053964904\n",
      "Epoch: 6478 Training Loss: 0.2172648310840632 Test Loss: 0.21369379761999574\n",
      "Epoch: 6479 Training Loss: 0.21726589573706387 Test Loss: 0.21376281135512903\n",
      "Epoch: 6480 Training Loss: 0.21726504276662384 Test Loss: 0.2138872279272194\n",
      "Epoch: 6481 Training Loss: 0.2172633653907856 Test Loss: 0.21379756451513884\n",
      "Epoch: 6482 Training Loss: 0.21726446313655765 Test Loss: 0.21379073314053043\n",
      "Epoch: 6483 Training Loss: 0.2172631726081334 Test Loss: 0.21372240643168605\n",
      "Epoch: 6484 Training Loss: 0.21726467723091875 Test Loss: 0.2136467428005867\n",
      "Epoch: 6485 Training Loss: 0.21726576115142568 Test Loss: 0.21380263295436444\n",
      "Epoch: 6486 Training Loss: 0.2172648302861069 Test Loss: 0.21374448201225177\n",
      "Epoch: 6487 Training Loss: 0.2172664568975801 Test Loss: 0.21371808983255017\n",
      "Epoch: 6488 Training Loss: 0.21726578417560255 Test Loss: 0.21378707764216312\n",
      "Epoch: 6489 Training Loss: 0.21726612293046016 Test Loss: 0.21382648443307317\n",
      "Epoch: 6490 Training Loss: 0.2172645608010275 Test Loss: 0.21379415530921472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6491 Training Loss: 0.21726394419599812 Test Loss: 0.21382505852945471\n",
      "Epoch: 6492 Training Loss: 0.21726440532507305 Test Loss: 0.21376544279544307\n",
      "Epoch: 6493 Training Loss: 0.2172648957185223 Test Loss: 0.21379084980537194\n",
      "Epoch: 6494 Training Loss: 0.2172633867024945 Test Loss: 0.2137153028391141\n",
      "Epoch: 6495 Training Loss: 0.21726469520734967 Test Loss: 0.2138807465471355\n",
      "Epoch: 6496 Training Loss: 0.21726324006681982 Test Loss: 0.21383514355686525\n",
      "Epoch: 6497 Training Loss: 0.21726435984156478 Test Loss: 0.21372690450946427\n",
      "Epoch: 6498 Training Loss: 0.21726402110463874 Test Loss: 0.21377871666185488\n",
      "Epoch: 6499 Training Loss: 0.21726417089627537 Test Loss: 0.2138155179379712\n",
      "Epoch: 6500 Training Loss: 0.21726408573909786 Test Loss: 0.21378098514488425\n",
      "Epoch: 6501 Training Loss: 0.21726539261217725 Test Loss: 0.21380530328295902\n",
      "Epoch: 6502 Training Loss: 0.21726411370343107 Test Loss: 0.21377043345810767\n",
      "Epoch: 6503 Training Loss: 0.21726515075073058 Test Loss: 0.2137582743890703\n",
      "Epoch: 6504 Training Loss: 0.21726453081938907 Test Loss: 0.2138855298056374\n",
      "Epoch: 6505 Training Loss: 0.21726569554866007 Test Loss: 0.2137613724887504\n",
      "Epoch: 6506 Training Loss: 0.2172643543455288 Test Loss: 0.21374124132220984\n",
      "Epoch: 6507 Training Loss: 0.21726509418549234 Test Loss: 0.21378019441651402\n",
      "Epoch: 6508 Training Loss: 0.21726504622742301 Test Loss: 0.213743419065918\n",
      "Epoch: 6509 Training Loss: 0.21726520848152295 Test Loss: 0.21378069996416055\n",
      "Epoch: 6510 Training Loss: 0.21726438089326544 Test Loss: 0.2136916458018079\n",
      "Epoch: 6511 Training Loss: 0.21726524192396054 Test Loss: 0.21389464262603536\n",
      "Epoch: 6512 Training Loss: 0.21726453905896015 Test Loss: 0.21374472830469496\n",
      "Epoch: 6513 Training Loss: 0.21726657351575315 Test Loss: 0.21376899459172904\n",
      "Epoch: 6514 Training Loss: 0.2172650741379614 Test Loss: 0.2137290304021318\n",
      "Epoch: 6515 Training Loss: 0.2172652473124069 Test Loss: 0.21372418232982904\n",
      "Epoch: 6516 Training Loss: 0.2172635287835423 Test Loss: 0.21375666700680948\n",
      "Epoch: 6517 Training Loss: 0.21726345284320817 Test Loss: 0.213706099279395\n",
      "Epoch: 6518 Training Loss: 0.21726715088643753 Test Loss: 0.21371513432323191\n",
      "Epoch: 6519 Training Loss: 0.21726417524468886 Test Loss: 0.21371842686431453\n",
      "Epoch: 6520 Training Loss: 0.2172646285017905 Test Loss: 0.21376605204517096\n",
      "Epoch: 6521 Training Loss: 0.2172646161110536 Test Loss: 0.2137326859004991\n",
      "Epoch: 6522 Training Loss: 0.2172649892676895 Test Loss: 0.21378777763121218\n",
      "Epoch: 6523 Training Loss: 0.21726653946364113 Test Loss: 0.2138222456104983\n",
      "Epoch: 6524 Training Loss: 0.2172642656468599 Test Loss: 0.21376034843069713\n",
      "Epoch: 6525 Training Loss: 0.21726489220392833 Test Loss: 0.21379269051731575\n",
      "Epoch: 6526 Training Loss: 0.21726361755393758 Test Loss: 0.21388910752744372\n",
      "Epoch: 6527 Training Loss: 0.2172639325673543 Test Loss: 0.21376562427408544\n",
      "Epoch: 6528 Training Loss: 0.21726506627495396 Test Loss: 0.2137945312292596\n",
      "Epoch: 6529 Training Loss: 0.2172661138481038 Test Loss: 0.21372193977232\n",
      "Epoch: 6530 Training Loss: 0.21726397268931358 Test Loss: 0.21383527318446693\n",
      "Epoch: 6531 Training Loss: 0.21726415083081282 Test Loss: 0.21383720463573191\n",
      "Epoch: 6532 Training Loss: 0.21726353925559783 Test Loss: 0.21375744477241956\n",
      "Epoch: 6533 Training Loss: 0.2172652144527464 Test Loss: 0.21377800371004566\n",
      "Epoch: 6534 Training Loss: 0.2172670963564361 Test Loss: 0.2137616447067139\n",
      "Epoch: 6535 Training Loss: 0.21726525704030095 Test Loss: 0.21373019705054688\n",
      "Epoch: 6536 Training Loss: 0.21726527019313097 Test Loss: 0.2137657668644473\n",
      "Epoch: 6537 Training Loss: 0.21726293736344784 Test Loss: 0.21378009071443269\n",
      "Epoch: 6538 Training Loss: 0.21726345939720867 Test Loss: 0.21382150673316874\n",
      "Epoch: 6539 Training Loss: 0.2172643151380812 Test Loss: 0.21380978839797707\n",
      "Epoch: 6540 Training Loss: 0.21726538916930968 Test Loss: 0.21408157859041493\n",
      "Epoch: 6541 Training Loss: 0.21726294224980935 Test Loss: 0.2138196400957046\n",
      "Epoch: 6542 Training Loss: 0.21726466175594628 Test Loss: 0.2136764404841311\n",
      "Epoch: 6543 Training Loss: 0.21726466406015713 Test Loss: 0.21366027592220188\n",
      "Epoch: 6544 Training Loss: 0.2172658996013241 Test Loss: 0.21371433063210152\n",
      "Epoch: 6545 Training Loss: 0.21726329741208275 Test Loss: 0.2138195104681029\n",
      "Epoch: 6546 Training Loss: 0.21726349644389825 Test Loss: 0.21377499634968675\n",
      "Epoch: 6547 Training Loss: 0.21726423497485484 Test Loss: 0.21374658197939897\n",
      "Epoch: 6548 Training Loss: 0.21726381500777212 Test Loss: 0.21366695174368827\n",
      "Epoch: 6549 Training Loss: 0.21726384439766766 Test Loss: 0.21367098316210045\n",
      "Epoch: 6550 Training Loss: 0.21726410183271058 Test Loss: 0.21377950739022514\n",
      "Epoch: 6551 Training Loss: 0.2172646823503911 Test Loss: 0.21381843455900898\n",
      "Epoch: 6552 Training Loss: 0.21726449151331767 Test Loss: 0.2137472949312082\n",
      "Epoch: 6553 Training Loss: 0.2172637701160067 Test Loss: 0.21388463537518584\n",
      "Epoch: 6554 Training Loss: 0.21726300561112474 Test Loss: 0.21421398022276866\n",
      "Epoch: 6555 Training Loss: 0.21726429598713035 Test Loss: 0.21390988683199266\n",
      "Epoch: 6556 Training Loss: 0.21726416571404242 Test Loss: 0.21388896493708187\n",
      "Epoch: 6557 Training Loss: 0.21726480850817637 Test Loss: 0.21366811839210337\n",
      "Epoch: 6558 Training Loss: 0.21726654458311348 Test Loss: 0.21379932745052166\n",
      "Epoch: 6559 Training Loss: 0.21726510424512102 Test Loss: 0.21405784377654774\n",
      "Epoch: 6560 Training Loss: 0.21726595625622033 Test Loss: 0.21376710202874455\n",
      "Epoch: 6561 Training Loss: 0.21726459037023904 Test Loss: 0.2137135399037313\n",
      "Epoch: 6562 Training Loss: 0.21726494754981765 Test Loss: 0.21368426999127244\n",
      "Epoch: 6563 Training Loss: 0.21726339186679586 Test Loss: 0.21373102666719762\n",
      "Epoch: 6564 Training Loss: 0.21726543546870583 Test Loss: 0.21378505545157694\n",
      "Epoch: 6565 Training Loss: 0.21726423049195434 Test Loss: 0.2137432764755562\n",
      "Epoch: 6566 Training Loss: 0.21726417025970352 Test Loss: 0.21367317386856882\n",
      "Epoch: 6567 Training Loss: 0.21726394121935222 Test Loss: 0.21369972160139242\n",
      "Epoch: 6568 Training Loss: 0.21726286103758427 Test Loss: 0.2137022234141048\n",
      "Epoch: 6569 Training Loss: 0.21726483999606935 Test Loss: 0.2137525189235558\n",
      "Epoch: 6570 Training Loss: 0.21726331313809763 Test Loss: 0.2138321491592665\n",
      "Epoch: 6571 Training Loss: 0.21726538685613303 Test Loss: 0.21383290099935623\n",
      "Epoch: 6572 Training Loss: 0.2172646394400677 Test Loss: 0.213789540566595\n",
      "Epoch: 6573 Training Loss: 0.21726412347615412 Test Loss: 0.21405309940632633\n",
      "Epoch: 6574 Training Loss: 0.2172637166977646 Test Loss: 0.21374261537478761\n",
      "Epoch: 6575 Training Loss: 0.2172642075932987 Test Loss: 0.21380689770245964\n",
      "Epoch: 6576 Training Loss: 0.21726291211575233 Test Loss: 0.2138235678120354\n",
      "Epoch: 6577 Training Loss: 0.21726354521785546 Test Loss: 0.21389461670051502\n",
      "Epoch: 6578 Training Loss: 0.2172631322172001 Test Loss: 0.21375599294328076\n",
      "Epoch: 6579 Training Loss: 0.21726364713211496 Test Loss: 0.213824773348731\n",
      "Epoch: 6580 Training Loss: 0.21726311161378947 Test Loss: 0.21375206522694992\n",
      "Epoch: 6581 Training Loss: 0.2172647002192324 Test Loss: 0.21377677224782973\n",
      "Epoch: 6582 Training Loss: 0.217262673347507 Test Loss: 0.21374264130030796\n",
      "Epoch: 6583 Training Loss: 0.21726527268562362 Test Loss: 0.21378833502989938\n",
      "Epoch: 6584 Training Loss: 0.21726413234333125 Test Loss: 0.21374360054456037\n",
      "Epoch: 6585 Training Loss: 0.21726442832235252 Test Loss: 0.21376965569249762\n",
      "Epoch: 6586 Training Loss: 0.21726619938184494 Test Loss: 0.21383629724252018\n",
      "Epoch: 6587 Training Loss: 0.21726402184880023 Test Loss: 0.21376616871001247\n",
      "Epoch: 6588 Training Loss: 0.21726681237365653 Test Loss: 0.21380145334318917\n",
      "Epoch: 6589 Training Loss: 0.21726404798411003 Test Loss: 0.21380237369916108\n",
      "Epoch: 6590 Training Loss: 0.21726470251447744 Test Loss: 0.21380665141001645\n",
      "Epoch: 6591 Training Loss: 0.21726378461370685 Test Loss: 0.21381424758747478\n",
      "Epoch: 6592 Training Loss: 0.21726569907221985 Test Loss: 0.21381847344728946\n",
      "Epoch: 6593 Training Loss: 0.21726421650530486 Test Loss: 0.21371198437251115\n",
      "Epoch: 6594 Training Loss: 0.21726524460473504 Test Loss: 0.2136764793724116\n",
      "Epoch: 6595 Training Loss: 0.21726506995093234 Test Loss: 0.2137227045751699\n",
      "Epoch: 6596 Training Loss: 0.21726566412352769 Test Loss: 0.21369721978868006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6597 Training Loss: 0.2172652837314904 Test Loss: 0.2137499911853231\n",
      "Epoch: 6598 Training Loss: 0.21726365109499898 Test Loss: 0.21372179718195816\n",
      "Epoch: 6599 Training Loss: 0.2172656912809388 Test Loss: 0.21378806281193585\n",
      "Epoch: 6600 Training Loss: 0.21726371624050875 Test Loss: 0.21379387012849102\n",
      "Epoch: 6601 Training Loss: 0.21726527848649685 Test Loss: 0.21379810895106588\n",
      "Epoch: 6602 Training Loss: 0.21726332335911072 Test Loss: 0.21385940984389934\n",
      "Epoch: 6603 Training Loss: 0.21726637441221128 Test Loss: 0.21371194548423064\n",
      "Epoch: 6604 Training Loss: 0.21726533150127791 Test Loss: 0.21373991912067272\n",
      "Epoch: 6605 Training Loss: 0.2172642076381277 Test Loss: 0.2137322451666534\n",
      "Epoch: 6606 Training Loss: 0.21726544574351375 Test Loss: 0.2137744908020402\n",
      "Epoch: 6607 Training Loss: 0.21726521360996112 Test Loss: 0.21367802194087157\n",
      "Epoch: 6608 Training Loss: 0.21726559618965383 Test Loss: 0.21375495592246735\n",
      "Epoch: 6609 Training Loss: 0.21726548106876953 Test Loss: 0.21363724109738372\n",
      "Epoch: 6610 Training Loss: 0.21726851310540624 Test Loss: 0.21388232800387597\n",
      "Epoch: 6611 Training Loss: 0.21726455364631833 Test Loss: 0.21376415948218647\n",
      "Epoch: 6612 Training Loss: 0.2172658886092521 Test Loss: 0.21377450376480037\n",
      "Epoch: 6613 Training Loss: 0.21726463006183988 Test Loss: 0.2136684683866279\n",
      "Epoch: 6614 Training Loss: 0.21726435364619634 Test Loss: 0.21378974797075767\n",
      "Epoch: 6615 Training Loss: 0.21726341510615194 Test Loss: 0.21372336567593847\n",
      "Epoch: 6616 Training Loss: 0.21726242798043216 Test Loss: 0.21369817903293245\n",
      "Epoch: 6617 Training Loss: 0.21726577086138812 Test Loss: 0.21631007150465914\n",
      "Epoch: 6618 Training Loss: 0.2172640018640299 Test Loss: 0.21412633900127429\n",
      "Epoch: 6619 Training Loss: 0.21726303171953712 Test Loss: 0.21382352892375492\n",
      "Epoch: 6620 Training Loss: 0.21726472615729456 Test Loss: 0.21379936633880217\n",
      "Epoch: 6621 Training Loss: 0.21726437531653725 Test Loss: 0.21374968007907905\n",
      "Epoch: 6622 Training Loss: 0.21726357699265406 Test Loss: 0.21368445146991477\n",
      "Epoch: 6623 Training Loss: 0.21726654833081827 Test Loss: 0.21369917716546538\n",
      "Epoch: 6624 Training Loss: 0.2172648237231406 Test Loss: 0.21377788704520415\n",
      "Epoch: 6625 Training Loss: 0.21726493255899845 Test Loss: 0.21371766206146464\n",
      "Epoch: 6626 Training Loss: 0.2172645328994549 Test Loss: 0.21374338017763753\n",
      "Epoch: 6627 Training Loss: 0.21726569189061326 Test Loss: 0.21373238775701525\n",
      "Epoch: 6628 Training Loss: 0.21726542442283905 Test Loss: 0.21373241368253557\n",
      "Epoch: 6629 Training Loss: 0.2172652227371465 Test Loss: 0.21376900755448922\n",
      "Epoch: 6630 Training Loss: 0.21726403340571768 Test Loss: 0.21642730670761653\n",
      "Epoch: 6631 Training Loss: 0.21726353383128824 Test Loss: 0.21387666327768265\n",
      "Epoch: 6632 Training Loss: 0.21726533766078318 Test Loss: 0.21376216321712063\n",
      "Epoch: 6633 Training Loss: 0.21726273383080028 Test Loss: 0.21367723121250132\n",
      "Epoch: 6634 Training Loss: 0.21726398831670465 Test Loss: 0.21365199271845467\n",
      "Epoch: 6635 Training Loss: 0.21726436156299858 Test Loss: 0.21368144410955586\n",
      "Epoch: 6636 Training Loss: 0.21726563099489316 Test Loss: 0.21367660900001328\n",
      "Epoch: 6637 Training Loss: 0.21726482258448387 Test Loss: 0.21370187341958027\n",
      "Epoch: 6638 Training Loss: 0.21726535552962448 Test Loss: 0.21370511410962223\n",
      "Epoch: 6639 Training Loss: 0.21726488777482267 Test Loss: 0.21369041433959196\n",
      "Epoch: 6640 Training Loss: 0.21726422867189676 Test Loss: 0.21371722132761894\n",
      "Epoch: 6641 Training Loss: 0.21726253145474103 Test Loss: 0.21361425812360624\n",
      "Epoch: 6642 Training Loss: 0.21726556667423708 Test Loss: 0.21707920391645427\n",
      "Epoch: 6643 Training Loss: 0.21726622570543658 Test Loss: 0.21405348828913137\n",
      "Epoch: 6644 Training Loss: 0.2172649890883735 Test Loss: 0.21400416498669295\n",
      "Epoch: 6645 Training Loss: 0.21726466796924634 Test Loss: 0.21382797515049246\n",
      "Epoch: 6646 Training Loss: 0.21726463146947064 Test Loss: 0.21383317321731973\n",
      "Epoch: 6647 Training Loss: 0.2172635437116009 Test Loss: 0.21383148805849794\n",
      "Epoch: 6648 Training Loss: 0.21726282448401377 Test Loss: 0.21387496515610066\n",
      "Epoch: 6649 Training Loss: 0.21726168711836732 Test Loss: 0.21375885771327785\n",
      "Epoch: 6650 Training Loss: 0.21726436127609294 Test Loss: 0.21379451826649942\n",
      "Epoch: 6651 Training Loss: 0.2172640811755052 Test Loss: 0.21375797624558643\n",
      "Epoch: 6652 Training Loss: 0.21726518255242658 Test Loss: 0.21393185871047707\n",
      "Epoch: 6653 Training Loss: 0.21726354060943376 Test Loss: 0.21378460175497108\n",
      "Epoch: 6654 Training Loss: 0.21726540090554314 Test Loss: 0.2137259193396915\n",
      "Epoch: 6655 Training Loss: 0.21726615640876096 Test Loss: 0.21367567568128118\n",
      "Epoch: 6656 Training Loss: 0.21726442210008665 Test Loss: 0.21365781299776998\n",
      "Epoch: 6657 Training Loss: 0.21726304203020824 Test Loss: 0.2137622928447223\n",
      "Epoch: 6658 Training Loss: 0.217264073886309 Test Loss: 0.2137676075763911\n",
      "Epoch: 6659 Training Loss: 0.2172639353467526 Test Loss: 0.21370466041301636\n",
      "Epoch: 6660 Training Loss: 0.21726504303559788 Test Loss: 0.21376034843069713\n",
      "Epoch: 6661 Training Loss: 0.21726528112244234 Test Loss: 0.21378775170569184\n",
      "Epoch: 6662 Training Loss: 0.21726286845230167 Test Loss: 0.21424603712866358\n",
      "Epoch: 6663 Training Loss: 0.21726225582808795 Test Loss: 0.214508520059301\n",
      "Epoch: 6664 Training Loss: 0.2172637935167472 Test Loss: 0.21396315081352207\n",
      "Epoch: 6665 Training Loss: 0.21726377492167603 Test Loss: 0.21387517256026337\n",
      "Epoch: 6666 Training Loss: 0.21726406870407605 Test Loss: 0.21377898887981842\n",
      "Epoch: 6667 Training Loss: 0.21726368929827686 Test Loss: 0.21369343466271107\n",
      "Epoch: 6668 Training Loss: 0.2172647983319923 Test Loss: 0.21374935601007486\n",
      "Epoch: 6669 Training Loss: 0.21726450178812556 Test Loss: 0.21376826867715965\n",
      "Epoch: 6670 Training Loss: 0.21726337508281646 Test Loss: 0.21372791560475735\n",
      "Epoch: 6671 Training Loss: 0.21726723191038078 Test Loss: 0.213704401157813\n",
      "Epoch: 6672 Training Loss: 0.21726572707241623 Test Loss: 0.21373873950949746\n",
      "Epoch: 6673 Training Loss: 0.21726509171093128 Test Loss: 0.21375410038029627\n",
      "Epoch: 6674 Training Loss: 0.21726635985175052 Test Loss: 0.21584000293269487\n",
      "Epoch: 6675 Training Loss: 0.2172662419873311 Test Loss: 0.21385850245068758\n",
      "Epoch: 6676 Training Loss: 0.21726504758125895 Test Loss: 0.21390384618575448\n",
      "Epoch: 6677 Training Loss: 0.21726449973495715 Test Loss: 0.21381261427969364\n",
      "Epoch: 6678 Training Loss: 0.21726564060623177 Test Loss: 0.2137861183979107\n",
      "Epoch: 6679 Training Loss: 0.2172651229746792 Test Loss: 0.21367697195729798\n",
      "Epoch: 6680 Training Loss: 0.2172641560309774 Test Loss: 0.21380646993137412\n",
      "Epoch: 6681 Training Loss: 0.2172653536737037 Test Loss: 0.213754852220386\n",
      "Epoch: 6682 Training Loss: 0.21726467919442916 Test Loss: 0.21367511828259397\n",
      "Epoch: 6683 Training Loss: 0.2172652536063992 Test Loss: 0.2136878995641194\n",
      "Epoch: 6684 Training Loss: 0.21726602615360463 Test Loss: 0.21371197140975098\n",
      "Epoch: 6685 Training Loss: 0.21726612380014287 Test Loss: 0.21380125890178667\n",
      "Epoch: 6686 Training Loss: 0.21726629398897668 Test Loss: 0.21390409247819767\n",
      "Epoch: 6687 Training Loss: 0.21726303059881202 Test Loss: 0.21425083334992567\n",
      "Epoch: 6688 Training Loss: 0.2172660652982916 Test Loss: 0.2139420992910096\n",
      "Epoch: 6689 Training Loss: 0.21726478931239654 Test Loss: 0.21390127955924126\n",
      "Epoch: 6690 Training Loss: 0.21726688539113953 Test Loss: 0.21390076104883454\n",
      "Epoch: 6691 Training Loss: 0.21726496626144426 Test Loss: 0.21378995537492035\n",
      "Epoch: 6692 Training Loss: 0.21726520924361603 Test Loss: 0.2137872072697648\n",
      "Epoch: 6693 Training Loss: 0.21726501306292526 Test Loss: 0.21367689418073696\n",
      "Epoch: 6694 Training Loss: 0.21726630247959017 Test Loss: 0.21377538523249176\n",
      "Epoch: 6695 Training Loss: 0.21726404597577062 Test Loss: 0.21374334128935701\n",
      "Epoch: 6696 Training Loss: 0.2172648722101922 Test Loss: 0.2136787348926808\n",
      "Epoch: 6697 Training Loss: 0.21726374320067224 Test Loss: 0.2154550219184719\n",
      "Epoch: 6698 Training Loss: 0.217264971273327 Test Loss: 0.21373282849086095\n",
      "Epoch: 6699 Training Loss: 0.21726467471152866 Test Loss: 0.21369228097705611\n",
      "Epoch: 6700 Training Loss: 0.21726432738536533 Test Loss: 0.2136869792081475\n",
      "Epoch: 6701 Training Loss: 0.21726412868528447 Test Loss: 0.21370989736812412\n",
      "Epoch: 6702 Training Loss: 0.21726308016175971 Test Loss: 0.21372103237910825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6703 Training Loss: 0.21726492724227847 Test Loss: 0.21393101613106616\n",
      "Epoch: 6704 Training Loss: 0.21726347692534956 Test Loss: 0.21376208544055963\n",
      "Epoch: 6705 Training Loss: 0.2172600649987604 Test Loss: 0.21435540393619917\n",
      "Epoch: 6706 Training Loss: 0.2172644900518921 Test Loss: 0.21395266394054635\n",
      "Epoch: 6707 Training Loss: 0.21726391203567008 Test Loss: 0.21378580729166666\n",
      "Epoch: 6708 Training Loss: 0.21726511479786875 Test Loss: 0.21378274808026707\n",
      "Epoch: 6709 Training Loss: 0.21726464629890543 Test Loss: 0.21377751112515928\n",
      "Epoch: 6710 Training Loss: 0.21726400396202733 Test Loss: 0.21369917716546538\n",
      "Epoch: 6711 Training Loss: 0.21726448075435653 Test Loss: 0.21363448002946797\n",
      "Epoch: 6712 Training Loss: 0.21726583468892513 Test Loss: 0.213721784219198\n",
      "Epoch: 6713 Training Loss: 0.21726458885501868 Test Loss: 0.21369089396171817\n",
      "Epoch: 6714 Training Loss: 0.21726524689998006 Test Loss: 0.21404743468013301\n",
      "Epoch: 6715 Training Loss: 0.21726276919191925 Test Loss: 0.21437050555179463\n",
      "Epoch: 6716 Training Loss: 0.21726316008290947 Test Loss: 0.21403274787286292\n",
      "Epoch: 6717 Training Loss: 0.2172629285859287 Test Loss: 0.21393646049033663\n",
      "Epoch: 6718 Training Loss: 0.2172658455285785 Test Loss: 0.21390133141028195\n",
      "Epoch: 6719 Training Loss: 0.21726423231201195 Test Loss: 0.21377108159611607\n",
      "Epoch: 6720 Training Loss: 0.21726427530302755 Test Loss: 0.21386237831597776\n",
      "Epoch: 6721 Training Loss: 0.21726341514201514 Test Loss: 0.21373380069787354\n",
      "Epoch: 6722 Training Loss: 0.2172648208182211 Test Loss: 0.21379033129496522\n",
      "Epoch: 6723 Training Loss: 0.21726493879919592 Test Loss: 0.21394466591752284\n",
      "Epoch: 6724 Training Loss: 0.21726482302380812 Test Loss: 0.21366745729133482\n",
      "Epoch: 6725 Training Loss: 0.21726739129542444 Test Loss: 0.21373732656863917\n",
      "Epoch: 6726 Training Loss: 0.217264156228225 Test Loss: 0.21370574928487043\n",
      "Epoch: 6727 Training Loss: 0.21726497858045477 Test Loss: 0.21379532195762982\n",
      "Epoch: 6728 Training Loss: 0.2172655240508193 Test Loss: 0.21369268282262133\n",
      "Epoch: 6729 Training Loss: 0.21726574884138095 Test Loss: 0.21372992483258335\n",
      "Epoch: 6730 Training Loss: 0.21726469509079424 Test Loss: 0.21371541950395562\n",
      "Epoch: 6731 Training Loss: 0.21726501368156553 Test Loss: 0.21437957948391206\n",
      "Epoch: 6732 Training Loss: 0.21726556437899203 Test Loss: 0.21382233634981948\n",
      "Epoch: 6733 Training Loss: 0.21726602997303582 Test Loss: 0.21365312047858925\n",
      "Epoch: 6734 Training Loss: 0.21726712600633985 Test Loss: 0.2137596354788879\n",
      "Epoch: 6735 Training Loss: 0.21726578838952898 Test Loss: 0.21376570205064643\n",
      "Epoch: 6736 Training Loss: 0.21726426250882958 Test Loss: 0.21370284562659286\n",
      "Epoch: 6737 Training Loss: 0.21726474095983195 Test Loss: 0.21369304577990603\n",
      "Epoch: 6738 Training Loss: 0.21726591031545622 Test Loss: 0.21382832514501698\n",
      "Epoch: 6739 Training Loss: 0.21726644987735794 Test Loss: 0.2138344435678162\n",
      "Epoch: 6740 Training Loss: 0.2172657035013255 Test Loss: 0.21378368139899914\n",
      "Epoch: 6741 Training Loss: 0.21726505412629366 Test Loss: 0.21370714926296858\n",
      "Epoch: 6742 Training Loss: 0.2172643179264453 Test Loss: 0.21369111432864102\n",
      "Epoch: 6743 Training Loss: 0.21726553211107438 Test Loss: 0.21370940478323777\n",
      "Epoch: 6744 Training Loss: 0.21726445378522724 Test Loss: 0.21375065228609164\n",
      "Epoch: 6745 Training Loss: 0.21726361052474963 Test Loss: 0.21523569201643306\n",
      "Epoch: 6746 Training Loss: 0.21725935367900973 Test Loss: 0.21526042496283318\n",
      "Epoch: 6747 Training Loss: 0.2172617544784299 Test Loss: 0.21442435285753159\n",
      "Epoch: 6748 Training Loss: 0.21726513282809448 Test Loss: 0.2140292868158981\n",
      "Epoch: 6749 Training Loss: 0.2172641494590453 Test Loss: 0.2138699226423954\n",
      "Epoch: 6750 Training Loss: 0.2172644908139852 Test Loss: 0.21377367414814963\n",
      "Epoch: 6751 Training Loss: 0.21726532717976185 Test Loss: 0.21373540808013433\n",
      "Epoch: 6752 Training Loss: 0.21726414839211497 Test Loss: 0.21386149684828634\n",
      "Epoch: 6753 Training Loss: 0.21726436982050126 Test Loss: 0.2137243249201909\n",
      "Epoch: 6754 Training Loss: 0.21726575703612303 Test Loss: 0.2136640480854107\n",
      "Epoch: 6755 Training Loss: 0.21726452277706562 Test Loss: 0.21369556055537858\n",
      "Epoch: 6756 Training Loss: 0.2172655929888629 Test Loss: 0.21374209686438092\n",
      "Epoch: 6757 Training Loss: 0.2172637820494878 Test Loss: 0.21369549574157773\n",
      "Epoch: 6758 Training Loss: 0.21726251224999538 Test Loss: 0.21370707148640755\n",
      "Epoch: 6759 Training Loss: 0.21726522753385 Test Loss: 0.21379475159618244\n",
      "Epoch: 6760 Training Loss: 0.21726628349002375 Test Loss: 0.21377113344715673\n",
      "Epoch: 6761 Training Loss: 0.21726464195049197 Test Loss: 0.21414698867822157\n",
      "Epoch: 6762 Training Loss: 0.21726435955465917 Test Loss: 0.21374033392899808\n",
      "Epoch: 6763 Training Loss: 0.21726572099360317 Test Loss: 0.2136546760098094\n",
      "Epoch: 6764 Training Loss: 0.2172639253498845 Test Loss: 0.21397567283984417\n",
      "Epoch: 6765 Training Loss: 0.21726488779275427 Test Loss: 0.21381483091168232\n",
      "Epoch: 6766 Training Loss: 0.21726402866280897 Test Loss: 0.2137269693232651\n",
      "Epoch: 6767 Training Loss: 0.21726274885748267 Test Loss: 0.21367574049508203\n",
      "Epoch: 6768 Training Loss: 0.21726597978248205 Test Loss: 0.21379908115807847\n",
      "Epoch: 6769 Training Loss: 0.21726485694143316 Test Loss: 0.2136479613000425\n",
      "Epoch: 6770 Training Loss: 0.21726604133270563 Test Loss: 0.21373065074715275\n",
      "Epoch: 6771 Training Loss: 0.21726498133295566 Test Loss: 0.21380834953159844\n",
      "Epoch: 6772 Training Loss: 0.2172647862460926 Test Loss: 0.21373635436162658\n",
      "Epoch: 6773 Training Loss: 0.2172646012367898 Test Loss: 0.21370139379745406\n",
      "Epoch: 6774 Training Loss: 0.21726572424818894 Test Loss: 0.213743419065918\n",
      "Epoch: 6775 Training Loss: 0.21726435555591192 Test Loss: 0.2138906889841842\n",
      "Epoch: 6776 Training Loss: 0.21726397807775996 Test Loss: 0.21368568293213072\n",
      "Epoch: 6777 Training Loss: 0.21726402738069941 Test Loss: 0.21375704292685435\n",
      "Epoch: 6778 Training Loss: 0.21726462696863857 Test Loss: 0.2137264119245779\n",
      "Epoch: 6779 Training Loss: 0.21726473993773066 Test Loss: 0.2137390376529813\n",
      "Epoch: 6780 Training Loss: 0.21726391627649394 Test Loss: 0.2137252841644433\n",
      "Epoch: 6781 Training Loss: 0.21726524966144675 Test Loss: 0.21371508247219126\n",
      "Epoch: 6782 Training Loss: 0.2172636548696012 Test Loss: 0.21471658532275406\n",
      "Epoch: 6783 Training Loss: 0.21726512878451823 Test Loss: 0.21368974027606324\n",
      "Epoch: 6784 Training Loss: 0.21726481732155872 Test Loss: 0.21371875093331874\n",
      "Epoch: 6785 Training Loss: 0.21726579074753463 Test Loss: 0.21369781607564778\n",
      "Epoch: 6786 Training Loss: 0.2172660595422474 Test Loss: 0.21376085397834368\n",
      "Epoch: 6787 Training Loss: 0.21726436054986306 Test Loss: 0.2137266452542609\n",
      "Epoch: 6788 Training Loss: 0.21726355245325682 Test Loss: 0.21373609510642322\n",
      "Epoch: 6789 Training Loss: 0.21726476720273136 Test Loss: 0.21373886913709914\n",
      "Epoch: 6790 Training Loss: 0.2172656464788314 Test Loss: 0.21376199470123844\n",
      "Epoch: 6791 Training Loss: 0.21726354056460476 Test Loss: 0.21378267030370607\n",
      "Epoch: 6792 Training Loss: 0.21726486540514928 Test Loss: 0.21374159131673437\n",
      "Epoch: 6793 Training Loss: 0.2172651340474434 Test Loss: 0.21377608522154082\n",
      "Epoch: 6794 Training Loss: 0.21726403811276318 Test Loss: 0.21370106972844988\n",
      "Epoch: 6795 Training Loss: 0.21726722714057467 Test Loss: 0.21373355440543035\n",
      "Epoch: 6796 Training Loss: 0.21726478461431684 Test Loss: 0.2138667467661543\n",
      "Epoch: 6797 Training Loss: 0.2172637605584629 Test Loss: 0.21379978114712753\n",
      "Epoch: 6798 Training Loss: 0.2172632773824834 Test Loss: 0.21368993471746575\n",
      "Epoch: 6799 Training Loss: 0.21726478340393368 Test Loss: 0.21379768117998035\n",
      "Epoch: 6800 Training Loss: 0.21726415946487915 Test Loss: 0.21378235919746202\n",
      "Epoch: 6801 Training Loss: 0.21726474255574452 Test Loss: 0.21367090538553946\n",
      "Epoch: 6802 Training Loss: 0.21726500842760615 Test Loss: 0.2137048418916587\n",
      "Epoch: 6803 Training Loss: 0.21726588519328194 Test Loss: 0.21399567437878306\n",
      "Epoch: 6804 Training Loss: 0.21726265174889248 Test Loss: 0.21370613816767547\n",
      "Epoch: 6805 Training Loss: 0.21726245122875404 Test Loss: 0.21369972160139242\n",
      "Epoch: 6806 Training Loss: 0.2172661325866278 Test Loss: 0.21369922901650606\n",
      "Epoch: 6807 Training Loss: 0.21726475257951 Test Loss: 0.21374319869899516\n",
      "Epoch: 6808 Training Loss: 0.21726484148439232 Test Loss: 0.2137017049036981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6809 Training Loss: 0.21726522989185565 Test Loss: 0.21396744148713762\n",
      "Epoch: 6810 Training Loss: 0.2172648422554512 Test Loss: 0.21375472259278433\n",
      "Epoch: 6811 Training Loss: 0.21726364403891363 Test Loss: 0.21374907082935118\n",
      "Epoch: 6812 Training Loss: 0.21726542984714864 Test Loss: 0.21374525977786185\n",
      "Epoch: 6813 Training Loss: 0.21726348986300034 Test Loss: 0.21370848442726584\n",
      "Epoch: 6814 Training Loss: 0.21726478616540038 Test Loss: 0.21377484079656472\n",
      "Epoch: 6815 Training Loss: 0.21726392571748235 Test Loss: 0.21370692889604573\n",
      "Epoch: 6816 Training Loss: 0.21726407307042112 Test Loss: 0.22052556111122174\n",
      "Epoch: 6817 Training Loss: 0.21726544233650938 Test Loss: 0.2145907946980859\n",
      "Epoch: 6818 Training Loss: 0.2172655409154909 Test Loss: 0.2140161296143278\n",
      "Epoch: 6819 Training Loss: 0.21726415004182234 Test Loss: 0.21384369897857597\n",
      "Epoch: 6820 Training Loss: 0.2172641905224137 Test Loss: 0.21379084980537194\n",
      "Epoch: 6821 Training Loss: 0.21726423129887645 Test Loss: 0.21378559988750398\n",
      "Epoch: 6822 Training Loss: 0.21726360796949637 Test Loss: 0.21381216058308777\n",
      "Epoch: 6823 Training Loss: 0.21726267410960007 Test Loss: 0.21374868194654614\n",
      "Epoch: 6824 Training Loss: 0.21726319946967307 Test Loss: 0.21379389605401136\n",
      "Epoch: 6825 Training Loss: 0.21726430530259755 Test Loss: 0.21380447366630828\n",
      "Epoch: 6826 Training Loss: 0.21726491195558784 Test Loss: 0.21372107126738876\n",
      "Epoch: 6827 Training Loss: 0.21726490549124536 Test Loss: 0.2138174623519964\n",
      "Epoch: 6828 Training Loss: 0.21726265425035096 Test Loss: 0.213673186831329\n",
      "Epoch: 6829 Training Loss: 0.21726464167255213 Test Loss: 0.213708497390026\n",
      "Epoch: 6830 Training Loss: 0.2172652753036375 Test Loss: 0.21370603446559414\n",
      "Epoch: 6831 Training Loss: 0.21726500297639917 Test Loss: 0.21370784925201763\n",
      "Epoch: 6832 Training Loss: 0.21726647813756256 Test Loss: 0.2136981271818918\n",
      "Epoch: 6833 Training Loss: 0.21726570000466314 Test Loss: 0.21377717409339492\n",
      "Epoch: 6834 Training Loss: 0.21726457252829515 Test Loss: 0.2138734614759212\n",
      "Epoch: 6835 Training Loss: 0.21726621213121391 Test Loss: 0.21380914025996867\n",
      "Epoch: 6836 Training Loss: 0.2172631287026061 Test Loss: 0.21460273340020045\n",
      "Epoch: 6837 Training Loss: 0.2172641102605635 Test Loss: 0.2137327636770601\n",
      "Epoch: 6838 Training Loss: 0.21726486363888647 Test Loss: 0.21367603863856588\n",
      "Epoch: 6839 Training Loss: 0.21726596290884465 Test Loss: 0.21376566316236592\n",
      "Epoch: 6840 Training Loss: 0.21726399578521685 Test Loss: 0.21375438556101997\n",
      "Epoch: 6841 Training Loss: 0.21726603843675193 Test Loss: 0.21374977081840024\n",
      "Epoch: 6842 Training Loss: 0.21726379740790483 Test Loss: 0.21368305149181666\n",
      "Epoch: 6843 Training Loss: 0.21726612259872555 Test Loss: 0.21378191846361633\n",
      "Epoch: 6844 Training Loss: 0.21726372181723694 Test Loss: 0.21379913300911915\n",
      "Epoch: 6845 Training Loss: 0.217263806409569 Test Loss: 0.2137011863932914\n",
      "Epoch: 6846 Training Loss: 0.21726462058498827 Test Loss: 0.21378986463559918\n",
      "Epoch: 6847 Training Loss: 0.21726450543720657 Test Loss: 0.21385895614729347\n",
      "Epoch: 6848 Training Loss: 0.21726185077113222 Test Loss: 0.21378425176044652\n",
      "Epoch: 6849 Training Loss: 0.21726389376336774 Test Loss: 0.21367951265829085\n",
      "Epoch: 6850 Training Loss: 0.21726446879397804 Test Loss: 0.21381056616358712\n",
      "Epoch: 6851 Training Loss: 0.21726341299918872 Test Loss: 0.21376708906598438\n",
      "Epoch: 6852 Training Loss: 0.21726383487598705 Test Loss: 0.21373999689723372\n",
      "Epoch: 6853 Training Loss: 0.21726472759182272 Test Loss: 0.2137735704460683\n",
      "Epoch: 6854 Training Loss: 0.2172642132238217 Test Loss: 0.21380219222051874\n",
      "Epoch: 6855 Training Loss: 0.21726665652113844 Test Loss: 0.21377035568154668\n",
      "Epoch: 6856 Training Loss: 0.21726492147726847 Test Loss: 0.21373194702316953\n",
      "Epoch: 6857 Training Loss: 0.2172652910296524 Test Loss: 0.2138018033377137\n",
      "Epoch: 6858 Training Loss: 0.21726516564292597 Test Loss: 0.2137400487482744\n",
      "Epoch: 6859 Training Loss: 0.21726379864518536 Test Loss: 0.21991480770315652\n",
      "Epoch: 6860 Training Loss: 0.21726505789193004 Test Loss: 0.21406523254984336\n",
      "Epoch: 6861 Training Loss: 0.2172652220288482 Test Loss: 0.21383019178248114\n",
      "Epoch: 6862 Training Loss: 0.21726433020062683 Test Loss: 0.21370477707785787\n",
      "Epoch: 6863 Training Loss: 0.21726460006226989 Test Loss: 0.21373320441090582\n",
      "Epoch: 6864 Training Loss: 0.2172646319625897 Test Loss: 0.21374453386329245\n",
      "Epoch: 6865 Training Loss: 0.21726347045204125 Test Loss: 0.2141668476267986\n",
      "Epoch: 6866 Training Loss: 0.21726413956080104 Test Loss: 0.21376262987648667\n",
      "Epoch: 6867 Training Loss: 0.21726341041703803 Test Loss: 0.21375721144273654\n",
      "Epoch: 6868 Training Loss: 0.21726210100663698 Test Loss: 0.21365159087288946\n",
      "Epoch: 6869 Training Loss: 0.21726651099722308 Test Loss: 0.2136460168860173\n",
      "Epoch: 6870 Training Loss: 0.21726480718123783 Test Loss: 0.21377144455340077\n",
      "Epoch: 6871 Training Loss: 0.21726517356869401 Test Loss: 0.2137400487482744\n",
      "Epoch: 6872 Training Loss: 0.217263166905884 Test Loss: 0.21378456286669056\n",
      "Epoch: 6873 Training Loss: 0.21726479800025766 Test Loss: 0.21368048486530344\n",
      "Epoch: 6874 Training Loss: 0.21726513156391652 Test Loss: 0.21362877641499417\n",
      "Epoch: 6875 Training Loss: 0.21726635013282228 Test Loss: 0.21371330657404827\n",
      "Epoch: 6876 Training Loss: 0.21726528951443203 Test Loss: 0.21373114333203913\n",
      "Epoch: 6877 Training Loss: 0.2172654612633152 Test Loss: 0.21366260921903207\n",
      "Epoch: 6878 Training Loss: 0.21726384451422306 Test Loss: 0.2137425505609868\n",
      "Epoch: 6879 Training Loss: 0.21726545329271815 Test Loss: 0.21380230888536025\n",
      "Epoch: 6880 Training Loss: 0.21726389838075524 Test Loss: 0.21393576050128757\n",
      "Epoch: 6881 Training Loss: 0.21726196118497101 Test Loss: 0.2137515596793034\n",
      "Epoch: 6882 Training Loss: 0.21726507523178912 Test Loss: 0.21419431571559414\n",
      "Epoch: 6883 Training Loss: 0.21726547959837816 Test Loss: 0.2137242341808697\n",
      "Epoch: 6884 Training Loss: 0.21726417161353945 Test Loss: 0.2136931754075077\n",
      "Epoch: 6885 Training Loss: 0.2172656183082848 Test Loss: 0.2137469578994438\n",
      "Epoch: 6886 Training Loss: 0.21726428899380562 Test Loss: 0.21372852485448524\n",
      "Epoch: 6887 Training Loss: 0.21726542800019363 Test Loss: 0.21373849321705427\n",
      "Epoch: 6888 Training Loss: 0.21726644308128082 Test Loss: 0.21372943224769697\n",
      "Epoch: 6889 Training Loss: 0.21726514005453004 Test Loss: 0.21380488847463364\n",
      "Epoch: 6890 Training Loss: 0.2172648890300348 Test Loss: 0.21378303326099077\n",
      "Epoch: 6891 Training Loss: 0.21726447457691966 Test Loss: 0.2137382858128916\n",
      "Epoch: 6892 Training Loss: 0.2172642888682844 Test Loss: 0.21364842795940853\n",
      "Epoch: 6893 Training Loss: 0.21726416022697223 Test Loss: 0.2137734797067471\n",
      "Epoch: 6894 Training Loss: 0.21726489599646215 Test Loss: 0.21370607335387465\n",
      "Epoch: 6895 Training Loss: 0.21726600481499833 Test Loss: 0.21372038424109988\n",
      "Epoch: 6896 Training Loss: 0.2172637204275378 Test Loss: 0.21368430887955295\n",
      "Epoch: 6897 Training Loss: 0.21726729699312994 Test Loss: 0.21372467491471542\n",
      "Epoch: 6898 Training Loss: 0.2172646843318331 Test Loss: 0.21429005866219336\n",
      "Epoch: 6899 Training Loss: 0.21726465088042973 Test Loss: 0.21377880740117605\n",
      "Epoch: 6900 Training Loss: 0.21726549842656018 Test Loss: 0.213776253737423\n",
      "Epoch: 6901 Training Loss: 0.21726521642522262 Test Loss: 0.21371915277888393\n",
      "Epoch: 6902 Training Loss: 0.21726489517160844 Test Loss: 0.21368004413145775\n",
      "Epoch: 6903 Training Loss: 0.21726767656927934 Test Loss: 0.21364998349062864\n",
      "Epoch: 6904 Training Loss: 0.21726281706033057 Test Loss: 0.21369588462438277\n",
      "Epoch: 6905 Training Loss: 0.21726631813387864 Test Loss: 0.21373247849633642\n",
      "Epoch: 6906 Training Loss: 0.2172647609535681 Test Loss: 0.21371206214907215\n",
      "Epoch: 6907 Training Loss: 0.2172661203303779 Test Loss: 0.2137636280090196\n",
      "Epoch: 6908 Training Loss: 0.21726466138834843 Test Loss: 0.21379900338151747\n",
      "Epoch: 6909 Training Loss: 0.21726224507809258 Test Loss: 0.21368902732425402\n",
      "Epoch: 6910 Training Loss: 0.2172650032453732 Test Loss: 0.21367774972290804\n",
      "Epoch: 6911 Training Loss: 0.21726552357563186 Test Loss: 0.21373092296511628\n",
      "Epoch: 6912 Training Loss: 0.21726494967471247 Test Loss: 0.21374222649198257\n",
      "Epoch: 6913 Training Loss: 0.21726449891906927 Test Loss: 0.21379888671667596\n",
      "Epoch: 6914 Training Loss: 0.21726317180121132 Test Loss: 0.2137451560757805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6915 Training Loss: 0.21726497285130797 Test Loss: 0.21382267338158384\n",
      "Epoch: 6916 Training Loss: 0.21726547427269238 Test Loss: 0.21380411070902358\n",
      "Epoch: 6917 Training Loss: 0.2172655652038457 Test Loss: 0.21405147906130534\n",
      "Epoch: 6918 Training Loss: 0.21726588578502481 Test Loss: 0.21377603337050016\n",
      "Epoch: 6919 Training Loss: 0.2172641983316263 Test Loss: 0.2136767256648548\n",
      "Epoch: 6920 Training Loss: 0.21726496232545764 Test Loss: 0.21388878345843954\n",
      "Epoch: 6921 Training Loss: 0.21726259707543824 Test Loss: 0.2137671668425454\n",
      "Epoch: 6922 Training Loss: 0.217263963660752 Test Loss: 0.2137394394985465\n",
      "Epoch: 6923 Training Loss: 0.21726573278363145 Test Loss: 0.2138079606487934\n",
      "Epoch: 6924 Training Loss: 0.21726415927659734 Test Loss: 0.2136828311248938\n",
      "Epoch: 6925 Training Loss: 0.21726653889879566 Test Loss: 0.21376347245589758\n",
      "Epoch: 6926 Training Loss: 0.21726493072100925 Test Loss: 0.21373416365515824\n",
      "Epoch: 6927 Training Loss: 0.2172664587893641 Test Loss: 0.2137538411250929\n",
      "Epoch: 6928 Training Loss: 0.2172648330206762 Test Loss: 0.21373813025976957\n",
      "Epoch: 6929 Training Loss: 0.21726582988325582 Test Loss: 0.21410580598916854\n",
      "Epoch: 6930 Training Loss: 0.21726515261561719 Test Loss: 0.21377117233543724\n",
      "Epoch: 6931 Training Loss: 0.21726453742718438 Test Loss: 0.21374418386876792\n",
      "Epoch: 6932 Training Loss: 0.21726429958241655 Test Loss: 0.2139600786393623\n",
      "Epoch: 6933 Training Loss: 0.21726572081428716 Test Loss: 0.21375443741206063\n",
      "Epoch: 6934 Training Loss: 0.2172632662469586 Test Loss: 0.21375753551174073\n",
      "Epoch: 6935 Training Loss: 0.21726440426710855 Test Loss: 0.21385460065987708\n",
      "Epoch: 6936 Training Loss: 0.21726339836700154 Test Loss: 0.21381892714389536\n",
      "Epoch: 6937 Training Loss: 0.21726354023287012 Test Loss: 0.2137839406542025\n",
      "Epoch: 6938 Training Loss: 0.21726440796998434 Test Loss: 0.21379467381962142\n",
      "Epoch: 6939 Training Loss: 0.21726456353559678 Test Loss: 0.21373278960258044\n",
      "Epoch: 6940 Training Loss: 0.21726276385726767 Test Loss: 0.2137549818479877\n",
      "Epoch: 6941 Training Loss: 0.21726449483962984 Test Loss: 0.21378562581302432\n",
      "Epoch: 6942 Training Loss: 0.2172638617285609 Test Loss: 0.21372508972304077\n",
      "Epoch: 6943 Training Loss: 0.21726303819284543 Test Loss: 0.21379562010111366\n",
      "Epoch: 6944 Training Loss: 0.21726475759139274 Test Loss: 0.21375187078554742\n",
      "Epoch: 6945 Training Loss: 0.2172641663775117 Test Loss: 0.21369745311836308\n",
      "Epoch: 6946 Training Loss: 0.2172638765579957 Test Loss: 0.21379788858414303\n",
      "Epoch: 6947 Training Loss: 0.21726401273954646 Test Loss: 0.21372577674932966\n",
      "Epoch: 6948 Training Loss: 0.21726444450562324 Test Loss: 0.213741409838092\n",
      "Epoch: 6949 Training Loss: 0.21726202766638514 Test Loss: 0.2173391979971395\n",
      "Epoch: 6950 Training Loss: 0.21726616815396022 Test Loss: 0.2139212551726598\n",
      "Epoch: 6951 Training Loss: 0.21726412374512813 Test Loss: 0.21381619200149996\n",
      "Epoch: 6952 Training Loss: 0.21726489296602142 Test Loss: 0.21378180179877482\n",
      "Epoch: 6953 Training Loss: 0.21726487845935546 Test Loss: 0.21379769414274052\n",
      "Epoch: 6954 Training Loss: 0.21726558978807195 Test Loss: 0.213805147729837\n",
      "Epoch: 6955 Training Loss: 0.21726257605960078 Test Loss: 0.2137036752432436\n",
      "Epoch: 6956 Training Loss: 0.21726548608065224 Test Loss: 0.21376155396739274\n",
      "Epoch: 6957 Training Loss: 0.21726617503072954 Test Loss: 0.2137439116508044\n",
      "Epoch: 6958 Training Loss: 0.2172630800452043 Test Loss: 0.21376415948218647\n",
      "Epoch: 6959 Training Loss: 0.21726359127517497 Test Loss: 0.2138670189841178\n",
      "Epoch: 6960 Training Loss: 0.21726519679011852 Test Loss: 0.21378351288311698\n",
      "Epoch: 6961 Training Loss: 0.21726388655486376 Test Loss: 0.21381809752724462\n",
      "Epoch: 6962 Training Loss: 0.21726551611608547 Test Loss: 0.21378324066515345\n",
      "Epoch: 6963 Training Loss: 0.21726329895420052 Test Loss: 0.21377854814597272\n",
      "Epoch: 6964 Training Loss: 0.21726355457815166 Test Loss: 0.21372235458064537\n",
      "Epoch: 6965 Training Loss: 0.21726540023310806 Test Loss: 0.2137186083429569\n",
      "Epoch: 6966 Training Loss: 0.21726271438397798 Test Loss: 0.2137137343451338\n",
      "Epoch: 6967 Training Loss: 0.2172617619290105 Test Loss: 0.2137204490549007\n",
      "Epoch: 6968 Training Loss: 0.21726509599658414 Test Loss: 0.21371736391798077\n",
      "Epoch: 6969 Training Loss: 0.217264872353645 Test Loss: 0.21374660790491928\n",
      "Epoch: 6970 Training Loss: 0.21726468185727202 Test Loss: 0.2139747654466324\n",
      "Epoch: 6971 Training Loss: 0.21726525708512995 Test Loss: 0.21369556055537858\n",
      "Epoch: 6972 Training Loss: 0.21726485504964915 Test Loss: 0.21364652243366386\n",
      "Epoch: 6973 Training Loss: 0.2172665139469716 Test Loss: 0.2137537633485319\n",
      "Epoch: 6974 Training Loss: 0.21726423963707134 Test Loss: 0.21378010367719286\n",
      "Epoch: 6975 Training Loss: 0.2172654145335606 Test Loss: 0.21378103699592493\n",
      "Epoch: 6976 Training Loss: 0.21726515314459943 Test Loss: 0.21384741929074413\n",
      "Epoch: 6977 Training Loss: 0.21726459982019325 Test Loss: 0.21370314377007674\n",
      "Epoch: 6978 Training Loss: 0.21726478801235538 Test Loss: 0.2137583910539118\n",
      "Epoch: 6979 Training Loss: 0.21726440631131116 Test Loss: 0.21373183035832802\n",
      "Epoch: 6980 Training Loss: 0.2172632277119461 Test Loss: 0.21374191538573856\n",
      "Epoch: 6981 Training Loss: 0.217263647625234 Test Loss: 0.21381671051190665\n",
      "Epoch: 6982 Training Loss: 0.21726407790298782 Test Loss: 0.21376449651395082\n",
      "Epoch: 6983 Training Loss: 0.21726476706824435 Test Loss: 0.21382608258750796\n",
      "Epoch: 6984 Training Loss: 0.2172639626386507 Test Loss: 0.2137209286770269\n",
      "Epoch: 6985 Training Loss: 0.21726311103997822 Test Loss: 0.21373973764203036\n",
      "Epoch: 6986 Training Loss: 0.21726547786797856 Test Loss: 0.21362278761979664\n",
      "Epoch: 6987 Training Loss: 0.21726412559208313 Test Loss: 0.21366979058816502\n",
      "Epoch: 6988 Training Loss: 0.2172636523322795 Test Loss: 0.21372983409326218\n",
      "Epoch: 6989 Training Loss: 0.2172665775055346 Test Loss: 0.21378016849099368\n",
      "Epoch: 6990 Training Loss: 0.21726465178597562 Test Loss: 0.21374798195749709\n",
      "Epoch: 6991 Training Loss: 0.21726414539753744 Test Loss: 0.21368277927385315\n",
      "Epoch: 6992 Training Loss: 0.2172652592369222 Test Loss: 0.21385593582417436\n",
      "Epoch: 6993 Training Loss: 0.21726423859703842 Test Loss: 0.21373858395637543\n",
      "Epoch: 6994 Training Loss: 0.21726467550948494 Test Loss: 0.2137396080144287\n",
      "Epoch: 6995 Training Loss: 0.21726570950841215 Test Loss: 0.21373459142624376\n",
      "Epoch: 6996 Training Loss: 0.21726554183000263 Test Loss: 0.2136569963438794\n",
      "Epoch: 6997 Training Loss: 0.21726530154653692 Test Loss: 0.21365130569216575\n",
      "Epoch: 6998 Training Loss: 0.21726517680534818 Test Loss: 0.21369693460795636\n",
      "Epoch: 6999 Training Loss: 0.21726571475340573 Test Loss: 0.2137962941646424\n",
      "Epoch: 7000 Training Loss: 0.21726599738234934 Test Loss: 0.21371484914250824\n",
      "Epoch: 7001 Training Loss: 0.21726715488518475 Test Loss: 0.21373320441090582\n",
      "Epoch: 7002 Training Loss: 0.21726582663763588 Test Loss: 0.21373916728058298\n",
      "Epoch: 7003 Training Loss: 0.21726512901762907 Test Loss: 0.2137729223080599\n",
      "Epoch: 7004 Training Loss: 0.21726560735207603 Test Loss: 0.21376299283377137\n",
      "Epoch: 7005 Training Loss: 0.21726412142298568 Test Loss: 0.21371163437798663\n",
      "Epoch: 7006 Training Loss: 0.2172662221998084 Test Loss: 0.21379346828292584\n",
      "Epoch: 7007 Training Loss: 0.21726575599609013 Test Loss: 0.21374916156867235\n",
      "Epoch: 7008 Training Loss: 0.21726373958745446 Test Loss: 0.21381146059403872\n",
      "Epoch: 7009 Training Loss: 0.21726486499272243 Test Loss: 0.21373317848538548\n",
      "Epoch: 7010 Training Loss: 0.2172652418163709 Test Loss: 0.2137126454732797\n",
      "Epoch: 7011 Training Loss: 0.21726409112754425 Test Loss: 0.2136743794052644\n",
      "Epoch: 7012 Training Loss: 0.21726515631849297 Test Loss: 0.21372152496399463\n",
      "Epoch: 7013 Training Loss: 0.21726624335909867 Test Loss: 0.21385046553938356\n",
      "Epoch: 7014 Training Loss: 0.21726578575358352 Test Loss: 0.21368872918077014\n",
      "Epoch: 7015 Training Loss: 0.21726486644518217 Test Loss: 0.21374637457523626\n",
      "Epoch: 7016 Training Loss: 0.21726463624824255 Test Loss: 0.2137388820998593\n",
      "Epoch: 7017 Training Loss: 0.2172637736754297 Test Loss: 0.2137228730910521\n",
      "Epoch: 7018 Training Loss: 0.21726483419519613 Test Loss: 0.21377700557751275\n",
      "Epoch: 7019 Training Loss: 0.21726387817183987 Test Loss: 0.2137423042685436\n",
      "Epoch: 7020 Training Loss: 0.21726548994491246 Test Loss: 0.21372963965185968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7021 Training Loss: 0.21726447153751313 Test Loss: 0.21379856264767177\n",
      "Epoch: 7022 Training Loss: 0.21726571985494647 Test Loss: 0.21375731514481788\n",
      "Epoch: 7023 Training Loss: 0.21726400701039966 Test Loss: 0.2138004033596156\n",
      "Epoch: 7024 Training Loss: 0.2172646263948273 Test Loss: 0.21382223264773814\n",
      "Epoch: 7025 Training Loss: 0.21726508289754892 Test Loss: 0.21375778180418392\n",
      "Epoch: 7026 Training Loss: 0.21726548986422026 Test Loss: 0.21366347772396332\n",
      "Epoch: 7027 Training Loss: 0.2172673133646825 Test Loss: 0.21381912158529787\n",
      "Epoch: 7028 Training Loss: 0.2172649957409978 Test Loss: 0.2136901550843886\n",
      "Epoch: 7029 Training Loss: 0.21726488400918625 Test Loss: 0.21373084518855529\n",
      "Epoch: 7030 Training Loss: 0.21726345200938868 Test Loss: 0.21383686760396756\n",
      "Epoch: 7031 Training Loss: 0.21726477255531454 Test Loss: 0.21372304160693426\n",
      "Epoch: 7032 Training Loss: 0.21726463615858455 Test Loss: 0.21367523494743548\n",
      "Epoch: 7033 Training Loss: 0.21726553011170077 Test Loss: 0.21368984397814458\n",
      "Epoch: 7034 Training Loss: 0.21726459338274817 Test Loss: 0.21370865294314803\n",
      "Epoch: 7035 Training Loss: 0.2172644843406769 Test Loss: 0.21377567041321546\n",
      "Epoch: 7036 Training Loss: 0.21726573694376308 Test Loss: 0.21374691901116333\n",
      "Epoch: 7037 Training Loss: 0.21726397426729455 Test Loss: 0.21363451891774848\n",
      "Epoch: 7038 Training Loss: 0.2172664440406215 Test Loss: 0.21374301722035283\n",
      "Epoch: 7039 Training Loss: 0.21726357530708348 Test Loss: 0.2137463227241956\n",
      "Epoch: 7040 Training Loss: 0.21726521484724165 Test Loss: 0.21390988683199266\n",
      "Epoch: 7041 Training Loss: 0.2172636673141329 Test Loss: 0.2141847103103098\n",
      "Epoch: 7042 Training Loss: 0.21726318187877158 Test Loss: 0.21397361176097748\n",
      "Epoch: 7043 Training Loss: 0.21726486175606827 Test Loss: 0.21384660263685357\n",
      "Epoch: 7044 Training Loss: 0.21726385767601888 Test Loss: 0.21383340654700275\n",
      "Epoch: 7045 Training Loss: 0.21726387706904635 Test Loss: 0.21377322045154376\n",
      "Epoch: 7046 Training Loss: 0.21726548714758256 Test Loss: 0.21376802238471648\n",
      "Epoch: 7047 Training Loss: 0.2172652836059692 Test Loss: 0.21378528878125996\n",
      "Epoch: 7048 Training Loss: 0.21726538308153084 Test Loss: 0.2137721186169295\n",
      "Epoch: 7049 Training Loss: 0.21726381899755354 Test Loss: 0.21374007467379474\n",
      "Epoch: 7050 Training Loss: 0.21726042462600267 Test Loss: 0.2136906347065148\n",
      "Epoch: 7051 Training Loss: 0.21726335187932358 Test Loss: 0.21393796417051608\n",
      "Epoch: 7052 Training Loss: 0.21726426908972749 Test Loss: 0.21381060505186764\n",
      "Epoch: 7053 Training Loss: 0.21726432866747486 Test Loss: 0.21382158450972974\n",
      "Epoch: 7054 Training Loss: 0.21726475670377843 Test Loss: 0.21391839040266275\n",
      "Epoch: 7055 Training Loss: 0.21726324266690208 Test Loss: 0.21381223835964877\n",
      "Epoch: 7056 Training Loss: 0.21726558806663815 Test Loss: 0.21378902205618827\n",
      "Epoch: 7057 Training Loss: 0.21726411393654188 Test Loss: 0.21371770094974515\n",
      "Epoch: 7058 Training Loss: 0.21726335374421019 Test Loss: 0.2137736482226293\n",
      "Epoch: 7059 Training Loss: 0.21726458543008273 Test Loss: 0.21375314113604385\n",
      "Epoch: 7060 Training Loss: 0.21726452985108258 Test Loss: 0.21373434513380057\n",
      "Epoch: 7061 Training Loss: 0.21726621215811132 Test Loss: 0.21368721253783052\n",
      "Epoch: 7062 Training Loss: 0.21726502652059249 Test Loss: 0.2136793441424087\n",
      "Epoch: 7063 Training Loss: 0.2172644151426251 Test Loss: 0.2137676853529521\n",
      "Epoch: 7064 Training Loss: 0.2172657799168471 Test Loss: 0.21362237281147128\n",
      "Epoch: 7065 Training Loss: 0.21726711639500124 Test Loss: 0.2137927164428361\n",
      "Epoch: 7066 Training Loss: 0.21726624352048307 Test Loss: 0.21375141708894155\n",
      "Epoch: 7067 Training Loss: 0.21726300281379485 Test Loss: 0.21411577435173754\n",
      "Epoch: 7068 Training Loss: 0.21726554524597277 Test Loss: 0.2139563972154747\n",
      "Epoch: 7069 Training Loss: 0.21726475881074167 Test Loss: 0.21387513367198285\n",
      "Epoch: 7070 Training Loss: 0.2172631148504436 Test Loss: 0.21376211136607995\n",
      "Epoch: 7071 Training Loss: 0.21726468329180018 Test Loss: 0.21375881882499734\n",
      "Epoch: 7072 Training Loss: 0.21726590688155445 Test Loss: 0.21373818211081025\n",
      "Epoch: 7073 Training Loss: 0.21726418386082358 Test Loss: 0.2139131793730753\n",
      "Epoch: 7074 Training Loss: 0.21726633121498226 Test Loss: 0.21380066261481895\n",
      "Epoch: 7075 Training Loss: 0.21726520070817354 Test Loss: 0.2137497448928799\n",
      "Epoch: 7076 Training Loss: 0.21726438573479795 Test Loss: 0.21374650420283794\n",
      "Epoch: 7077 Training Loss: 0.21726418829889504 Test Loss: 0.21369066063203515\n",
      "Epoch: 7078 Training Loss: 0.21726246829963908 Test Loss: 0.213719930544494\n",
      "Epoch: 7079 Training Loss: 0.21726144243269335 Test Loss: 0.21382316596647022\n",
      "Epoch: 7080 Training Loss: 0.21726582155402674 Test Loss: 0.21384189715491267\n",
      "Epoch: 7081 Training Loss: 0.21726457383730208 Test Loss: 0.2137354210428945\n",
      "Epoch: 7082 Training Loss: 0.21726676434386077 Test Loss: 0.21381658088430497\n",
      "Epoch: 7083 Training Loss: 0.21726425530032561 Test Loss: 0.21380831064331793\n",
      "Epoch: 7084 Training Loss: 0.21726189606635865 Test Loss: 0.21378256660162473\n",
      "Epoch: 7085 Training Loss: 0.2172635297249514 Test Loss: 0.21377837963009053\n",
      "Epoch: 7086 Training Loss: 0.21726444744640597 Test Loss: 0.21372826559928187\n",
      "Epoch: 7087 Training Loss: 0.21726684240012392 Test Loss: 0.21379105720953462\n",
      "Epoch: 7088 Training Loss: 0.2172644993852909 Test Loss: 0.2137538411250929\n",
      "Epoch: 7089 Training Loss: 0.21726436705903457 Test Loss: 0.21373701546239512\n",
      "Epoch: 7090 Training Loss: 0.21726592984297072 Test Loss: 0.21390056660743204\n",
      "Epoch: 7091 Training Loss: 0.2172646366158404 Test Loss: 0.21381559571453224\n",
      "Epoch: 7092 Training Loss: 0.21726664706221843 Test Loss: 0.21379477752170278\n",
      "Epoch: 7093 Training Loss: 0.21726619015603577 Test Loss: 0.21376081509006317\n",
      "Epoch: 7094 Training Loss: 0.21726486615827653 Test Loss: 0.21385090627322928\n",
      "Epoch: 7095 Training Loss: 0.21726314606039676 Test Loss: 0.2138521377354452\n",
      "Epoch: 7096 Training Loss: 0.21726549049182634 Test Loss: 0.2137615669301529\n",
      "Epoch: 7097 Training Loss: 0.21726458681081606 Test Loss: 0.21372324901109696\n",
      "Epoch: 7098 Training Loss: 0.21726509002536068 Test Loss: 0.21367939599344934\n",
      "Epoch: 7099 Training Loss: 0.2172642095926723 Test Loss: 0.21373258219841776\n",
      "Epoch: 7100 Training Loss: 0.2172639251526369 Test Loss: 0.21372034535281936\n",
      "Epoch: 7101 Training Loss: 0.21726471576593126 Test Loss: 0.21371728614141977\n",
      "Epoch: 7102 Training Loss: 0.21726351070848757 Test Loss: 0.21373282849086095\n",
      "Epoch: 7103 Training Loss: 0.2172656866635513 Test Loss: 0.2137397635675507\n",
      "Epoch: 7104 Training Loss: 0.21726542116825331 Test Loss: 0.21374883749966817\n",
      "Epoch: 7105 Training Loss: 0.21726422841188853 Test Loss: 0.21380219222051874\n",
      "Epoch: 7106 Training Loss: 0.21726507183375054 Test Loss: 0.21386615047918658\n",
      "Epoch: 7107 Training Loss: 0.2172649761417569 Test Loss: 0.21380284035852712\n",
      "Epoch: 7108 Training Loss: 0.21726549566509348 Test Loss: 0.21412917784575103\n",
      "Epoch: 7109 Training Loss: 0.21726165848159903 Test Loss: 0.2143421948835882\n",
      "Epoch: 7110 Training Loss: 0.21726473935495358 Test Loss: 0.2139684914707112\n",
      "Epoch: 7111 Training Loss: 0.21726421463145246 Test Loss: 0.21383522133342625\n",
      "Epoch: 7112 Training Loss: 0.2172641543454068 Test Loss: 0.21382054748891632\n",
      "Epoch: 7113 Training Loss: 0.21726479498774853 Test Loss: 0.2149540630890278\n",
      "Epoch: 7114 Training Loss: 0.217265276271944 Test Loss: 0.21379066832672958\n",
      "Epoch: 7115 Training Loss: 0.21726551342634517 Test Loss: 0.21378785540777318\n",
      "Epoch: 7116 Training Loss: 0.2172655772718138 Test Loss: 0.21374061910972178\n",
      "Epoch: 7117 Training Loss: 0.21726541228314455 Test Loss: 0.2136753905005575\n",
      "Epoch: 7118 Training Loss: 0.21726463709999363 Test Loss: 0.21370018826075846\n",
      "Epoch: 7119 Training Loss: 0.21726674469082508 Test Loss: 0.21369184024321042\n",
      "Epoch: 7120 Training Loss: 0.21726603655393373 Test Loss: 0.21375998547341243\n",
      "Epoch: 7121 Training Loss: 0.21726426250882958 Test Loss: 0.21380995691385923\n",
      "Epoch: 7122 Training Loss: 0.21726526743166427 Test Loss: 0.2137183620505137\n",
      "Epoch: 7123 Training Loss: 0.21726392652440443 Test Loss: 0.2137303526036689\n",
      "Epoch: 7124 Training Loss: 0.2172657045323926 Test Loss: 0.21376577982720743\n",
      "Epoch: 7125 Training Loss: 0.2172645983408361 Test Loss: 0.21377193713828713\n",
      "Epoch: 7126 Training Loss: 0.21726476448609366 Test Loss: 0.21372349530354015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7127 Training Loss: 0.21726383890163167 Test Loss: 0.21375977806924976\n",
      "Epoch: 7128 Training Loss: 0.2172647040028004 Test Loss: 0.21376397800354413\n",
      "Epoch: 7129 Training Loss: 0.21726588798164603 Test Loss: 0.213911766432217\n",
      "Epoch: 7130 Training Loss: 0.21726450185985197 Test Loss: 0.21378557396198364\n",
      "Epoch: 7131 Training Loss: 0.2172638206920899 Test Loss: 0.2137618650736368\n",
      "Epoch: 7132 Training Loss: 0.2172662028067809 Test Loss: 0.21381686606502867\n",
      "Epoch: 7133 Training Loss: 0.21726391676064719 Test Loss: 0.21377857407149303\n",
      "Epoch: 7134 Training Loss: 0.21726465496883496 Test Loss: 0.21373409884135738\n",
      "Epoch: 7135 Training Loss: 0.2172634801440721 Test Loss: 0.21370577521039077\n",
      "Epoch: 7136 Training Loss: 0.21726488594579926 Test Loss: 0.21367187759255205\n",
      "Epoch: 7137 Training Loss: 0.21726489222185993 Test Loss: 0.21369794570324943\n",
      "Epoch: 7138 Training Loss: 0.21726384456801787 Test Loss: 0.21361099150804397\n",
      "Epoch: 7139 Training Loss: 0.21726517669775855 Test Loss: 0.21367465162322793\n",
      "Epoch: 7140 Training Loss: 0.21726724864056537 Test Loss: 0.21369909938890438\n",
      "Epoch: 7141 Training Loss: 0.21726391427712033 Test Loss: 0.21365726856184294\n",
      "Epoch: 7142 Training Loss: 0.21726719809137957 Test Loss: 0.21381620496426013\n",
      "Epoch: 7143 Training Loss: 0.2172650911998806 Test Loss: 0.2142814773149623\n",
      "Epoch: 7144 Training Loss: 0.2172652398618263 Test Loss: 0.21389910181553307\n",
      "Epoch: 7145 Training Loss: 0.21726443019620492 Test Loss: 0.21377000568702215\n",
      "Epoch: 7146 Training Loss: 0.21726522390270062 Test Loss: 0.21376667425765902\n",
      "Epoch: 7147 Training Loss: 0.21726541563635413 Test Loss: 0.21371339731336944\n",
      "Epoch: 7148 Training Loss: 0.21726556722115092 Test Loss: 0.2138015570452705\n",
      "Epoch: 7149 Training Loss: 0.21726447234443522 Test Loss: 0.21378378510108048\n",
      "Epoch: 7150 Training Loss: 0.2172650368760926 Test Loss: 0.21375953177680657\n",
      "Epoch: 7151 Training Loss: 0.21726375390583857 Test Loss: 0.2137139287865363\n",
      "Epoch: 7152 Training Loss: 0.21726492259799357 Test Loss: 0.21374061910972178\n",
      "Epoch: 7153 Training Loss: 0.2172659669255235 Test Loss: 0.2138440489731005\n",
      "Epoch: 7154 Training Loss: 0.2172645728779614 Test Loss: 0.2137994700408835\n",
      "Epoch: 7155 Training Loss: 0.21726332326945272 Test Loss: 0.21378807577469602\n",
      "Epoch: 7156 Training Loss: 0.2172642309043812 Test Loss: 0.2138061847506504\n",
      "Epoch: 7157 Training Loss: 0.21726330501508195 Test Loss: 0.213754618890703\n",
      "Epoch: 7158 Training Loss: 0.21726494494076956 Test Loss: 0.21375495592246735\n",
      "Epoch: 7159 Training Loss: 0.2172641830628673 Test Loss: 0.21373250442185676\n",
      "Epoch: 7160 Training Loss: 0.21726388993497073 Test Loss: 0.21373120814583996\n",
      "Epoch: 7161 Training Loss: 0.21726450062257144 Test Loss: 0.21390905721534192\n",
      "Epoch: 7162 Training Loss: 0.21726583372061864 Test Loss: 0.21374895416450967\n",
      "Epoch: 7163 Training Loss: 0.217263290373929 Test Loss: 0.21375604479432145\n",
      "Epoch: 7164 Training Loss: 0.21726364976806045 Test Loss: 0.21374916156867235\n",
      "Epoch: 7165 Training Loss: 0.21726448960360206 Test Loss: 0.2137715482554821\n",
      "Epoch: 7166 Training Loss: 0.21726322233246553 Test Loss: 0.21373183035832802\n",
      "Epoch: 7167 Training Loss: 0.21726628062993325 Test Loss: 0.21373689879755361\n",
      "Epoch: 7168 Training Loss: 0.21726531601733967 Test Loss: 0.21372611378109405\n",
      "Epoch: 7169 Training Loss: 0.2172653052673443 Test Loss: 0.21372506379752043\n",
      "Epoch: 7170 Training Loss: 0.21726360750327472 Test Loss: 0.21364050771294602\n",
      "Epoch: 7171 Training Loss: 0.2172626041673868 Test Loss: 0.21502559159963364\n",
      "Epoch: 7172 Training Loss: 0.21726699477391123 Test Loss: 0.21387154298741637\n",
      "Epoch: 7173 Training Loss: 0.21726462765003943 Test Loss: 0.21383851387450886\n",
      "Epoch: 7174 Training Loss: 0.21726558855079142 Test Loss: 0.21397079884202108\n",
      "Epoch: 7175 Training Loss: 0.21726427282846647 Test Loss: 0.21450181831229426\n",
      "Epoch: 7176 Training Loss: 0.2172633483557638 Test Loss: 0.21381913454805804\n",
      "Epoch: 7177 Training Loss: 0.2172633698916177 Test Loss: 0.21385018035865988\n",
      "Epoch: 7178 Training Loss: 0.21726434569353087 Test Loss: 0.21370217156306415\n",
      "Epoch: 7179 Training Loss: 0.21726321313355373 Test Loss: 0.21379611268600004\n",
      "Epoch: 7180 Training Loss: 0.21726322603534132 Test Loss: 0.2136621944107067\n",
      "Epoch: 7181 Training Loss: 0.2172645487868542 Test Loss: 0.21374791714369623\n",
      "Epoch: 7182 Training Loss: 0.21726460152369545 Test Loss: 0.2137261656321347\n",
      "Epoch: 7183 Training Loss: 0.21726492766367111 Test Loss: 0.21365203160673515\n",
      "Epoch: 7184 Training Loss: 0.21726614598153443 Test Loss: 0.2136411558509544\n",
      "Epoch: 7185 Training Loss: 0.21726352534067472 Test Loss: 0.21370840665070484\n",
      "Epoch: 7186 Training Loss: 0.2172657923703446 Test Loss: 0.2137470097504845\n",
      "Epoch: 7187 Training Loss: 0.21726509908978545 Test Loss: 0.21384289528744557\n",
      "Epoch: 7188 Training Loss: 0.2172656266733771 Test Loss: 0.21452994750185833\n",
      "Epoch: 7189 Training Loss: 0.21726436947083502 Test Loss: 0.21369641609754964\n",
      "Epoch: 7190 Training Loss: 0.21726616256826622 Test Loss: 0.2138071180693825\n",
      "Epoch: 7191 Training Loss: 0.2172641028458461 Test Loss: 0.21376929273521292\n",
      "Epoch: 7192 Training Loss: 0.21726439392954003 Test Loss: 0.213708108507221\n",
      "Epoch: 7193 Training Loss: 0.217262596636114 Test Loss: 0.21370227526514549\n",
      "Epoch: 7194 Training Loss: 0.21726614872506952 Test Loss: 0.21374111169460816\n",
      "Epoch: 7195 Training Loss: 0.2172635314822484 Test Loss: 0.21374183760917756\n",
      "Epoch: 7196 Training Loss: 0.21726585331985954 Test Loss: 0.21380322924133216\n",
      "Epoch: 7197 Training Loss: 0.21726391650960478 Test Loss: 0.2139422678068918\n",
      "Epoch: 7198 Training Loss: 0.21726352841594446 Test Loss: 0.21375176708346608\n",
      "Epoch: 7199 Training Loss: 0.21726524571649433 Test Loss: 0.2140534364380907\n",
      "Epoch: 7200 Training Loss: 0.2172614018265808 Test Loss: 0.2143551446809958\n",
      "Epoch: 7201 Training Loss: 0.21726426798693396 Test Loss: 0.21401711478410057\n",
      "Epoch: 7202 Training Loss: 0.2172639057147804 Test Loss: 0.21383667316256505\n",
      "Epoch: 7203 Training Loss: 0.21726626578256686 Test Loss: 0.21388562054495858\n",
      "Epoch: 7204 Training Loss: 0.21726527726714792 Test Loss: 0.21390623133362535\n",
      "Epoch: 7205 Training Loss: 0.21726589907234184 Test Loss: 0.21374997822256292\n",
      "Epoch: 7206 Training Loss: 0.21726366393402596 Test Loss: 0.21375423000789795\n",
      "Epoch: 7207 Training Loss: 0.21726521661350442 Test Loss: 0.21369060878099447\n",
      "Epoch: 7208 Training Loss: 0.21726548721930897 Test Loss: 0.21369412168899996\n",
      "Epoch: 7209 Training Loss: 0.21726445225207527 Test Loss: 0.21368557923004938\n",
      "Epoch: 7210 Training Loss: 0.21726439886969637 Test Loss: 0.21372208236268186\n",
      "Epoch: 7211 Training Loss: 0.2172654685973404 Test Loss: 0.2137382858128916\n",
      "Epoch: 7212 Training Loss: 0.21726397129064862 Test Loss: 0.2136903236002708\n",
      "Epoch: 7213 Training Loss: 0.21726269466818168 Test Loss: 0.2136736275651747\n",
      "Epoch: 7214 Training Loss: 0.21726547091948284 Test Loss: 0.21382425483832432\n",
      "Epoch: 7215 Training Loss: 0.21726494358693363 Test Loss: 0.2137629409827307\n",
      "Epoch: 7216 Training Loss: 0.21726676160929148 Test Loss: 0.21385690803118695\n",
      "Epoch: 7217 Training Loss: 0.21726411621385533 Test Loss: 0.21634494132951046\n",
      "Epoch: 7218 Training Loss: 0.21726449133400166 Test Loss: 0.213828247368456\n",
      "Epoch: 7219 Training Loss: 0.2172648628588618 Test Loss: 0.21366323143152013\n",
      "Epoch: 7220 Training Loss: 0.21726590188760334 Test Loss: 0.21371335842508893\n",
      "Epoch: 7221 Training Loss: 0.21726485634072448 Test Loss: 0.21372969150290033\n",
      "Epoch: 7222 Training Loss: 0.2172640082297486 Test Loss: 0.21369329207234922\n",
      "Epoch: 7223 Training Loss: 0.21726397169410966 Test Loss: 0.21370397338672745\n",
      "Epoch: 7224 Training Loss: 0.21726623152424138 Test Loss: 0.2137494208238757\n",
      "Epoch: 7225 Training Loss: 0.21726396548080962 Test Loss: 0.2137457653255084\n",
      "Epoch: 7226 Training Loss: 0.21726540766575705 Test Loss: 0.2148246299287525\n",
      "Epoch: 7227 Training Loss: 0.21726562524781473 Test Loss: 0.21373216739009238\n",
      "Epoch: 7228 Training Loss: 0.21726516008412938 Test Loss: 0.21376553353476427\n",
      "Epoch: 7229 Training Loss: 0.21726527768854056 Test Loss: 0.213700616031844\n",
      "Epoch: 7230 Training Loss: 0.21726344693474534 Test Loss: 0.21370735666713125\n",
      "Epoch: 7231 Training Loss: 0.21726493137551273 Test Loss: 0.21382137710556706\n",
      "Epoch: 7232 Training Loss: 0.21726352950080638 Test Loss: 0.21379357198500717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7233 Training Loss: 0.2172633343242853 Test Loss: 0.21371070105925455\n",
      "Epoch: 7234 Training Loss: 0.2172658147041548 Test Loss: 0.21383467689749922\n",
      "Epoch: 7235 Training Loss: 0.2172649768769526 Test Loss: 0.21374023022691674\n",
      "Epoch: 7236 Training Loss: 0.21726431535326043 Test Loss: 0.21380278850748646\n",
      "Epoch: 7237 Training Loss: 0.2172633090586582 Test Loss: 0.2138001959554529\n",
      "Epoch: 7238 Training Loss: 0.21726459424346506 Test Loss: 0.21376601315689048\n",
      "Epoch: 7239 Training Loss: 0.21726682575063155 Test Loss: 0.21374002282275406\n",
      "Epoch: 7240 Training Loss: 0.2172643952564786 Test Loss: 0.21377193713828713\n",
      "Epoch: 7241 Training Loss: 0.21726735220453225 Test Loss: 0.21374987452048158\n",
      "Epoch: 7242 Training Loss: 0.21726691845701346 Test Loss: 0.21373307478330414\n",
      "Epoch: 7243 Training Loss: 0.21726599735545193 Test Loss: 0.21421381170688647\n",
      "Epoch: 7244 Training Loss: 0.21726150623333296 Test Loss: 0.21463149776501275\n",
      "Epoch: 7245 Training Loss: 0.2172658550592249 Test Loss: 0.2141575922160388\n",
      "Epoch: 7246 Training Loss: 0.21726315333166135 Test Loss: 0.2139182089240204\n",
      "Epoch: 7247 Training Loss: 0.21726544947328694 Test Loss: 0.21382967327207444\n",
      "Epoch: 7248 Training Loss: 0.2172661409158569 Test Loss: 0.21391971260419984\n",
      "Epoch: 7249 Training Loss: 0.2172644797053578 Test Loss: 0.2137641465194263\n",
      "Epoch: 7250 Training Loss: 0.2172650358270939 Test Loss: 0.21369493834289052\n",
      "Epoch: 7251 Training Loss: 0.21726558052639955 Test Loss: 0.21367054242825476\n",
      "Epoch: 7252 Training Loss: 0.21726674525567052 Test Loss: 0.21363035787173462\n",
      "Epoch: 7253 Training Loss: 0.21726504459564724 Test Loss: 0.21370096602636854\n",
      "Epoch: 7254 Training Loss: 0.2172659119293004 Test Loss: 0.21365658153555406\n",
      "Epoch: 7255 Training Loss: 0.21726594337236438 Test Loss: 0.21371902315128224\n",
      "Epoch: 7256 Training Loss: 0.2172656557494696 Test Loss: 0.2136943031676423\n",
      "Epoch: 7257 Training Loss: 0.21726444449665744 Test Loss: 0.21370254748310902\n",
      "Epoch: 7258 Training Loss: 0.21726655829182315 Test Loss: 0.21374542829374402\n",
      "Epoch: 7259 Training Loss: 0.2172638299178991 Test Loss: 0.21372838226412338\n",
      "Epoch: 7260 Training Loss: 0.21726485116745733 Test Loss: 0.21372288605381226\n",
      "Epoch: 7261 Training Loss: 0.21726581991328517 Test Loss: 0.21370904182595307\n",
      "Epoch: 7262 Training Loss: 0.2172655147353521 Test Loss: 0.2140277183219178\n",
      "Epoch: 7263 Training Loss: 0.2172659577983381 Test Loss: 0.21365039829895402\n",
      "Epoch: 7264 Training Loss: 0.21726514089731533 Test Loss: 0.2136515001335683\n",
      "Epoch: 7265 Training Loss: 0.21726666428552208 Test Loss: 0.21372078608666506\n",
      "Epoch: 7266 Training Loss: 0.21726541114448783 Test Loss: 0.21378077774072157\n",
      "Epoch: 7267 Training Loss: 0.21726453538298177 Test Loss: 0.21364312619049988\n",
      "Epoch: 7268 Training Loss: 0.2172634428194427 Test Loss: 0.21770945331581182\n",
      "Epoch: 7269 Training Loss: 0.2172671086037202 Test Loss: 0.21392093110365562\n",
      "Epoch: 7270 Training Loss: 0.21726431069104393 Test Loss: 0.21367117760350296\n",
      "Epoch: 7271 Training Loss: 0.21726595793282513 Test Loss: 0.21370333821147924\n",
      "Epoch: 7272 Training Loss: 0.2172642524223035 Test Loss: 0.2137266971053016\n",
      "Epoch: 7273 Training Loss: 0.21726691568658096 Test Loss: 0.21374585606482957\n",
      "Epoch: 7274 Training Loss: 0.2172643973186128 Test Loss: 0.2137839406542025\n",
      "Epoch: 7275 Training Loss: 0.21726519525696655 Test Loss: 0.2136940957634796\n",
      "Epoch: 7276 Training Loss: 0.21726428171357523 Test Loss: 0.21372319716005628\n",
      "Epoch: 7277 Training Loss: 0.21726750207686107 Test Loss: 0.21368718661231018\n",
      "Epoch: 7278 Training Loss: 0.21726511583790165 Test Loss: 0.213793559022247\n",
      "Epoch: 7279 Training Loss: 0.21726552422116954 Test Loss: 0.21370831591138367\n",
      "Epoch: 7280 Training Loss: 0.2172630202791751 Test Loss: 0.21373558955877667\n",
      "Epoch: 7281 Training Loss: 0.2172657671764439 Test Loss: 0.21368594218733408\n",
      "Epoch: 7282 Training Loss: 0.2172652424260454 Test Loss: 0.2137021456375438\n",
      "Epoch: 7283 Training Loss: 0.21726474284265015 Test Loss: 0.21381146059403872\n",
      "Epoch: 7284 Training Loss: 0.21726343657924524 Test Loss: 0.21370274192451152\n",
      "Epoch: 7285 Training Loss: 0.2172655977766006 Test Loss: 0.21380390330486088\n",
      "Epoch: 7286 Training Loss: 0.21726394598019252 Test Loss: 0.21377377785023097\n",
      "Epoch: 7287 Training Loss: 0.21726354335296885 Test Loss: 0.21382263449330333\n",
      "Epoch: 7288 Training Loss: 0.21726243049085642 Test Loss: 0.21381223835964877\n",
      "Epoch: 7289 Training Loss: 0.21726334284179621 Test Loss: 0.2139487362242155\n",
      "Epoch: 7290 Training Loss: 0.2172654042228895 Test Loss: 0.21375892252707868\n",
      "Epoch: 7291 Training Loss: 0.21726476858346472 Test Loss: 0.2137371969410375\n",
      "Epoch: 7292 Training Loss: 0.21726549443677876 Test Loss: 0.21378897020514762\n",
      "Epoch: 7293 Training Loss: 0.21726490162698514 Test Loss: 0.21368684958054582\n",
      "Epoch: 7294 Training Loss: 0.21726389734968812 Test Loss: 0.21368281816213364\n",
      "Epoch: 7295 Training Loss: 0.21726377957492674 Test Loss: 0.2136931754075077\n",
      "Epoch: 7296 Training Loss: 0.2172654753754859 Test Loss: 0.21377513894004857\n",
      "Epoch: 7297 Training Loss: 0.21726416472780433 Test Loss: 0.21373499327180898\n",
      "Epoch: 7298 Training Loss: 0.21726429491123425 Test Loss: 0.21374432645912977\n",
      "Epoch: 7299 Training Loss: 0.21726313964984909 Test Loss: 0.21827045565035308\n",
      "Epoch: 7300 Training Loss: 0.21726697740715475 Test Loss: 0.2139066202164304\n",
      "Epoch: 7301 Training Loss: 0.21726584879213004 Test Loss: 0.21375688737373236\n",
      "Epoch: 7302 Training Loss: 0.21726536731965276 Test Loss: 0.21369545685329722\n",
      "Epoch: 7303 Training Loss: 0.2172657164569079 Test Loss: 0.21378111477248593\n",
      "Epoch: 7304 Training Loss: 0.2172658446857932 Test Loss: 0.21372874522140808\n",
      "Epoch: 7305 Training Loss: 0.21726417243839313 Test Loss: 0.2137493430473147\n",
      "Epoch: 7306 Training Loss: 0.21726480232177373 Test Loss: 0.21371596393988265\n",
      "Epoch: 7307 Training Loss: 0.2172639617331048 Test Loss: 0.21386630603230858\n",
      "Epoch: 7308 Training Loss: 0.2172646966060146 Test Loss: 0.2137204231293804\n",
      "Epoch: 7309 Training Loss: 0.21726540644640813 Test Loss: 0.21379853672215143\n",
      "Epoch: 7310 Training Loss: 0.21726304610964767 Test Loss: 0.2137791185074201\n",
      "Epoch: 7311 Training Loss: 0.21726434086096416 Test Loss: 0.21374404127840607\n",
      "Epoch: 7312 Training Loss: 0.21726494551458084 Test Loss: 0.21375478740658516\n",
      "Epoch: 7313 Training Loss: 0.21726499988319786 Test Loss: 0.21377425747235718\n",
      "Epoch: 7314 Training Loss: 0.2172637654268928 Test Loss: 0.21374204501334024\n",
      "Epoch: 7315 Training Loss: 0.21726513433434902 Test Loss: 0.2137829684471899\n",
      "Epoch: 7316 Training Loss: 0.21726366668652686 Test Loss: 0.21373443587312174\n",
      "Epoch: 7317 Training Loss: 0.21726436523001116 Test Loss: 0.21370333821147924\n",
      "Epoch: 7318 Training Loss: 0.21726466807683595 Test Loss: 0.21372191384679967\n",
      "Epoch: 7319 Training Loss: 0.2172657016812679 Test Loss: 0.2137234693780198\n",
      "Epoch: 7320 Training Loss: 0.2172652398349289 Test Loss: 0.21377528153041042\n",
      "Epoch: 7321 Training Loss: 0.21726589100312096 Test Loss: 0.21377859999701337\n",
      "Epoch: 7322 Training Loss: 0.21726548500475615 Test Loss: 0.21380657363345545\n",
      "Epoch: 7323 Training Loss: 0.2172640972960153 Test Loss: 0.21375170226966522\n",
      "Epoch: 7324 Training Loss: 0.21726544808358778 Test Loss: 0.2138608616730381\n",
      "Epoch: 7325 Training Loss: 0.21726541196934152 Test Loss: 0.2137510800571772\n",
      "Epoch: 7326 Training Loss: 0.2172638456528798 Test Loss: 0.21380346257101518\n",
      "Epoch: 7327 Training Loss: 0.21726385384762187 Test Loss: 0.21386001909362723\n",
      "Epoch: 7328 Training Loss: 0.21726163520637976 Test Loss: 0.21414019619189364\n",
      "Epoch: 7329 Training Loss: 0.2172627444463086 Test Loss: 0.21515818767338987\n",
      "Epoch: 7330 Training Loss: 0.21726697915548596 Test Loss: 0.2139889726317763\n",
      "Epoch: 7331 Training Loss: 0.21726327623486089 Test Loss: 0.2150948905154906\n",
      "Epoch: 7332 Training Loss: 0.21726519536455616 Test Loss: 0.2137985107966311\n",
      "Epoch: 7333 Training Loss: 0.2172627125549546 Test Loss: 0.2137138121216948\n",
      "Epoch: 7334 Training Loss: 0.21726562437813204 Test Loss: 0.21380477180979213\n",
      "Epoch: 7335 Training Loss: 0.2172650469984819 Test Loss: 0.2137864813551954\n",
      "Epoch: 7336 Training Loss: 0.21726298963406743 Test Loss: 0.21381626977806095\n",
      "Epoch: 7337 Training Loss: 0.21726226682912572 Test Loss: 0.21374733381948868\n",
      "Epoch: 7338 Training Loss: 0.21726395374457616 Test Loss: 0.2137284989289649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7339 Training Loss: 0.21726678264306054 Test Loss: 0.21383526022170676\n",
      "Epoch: 7340 Training Loss: 0.21726420538771166 Test Loss: 0.2137583910539118\n",
      "Epoch: 7341 Training Loss: 0.21726438213951177 Test Loss: 0.2137352525270123\n",
      "Epoch: 7342 Training Loss: 0.21726437603380133 Test Loss: 0.21372009906037617\n",
      "Epoch: 7343 Training Loss: 0.2172652911193104 Test Loss: 0.21377499634968675\n",
      "Epoch: 7344 Training Loss: 0.21726552323493142 Test Loss: 0.21383723056125226\n",
      "Epoch: 7345 Training Loss: 0.2172640689820159 Test Loss: 0.21380903655788733\n",
      "Epoch: 7346 Training Loss: 0.21726544938362893 Test Loss: 0.2153743935502283\n",
      "Epoch: 7347 Training Loss: 0.21726500246534852 Test Loss: 0.21378915168378995\n",
      "Epoch: 7348 Training Loss: 0.21726636660299864 Test Loss: 0.213739400610266\n",
      "Epoch: 7349 Training Loss: 0.21726505101516072 Test Loss: 0.21369211246117395\n",
      "Epoch: 7350 Training Loss: 0.21726566583599569 Test Loss: 0.21374453386329245\n",
      "Epoch: 7351 Training Loss: 0.21726418569881276 Test Loss: 0.21364361877538626\n",
      "Epoch: 7352 Training Loss: 0.21726507210272458 Test Loss: 0.2136960401775048\n",
      "Epoch: 7353 Training Loss: 0.21726585277294566 Test Loss: 0.21371575653571998\n",
      "Epoch: 7354 Training Loss: 0.21726519985642243 Test Loss: 0.21385240995340873\n",
      "Epoch: 7355 Training Loss: 0.21726444543806656 Test Loss: 0.21382425483832432\n",
      "Epoch: 7356 Training Loss: 0.2172628276399757 Test Loss: 0.21366311476667862\n",
      "Epoch: 7357 Training Loss: 0.21726652206998726 Test Loss: 0.2137268526584236\n",
      "Epoch: 7358 Training Loss: 0.21726490495329728 Test Loss: 0.21370888627283105\n",
      "Epoch: 7359 Training Loss: 0.21726465924552202 Test Loss: 0.21371693614689524\n",
      "Epoch: 7360 Training Loss: 0.2172652080421987 Test Loss: 0.21380499217671498\n",
      "Epoch: 7361 Training Loss: 0.21726551418843826 Test Loss: 0.21404918465275566\n",
      "Epoch: 7362 Training Loss: 0.2172629250623689 Test Loss: 0.2138253048218979\n",
      "Epoch: 7363 Training Loss: 0.2172646094225661 Test Loss: 0.21380137556662818\n",
      "Epoch: 7364 Training Loss: 0.21726455321595986 Test Loss: 0.21374807269681825\n",
      "Epoch: 7365 Training Loss: 0.21726321305286153 Test Loss: 0.21371374730789397\n",
      "Epoch: 7366 Training Loss: 0.217264758210033 Test Loss: 0.2137108695751367\n",
      "Epoch: 7367 Training Loss: 0.21726471198236325 Test Loss: 0.21380841434539927\n",
      "Epoch: 7368 Training Loss: 0.2172660250149479 Test Loss: 0.21414737756102661\n",
      "Epoch: 7369 Training Loss: 0.21726596836005163 Test Loss: 0.2137615021163521\n",
      "Epoch: 7370 Training Loss: 0.21726493874540112 Test Loss: 0.21375661515576883\n",
      "Epoch: 7371 Training Loss: 0.21726434948606468 Test Loss: 0.21373214146457206\n",
      "Epoch: 7372 Training Loss: 0.21726478134179947 Test Loss: 0.21370860109210735\n",
      "Epoch: 7373 Training Loss: 0.21726358147555452 Test Loss: 0.21371567875915898\n",
      "Epoch: 7374 Training Loss: 0.21726551676162315 Test Loss: 0.21372827856204205\n",
      "Epoch: 7375 Training Loss: 0.21726500275225416 Test Loss: 0.2136881847448431\n",
      "Epoch: 7376 Training Loss: 0.2172646673685377 Test Loss: 0.2137053215137849\n",
      "Epoch: 7377 Training Loss: 0.21726501376225774 Test Loss: 0.21373416365515824\n",
      "Epoch: 7378 Training Loss: 0.21726462773969743 Test Loss: 0.2137023400789463\n",
      "Epoch: 7379 Training Loss: 0.21726443587155692 Test Loss: 0.2137228730910521\n",
      "Epoch: 7380 Training Loss: 0.2172632548693572 Test Loss: 0.2140334348991518\n",
      "Epoch: 7381 Training Loss: 0.21726509812147896 Test Loss: 0.21518329653983487\n",
      "Epoch: 7382 Training Loss: 0.21726344248770807 Test Loss: 0.21374120243392933\n",
      "Epoch: 7383 Training Loss: 0.21726627143998725 Test Loss: 0.21383269359519352\n",
      "Epoch: 7384 Training Loss: 0.21726378355574236 Test Loss: 0.21385863207828926\n",
      "Epoch: 7385 Training Loss: 0.21726378290123888 Test Loss: 0.21373765063764336\n",
      "Epoch: 7386 Training Loss: 0.21726463792484732 Test Loss: 0.21375535776803256\n",
      "Epoch: 7387 Training Loss: 0.21726491915512602 Test Loss: 0.21379100535849394\n",
      "Epoch: 7388 Training Loss: 0.21726581987742197 Test Loss: 0.2142053081362164\n",
      "Epoch: 7389 Training Loss: 0.21726528478945492 Test Loss: 0.21671458739845492\n",
      "Epoch: 7390 Training Loss: 0.21726437510135801 Test Loss: 0.21380649585689446\n",
      "Epoch: 7391 Training Loss: 0.2172629070769722 Test Loss: 0.2137991589346395\n",
      "Epoch: 7392 Training Loss: 0.21726465009143922 Test Loss: 0.21374863009550546\n",
      "Epoch: 7393 Training Loss: 0.21726416662855413 Test Loss: 0.21377316860050308\n",
      "Epoch: 7394 Training Loss: 0.21726505998096168 Test Loss: 0.21373657472854943\n",
      "Epoch: 7395 Training Loss: 0.21726552825577997 Test Loss: 0.21372361196838166\n",
      "Epoch: 7396 Training Loss: 0.21726470906847795 Test Loss: 0.21379031833220505\n",
      "Epoch: 7397 Training Loss: 0.21726038411851395 Test Loss: 0.21479897662638048\n",
      "Epoch: 7398 Training Loss: 0.21726719209325873 Test Loss: 0.2138741096139296\n",
      "Epoch: 7399 Training Loss: 0.21726361351036136 Test Loss: 0.21388339095020972\n",
      "Epoch: 7400 Training Loss: 0.21726230530137763 Test Loss: 0.21376968161801796\n",
      "Epoch: 7401 Training Loss: 0.21726437683175762 Test Loss: 0.21377215750521\n",
      "Epoch: 7402 Training Loss: 0.21726468463667034 Test Loss: 0.2138176956816794\n",
      "Epoch: 7403 Training Loss: 0.21726524124255966 Test Loss: 0.21379812191382605\n",
      "Epoch: 7404 Training Loss: 0.217263656860009 Test Loss: 0.21377701854027292\n",
      "Epoch: 7405 Training Loss: 0.21726423524382887 Test Loss: 0.21378425176044652\n",
      "Epoch: 7406 Training Loss: 0.21726459808082788 Test Loss: 0.21383685464120739\n",
      "Epoch: 7407 Training Loss: 0.21726464895278252 Test Loss: 0.2138172549478337\n",
      "Epoch: 7408 Training Loss: 0.21726460342444523 Test Loss: 0.21381238095001062\n",
      "Epoch: 7409 Training Loss: 0.21726365633102673 Test Loss: 0.21375861142083466\n",
      "Epoch: 7410 Training Loss: 0.21726563561228066 Test Loss: 0.21376561131132527\n",
      "Epoch: 7411 Training Loss: 0.21726456639568728 Test Loss: 0.21392907171704098\n",
      "Epoch: 7412 Training Loss: 0.2172625067808568 Test Loss: 0.21417463824565944\n",
      "Epoch: 7413 Training Loss: 0.21726551099661312 Test Loss: 0.21396079159117154\n",
      "Epoch: 7414 Training Loss: 0.21726452210463054 Test Loss: 0.21388751310794307\n",
      "Epoch: 7415 Training Loss: 0.21726629379172904 Test Loss: 0.21380058483825795\n",
      "Epoch: 7416 Training Loss: 0.21726446321724985 Test Loss: 0.213837982401342\n",
      "Epoch: 7417 Training Loss: 0.21726370769610043 Test Loss: 0.21386910598850484\n",
      "Epoch: 7418 Training Loss: 0.21726255021119664 Test Loss: 0.21741605420217425\n",
      "Epoch: 7419 Training Loss: 0.21726704242714331 Test Loss: 0.21392071073673277\n",
      "Epoch: 7420 Training Loss: 0.21726491200938264 Test Loss: 0.21372428603191038\n",
      "Epoch: 7421 Training Loss: 0.21726444262280503 Test Loss: 0.21378414805836518\n",
      "Epoch: 7422 Training Loss: 0.2172647384045787 Test Loss: 0.21371427878106083\n",
      "Epoch: 7423 Training Loss: 0.2172647096871182 Test Loss: 0.2136936161413534\n",
      "Epoch: 7424 Training Loss: 0.21726502453018467 Test Loss: 0.21368647366050095\n",
      "Epoch: 7425 Training Loss: 0.21726470910434115 Test Loss: 0.21364880387945337\n",
      "Epoch: 7426 Training Loss: 0.21726664436351234 Test Loss: 0.21380195889083572\n",
      "Epoch: 7427 Training Loss: 0.2172646618904333 Test Loss: 0.21379056462464824\n",
      "Epoch: 7428 Training Loss: 0.21726245876899264 Test Loss: 0.2140388662956621\n",
      "Epoch: 7429 Training Loss: 0.21726602763296177 Test Loss: 0.21377160010652277\n",
      "Epoch: 7430 Training Loss: 0.21726550202184636 Test Loss: 0.21376493724779655\n",
      "Epoch: 7431 Training Loss: 0.21726600719990138 Test Loss: 0.21379765525446\n",
      "Epoch: 7432 Training Loss: 0.2172651011698513 Test Loss: 0.21371724725313926\n",
      "Epoch: 7433 Training Loss: 0.21726432082239902 Test Loss: 0.21376758165087076\n",
      "Epoch: 7434 Training Loss: 0.21726378701654153 Test Loss: 0.21494056885569315\n",
      "Epoch: 7435 Training Loss: 0.2172653621105224 Test Loss: 0.21375166338138474\n",
      "Epoch: 7436 Training Loss: 0.21726444402147 Test Loss: 0.2138717763170994\n",
      "Epoch: 7437 Training Loss: 0.21726431118416298 Test Loss: 0.21377043345810767\n",
      "Epoch: 7438 Training Loss: 0.21726468224280146 Test Loss: 0.21396808962514602\n",
      "Epoch: 7439 Training Loss: 0.21726395187072375 Test Loss: 0.21427427002030902\n",
      "Epoch: 7440 Training Loss: 0.21726330580407244 Test Loss: 0.2138726966730713\n",
      "Epoch: 7441 Training Loss: 0.21726380404259754 Test Loss: 0.21372835633860307\n",
      "Epoch: 7442 Training Loss: 0.2172646770605685 Test Loss: 0.21378080366624191\n",
      "Epoch: 7443 Training Loss: 0.21726367133977756 Test Loss: 0.21374287462999098\n",
      "Epoch: 7444 Training Loss: 0.21726357954790732 Test Loss: 0.2137453634799432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7445 Training Loss: 0.21726486400648432 Test Loss: 0.21367552012815919\n",
      "Epoch: 7446 Training Loss: 0.21726543573767987 Test Loss: 0.21515410440393704\n",
      "Epoch: 7447 Training Loss: 0.2172645458281399 Test Loss: 0.21369178839216973\n",
      "Epoch: 7448 Training Loss: 0.21726482506801076 Test Loss: 0.2137530892850032\n",
      "Epoch: 7449 Training Loss: 0.2172639913919744 Test Loss: 0.213700616031844\n",
      "Epoch: 7450 Training Loss: 0.2172650762807878 Test Loss: 0.21372158977779548\n",
      "Epoch: 7451 Training Loss: 0.21726393868203053 Test Loss: 0.21371497877010992\n",
      "Epoch: 7452 Training Loss: 0.2172627921354039 Test Loss: 0.21371959351272965\n",
      "Epoch: 7453 Training Loss: 0.2172597030942047 Test Loss: 0.213750833764734\n",
      "Epoch: 7454 Training Loss: 0.21726168300306467 Test Loss: 0.21498373484705188\n",
      "Epoch: 7455 Training Loss: 0.21726346451668102 Test Loss: 0.21375959659060742\n",
      "Epoch: 7456 Training Loss: 0.217263466094662 Test Loss: 0.21383981015052564\n",
      "Epoch: 7457 Training Loss: 0.21726184055011913 Test Loss: 0.21376138545151058\n",
      "Epoch: 7458 Training Loss: 0.21726332442604104 Test Loss: 0.2137732074887836\n",
      "Epoch: 7459 Training Loss: 0.21726501230979797 Test Loss: 0.2137590003036397\n",
      "Epoch: 7460 Training Loss: 0.21726225421424378 Test Loss: 0.21392406809161624\n",
      "Epoch: 7461 Training Loss: 0.21726531143581537 Test Loss: 0.2138364139073617\n",
      "Epoch: 7462 Training Loss: 0.21726261957063284 Test Loss: 0.21374163020501485\n",
      "Epoch: 7463 Training Loss: 0.21726258230876405 Test Loss: 0.21368054967910427\n",
      "Epoch: 7464 Training Loss: 0.21726458313483768 Test Loss: 0.2138048625491133\n",
      "Epoch: 7465 Training Loss: 0.21726403419470816 Test Loss: 0.21707001331949533\n",
      "Epoch: 7466 Training Loss: 0.217265704335145 Test Loss: 0.21376006324997346\n",
      "Epoch: 7467 Training Loss: 0.21726537729858922 Test Loss: 0.2136724868422799\n",
      "Epoch: 7468 Training Loss: 0.21726540204419986 Test Loss: 0.21367063316757592\n",
      "Epoch: 7469 Training Loss: 0.21726530447835382 Test Loss: 0.213797344148216\n",
      "Epoch: 7470 Training Loss: 0.21726432936680734 Test Loss: 0.2137246878774756\n",
      "Epoch: 7471 Training Loss: 0.21726213641258496 Test Loss: 0.21379783673310238\n",
      "Epoch: 7472 Training Loss: 0.21726191164892072 Test Loss: 0.2137887628009849\n",
      "Epoch: 7473 Training Loss: 0.21726428295982156 Test Loss: 0.2140159740612058\n",
      "Epoch: 7474 Training Loss: 0.2172646657546935 Test Loss: 0.21379608676047973\n",
      "Epoch: 7475 Training Loss: 0.21726329861350008 Test Loss: 0.21393342720445738\n",
      "Epoch: 7476 Training Loss: 0.21726474425028092 Test Loss: 0.21372975631670119\n",
      "Epoch: 7477 Training Loss: 0.21726373512248556 Test Loss: 0.21376572797616678\n",
      "Epoch: 7478 Training Loss: 0.21726407327663455 Test Loss: 0.21373601732986222\n",
      "Epoch: 7479 Training Loss: 0.21726349475832765 Test Loss: 0.21367109982694196\n",
      "Epoch: 7480 Training Loss: 0.21726248420496996 Test Loss: 0.21436758893075689\n",
      "Epoch: 7481 Training Loss: 0.21726608354369656 Test Loss: 0.2138084402709196\n",
      "Epoch: 7482 Training Loss: 0.21726320749406494 Test Loss: 0.21901975503909155\n",
      "Epoch: 7483 Training Loss: 0.21726646329019617 Test Loss: 0.21389805183195948\n",
      "Epoch: 7484 Training Loss: 0.21726520198131727 Test Loss: 0.21378163328289265\n",
      "Epoch: 7485 Training Loss: 0.21726412136919088 Test Loss: 0.21368132744471435\n",
      "Epoch: 7486 Training Loss: 0.21726404385087578 Test Loss: 0.2137516763441449\n",
      "Epoch: 7487 Training Loss: 0.2172638578463691 Test Loss: 0.2136930587426662\n",
      "Epoch: 7488 Training Loss: 0.217263611815825 Test Loss: 0.2137259971162525\n",
      "Epoch: 7489 Training Loss: 0.21726516843129007 Test Loss: 0.21367408126178056\n",
      "Epoch: 7490 Training Loss: 0.2172652294345998 Test Loss: 0.21359543619584262\n",
      "Epoch: 7491 Training Loss: 0.2172639673367304 Test Loss: 0.21368077004602712\n",
      "Epoch: 7492 Training Loss: 0.21726295769788442 Test Loss: 0.21464029947916666\n",
      "Epoch: 7493 Training Loss: 0.21726397993368074 Test Loss: 0.21373720990379766\n",
      "Epoch: 7494 Training Loss: 0.21726362278099956 Test Loss: 0.21376076323902252\n",
      "Epoch: 7495 Training Loss: 0.21726467269422345 Test Loss: 0.21369387539655677\n",
      "Epoch: 7496 Training Loss: 0.2172634679236854 Test Loss: 0.2137842647232067\n",
      "Epoch: 7497 Training Loss: 0.21726398580628037 Test Loss: 0.21375894845259902\n",
      "Epoch: 7498 Training Loss: 0.21726437844560179 Test Loss: 0.21378034996963602\n",
      "Epoch: 7499 Training Loss: 0.21726363971739757 Test Loss: 0.2136197543339174\n",
      "Epoch: 7500 Training Loss: 0.21726573907762373 Test Loss: 0.21374380794872305\n",
      "Epoch: 7501 Training Loss: 0.21726579777672259 Test Loss: 0.21365231678745886\n",
      "Epoch: 7502 Training Loss: 0.2172644918181549 Test Loss: 0.21362451166689897\n",
      "Epoch: 7503 Training Loss: 0.2172633100897253 Test Loss: 0.21359735468434746\n",
      "Epoch: 7504 Training Loss: 0.2172651049803167 Test Loss: 0.2136706461303361\n",
      "Epoch: 7505 Training Loss: 0.2172654883041709 Test Loss: 0.21371346212717027\n",
      "Epoch: 7506 Training Loss: 0.21726558044570735 Test Loss: 0.21362628756504196\n",
      "Epoch: 7507 Training Loss: 0.2172645066744871 Test Loss: 0.21376313542413322\n",
      "Epoch: 7508 Training Loss: 0.2172647938490918 Test Loss: 0.213684775538919\n",
      "Epoch: 7509 Training Loss: 0.21726524113497006 Test Loss: 0.21373459142624376\n",
      "Epoch: 7510 Training Loss: 0.21726522843043009 Test Loss: 0.21378757022704947\n",
      "Epoch: 7511 Training Loss: 0.21726773046270892 Test Loss: 0.21373131184792132\n",
      "Epoch: 7512 Training Loss: 0.2172655276371397 Test Loss: 0.21372099349082777\n",
      "Epoch: 7513 Training Loss: 0.21726478118041506 Test Loss: 0.21378058329931904\n",
      "Epoch: 7514 Training Loss: 0.21726505829539108 Test Loss: 0.21376929273521292\n",
      "Epoch: 7515 Training Loss: 0.2172652116464507 Test Loss: 0.2137908627681321\n",
      "Epoch: 7516 Training Loss: 0.21726486515410684 Test Loss: 0.21385051739042424\n",
      "Epoch: 7517 Training Loss: 0.21726535209572273 Test Loss: 0.21383082695772937\n",
      "Epoch: 7518 Training Loss: 0.2172641497280193 Test Loss: 0.2136627647721541\n",
      "Epoch: 7519 Training Loss: 0.21726758850718234 Test Loss: 0.2137579114317856\n",
      "Epoch: 7520 Training Loss: 0.2172653660554748 Test Loss: 0.21376434096082883\n",
      "Epoch: 7521 Training Loss: 0.21726617436726028 Test Loss: 0.21369934568134757\n",
      "Epoch: 7522 Training Loss: 0.21726532086783798 Test Loss: 0.21368214409860492\n",
      "Epoch: 7523 Training Loss: 0.2172648312006186 Test Loss: 0.2184723376772061\n",
      "Epoch: 7524 Training Loss: 0.21726637172247099 Test Loss: 0.2138974425822316\n",
      "Epoch: 7525 Training Loss: 0.21726611602679344 Test Loss: 0.21376102249422588\n",
      "Epoch: 7526 Training Loss: 0.21726527231802578 Test Loss: 0.21370967700120128\n",
      "Epoch: 7527 Training Loss: 0.2172646309942832 Test Loss: 0.2136912439562427\n",
      "Epoch: 7528 Training Loss: 0.21726677871603972 Test Loss: 0.21368027746114077\n",
      "Epoch: 7529 Training Loss: 0.2172649911146445 Test Loss: 0.2136628814369956\n",
      "Epoch: 7530 Training Loss: 0.21726639502458767 Test Loss: 0.21378342214379578\n",
      "Epoch: 7531 Training Loss: 0.21726417883997504 Test Loss: 0.21376135952599024\n",
      "Epoch: 7532 Training Loss: 0.21726477802445313 Test Loss: 0.21371668985445205\n",
      "Epoch: 7533 Training Loss: 0.2172651668712407 Test Loss: 0.21375958362784725\n",
      "Epoch: 7534 Training Loss: 0.21726545255752247 Test Loss: 0.21370466041301636\n",
      "Epoch: 7535 Training Loss: 0.21726603195447783 Test Loss: 0.21377134085131944\n",
      "Epoch: 7536 Training Loss: 0.21726568593732143 Test Loss: 0.21372788967923703\n",
      "Epoch: 7537 Training Loss: 0.2172655810195186 Test Loss: 0.21365958889591297\n",
      "Epoch: 7538 Training Loss: 0.21726506603287732 Test Loss: 0.21365975741179516\n",
      "Epoch: 7539 Training Loss: 0.2172645196569669 Test Loss: 0.2136995790110306\n",
      "Epoch: 7540 Training Loss: 0.21726699879059005 Test Loss: 0.21371166030350697\n",
      "Epoch: 7541 Training Loss: 0.2172657389789999 Test Loss: 0.21377542412077227\n",
      "Epoch: 7542 Training Loss: 0.21726482306863715 Test Loss: 0.21368599403837474\n",
      "Epoch: 7543 Training Loss: 0.2172648986951682 Test Loss: 0.21373074148647392\n",
      "Epoch: 7544 Training Loss: 0.21726450813591264 Test Loss: 0.21376999272426198\n",
      "Epoch: 7545 Training Loss: 0.21726433977610224 Test Loss: 0.21372221199028352\n",
      "Epoch: 7546 Training Loss: 0.21726432517977828 Test Loss: 0.215973519362423\n",
      "Epoch: 7547 Training Loss: 0.21726475593271954 Test Loss: 0.21378585914270734\n",
      "Epoch: 7548 Training Loss: 0.21726494936090945 Test Loss: 0.21383433986573483\n",
      "Epoch: 7549 Training Loss: 0.2172639225615204 Test Loss: 0.21378782948225283\n",
      "Epoch: 7550 Training Loss: 0.2172650357822649 Test Loss: 0.21371805094426968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7551 Training Loss: 0.21726483747667927 Test Loss: 0.21372188792127933\n",
      "Epoch: 7552 Training Loss: 0.21726486057258254 Test Loss: 0.2138265881351545\n",
      "Epoch: 7553 Training Loss: 0.21726321533914078 Test Loss: 0.21379519233002814\n",
      "Epoch: 7554 Training Loss: 0.21726488948729064 Test Loss: 0.21375708181513486\n",
      "Epoch: 7555 Training Loss: 0.2172658268528151 Test Loss: 0.21372413047878835\n",
      "Epoch: 7556 Training Loss: 0.2172648227817315 Test Loss: 0.21391934964691514\n",
      "Epoch: 7557 Training Loss: 0.21726138910410925 Test Loss: 0.21406967877658092\n",
      "Epoch: 7558 Training Loss: 0.21726671358846156 Test Loss: 0.21395928791099209\n",
      "Epoch: 7559 Training Loss: 0.2172635288373371 Test Loss: 0.21372213421372252\n",
      "Epoch: 7560 Training Loss: 0.2172655387905961 Test Loss: 0.21417839744610812\n",
      "Epoch: 7561 Training Loss: 0.21726417319152042 Test Loss: 0.21368000524317723\n",
      "Epoch: 7562 Training Loss: 0.21726521027468315 Test Loss: 0.21365960185867314\n",
      "Epoch: 7563 Training Loss: 0.21726632149605402 Test Loss: 0.21372965261461985\n",
      "Epoch: 7564 Training Loss: 0.21726593301686425 Test Loss: 0.2136897921271039\n",
      "Epoch: 7565 Training Loss: 0.21726361786774062 Test Loss: 0.21380295702336863\n",
      "Epoch: 7566 Training Loss: 0.21726386563765013 Test Loss: 0.2137301840877867\n",
      "Epoch: 7567 Training Loss: 0.21726505093446852 Test Loss: 0.21380242555020176\n",
      "Epoch: 7568 Training Loss: 0.21726524555510993 Test Loss: 0.21366273884663375\n",
      "Epoch: 7569 Training Loss: 0.21726162843720004 Test Loss: 0.2143970273590979\n",
      "Epoch: 7570 Training Loss: 0.21726680738867118 Test Loss: 0.21384030273541202\n",
      "Epoch: 7571 Training Loss: 0.2172635464641018 Test Loss: 0.2137869609773216\n",
      "Epoch: 7572 Training Loss: 0.217266680370169 Test Loss: 0.21376672610869968\n",
      "Epoch: 7573 Training Loss: 0.21726336726463802 Test Loss: 0.2138501285076192\n",
      "Epoch: 7574 Training Loss: 0.21726379504989918 Test Loss: 0.21382682146483753\n",
      "Epoch: 7575 Training Loss: 0.21726349290240685 Test Loss: 0.21383852683726903\n",
      "Epoch: 7576 Training Loss: 0.21726507355518432 Test Loss: 0.21373579696293937\n",
      "Epoch: 7577 Training Loss: 0.21726381411119203 Test Loss: 0.21372059164526255\n",
      "Epoch: 7578 Training Loss: 0.21726312614735283 Test Loss: 0.2136576833701683\n",
      "Epoch: 7579 Training Loss: 0.21726345261009736 Test Loss: 0.21369250134397896\n",
      "Epoch: 7580 Training Loss: 0.2172659409515981 Test Loss: 0.21374566162342704\n",
      "Epoch: 7581 Training Loss: 0.21726508586522905 Test Loss: 0.21378469249429224\n",
      "Epoch: 7582 Training Loss: 0.2172663838980287 Test Loss: 0.21368962361122174\n",
      "Epoch: 7583 Training Loss: 0.21726555642632658 Test Loss: 0.21381454573095865\n",
      "Epoch: 7584 Training Loss: 0.21726577867060076 Test Loss: 0.21374325055003585\n",
      "Epoch: 7585 Training Loss: 0.2172639765356422 Test Loss: 0.21374914860591218\n",
      "Epoch: 7586 Training Loss: 0.21726502700474576 Test Loss: 0.2137549818479877\n",
      "Epoch: 7587 Training Loss: 0.21726586941347226 Test Loss: 0.21371886759816025\n",
      "Epoch: 7588 Training Loss: 0.21726591791845545 Test Loss: 0.21369056989271398\n",
      "Epoch: 7589 Training Loss: 0.21726441345705452 Test Loss: 0.21375675774613068\n",
      "Epoch: 7590 Training Loss: 0.21726415464127824 Test Loss: 0.21371597690264282\n",
      "Epoch: 7591 Training Loss: 0.21726552727850765 Test Loss: 0.21372965261461985\n",
      "Epoch: 7592 Training Loss: 0.2172660334876298 Test Loss: 0.21378863317338323\n",
      "Epoch: 7593 Training Loss: 0.21726280469649104 Test Loss: 0.21368014783353909\n",
      "Epoch: 7594 Training Loss: 0.2172665291350384 Test Loss: 0.21376509280091854\n",
      "Epoch: 7595 Training Loss: 0.217266179289485 Test Loss: 0.21372048794318121\n",
      "Epoch: 7596 Training Loss: 0.2172641655885212 Test Loss: 0.21373275071429992\n",
      "Epoch: 7597 Training Loss: 0.21726551857271492 Test Loss: 0.2137680871985173\n",
      "Epoch: 7598 Training Loss: 0.21726677980090164 Test Loss: 0.21377390747783265\n",
      "Epoch: 7599 Training Loss: 0.21726596948077675 Test Loss: 0.21379368864984868\n",
      "Epoch: 7600 Training Loss: 0.21726416982037927 Test Loss: 0.21371834908775353\n",
      "Epoch: 7601 Training Loss: 0.21726744643510035 Test Loss: 0.21383395098292982\n",
      "Epoch: 7602 Training Loss: 0.2172639588819801 Test Loss: 0.21374811158509877\n",
      "Epoch: 7603 Training Loss: 0.21726641617491213 Test Loss: 0.21376062064866067\n",
      "Epoch: 7604 Training Loss: 0.21726248572915613 Test Loss: 0.2142638609238943\n",
      "Epoch: 7605 Training Loss: 0.2172663465285703 Test Loss: 0.21381401425779176\n",
      "Epoch: 7606 Training Loss: 0.21726439555235 Test Loss: 0.21377319452602342\n",
      "Epoch: 7607 Training Loss: 0.21726488477127934 Test Loss: 0.21374321166175533\n",
      "Epoch: 7608 Training Loss: 0.21726620375715583 Test Loss: 0.21378706467940295\n",
      "Epoch: 7609 Training Loss: 0.21726405648368935 Test Loss: 0.2137448708950568\n",
      "Epoch: 7610 Training Loss: 0.21726470264896447 Test Loss: 0.21369719386315972\n",
      "Epoch: 7611 Training Loss: 0.21726571698589014 Test Loss: 0.21401990177753663\n",
      "Epoch: 7612 Training Loss: 0.217262837672707 Test Loss: 0.214345578163992\n",
      "Epoch: 7613 Training Loss: 0.21726195039014667 Test Loss: 0.21502507308922694\n",
      "Epoch: 7614 Training Loss: 0.21726578540391728 Test Loss: 0.2140083260327068\n",
      "Epoch: 7615 Training Loss: 0.21726415017630937 Test Loss: 0.21390938128434614\n",
      "Epoch: 7616 Training Loss: 0.21726203742117658 Test Loss: 0.21374243389614528\n",
      "Epoch: 7617 Training Loss: 0.21726621699067802 Test Loss: 0.21416463099480992\n",
      "Epoch: 7618 Training Loss: 0.21726544528625788 Test Loss: 0.2137734797067471\n",
      "Epoch: 7619 Training Loss: 0.21726494879606398 Test Loss: 0.21371845278983487\n",
      "Epoch: 7620 Training Loss: 0.21726634782861143 Test Loss: 0.21381783827204126\n",
      "Epoch: 7621 Training Loss: 0.21726475713413687 Test Loss: 0.21366757395617633\n",
      "Epoch: 7622 Training Loss: 0.21726536271123106 Test Loss: 0.21369925494202638\n",
      "Epoch: 7623 Training Loss: 0.21726404519574594 Test Loss: 0.21374043763107944\n",
      "Epoch: 7624 Training Loss: 0.21726631655589768 Test Loss: 0.21374002282275406\n",
      "Epoch: 7625 Training Loss: 0.21726334860680624 Test Loss: 0.21378859428510275\n",
      "Epoch: 7626 Training Loss: 0.21726440505609904 Test Loss: 0.2137886850244239\n",
      "Epoch: 7627 Training Loss: 0.21726338923085037 Test Loss: 0.21398429307535574\n",
      "Epoch: 7628 Training Loss: 0.21726409422971138 Test Loss: 0.21374987452048158\n",
      "Epoch: 7629 Training Loss: 0.2172637821839748 Test Loss: 0.2139271013774955\n",
      "Epoch: 7630 Training Loss: 0.21726468664500975 Test Loss: 0.21379930152500132\n",
      "Epoch: 7631 Training Loss: 0.21726502331083575 Test Loss: 0.21374488385781698\n",
      "Epoch: 7632 Training Loss: 0.21726410627078208 Test Loss: 0.21375403556649544\n",
      "Epoch: 7633 Training Loss: 0.21726510679140848 Test Loss: 0.2137445597888128\n",
      "Epoch: 7634 Training Loss: 0.21726363203370613 Test Loss: 0.2137614502653114\n",
      "Epoch: 7635 Training Loss: 0.21726387600211602 Test Loss: 0.2138187845535335\n",
      "Epoch: 7636 Training Loss: 0.21726450414613122 Test Loss: 0.2137787036990947\n",
      "Epoch: 7637 Training Loss: 0.21726432386180555 Test Loss: 0.21386508753285283\n",
      "Epoch: 7638 Training Loss: 0.21726538916930968 Test Loss: 0.21370642334839918\n",
      "Epoch: 7639 Training Loss: 0.21726540580983628 Test Loss: 0.21371412322793884\n",
      "Epoch: 7640 Training Loss: 0.2172642205937101 Test Loss: 0.21376015398929463\n",
      "Epoch: 7641 Training Loss: 0.21726600756749923 Test Loss: 0.2137637446738611\n",
      "Epoch: 7642 Training Loss: 0.2172652809969211 Test Loss: 0.21374833195202161\n",
      "Epoch: 7643 Training Loss: 0.21726373105201194 Test Loss: 0.21370559373174844\n",
      "Epoch: 7644 Training Loss: 0.21726542226208104 Test Loss: 0.213756939224773\n",
      "Epoch: 7645 Training Loss: 0.21726362200994068 Test Loss: 0.21407184355752892\n",
      "Epoch: 7646 Training Loss: 0.21726296849270876 Test Loss: 0.2138432841702506\n",
      "Epoch: 7647 Training Loss: 0.2172643049887945 Test Loss: 0.21389622408277584\n",
      "Epoch: 7648 Training Loss: 0.2172649125114675 Test Loss: 0.21383804721514282\n",
      "Epoch: 7649 Training Loss: 0.2172642391618839 Test Loss: 0.213844087861381\n",
      "Epoch: 7650 Training Loss: 0.21726285156073266 Test Loss: 0.21373084518855529\n",
      "Epoch: 7651 Training Loss: 0.21726382374046224 Test Loss: 0.21376465206707285\n",
      "Epoch: 7652 Training Loss: 0.2172644508534103 Test Loss: 0.2137960608349594\n",
      "Epoch: 7653 Training Loss: 0.2172666073616518 Test Loss: 0.21373810433424922\n",
      "Epoch: 7654 Training Loss: 0.21726179953157973 Test Loss: 0.2137115306759053\n",
      "Epoch: 7655 Training Loss: 0.21726598140529205 Test Loss: 0.21379261274075476\n",
      "Epoch: 7656 Training Loss: 0.21726341902420696 Test Loss: 0.2137198138796525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7657 Training Loss: 0.21726619763351376 Test Loss: 0.21469994113869864\n",
      "Epoch: 7658 Training Loss: 0.2172609952723363 Test Loss: 0.21523509572946534\n",
      "Epoch: 7659 Training Loss: 0.21726250266555414 Test Loss: 0.2144794316254845\n",
      "Epoch: 7660 Training Loss: 0.21726501963485736 Test Loss: 0.21385627285593872\n",
      "Epoch: 7661 Training Loss: 0.2172642103906286 Test Loss: 0.21383436579125517\n",
      "Epoch: 7662 Training Loss: 0.21726467922132656 Test Loss: 0.2137666353693785\n",
      "Epoch: 7663 Training Loss: 0.2172642466752251 Test Loss: 0.21375459296518265\n",
      "Epoch: 7664 Training Loss: 0.21726439015493784 Test Loss: 0.21373120814583996\n",
      "Epoch: 7665 Training Loss: 0.2172649834399189 Test Loss: 0.2137929108842386\n",
      "Epoch: 7666 Training Loss: 0.21726388989910753 Test Loss: 0.21389172600499762\n",
      "Epoch: 7667 Training Loss: 0.217266061407134 Test Loss: 0.21376922792141206\n",
      "Epoch: 7668 Training Loss: 0.21726449339613588 Test Loss: 0.21376355023245858\n",
      "Epoch: 7669 Training Loss: 0.21726594581106223 Test Loss: 0.2137106751337342\n",
      "Epoch: 7670 Training Loss: 0.21726607157435227 Test Loss: 0.21372358604286132\n",
      "Epoch: 7671 Training Loss: 0.21726463133498364 Test Loss: 0.21818376071035095\n",
      "Epoch: 7672 Training Loss: 0.2172667505813563 Test Loss: 0.2139266087926091\n",
      "Epoch: 7673 Training Loss: 0.21726540390908647 Test Loss: 0.21380339775721435\n",
      "Epoch: 7674 Training Loss: 0.21726400316407105 Test Loss: 0.2140812804469311\n",
      "Epoch: 7675 Training Loss: 0.21726578317143283 Test Loss: 0.21416193474069503\n",
      "Epoch: 7676 Training Loss: 0.21726201636051012 Test Loss: 0.21396549707311247\n",
      "Epoch: 7677 Training Loss: 0.21726599859273246 Test Loss: 0.21392216256587157\n",
      "Epoch: 7678 Training Loss: 0.2172635820493658 Test Loss: 0.21381961417018425\n",
      "Epoch: 7679 Training Loss: 0.21726235121524434 Test Loss: 0.2138795021221594\n",
      "Epoch: 7680 Training Loss: 0.21726333756093943 Test Loss: 0.21379948300364368\n",
      "Epoch: 7681 Training Loss: 0.21726311889401986 Test Loss: 0.2138662152929874\n",
      "Epoch: 7682 Training Loss: 0.2172641299135992 Test Loss: 0.21374094317872597\n",
      "Epoch: 7683 Training Loss: 0.2172638896480651 Test Loss: 0.21375848179323298\n",
      "Epoch: 7684 Training Loss: 0.2172646489079535 Test Loss: 0.21379153683166083\n",
      "Epoch: 7685 Training Loss: 0.21726501362777073 Test Loss: 0.21371837501327387\n",
      "Epoch: 7686 Training Loss: 0.2172628695909584 Test Loss: 0.21369311059370685\n",
      "Epoch: 7687 Training Loss: 0.21726520553177445 Test Loss: 0.21369978641519327\n",
      "Epoch: 7688 Training Loss: 0.21726728696936448 Test Loss: 0.21368786067583892\n",
      "Epoch: 7689 Training Loss: 0.21726602396594918 Test Loss: 0.213754929996947\n",
      "Epoch: 7690 Training Loss: 0.21726492503669145 Test Loss: 0.21367850156299778\n",
      "Epoch: 7691 Training Loss: 0.21726547031877416 Test Loss: 0.21385800986580122\n",
      "Epoch: 7692 Training Loss: 0.21726381987620202 Test Loss: 0.21385571545725152\n",
      "Epoch: 7693 Training Loss: 0.2172657558616031 Test Loss: 0.21382616036406898\n",
      "Epoch: 7694 Training Loss: 0.2172647546237126 Test Loss: 0.21374497459713815\n",
      "Epoch: 7695 Training Loss: 0.2172659717760218 Test Loss: 0.21391048311896038\n",
      "Epoch: 7696 Training Loss: 0.21726482691496574 Test Loss: 0.21391652376519857\n",
      "Epoch: 7697 Training Loss: 0.2172633782746416 Test Loss: 0.2137861572861912\n",
      "Epoch: 7698 Training Loss: 0.21726566350488744 Test Loss: 0.2137832147396331\n",
      "Epoch: 7699 Training Loss: 0.21726497539759543 Test Loss: 0.21378908686998913\n",
      "Epoch: 7700 Training Loss: 0.2172638962379288 Test Loss: 0.2137423042685436\n",
      "Epoch: 7701 Training Loss: 0.21726401850455648 Test Loss: 0.21377971479438782\n",
      "Epoch: 7702 Training Loss: 0.2172640001156987 Test Loss: 0.2138049792139548\n",
      "Epoch: 7703 Training Loss: 0.2172639924858021 Test Loss: 0.21381082541879048\n",
      "Epoch: 7704 Training Loss: 0.2172657827231428 Test Loss: 0.21371829723671287\n",
      "Epoch: 7705 Training Loss: 0.2172645583354322 Test Loss: 0.21373688583479347\n",
      "Epoch: 7706 Training Loss: 0.21726500221430609 Test Loss: 0.2137070974119279\n",
      "Epoch: 7707 Training Loss: 0.21726504862129187 Test Loss: 0.21367400348521956\n",
      "Epoch: 7708 Training Loss: 0.21726388909218544 Test Loss: 0.21367085353449877\n",
      "Epoch: 7709 Training Loss: 0.2172656305824663 Test Loss: 0.21368587737353323\n",
      "Epoch: 7710 Training Loss: 0.21726500440196153 Test Loss: 0.2137117640055883\n",
      "Epoch: 7711 Training Loss: 0.21726474093293455 Test Loss: 0.213677283063542\n",
      "Epoch: 7712 Training Loss: 0.21726328038602674 Test Loss: 0.21371468062662605\n",
      "Epoch: 7713 Training Loss: 0.2172627452442649 Test Loss: 0.21371846575259504\n",
      "Epoch: 7714 Training Loss: 0.21726358990340744 Test Loss: 0.21368885880837182\n",
      "Epoch: 7715 Training Loss: 0.2172641206160636 Test Loss: 0.21364733908755443\n",
      "Epoch: 7716 Training Loss: 0.21726499002081678 Test Loss: 0.21366409993645136\n",
      "Epoch: 7717 Training Loss: 0.21726441248874803 Test Loss: 0.21375093746681534\n",
      "Epoch: 7718 Training Loss: 0.21726477605197692 Test Loss: 0.21445784862980513\n",
      "Epoch: 7719 Training Loss: 0.21726383052757356 Test Loss: 0.21377659076918737\n",
      "Epoch: 7720 Training Loss: 0.21726401074017285 Test Loss: 0.21379982003540804\n",
      "Epoch: 7721 Training Loss: 0.21726535252608117 Test Loss: 0.213735537707736\n",
      "Epoch: 7722 Training Loss: 0.21726576567018935 Test Loss: 0.21371466766386588\n",
      "Epoch: 7723 Training Loss: 0.2172641400987491 Test Loss: 0.21375874104843634\n",
      "Epoch: 7724 Training Loss: 0.2172649226876516 Test Loss: 0.2137119195587103\n",
      "Epoch: 7725 Training Loss: 0.21726276556973567 Test Loss: 0.21369644202306998\n",
      "Epoch: 7726 Training Loss: 0.2172662978711685 Test Loss: 0.2137181157580705\n",
      "Epoch: 7727 Training Loss: 0.21726544405794315 Test Loss: 0.2136625184797109\n",
      "Epoch: 7728 Training Loss: 0.21726521428239617 Test Loss: 0.21375179300898642\n",
      "Epoch: 7729 Training Loss: 0.21726508532728098 Test Loss: 0.2137627854296087\n",
      "Epoch: 7730 Training Loss: 0.21726408430456973 Test Loss: 0.2155387483863956\n",
      "Epoch: 7731 Training Loss: 0.21726477824859813 Test Loss: 0.21374850046790378\n",
      "Epoch: 7732 Training Loss: 0.21726575061660955 Test Loss: 0.2137698630966603\n",
      "Epoch: 7733 Training Loss: 0.21726583875043298 Test Loss: 0.21375925955884306\n",
      "Epoch: 7734 Training Loss: 0.21726450797452823 Test Loss: 0.21370634557183815\n",
      "Epoch: 7735 Training Loss: 0.2172644870124856 Test Loss: 0.21365086495832006\n",
      "Epoch: 7736 Training Loss: 0.2172640784678333 Test Loss: 0.21374389868804422\n",
      "Epoch: 7737 Training Loss: 0.21726716886286845 Test Loss: 0.21376979828285947\n",
      "Epoch: 7738 Training Loss: 0.21726428171357523 Test Loss: 0.21371990461897367\n",
      "Epoch: 7739 Training Loss: 0.21726522126675513 Test Loss: 0.2138275214538866\n",
      "Epoch: 7740 Training Loss: 0.21726531188410542 Test Loss: 0.2137289785510911\n",
      "Epoch: 7741 Training Loss: 0.21726534800731748 Test Loss: 0.21378230734642137\n",
      "Epoch: 7742 Training Loss: 0.2172644656649135 Test Loss: 0.2137621372916003\n",
      "Epoch: 7743 Training Loss: 0.21726385914641022 Test Loss: 0.2141388091765557\n",
      "Epoch: 7744 Training Loss: 0.21726515856890902 Test Loss: 0.2139437974125916\n",
      "Epoch: 7745 Training Loss: 0.21726491191972463 Test Loss: 0.2139289420894393\n",
      "Epoch: 7746 Training Loss: 0.2172645890970953 Test Loss: 0.21382049563787564\n",
      "Epoch: 7747 Training Loss: 0.21726393351772919 Test Loss: 0.21372695636050493\n",
      "Epoch: 7748 Training Loss: 0.21726464001387896 Test Loss: 0.21387679290528433\n",
      "Epoch: 7749 Training Loss: 0.21726410931915438 Test Loss: 0.21387447257121428\n",
      "Epoch: 7750 Training Loss: 0.21726354665238362 Test Loss: 0.21387111521633084\n",
      "Epoch: 7751 Training Loss: 0.21726489008799932 Test Loss: 0.21380290517232797\n",
      "Epoch: 7752 Training Loss: 0.21726396887884816 Test Loss: 0.21375679663441116\n",
      "Epoch: 7753 Training Loss: 0.21726442273665852 Test Loss: 0.21375286891808035\n",
      "Epoch: 7754 Training Loss: 0.2172645122422495 Test Loss: 0.21376999272426198\n",
      "Epoch: 7755 Training Loss: 0.21726593875497688 Test Loss: 0.21365737226392428\n",
      "Epoch: 7756 Training Loss: 0.2172643824174516 Test Loss: 0.21374334128935701\n",
      "Epoch: 7757 Training Loss: 0.21726335331385174 Test Loss: 0.2143960681148455\n",
      "Epoch: 7758 Training Loss: 0.2172645002460078 Test Loss: 0.21375795032006611\n",
      "Epoch: 7759 Training Loss: 0.21726621256157236 Test Loss: 0.21383139731917675\n",
      "Epoch: 7760 Training Loss: 0.2172629715948759 Test Loss: 0.2141201039136336\n",
      "Epoch: 7761 Training Loss: 0.21726490768786658 Test Loss: 0.2139124534585059\n",
      "Epoch: 7762 Training Loss: 0.2172629874643436 Test Loss: 0.2137119195587103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7763 Training Loss: 0.21726701214066768 Test Loss: 0.21374319869899516\n",
      "Epoch: 7764 Training Loss: 0.21726506979851373 Test Loss: 0.2139668063118894\n",
      "Epoch: 7765 Training Loss: 0.2172645431832286 Test Loss: 0.2137726760156167\n",
      "Epoch: 7766 Training Loss: 0.21726378265916224 Test Loss: 0.21375459296518265\n",
      "Epoch: 7767 Training Loss: 0.21726464844173185 Test Loss: 0.21378337029275513\n",
      "Epoch: 7768 Training Loss: 0.21726398177166995 Test Loss: 0.2137582743890703\n",
      "Epoch: 7769 Training Loss: 0.21726442288907713 Test Loss: 0.21371837501327387\n",
      "Epoch: 7770 Training Loss: 0.21726434756738328 Test Loss: 0.2137363413988664\n",
      "Epoch: 7771 Training Loss: 0.21726440644579817 Test Loss: 0.21376645389073617\n",
      "Epoch: 7772 Training Loss: 0.21726449050018218 Test Loss: 0.21395704535348306\n",
      "Epoch: 7773 Training Loss: 0.2172645245881574 Test Loss: 0.2138979610926383\n",
      "Epoch: 7774 Training Loss: 0.21726466302909 Test Loss: 0.21388038358985081\n",
      "Epoch: 7775 Training Loss: 0.2172627733699825 Test Loss: 0.2138766373521623\n",
      "Epoch: 7776 Training Loss: 0.21726466113730603 Test Loss: 0.21374767085125304\n",
      "Epoch: 7777 Training Loss: 0.21726334667915903 Test Loss: 0.2137094695970386\n",
      "Epoch: 7778 Training Loss: 0.21726522257576206 Test Loss: 0.21385200810784352\n",
      "Epoch: 7779 Training Loss: 0.2172645851880061 Test Loss: 0.21371223066495434\n",
      "Epoch: 7780 Training Loss: 0.2172649085306519 Test Loss: 0.2137035585784021\n",
      "Epoch: 7781 Training Loss: 0.21726384171689317 Test Loss: 0.21397995055069954\n",
      "Epoch: 7782 Training Loss: 0.21726453316842892 Test Loss: 0.21422623003112723\n",
      "Epoch: 7783 Training Loss: 0.21726362917361564 Test Loss: 0.21406004744577625\n",
      "Epoch: 7784 Training Loss: 0.21726341985802644 Test Loss: 0.213749135643152\n",
      "Epoch: 7785 Training Loss: 0.2172663833152516 Test Loss: 0.21378298140995008\n",
      "Epoch: 7786 Training Loss: 0.21726471200029485 Test Loss: 0.21381060505186764\n",
      "Epoch: 7787 Training Loss: 0.21726466982516715 Test Loss: 0.21372606193005336\n",
      "Epoch: 7788 Training Loss: 0.21726468858162276 Test Loss: 0.2136838811084674\n",
      "Epoch: 7789 Training Loss: 0.2172619055970051 Test Loss: 0.21440774756175665\n",
      "Epoch: 7790 Training Loss: 0.21726720246669043 Test Loss: 0.21383711389641075\n",
      "Epoch: 7791 Training Loss: 0.2172643944316249 Test Loss: 0.21380731251078502\n",
      "Epoch: 7792 Training Loss: 0.21726429154905888 Test Loss: 0.21374987452048158\n",
      "Epoch: 7793 Training Loss: 0.21726292188847537 Test Loss: 0.2137906294384491\n",
      "Epoch: 7794 Training Loss: 0.2172643411299382 Test Loss: 0.21377370007366997\n",
      "Epoch: 7795 Training Loss: 0.2172636260086879 Test Loss: 0.2137803240441157\n",
      "Epoch: 7796 Training Loss: 0.21726435719665352 Test Loss: 0.2139491510325409\n",
      "Epoch: 7797 Training Loss: 0.21726558806663815 Test Loss: 0.21377000568702215\n",
      "Epoch: 7798 Training Loss: 0.21726461256956223 Test Loss: 0.21367296646440612\n",
      "Epoch: 7799 Training Loss: 0.21726561612062936 Test Loss: 0.2137556559115164\n",
      "Epoch: 7800 Training Loss: 0.21726358822680267 Test Loss: 0.21378046663447753\n",
      "Epoch: 7801 Training Loss: 0.21726301014782 Test Loss: 0.21373762471212301\n",
      "Epoch: 7802 Training Loss: 0.21726385304069978 Test Loss: 0.21377001864978232\n",
      "Epoch: 7803 Training Loss: 0.21726249311697612 Test Loss: 0.22045688440785283\n",
      "Epoch: 7804 Training Loss: 0.21726642549037933 Test Loss: 0.21408634888615669\n",
      "Epoch: 7805 Training Loss: 0.21726517860747416 Test Loss: 0.21448137603950967\n",
      "Epoch: 7806 Training Loss: 0.21726335506218292 Test Loss: 0.21454978052491505\n",
      "Epoch: 7807 Training Loss: 0.2172644021242821 Test Loss: 0.21412554827290406\n",
      "Epoch: 7808 Training Loss: 0.21726251407005295 Test Loss: 0.21379691637713047\n",
      "Epoch: 7809 Training Loss: 0.21726577477944314 Test Loss: 0.21381260131693347\n",
      "Epoch: 7810 Training Loss: 0.21726382005551806 Test Loss: 0.2137161843068055\n",
      "Epoch: 7811 Training Loss: 0.2172658951184236 Test Loss: 0.21365781299776998\n",
      "Epoch: 7812 Training Loss: 0.21726476118667892 Test Loss: 0.2137420190878199\n",
      "Epoch: 7813 Training Loss: 0.21726440916243586 Test Loss: 0.21375530591699188\n",
      "Epoch: 7814 Training Loss: 0.2172643292681835 Test Loss: 0.21366815728038388\n",
      "Epoch: 7815 Training Loss: 0.2172646169897021 Test Loss: 0.21372827856204205\n",
      "Epoch: 7816 Training Loss: 0.21726437728901346 Test Loss: 0.21373291923018212\n",
      "Epoch: 7817 Training Loss: 0.21726387164473676 Test Loss: 0.21379586639355685\n",
      "Epoch: 7818 Training Loss: 0.21726288580112652 Test Loss: 0.21436840558464745\n",
      "Epoch: 7819 Training Loss: 0.21726541332317748 Test Loss: 0.213844165637942\n",
      "Epoch: 7820 Training Loss: 0.2172647236827335 Test Loss: 0.21374086540216497\n",
      "Epoch: 7821 Training Loss: 0.21726374266272416 Test Loss: 0.2145980797693002\n",
      "Epoch: 7822 Training Loss: 0.21726047095229623 Test Loss: 0.21490193983039318\n",
      "Epoch: 7823 Training Loss: 0.2172645947455499 Test Loss: 0.2142259967014442\n",
      "Epoch: 7824 Training Loss: 0.21726529945750528 Test Loss: 0.21386640973438995\n",
      "Epoch: 7825 Training Loss: 0.21726466615815454 Test Loss: 0.21400880565483302\n",
      "Epoch: 7826 Training Loss: 0.2172645000218628 Test Loss: 0.2138281177408543\n",
      "Epoch: 7827 Training Loss: 0.21726300781671176 Test Loss: 0.21369025878646994\n",
      "Epoch: 7828 Training Loss: 0.21726552232938554 Test Loss: 0.2137285378172454\n",
      "Epoch: 7829 Training Loss: 0.21726467944547156 Test Loss: 0.21370634557183815\n",
      "Epoch: 7830 Training Loss: 0.2172650115028759 Test Loss: 0.21376598723137014\n",
      "Epoch: 7831 Training Loss: 0.2172645508579542 Test Loss: 0.2137249989837196\n",
      "Epoch: 7832 Training Loss: 0.21726546734212826 Test Loss: 0.21383721759849209\n",
      "Epoch: 7833 Training Loss: 0.21726371932474428 Test Loss: 0.21375137820066104\n",
      "Epoch: 7834 Training Loss: 0.2172644150350355 Test Loss: 0.21371425285554052\n",
      "Epoch: 7835 Training Loss: 0.21726458780601998 Test Loss: 0.21371020847436817\n",
      "Epoch: 7836 Training Loss: 0.217261643168011 Test Loss: 0.2144133215486288\n",
      "Epoch: 7837 Training Loss: 0.2172656576053904 Test Loss: 0.21378066107588006\n",
      "Epoch: 7838 Training Loss: 0.21726510937355917 Test Loss: 0.21379490714930444\n",
      "Epoch: 7839 Training Loss: 0.21726535270539718 Test Loss: 0.2137169491096554\n",
      "Epoch: 7840 Training Loss: 0.21726478577987096 Test Loss: 0.21377499634968675\n",
      "Epoch: 7841 Training Loss: 0.21726499048703846 Test Loss: 0.21363160229671074\n",
      "Epoch: 7842 Training Loss: 0.21726556398449678 Test Loss: 0.2137489412017495\n",
      "Epoch: 7843 Training Loss: 0.21726293827795953 Test Loss: 0.21368853473936764\n",
      "Epoch: 7844 Training Loss: 0.21726541689156625 Test Loss: 0.21385111367739196\n",
      "Epoch: 7845 Training Loss: 0.2172645411210944 Test Loss: 0.21377826296524902\n",
      "Epoch: 7846 Training Loss: 0.2172639793509037 Test Loss: 0.21364019660670197\n",
      "Epoch: 7847 Training Loss: 0.2172653903348638 Test Loss: 0.21367427570318306\n",
      "Epoch: 7848 Training Loss: 0.21726497171265124 Test Loss: 0.2137873628228868\n",
      "Epoch: 7849 Training Loss: 0.2172651181869415 Test Loss: 0.2137526744766778\n",
      "Epoch: 7850 Training Loss: 0.2172620805466792 Test Loss: 0.2137494208238757\n",
      "Epoch: 7851 Training Loss: 0.21726461843319606 Test Loss: 0.21378342214379578\n",
      "Epoch: 7852 Training Loss: 0.21726370968650824 Test Loss: 0.213746815309082\n",
      "Epoch: 7853 Training Loss: 0.21726208020597876 Test Loss: 0.21477684919477408\n",
      "Epoch: 7854 Training Loss: 0.21726625087243986 Test Loss: 0.2138564024835404\n",
      "Epoch: 7855 Training Loss: 0.21726371045756712 Test Loss: 0.2137369895368748\n",
      "Epoch: 7856 Training Loss: 0.21726488417953646 Test Loss: 0.21433594683318732\n",
      "Epoch: 7857 Training Loss: 0.217263599066456 Test Loss: 0.21372339160145878\n",
      "Epoch: 7858 Training Loss: 0.2172665505543369 Test Loss: 0.21372712487638712\n",
      "Epoch: 7859 Training Loss: 0.21726610998384357 Test Loss: 0.21365337973379261\n",
      "Epoch: 7860 Training Loss: 0.21726502954206742 Test Loss: 0.21376303172205188\n",
      "Epoch: 7861 Training Loss: 0.21726441455984805 Test Loss: 0.2136646832606589\n",
      "Epoch: 7862 Training Loss: 0.21726531762221804 Test Loss: 0.21370603446559414\n",
      "Epoch: 7863 Training Loss: 0.2172652443716242 Test Loss: 0.2139551787160189\n",
      "Epoch: 7864 Training Loss: 0.2172660092261724 Test Loss: 0.21405742896822236\n",
      "Epoch: 7865 Training Loss: 0.21726447671078028 Test Loss: 0.21382174006285176\n",
      "Epoch: 7866 Training Loss: 0.2172637813501553 Test Loss: 0.21382430668936497\n",
      "Epoch: 7867 Training Loss: 0.21726437230402812 Test Loss: 0.21384211752183552\n",
      "Epoch: 7868 Training Loss: 0.21726401361819495 Test Loss: 0.21382135118004672\n",
      "Epoch: 7869 Training Loss: 0.2172646723176598 Test Loss: 0.2138695207968302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7870 Training Loss: 0.21726479704988275 Test Loss: 0.21381059208910747\n",
      "Epoch: 7871 Training Loss: 0.21726348066408854 Test Loss: 0.21376727054462674\n",
      "Epoch: 7872 Training Loss: 0.21726559933664996 Test Loss: 0.21371471951490656\n",
      "Epoch: 7873 Training Loss: 0.21726484645144603 Test Loss: 0.21369766052252576\n",
      "Epoch: 7874 Training Loss: 0.21726442013657624 Test Loss: 0.21370944367151826\n",
      "Epoch: 7875 Training Loss: 0.21726633676481305 Test Loss: 0.21380455144286928\n",
      "Epoch: 7876 Training Loss: 0.2172641328454161 Test Loss: 0.21377648706710603\n",
      "Epoch: 7877 Training Loss: 0.21726508339963377 Test Loss: 0.2137569910758137\n",
      "Epoch: 7878 Training Loss: 0.2172625818335766 Test Loss: 0.21378055737379872\n",
      "Epoch: 7879 Training Loss: 0.21726537395434545 Test Loss: 0.21375137820066104\n",
      "Epoch: 7880 Training Loss: 0.2172652508538983 Test Loss: 0.21380413663454392\n",
      "Epoch: 7881 Training Loss: 0.2172656174923969 Test Loss: 0.21380201074187638\n",
      "Epoch: 7882 Training Loss: 0.21726554287003552 Test Loss: 0.21376653166729717\n",
      "Epoch: 7883 Training Loss: 0.2172643701612017 Test Loss: 0.21376838534200115\n",
      "Epoch: 7884 Training Loss: 0.21726257940384455 Test Loss: 0.21375858549531432\n",
      "Epoch: 7885 Training Loss: 0.21726500768344467 Test Loss: 0.2138408342085789\n",
      "Epoch: 7886 Training Loss: 0.21726392151252172 Test Loss: 0.21377743334859828\n",
      "Epoch: 7887 Training Loss: 0.21726566476009956 Test Loss: 0.21375321891260488\n",
      "Epoch: 7888 Training Loss: 0.21726494966574667 Test Loss: 0.21379113498609562\n",
      "Epoch: 7889 Training Loss: 0.21726272720507336 Test Loss: 0.21377092604299405\n",
      "Epoch: 7890 Training Loss: 0.21726286246314663 Test Loss: 0.21640666999342945\n",
      "Epoch: 7891 Training Loss: 0.21726388009052128 Test Loss: 0.2142191523640756\n",
      "Epoch: 7892 Training Loss: 0.21726457942299607 Test Loss: 0.21392365328329085\n",
      "Epoch: 7893 Training Loss: 0.21726385812430893 Test Loss: 0.21376968161801796\n",
      "Epoch: 7894 Training Loss: 0.21726385730842104 Test Loss: 0.21373405995307687\n",
      "Epoch: 7895 Training Loss: 0.21726498504479727 Test Loss: 0.213653872318679\n",
      "Epoch: 7896 Training Loss: 0.2172645925668603 Test Loss: 0.21372671006806174\n",
      "Epoch: 7897 Training Loss: 0.21726375481138446 Test Loss: 0.21371674170549274\n",
      "Epoch: 7898 Training Loss: 0.21726381408429463 Test Loss: 0.2136885477021278\n",
      "Epoch: 7899 Training Loss: 0.217266916224529 Test Loss: 0.2137674131349886\n",
      "Epoch: 7900 Training Loss: 0.2172634848242202 Test Loss: 0.2137057363221103\n",
      "Epoch: 7901 Training Loss: 0.21726530434386682 Test Loss: 0.21376776312951312\n",
      "Epoch: 7902 Training Loss: 0.21726380122733605 Test Loss: 0.21381575126765423\n",
      "Epoch: 7903 Training Loss: 0.21726298429941585 Test Loss: 0.21375776884142375\n",
      "Epoch: 7904 Training Loss: 0.2172656586005943 Test Loss: 0.2137738037757513\n",
      "Epoch: 7905 Training Loss: 0.2172628154823496 Test Loss: 0.2137459597669109\n",
      "Epoch: 7906 Training Loss: 0.21726510552723055 Test Loss: 0.21379982003540804\n",
      "Epoch: 7907 Training Loss: 0.2172646169269415 Test Loss: 0.21373784507904586\n",
      "Epoch: 7908 Training Loss: 0.21726665538248172 Test Loss: 0.21376222803092146\n",
      "Epoch: 7909 Training Loss: 0.2172653294301779 Test Loss: 0.2137988219028751\n",
      "Epoch: 7910 Training Loss: 0.2172650033170996 Test Loss: 0.2137360302926224\n",
      "Epoch: 7911 Training Loss: 0.21726239303174003 Test Loss: 0.21458465034976637\n",
      "Epoch: 7912 Training Loss: 0.21726752164920457 Test Loss: 0.21386768008488638\n",
      "Epoch: 7913 Training Loss: 0.21726395106380167 Test Loss: 0.21382430668936497\n",
      "Epoch: 7914 Training Loss: 0.21726520403448568 Test Loss: 0.21400373721560742\n",
      "Epoch: 7915 Training Loss: 0.2172637362342449 Test Loss: 0.2138114735567989\n",
      "Epoch: 7916 Training Loss: 0.2172651179179675 Test Loss: 0.2138044995918286\n",
      "Epoch: 7917 Training Loss: 0.2172639589985355 Test Loss: 0.213793481245686\n",
      "Epoch: 7918 Training Loss: 0.2172643336972892 Test Loss: 0.2137532448381252\n",
      "Epoch: 7919 Training Loss: 0.2172638784139165 Test Loss: 0.2137555781349554\n",
      "Epoch: 7920 Training Loss: 0.21726405636713392 Test Loss: 0.21376697240114287\n",
      "Epoch: 7921 Training Loss: 0.2172587887528571 Test Loss: 0.21490923786436764\n",
      "Epoch: 7922 Training Loss: 0.21726679859322043 Test Loss: 0.21384399712205984\n",
      "Epoch: 7923 Training Loss: 0.21726615788811812 Test Loss: 0.21383024363352182\n",
      "Epoch: 7924 Training Loss: 0.21726494122892798 Test Loss: 0.2137145380362642\n",
      "Epoch: 7925 Training Loss: 0.21726548658273712 Test Loss: 0.21377465931792236\n",
      "Epoch: 7926 Training Loss: 0.21726376357993782 Test Loss: 0.21776801906624987\n",
      "Epoch: 7927 Training Loss: 0.21726541284799003 Test Loss: 0.21392772358998355\n",
      "Epoch: 7928 Training Loss: 0.21726439667307512 Test Loss: 0.21376682981078104\n",
      "Epoch: 7929 Training Loss: 0.21726461759041077 Test Loss: 0.21372304160693426\n",
      "Epoch: 7930 Training Loss: 0.21726536163533494 Test Loss: 0.2136439298816303\n",
      "Epoch: 7931 Training Loss: 0.21726591978334203 Test Loss: 0.21375395778993442\n",
      "Epoch: 7932 Training Loss: 0.2172642392784393 Test Loss: 0.2137816851339333\n",
      "Epoch: 7933 Training Loss: 0.21726366646238182 Test Loss: 0.21391870150890677\n",
      "Epoch: 7934 Training Loss: 0.21726550004937015 Test Loss: 0.21389381300938462\n",
      "Epoch: 7935 Training Loss: 0.21726395941992815 Test Loss: 0.21386070611991612\n",
      "Epoch: 7936 Training Loss: 0.21726527399463058 Test Loss: 0.21392384772469336\n",
      "Epoch: 7937 Training Loss: 0.21726348333589723 Test Loss: 0.21386169128968885\n",
      "Epoch: 7938 Training Loss: 0.21726371807849795 Test Loss: 0.21388091506301768\n",
      "Epoch: 7939 Training Loss: 0.21726393605505087 Test Loss: 0.21377325933982425\n",
      "Epoch: 7940 Training Loss: 0.2172646021244041 Test Loss: 0.2137759685566993\n",
      "Epoch: 7941 Training Loss: 0.21726430043416764 Test Loss: 0.21368385518294705\n",
      "Epoch: 7942 Training Loss: 0.21726791548994331 Test Loss: 0.21368029042390094\n",
      "Epoch: 7943 Training Loss: 0.21726549944866147 Test Loss: 0.2184552786848253\n",
      "Epoch: 7944 Training Loss: 0.21726342772999968 Test Loss: 0.21452942899145164\n",
      "Epoch: 7945 Training Loss: 0.217265874559842 Test Loss: 0.21399748916520653\n",
      "Epoch: 7946 Training Loss: 0.21726438482028626 Test Loss: 0.21386861340361846\n",
      "Epoch: 7947 Training Loss: 0.21726506778120852 Test Loss: 0.21370819924654216\n",
      "Epoch: 7948 Training Loss: 0.21726565127553493 Test Loss: 0.2137536466836904\n",
      "Epoch: 7949 Training Loss: 0.21726410997365786 Test Loss: 0.21374423571980858\n",
      "Epoch: 7950 Training Loss: 0.21726534414305726 Test Loss: 0.21376775016675295\n",
      "Epoch: 7951 Training Loss: 0.2172650865197325 Test Loss: 0.2137174416945418\n",
      "Epoch: 7952 Training Loss: 0.21726471792668928 Test Loss: 0.2141533793189843\n",
      "Epoch: 7953 Training Loss: 0.2172662878832662 Test Loss: 0.21378092033108342\n",
      "Epoch: 7954 Training Loss: 0.21726654361480696 Test Loss: 0.21383268063243335\n",
      "Epoch: 7955 Training Loss: 0.2172639691029932 Test Loss: 0.21391460527669376\n",
      "Epoch: 7956 Training Loss: 0.2172644899891315 Test Loss: 0.21378439435080837\n",
      "Epoch: 7957 Training Loss: 0.2172651517549003 Test Loss: 0.2137680871985173\n",
      "Epoch: 7958 Training Loss: 0.21726608977492823 Test Loss: 0.21380726065974434\n",
      "Epoch: 7959 Training Loss: 0.21726498378061931 Test Loss: 0.21396096010705373\n",
      "Epoch: 7960 Training Loss: 0.21726362428725413 Test Loss: 0.21725495301880907\n",
      "Epoch: 7961 Training Loss: 0.2172667749235059 Test Loss: 0.21394794549584528\n",
      "Epoch: 7962 Training Loss: 0.2172639907733341 Test Loss: 0.2137654039071626\n",
      "Epoch: 7963 Training Loss: 0.2172659725381149 Test Loss: 0.21375865030911517\n",
      "Epoch: 7964 Training Loss: 0.21726655962772748 Test Loss: 0.21380990506281858\n",
      "Epoch: 7965 Training Loss: 0.21726409924159412 Test Loss: 0.2137565114536875\n",
      "Epoch: 7966 Training Loss: 0.2172674761029357 Test Loss: 0.21377683706163056\n",
      "Epoch: 7967 Training Loss: 0.2172663474699794 Test Loss: 0.21379672193572793\n",
      "Epoch: 7968 Training Loss: 0.2172643581828916 Test Loss: 0.2137677242412326\n",
      "Epoch: 7969 Training Loss: 0.21726433974023904 Test Loss: 0.21472895179595414\n",
      "Epoch: 7970 Training Loss: 0.21726756082078896 Test Loss: 0.21379808302554557\n",
      "Epoch: 7971 Training Loss: 0.21726472694628504 Test Loss: 0.2137478523298954\n",
      "Epoch: 7972 Training Loss: 0.21726650389630872 Test Loss: 0.21371720836485877\n",
      "Epoch: 7973 Training Loss: 0.2172653098936976 Test Loss: 0.21383614168939816\n",
      "Epoch: 7974 Training Loss: 0.21726602336524053 Test Loss: 0.21366976466264467\n",
      "Epoch: 7975 Training Loss: 0.21726515301907823 Test Loss: 0.2136967142410335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7976 Training Loss: 0.21726532845290558 Test Loss: 0.2137439116508044\n",
      "Epoch: 7977 Training Loss: 0.2172648086247318 Test Loss: 0.2138298158624363\n",
      "Epoch: 7978 Training Loss: 0.21726576381426857 Test Loss: 0.21373552474497584\n",
      "Epoch: 7979 Training Loss: 0.21726418092004085 Test Loss: 0.21386289682638446\n",
      "Epoch: 7980 Training Loss: 0.21726273369631324 Test Loss: 0.21381446795439762\n",
      "Epoch: 7981 Training Loss: 0.21726250104274417 Test Loss: 0.2137384284032534\n",
      "Epoch: 7982 Training Loss: 0.21726271459019142 Test Loss: 0.21367951265829085\n",
      "Epoch: 7983 Training Loss: 0.21726705549031533 Test Loss: 0.21368417925195127\n",
      "Epoch: 7984 Training Loss: 0.2172644264036711 Test Loss: 0.21369959197379076\n",
      "Epoch: 7985 Training Loss: 0.217265455641758 Test Loss: 0.2137268526584236\n",
      "Epoch: 7986 Training Loss: 0.21726307388569904 Test Loss: 0.21377616299810184\n",
      "Epoch: 7987 Training Loss: 0.2172631482211548 Test Loss: 0.21457732639027158\n",
      "Epoch: 7988 Training Loss: 0.2172670833649905 Test Loss: 0.21383864350211054\n",
      "Epoch: 7989 Training Loss: 0.21726470263999864 Test Loss: 0.2137124510318772\n",
      "Epoch: 7990 Training Loss: 0.21726631837595528 Test Loss: 0.21376555946028458\n",
      "Epoch: 7991 Training Loss: 0.21726437928838707 Test Loss: 0.2137214990384743\n",
      "Epoch: 7992 Training Loss: 0.2172654043125475 Test Loss: 0.21375840401667198\n",
      "Epoch: 7993 Training Loss: 0.21726551433189106 Test Loss: 0.2168573851644632\n",
      "Epoch: 7994 Training Loss: 0.21726606793423708 Test Loss: 0.21383340654700275\n",
      "Epoch: 7995 Training Loss: 0.21726490920308694 Test Loss: 0.21371026032540882\n",
      "Epoch: 7996 Training Loss: 0.21726538768098672 Test Loss: 0.2137405672586811\n",
      "Epoch: 7997 Training Loss: 0.2172645431742628 Test Loss: 0.21371528987635394\n",
      "Epoch: 7998 Training Loss: 0.21726480300317458 Test Loss: 0.213724104553268\n",
      "Epoch: 7999 Training Loss: 0.21726520069024194 Test Loss: 0.21371223066495434\n",
      "Epoch: 8000 Training Loss: 0.21726441256047443 Test Loss: 0.2136435280360651\n",
      "Epoch: 8001 Training Loss: 0.21726665865499908 Test Loss: 0.21369651979963097\n",
      "Epoch: 8002 Training Loss: 0.2172646677182039 Test Loss: 0.2148099171959621\n",
      "Epoch: 8003 Training Loss: 0.2172658249879285 Test Loss: 0.21374583013930923\n",
      "Epoch: 8004 Training Loss: 0.21726523299402278 Test Loss: 0.21364407247199213\n",
      "Epoch: 8005 Training Loss: 0.2172663961184154 Test Loss: 0.2137023400789463\n",
      "Epoch: 8006 Training Loss: 0.21726533171645715 Test Loss: 0.21366170182582034\n",
      "Epoch: 8007 Training Loss: 0.21726521955428715 Test Loss: 0.21376262987648667\n",
      "Epoch: 8008 Training Loss: 0.21726097808489586 Test Loss: 0.21463552918342493\n",
      "Epoch: 8009 Training Loss: 0.2172660960240915 Test Loss: 0.21387873731930948\n",
      "Epoch: 8010 Training Loss: 0.21726479416289485 Test Loss: 0.2139035610050308\n",
      "Epoch: 8011 Training Loss: 0.21726507092820466 Test Loss: 0.21378722023252494\n",
      "Epoch: 8012 Training Loss: 0.21726459069300788 Test Loss: 0.21382382706723876\n",
      "Epoch: 8013 Training Loss: 0.21726454574744766 Test Loss: 0.213782125867779\n",
      "Epoch: 8014 Training Loss: 0.21726454927997324 Test Loss: 0.21371215288839332\n",
      "Epoch: 8015 Training Loss: 0.2172650061413269 Test Loss: 0.21372118793223027\n",
      "Epoch: 8016 Training Loss: 0.21726420134413543 Test Loss: 0.2136980753308511\n",
      "Epoch: 8017 Training Loss: 0.21726677060198984 Test Loss: 0.21373453957520308\n",
      "Epoch: 8018 Training Loss: 0.21726467436186242 Test Loss: 0.21369869754333917\n",
      "Epoch: 8019 Training Loss: 0.21726440424021115 Test Loss: 0.2137065400132407\n",
      "Epoch: 8020 Training Loss: 0.21726504788609619 Test Loss: 0.21371163437798663\n",
      "Epoch: 8021 Training Loss: 0.21726457068134014 Test Loss: 0.21388180949346927\n",
      "Epoch: 8022 Training Loss: 0.21726458642528662 Test Loss: 0.21367352386309335\n",
      "Epoch: 8023 Training Loss: 0.21726522073777288 Test Loss: 0.21373409884135738\n",
      "Epoch: 8024 Training Loss: 0.21726167700494384 Test Loss: 0.21465760476399065\n",
      "Epoch: 8025 Training Loss: 0.21726644244470894 Test Loss: 0.21382949179343208\n",
      "Epoch: 8026 Training Loss: 0.21726377441959116 Test Loss: 0.21390190177172932\n",
      "Epoch: 8027 Training Loss: 0.21726464425470282 Test Loss: 0.21402095176111022\n",
      "Epoch: 8028 Training Loss: 0.21726529052756755 Test Loss: 0.21388746125690242\n",
      "Epoch: 8029 Training Loss: 0.21726412185334412 Test Loss: 0.2139103275658384\n",
      "Epoch: 8030 Training Loss: 0.2172641562999514 Test Loss: 0.21380961988209488\n",
      "Epoch: 8031 Training Loss: 0.21726515990481338 Test Loss: 0.21380471995875147\n",
      "Epoch: 8032 Training Loss: 0.21726489788824613 Test Loss: 0.2137061252049153\n",
      "Epoch: 8033 Training Loss: 0.21726487845935546 Test Loss: 0.21364975016094562\n",
      "Epoch: 8034 Training Loss: 0.21726581202338033 Test Loss: 0.2136810163384703\n",
      "Epoch: 8035 Training Loss: 0.21726469716189428 Test Loss: 0.21373218035285255\n",
      "Epoch: 8036 Training Loss: 0.2172637299133552 Test Loss: 0.2149035861009345\n",
      "Epoch: 8037 Training Loss: 0.2172647797907159 Test Loss: 0.2137495504514774\n",
      "Epoch: 8038 Training Loss: 0.21726539550813095 Test Loss: 0.21363178377535308\n",
      "Epoch: 8039 Training Loss: 0.2172629128150848 Test Loss: 0.21371188067042982\n",
      "Epoch: 8040 Training Loss: 0.21726469869504622 Test Loss: 0.2137434709169587\n",
      "Epoch: 8041 Training Loss: 0.21726380007074772 Test Loss: 0.21391349047931932\n",
      "Epoch: 8042 Training Loss: 0.2172636260535169 Test Loss: 0.21368966249950222\n",
      "Epoch: 8043 Training Loss: 0.2172639914368034 Test Loss: 0.21372838226412338\n",
      "Epoch: 8044 Training Loss: 0.21726499439612768 Test Loss: 0.21373727471759849\n",
      "Epoch: 8045 Training Loss: 0.2172648442099958 Test Loss: 0.2137762148491425\n",
      "Epoch: 8046 Training Loss: 0.2172635363865415 Test Loss: 0.21368023857286025\n",
      "Epoch: 8047 Training Loss: 0.21726414573823788 Test Loss: 0.21370187341958027\n",
      "Epoch: 8048 Training Loss: 0.21726484104506805 Test Loss: 0.21369637720926915\n",
      "Epoch: 8049 Training Loss: 0.21726396756087543 Test Loss: 0.2137233527131783\n",
      "Epoch: 8050 Training Loss: 0.21726477387328727 Test Loss: 0.2137753333814511\n",
      "Epoch: 8051 Training Loss: 0.21726562445882425 Test Loss: 0.21373262108669827\n",
      "Epoch: 8052 Training Loss: 0.21726468936164745 Test Loss: 0.21367666085105394\n",
      "Epoch: 8053 Training Loss: 0.21726561487438303 Test Loss: 0.21454101769904163\n",
      "Epoch: 8054 Training Loss: 0.21726247552607464 Test Loss: 0.2148811216375637\n",
      "Epoch: 8055 Training Loss: 0.21726134216814122 Test Loss: 0.21427896253948975\n",
      "Epoch: 8056 Training Loss: 0.2172613896061941 Test Loss: 0.21399063186507777\n",
      "Epoch: 8057 Training Loss: 0.217265509875888 Test Loss: 0.21384493044079192\n",
      "Epoch: 8058 Training Loss: 0.21726462244090908 Test Loss: 0.21369772533632658\n",
      "Epoch: 8059 Training Loss: 0.2172632559452533 Test Loss: 0.21374251167270628\n",
      "Epoch: 8060 Training Loss: 0.21726333774922127 Test Loss: 0.21379458308030025\n",
      "Epoch: 8061 Training Loss: 0.21726337200754672 Test Loss: 0.21374675049528113\n",
      "Epoch: 8062 Training Loss: 0.21726447471140667 Test Loss: 0.21366117035265345\n",
      "Epoch: 8063 Training Loss: 0.21726371884059104 Test Loss: 0.2136428021214957\n",
      "Epoch: 8064 Training Loss: 0.2172658230961445 Test Loss: 0.21368646069774078\n",
      "Epoch: 8065 Training Loss: 0.21726428285223195 Test Loss: 0.21364237435041017\n",
      "Epoch: 8066 Training Loss: 0.21726546916218584 Test Loss: 0.21371990461897367\n",
      "Epoch: 8067 Training Loss: 0.21726620716416017 Test Loss: 0.2137133713878491\n",
      "Epoch: 8068 Training Loss: 0.21726449162987307 Test Loss: 0.21370318265835722\n",
      "Epoch: 8069 Training Loss: 0.21726458676598706 Test Loss: 0.21374570051170755\n",
      "Epoch: 8070 Training Loss: 0.21726608308644071 Test Loss: 0.21588036896785734\n",
      "Epoch: 8071 Training Loss: 0.21726627517872626 Test Loss: 0.214369066685416\n",
      "Epoch: 8072 Training Loss: 0.21726471580179446 Test Loss: 0.21400343907212355\n",
      "Epoch: 8073 Training Loss: 0.21726445981024547 Test Loss: 0.21387037633900127\n",
      "Epoch: 8074 Training Loss: 0.21726421743774815 Test Loss: 0.21383448245609668\n",
      "Epoch: 8075 Training Loss: 0.2172642975292481 Test Loss: 0.21381792901136243\n",
      "Epoch: 8076 Training Loss: 0.217263817670615 Test Loss: 0.21379812191382605\n",
      "Epoch: 8077 Training Loss: 0.21726433263035888 Test Loss: 0.21373973764203036\n",
      "Epoch: 8078 Training Loss: 0.21726548926351158 Test Loss: 0.21376649277901666\n",
      "Epoch: 8079 Training Loss: 0.21726438367266374 Test Loss: 0.21393757528771107\n",
      "Epoch: 8080 Training Loss: 0.217263209878968 Test Loss: 0.21371549728051661\n",
      "Epoch: 8081 Training Loss: 0.21726476330260794 Test Loss: 0.213772313058332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8082 Training Loss: 0.21726598627372196 Test Loss: 0.21378584617994717\n",
      "Epoch: 8083 Training Loss: 0.21726603370280903 Test Loss: 0.2137814906925308\n",
      "Epoch: 8084 Training Loss: 0.21726518258828978 Test Loss: 0.21372571193552883\n",
      "Epoch: 8085 Training Loss: 0.21726436416308084 Test Loss: 0.21415829220508786\n",
      "Epoch: 8086 Training Loss: 0.21726482001129901 Test Loss: 0.21445762826288228\n",
      "Epoch: 8087 Training Loss: 0.21726359851057636 Test Loss: 0.21410154124107333\n",
      "Epoch: 8088 Training Loss: 0.21726248653607821 Test Loss: 0.21384290825020574\n",
      "Epoch: 8089 Training Loss: 0.2172658484066006 Test Loss: 0.21384595449884516\n",
      "Epoch: 8090 Training Loss: 0.21726328544273846 Test Loss: 0.21370259933414967\n",
      "Epoch: 8091 Training Loss: 0.21726511296884535 Test Loss: 0.2136569963438794\n",
      "Epoch: 8092 Training Loss: 0.21726462793694507 Test Loss: 0.21376259098820616\n",
      "Epoch: 8093 Training Loss: 0.21726561412125575 Test Loss: 0.21369215134945443\n",
      "Epoch: 8094 Training Loss: 0.2172638935212911 Test Loss: 0.21400219464714745\n",
      "Epoch: 8095 Training Loss: 0.21726581303651582 Test Loss: 0.21376019287757514\n",
      "Epoch: 8096 Training Loss: 0.21726564700781367 Test Loss: 0.21367030909857174\n",
      "Epoch: 8097 Training Loss: 0.2172661578253575 Test Loss: 0.2137436653583612\n",
      "Epoch: 8098 Training Loss: 0.2172656089659202 Test Loss: 0.21370904182595307\n",
      "Epoch: 8099 Training Loss: 0.21726577282489853 Test Loss: 0.2137618650736368\n",
      "Epoch: 8100 Training Loss: 0.21726607605725276 Test Loss: 0.2138924130312865\n",
      "Epoch: 8101 Training Loss: 0.2172646460209656 Test Loss: 0.2140467865421246\n",
      "Epoch: 8102 Training Loss: 0.21726243278610147 Test Loss: 0.2142640553652968\n",
      "Epoch: 8103 Training Loss: 0.21726577010826084 Test Loss: 0.2140202776975815\n",
      "Epoch: 8104 Training Loss: 0.21726333004759824 Test Loss: 0.21382709368280106\n",
      "Epoch: 8105 Training Loss: 0.21726291617726018 Test Loss: 0.21377345378122678\n",
      "Epoch: 8106 Training Loss: 0.21726573964246917 Test Loss: 0.21378562581302432\n",
      "Epoch: 8107 Training Loss: 0.21726396226208705 Test Loss: 0.21364179102620262\n",
      "Epoch: 8108 Training Loss: 0.21726547501685387 Test Loss: 0.21382625110339015\n",
      "Epoch: 8109 Training Loss: 0.21726413569654082 Test Loss: 0.21372650266389906\n",
      "Epoch: 8110 Training Loss: 0.21726339105090797 Test Loss: 0.21377329822810476\n",
      "Epoch: 8111 Training Loss: 0.21726214307417507 Test Loss: 0.21378189253809599\n",
      "Epoch: 8112 Training Loss: 0.21726425044086148 Test Loss: 0.21378451101564988\n",
      "Epoch: 8113 Training Loss: 0.21726514945068945 Test Loss: 0.2136654091752283\n",
      "Epoch: 8114 Training Loss: 0.21726480624879455 Test Loss: 0.21370409005156898\n",
      "Epoch: 8115 Training Loss: 0.21726439679859635 Test Loss: 0.2137194509223678\n",
      "Epoch: 8116 Training Loss: 0.21726506749430288 Test Loss: 0.21374112465736833\n",
      "Epoch: 8117 Training Loss: 0.21726607616484236 Test Loss: 0.21380535513399967\n",
      "Epoch: 8118 Training Loss: 0.21726409857812484 Test Loss: 0.2137218231074785\n",
      "Epoch: 8119 Training Loss: 0.2172642580886897 Test Loss: 0.21370730481609057\n",
      "Epoch: 8120 Training Loss: 0.21726243228401662 Test Loss: 0.21368608477769593\n",
      "Epoch: 8121 Training Loss: 0.21726636316909687 Test Loss: 0.21381890121837502\n",
      "Epoch: 8122 Training Loss: 0.21726338930257677 Test Loss: 0.2137686186716842\n",
      "Epoch: 8123 Training Loss: 0.21726351074435077 Test Loss: 0.21383041214940401\n",
      "Epoch: 8124 Training Loss: 0.21726283916102992 Test Loss: 0.2141112114601585\n",
      "Epoch: 8125 Training Loss: 0.21726557606143068 Test Loss: 0.21400087244561034\n",
      "Epoch: 8126 Training Loss: 0.21726072709626382 Test Loss: 0.21492054139123393\n",
      "Epoch: 8127 Training Loss: 0.2172657960552888 Test Loss: 0.21388676126785336\n",
      "Epoch: 8128 Training Loss: 0.217263915057145 Test Loss: 0.21381179762580307\n",
      "Epoch: 8129 Training Loss: 0.21726507728495753 Test Loss: 0.2136589796461851\n",
      "Epoch: 8130 Training Loss: 0.2172658335592342 Test Loss: 0.2137526744766778\n",
      "Epoch: 8131 Training Loss: 0.21726484878255428 Test Loss: 0.2138006366892986\n",
      "Epoch: 8132 Training Loss: 0.21726427720377736 Test Loss: 0.21379772006826087\n",
      "Epoch: 8133 Training Loss: 0.2172638409996291 Test Loss: 0.21374312092243417\n",
      "Epoch: 8134 Training Loss: 0.2172645607472327 Test Loss: 0.2138341583870925\n",
      "Epoch: 8135 Training Loss: 0.21726459842152832 Test Loss: 0.21371885463540008\n",
      "Epoch: 8136 Training Loss: 0.2172634458140202 Test Loss: 0.21411612434626207\n",
      "Epoch: 8137 Training Loss: 0.21726551607125646 Test Loss: 0.21389443522187268\n",
      "Epoch: 8138 Training Loss: 0.21726504851370226 Test Loss: 0.21383546762586944\n",
      "Epoch: 8139 Training Loss: 0.21726373499696436 Test Loss: 0.21384318046816927\n",
      "Epoch: 8140 Training Loss: 0.21726470237102463 Test Loss: 0.21373509697389032\n",
      "Epoch: 8141 Training Loss: 0.21726347778606644 Test Loss: 0.21377673335954922\n",
      "Epoch: 8142 Training Loss: 0.21726298514220116 Test Loss: 0.2137271767274278\n",
      "Epoch: 8143 Training Loss: 0.21726443584465951 Test Loss: 0.21378777763121218\n",
      "Epoch: 8144 Training Loss: 0.217264331724813 Test Loss: 0.21375855956979398\n",
      "Epoch: 8145 Training Loss: 0.21726337117372724 Test Loss: 0.2137289526255708\n",
      "Epoch: 8146 Training Loss: 0.21726449314509344 Test Loss: 0.21372278235173092\n",
      "Epoch: 8147 Training Loss: 0.21726464712375912 Test Loss: 0.21370240489274717\n",
      "Epoch: 8148 Training Loss: 0.2172643731557792 Test Loss: 0.21375721144273654\n",
      "Epoch: 8149 Training Loss: 0.2172663374372481 Test Loss: 0.21371045476681136\n",
      "Epoch: 8150 Training Loss: 0.21726482785637485 Test Loss: 0.21369457538560582\n",
      "Epoch: 8151 Training Loss: 0.21726415103702626 Test Loss: 0.21375314113604385\n",
      "Epoch: 8152 Training Loss: 0.21726338032781 Test Loss: 0.21369676609207416\n",
      "Epoch: 8153 Training Loss: 0.2172633437832053 Test Loss: 0.21373084518855529\n",
      "Epoch: 8154 Training Loss: 0.21726393226251706 Test Loss: 0.21373514882493097\n",
      "Epoch: 8155 Training Loss: 0.21726803274468826 Test Loss: 0.21378823132781805\n",
      "Epoch: 8156 Training Loss: 0.21726459898637376 Test Loss: 0.21374524681510168\n",
      "Epoch: 8157 Training Loss: 0.21726618976154052 Test Loss: 0.2137664020396955\n",
      "Epoch: 8158 Training Loss: 0.2172646769709105 Test Loss: 0.21383454726989753\n",
      "Epoch: 8159 Training Loss: 0.2172649704126101 Test Loss: 0.21454869165306095\n",
      "Epoch: 8160 Training Loss: 0.21726740824078825 Test Loss: 0.21391968667867953\n",
      "Epoch: 8161 Training Loss: 0.21726449313612764 Test Loss: 0.21386245609253876\n",
      "Epoch: 8162 Training Loss: 0.21726569656179556 Test Loss: 0.21383443060505603\n",
      "Epoch: 8163 Training Loss: 0.2172650442997758 Test Loss: 0.213851891443002\n",
      "Epoch: 8164 Training Loss: 0.2172635952021958 Test Loss: 0.2137653520561219\n",
      "Epoch: 8165 Training Loss: 0.21726474681449998 Test Loss: 0.21374852639342412\n",
      "Epoch: 8166 Training Loss: 0.21726486326232283 Test Loss: 0.213754696667264\n",
      "Epoch: 8167 Training Loss: 0.21726563279701916 Test Loss: 0.2136922161632553\n",
      "Epoch: 8168 Training Loss: 0.21726516611811342 Test Loss: 0.2137892553858713\n",
      "Epoch: 8169 Training Loss: 0.21726496093575848 Test Loss: 0.213716223195086\n",
      "Epoch: 8170 Training Loss: 0.2172639646021611 Test Loss: 0.21498965882844856\n",
      "Epoch: 8171 Training Loss: 0.21726397504731923 Test Loss: 0.2137801555282335\n",
      "Epoch: 8172 Training Loss: 0.2172646750253317 Test Loss: 0.21376745202326908\n",
      "Epoch: 8173 Training Loss: 0.21726320557538353 Test Loss: 0.21367316090580865\n",
      "Epoch: 8174 Training Loss: 0.2172657847942428 Test Loss: 0.21376304468481205\n",
      "Epoch: 8175 Training Loss: 0.21726542924643996 Test Loss: 0.21373548585669533\n",
      "Epoch: 8176 Training Loss: 0.2172655096696746 Test Loss: 0.2137025085948285\n",
      "Epoch: 8177 Training Loss: 0.2172627548018087 Test Loss: 0.21372843411516407\n",
      "Epoch: 8178 Training Loss: 0.21726329264227665 Test Loss: 0.21375031525432728\n",
      "Epoch: 8179 Training Loss: 0.2172644730885967 Test Loss: 0.21369934568134757\n",
      "Epoch: 8180 Training Loss: 0.21726701629183354 Test Loss: 0.21371901018852207\n",
      "Epoch: 8181 Training Loss: 0.21726493006650577 Test Loss: 0.21378531470678028\n",
      "Epoch: 8182 Training Loss: 0.21726494733463844 Test Loss: 0.21366133886853564\n",
      "Epoch: 8183 Training Loss: 0.21726441861239007 Test Loss: 0.2137345266124429\n",
      "Epoch: 8184 Training Loss: 0.21726477942311806 Test Loss: 0.21369628646994795\n",
      "Epoch: 8185 Training Loss: 0.21726591217137703 Test Loss: 0.2137309618533968\n",
      "Epoch: 8186 Training Loss: 0.21726368797133833 Test Loss: 0.21377989627303015\n",
      "Epoch: 8187 Training Loss: 0.2172648478321794 Test Loss: 0.21368626625633827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8188 Training Loss: 0.21726501312568586 Test Loss: 0.2137816851339333\n",
      "Epoch: 8189 Training Loss: 0.21726620412475364 Test Loss: 0.2137866369083174\n",
      "Epoch: 8190 Training Loss: 0.21726551394636162 Test Loss: 0.21371644356200886\n",
      "Epoch: 8191 Training Loss: 0.2172644542873121 Test Loss: 0.21372296383037326\n",
      "Epoch: 8192 Training Loss: 0.21726483532488705 Test Loss: 0.21370285858935303\n",
      "Epoch: 8193 Training Loss: 0.21726551987275608 Test Loss: 0.21375916881952187\n",
      "Epoch: 8194 Training Loss: 0.21726519230721802 Test Loss: 0.21393321980029467\n",
      "Epoch: 8195 Training Loss: 0.2172636934135795 Test Loss: 0.21377478894552404\n",
      "Epoch: 8196 Training Loss: 0.21726673515121286 Test Loss: 0.21378324066515345\n",
      "Epoch: 8197 Training Loss: 0.21726449678520865 Test Loss: 0.213717999093229\n",
      "Epoch: 8198 Training Loss: 0.2172648135738539 Test Loss: 0.21369674016655385\n",
      "Epoch: 8199 Training Loss: 0.2172651425470227 Test Loss: 0.21373894691366013\n",
      "Epoch: 8200 Training Loss: 0.21726456163484698 Test Loss: 0.21371815464635102\n",
      "Epoch: 8201 Training Loss: 0.21726454715507842 Test Loss: 0.2137621372916003\n",
      "Epoch: 8202 Training Loss: 0.21726496725664815 Test Loss: 0.21382369743963708\n",
      "Epoch: 8203 Training Loss: 0.21726370273801252 Test Loss: 0.21373084518855529\n",
      "Epoch: 8204 Training Loss: 0.21726116935232773 Test Loss: 0.2136977642246071\n",
      "Epoch: 8205 Training Loss: 0.2172620787804164 Test Loss: 0.21448766297819102\n",
      "Epoch: 8206 Training Loss: 0.21726542052271564 Test Loss: 0.21426790530506665\n",
      "Epoch: 8207 Training Loss: 0.21726328415166313 Test Loss: 0.21428680500939126\n",
      "Epoch: 8208 Training Loss: 0.2172638990621561 Test Loss: 0.21387618365555644\n",
      "Epoch: 8209 Training Loss: 0.21726239255655255 Test Loss: 0.2137558244273986\n",
      "Epoch: 8210 Training Loss: 0.21726572021357848 Test Loss: 0.21382251782846182\n",
      "Epoch: 8211 Training Loss: 0.21726361490006052 Test Loss: 0.21367198129463338\n",
      "Epoch: 8212 Training Loss: 0.2172647059394134 Test Loss: 0.2137686186716842\n",
      "Epoch: 8213 Training Loss: 0.21726371410664813 Test Loss: 0.2136943420559228\n",
      "Epoch: 8214 Training Loss: 0.21726440525334664 Test Loss: 0.21374943378663588\n",
      "Epoch: 8215 Training Loss: 0.21726408934334987 Test Loss: 0.2137932738415233\n",
      "Epoch: 8216 Training Loss: 0.21726521941980012 Test Loss: 0.21374070984904295\n",
      "Epoch: 8217 Training Loss: 0.21726570314269347 Test Loss: 0.213696597576192\n",
      "Epoch: 8218 Training Loss: 0.21726312439902165 Test Loss: 0.21603103512928745\n",
      "Epoch: 8219 Training Loss: 0.2172640374223965 Test Loss: 0.21378414805836518\n",
      "Epoch: 8220 Training Loss: 0.217264124471358 Test Loss: 0.21374023022691674\n",
      "Epoch: 8221 Training Loss: 0.2172652798672302 Test Loss: 0.21374527274062202\n",
      "Epoch: 8222 Training Loss: 0.21726468181244302 Test Loss: 0.21371083068685623\n",
      "Epoch: 8223 Training Loss: 0.21726384419145423 Test Loss: 0.21368616255425693\n",
      "Epoch: 8224 Training Loss: 0.21726603962920346 Test Loss: 0.2137337229213125\n",
      "Epoch: 8225 Training Loss: 0.21726510706038252 Test Loss: 0.21376199470123844\n",
      "Epoch: 8226 Training Loss: 0.21726452593302756 Test Loss: 0.21373796174388737\n",
      "Epoch: 8227 Training Loss: 0.21726387169853156 Test Loss: 0.2163174084269141\n",
      "Epoch: 8228 Training Loss: 0.21726601742988028 Test Loss: 0.21374697086220398\n",
      "Epoch: 8229 Training Loss: 0.21726468587395087 Test Loss: 0.21377929998606243\n",
      "Epoch: 8230 Training Loss: 0.21726568994503445 Test Loss: 0.21374713937808618\n",
      "Epoch: 8231 Training Loss: 0.21726498236402278 Test Loss: 0.21371925648096526\n",
      "Epoch: 8232 Training Loss: 0.2172637232338335 Test Loss: 0.21370502337030106\n",
      "Epoch: 8233 Training Loss: 0.21726535246332057 Test Loss: 0.2138047069959913\n",
      "Epoch: 8234 Training Loss: 0.2172644947948008 Test Loss: 0.21367658307449294\n",
      "Epoch: 8235 Training Loss: 0.21726688806294822 Test Loss: 0.21374811158509877\n",
      "Epoch: 8236 Training Loss: 0.21726505105102392 Test Loss: 0.2137394394985465\n",
      "Epoch: 8237 Training Loss: 0.2172643140711509 Test Loss: 0.21376244839784433\n",
      "Epoch: 8238 Training Loss: 0.21726425591000006 Test Loss: 0.21375806698490762\n",
      "Epoch: 8239 Training Loss: 0.21726344059592406 Test Loss: 0.21368700513366784\n",
      "Epoch: 8240 Training Loss: 0.21726560660791455 Test Loss: 0.2137392061688635\n",
      "Epoch: 8241 Training Loss: 0.21726501690028807 Test Loss: 0.2137403728172786\n",
      "Epoch: 8242 Training Loss: 0.2172657680102634 Test Loss: 0.21391104051764762\n",
      "Epoch: 8243 Training Loss: 0.21726458019405495 Test Loss: 0.21370791406581846\n",
      "Epoch: 8244 Training Loss: 0.21726397088718757 Test Loss: 0.21370227526514549\n",
      "Epoch: 8245 Training Loss: 0.21726466688438442 Test Loss: 0.21375910400572104\n",
      "Epoch: 8246 Training Loss: 0.21726342472645638 Test Loss: 0.21374192834849873\n",
      "Epoch: 8247 Training Loss: 0.21726458182583072 Test Loss: 0.21375933733540406\n",
      "Epoch: 8248 Training Loss: 0.21726514612437728 Test Loss: 0.21377804259832617\n",
      "Epoch: 8249 Training Loss: 0.2172660158877625 Test Loss: 0.2136861366287366\n",
      "Epoch: 8250 Training Loss: 0.2172652855605138 Test Loss: 0.21369457538560582\n",
      "Epoch: 8251 Training Loss: 0.21726541315282724 Test Loss: 0.21369261800882047\n",
      "Epoch: 8252 Training Loss: 0.21726400443721477 Test Loss: 0.21370262525967001\n",
      "Epoch: 8253 Training Loss: 0.21726476165290057 Test Loss: 0.21369934568134757\n",
      "Epoch: 8254 Training Loss: 0.21726607854077962 Test Loss: 0.21372200458612084\n",
      "Epoch: 8255 Training Loss: 0.2172644657545715 Test Loss: 0.21374331536383667\n",
      "Epoch: 8256 Training Loss: 0.21726594336339858 Test Loss: 0.21374852639342412\n",
      "Epoch: 8257 Training Loss: 0.21726476551716079 Test Loss: 0.21397737096142613\n",
      "Epoch: 8258 Training Loss: 0.21726855273424647 Test Loss: 0.21380580883060554\n",
      "Epoch: 8259 Training Loss: 0.2172647384494077 Test Loss: 0.21374505237369917\n",
      "Epoch: 8260 Training Loss: 0.21726607512480947 Test Loss: 0.21375636886332564\n",
      "Epoch: 8261 Training Loss: 0.21726414108498718 Test Loss: 0.21373652287750877\n",
      "Epoch: 8262 Training Loss: 0.21726414462647856 Test Loss: 0.21376130767494955\n",
      "Epoch: 8263 Training Loss: 0.21726531763118384 Test Loss: 0.213743107959674\n",
      "Epoch: 8264 Training Loss: 0.21726548919178518 Test Loss: 0.2137242341808697\n",
      "Epoch: 8265 Training Loss: 0.2172646190159731 Test Loss: 0.213743030183113\n",
      "Epoch: 8266 Training Loss: 0.21726414631204916 Test Loss: 0.21369899568682305\n",
      "Epoch: 8267 Training Loss: 0.2172653779620585 Test Loss: 0.21375540961907322\n",
      "Epoch: 8268 Training Loss: 0.2172657632583889 Test Loss: 0.2136988012454205\n",
      "Epoch: 8269 Training Loss: 0.2172650600616539 Test Loss: 0.21366771654653818\n",
      "Epoch: 8270 Training Loss: 0.2172667721172102 Test Loss: 0.21373584881398003\n",
      "Epoch: 8271 Training Loss: 0.21726606537898382 Test Loss: 0.2136941735400406\n",
      "Epoch: 8272 Training Loss: 0.217264646092692 Test Loss: 0.21377192417552698\n",
      "Epoch: 8273 Training Loss: 0.21726351975498073 Test Loss: 0.21398529120788867\n",
      "Epoch: 8274 Training Loss: 0.21726243503651751 Test Loss: 0.2140158444336041\n",
      "Epoch: 8275 Training Loss: 0.21726498876560466 Test Loss: 0.213871283732213\n",
      "Epoch: 8276 Training Loss: 0.21726379366020002 Test Loss: 0.21378632580207338\n",
      "Epoch: 8277 Training Loss: 0.217263282986109 Test Loss: 0.21371491395630907\n",
      "Epoch: 8278 Training Loss: 0.21726442865408715 Test Loss: 0.21372047498042104\n",
      "Epoch: 8279 Training Loss: 0.21726361151098775 Test Loss: 0.21370735666713125\n",
      "Epoch: 8280 Training Loss: 0.2172644357908647 Test Loss: 0.2139993169143902\n",
      "Epoch: 8281 Training Loss: 0.21726484416516678 Test Loss: 0.21368000524317723\n",
      "Epoch: 8282 Training Loss: 0.2172648787372953 Test Loss: 0.21381890121837502\n",
      "Epoch: 8283 Training Loss: 0.21726535487512103 Test Loss: 0.21373696361135447\n",
      "Epoch: 8284 Training Loss: 0.21726442522018538 Test Loss: 0.21372542675480513\n",
      "Epoch: 8285 Training Loss: 0.21726430829717508 Test Loss: 0.21377874258737523\n",
      "Epoch: 8286 Training Loss: 0.2172652574616936 Test Loss: 0.21383968052292396\n",
      "Epoch: 8287 Training Loss: 0.21726462668173294 Test Loss: 0.2137654298326829\n",
      "Epoch: 8288 Training Loss: 0.21726444850437046 Test Loss: 0.21378799799813503\n",
      "Epoch: 8289 Training Loss: 0.21726396701396158 Test Loss: 0.21369746608112322\n",
      "Epoch: 8290 Training Loss: 0.21726494836570553 Test Loss: 0.21370445300885366\n",
      "Epoch: 8291 Training Loss: 0.21726590004064833 Test Loss: 0.21374562273514655\n",
      "Epoch: 8292 Training Loss: 0.21726452138736646 Test Loss: 0.2137005771435635\n",
      "Epoch: 8293 Training Loss: 0.21726618067021836 Test Loss: 0.21378967019419667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8294 Training Loss: 0.21726544142199766 Test Loss: 0.2136994104951484\n",
      "Epoch: 8295 Training Loss: 0.2172657379031038 Test Loss: 0.21521585899337634\n",
      "Epoch: 8296 Training Loss: 0.2172661418931292 Test Loss: 0.21375034117984762\n",
      "Epoch: 8297 Training Loss: 0.21726483534281865 Test Loss: 0.21369867161781883\n",
      "Epoch: 8298 Training Loss: 0.21726483429381993 Test Loss: 0.21377891110325742\n",
      "Epoch: 8299 Training Loss: 0.21726411179371546 Test Loss: 0.2136709313110598\n",
      "Epoch: 8300 Training Loss: 0.21726589034861749 Test Loss: 0.21369685683139536\n",
      "Epoch: 8301 Training Loss: 0.2172646168910783 Test Loss: 0.21389300931825422\n",
      "Epoch: 8302 Training Loss: 0.21726292316161913 Test Loss: 0.2140704176539105\n",
      "Epoch: 8303 Training Loss: 0.21726538494641742 Test Loss: 0.21400898713347535\n",
      "Epoch: 8304 Training Loss: 0.21726280249986982 Test Loss: 0.21376898162896887\n",
      "Epoch: 8305 Training Loss: 0.21726288391830834 Test Loss: 0.21377525560489008\n",
      "Epoch: 8306 Training Loss: 0.21726101838617118 Test Loss: 0.2143265358693055\n",
      "Epoch: 8307 Training Loss: 0.21726550575161957 Test Loss: 0.2137618650736368\n",
      "Epoch: 8308 Training Loss: 0.21726366476784545 Test Loss: 0.21372327493661727\n",
      "Epoch: 8309 Training Loss: 0.21726489272394478 Test Loss: 0.21368779586203807\n",
      "Epoch: 8310 Training Loss: 0.2172649573942671 Test Loss: 0.21423590025021239\n",
      "Epoch: 8311 Training Loss: 0.217265769695834 Test Loss: 0.21376146322807157\n",
      "Epoch: 8312 Training Loss: 0.21726441195079996 Test Loss: 0.21374598569243125\n",
      "Epoch: 8313 Training Loss: 0.2172654165150026 Test Loss: 0.21374746344709036\n",
      "Epoch: 8314 Training Loss: 0.2172643749937684 Test Loss: 0.21380077927966046\n",
      "Epoch: 8315 Training Loss: 0.21726467743713215 Test Loss: 0.21375480036934533\n",
      "Epoch: 8316 Training Loss: 0.21726495899017967 Test Loss: 0.21374023022691674\n",
      "Epoch: 8317 Training Loss: 0.217265132352907 Test Loss: 0.21377185936172613\n",
      "Epoch: 8318 Training Loss: 0.2172635335533484 Test Loss: 0.21378894427962727\n",
      "Epoch: 8319 Training Loss: 0.2172658716369909 Test Loss: 0.21382544741225976\n",
      "Epoch: 8320 Training Loss: 0.21726381560848076 Test Loss: 0.21382109192484336\n",
      "Epoch: 8321 Training Loss: 0.21726382193833627 Test Loss: 0.2137726371273362\n",
      "Epoch: 8322 Training Loss: 0.21726538010488491 Test Loss: 0.21377559263665447\n",
      "Epoch: 8323 Training Loss: 0.2172628078614188 Test Loss: 0.21453576778117367\n",
      "Epoch: 8324 Training Loss: 0.2172659913931943 Test Loss: 0.21380779213291123\n",
      "Epoch: 8325 Training Loss: 0.2172633052123296 Test Loss: 0.2137261656321347\n",
      "Epoch: 8326 Training Loss: 0.2172634707568785 Test Loss: 0.21363978179837662\n",
      "Epoch: 8327 Training Loss: 0.21726425258368792 Test Loss: 0.21375270040219815\n",
      "Epoch: 8328 Training Loss: 0.21726381733888037 Test Loss: 0.21368237742828794\n",
      "Epoch: 8329 Training Loss: 0.21726298247039247 Test Loss: 0.21373053408231124\n",
      "Epoch: 8330 Training Loss: 0.2172637427254848 Test Loss: 0.2137464523517973\n",
      "Epoch: 8331 Training Loss: 0.21726459767736683 Test Loss: 0.21391971260419984\n",
      "Epoch: 8332 Training Loss: 0.21726436512242156 Test Loss: 0.21379406456989355\n",
      "Epoch: 8333 Training Loss: 0.21726532563764409 Test Loss: 0.21378330547895427\n",
      "Epoch: 8334 Training Loss: 0.21726325436727234 Test Loss: 0.213729587800819\n",
      "Epoch: 8335 Training Loss: 0.21726514383809806 Test Loss: 0.21381595867181694\n",
      "Epoch: 8336 Training Loss: 0.21726438369956114 Test Loss: 0.21380334590617367\n",
      "Epoch: 8337 Training Loss: 0.21726351070848757 Test Loss: 0.2137203712783397\n",
      "Epoch: 8338 Training Loss: 0.21726299716534023 Test Loss: 0.21373150628932383\n",
      "Epoch: 8339 Training Loss: 0.21726414006288589 Test Loss: 0.2137836943617593\n",
      "Epoch: 8340 Training Loss: 0.21726468875197297 Test Loss: 0.21374798195749709\n",
      "Epoch: 8341 Training Loss: 0.21726474076258434 Test Loss: 0.21370087528704737\n",
      "Epoch: 8342 Training Loss: 0.21726440180151327 Test Loss: 0.2136606259167264\n",
      "Epoch: 8343 Training Loss: 0.21726532132509382 Test Loss: 0.21372586748865086\n",
      "Epoch: 8344 Training Loss: 0.21726545674455153 Test Loss: 0.21364792241176198\n",
      "Epoch: 8345 Training Loss: 0.2172662495813645 Test Loss: 0.21391137754941197\n",
      "Epoch: 8346 Training Loss: 0.2172643682783835 Test Loss: 0.21770934961373048\n",
      "Epoch: 8347 Training Loss: 0.21726578641705277 Test Loss: 0.21385340808594164\n",
      "Epoch: 8348 Training Loss: 0.2172674826748678 Test Loss: 0.21376230580748248\n",
      "Epoch: 8349 Training Loss: 0.21726476413642742 Test Loss: 0.21375736699585857\n",
      "Epoch: 8350 Training Loss: 0.21726689781773964 Test Loss: 0.21376023176585562\n",
      "Epoch: 8351 Training Loss: 0.2172664765506158 Test Loss: 0.21369199579633244\n",
      "Epoch: 8352 Training Loss: 0.2172659763665119 Test Loss: 0.2137970589674923\n",
      "Epoch: 8353 Training Loss: 0.21726532433760296 Test Loss: 0.2141233964547162\n",
      "Epoch: 8354 Training Loss: 0.21726268559479112 Test Loss: 0.21439714402393942\n",
      "Epoch: 8355 Training Loss: 0.21726339461033095 Test Loss: 0.21400420387497346\n",
      "Epoch: 8356 Training Loss: 0.21726580944122964 Test Loss: 0.21390270546285972\n",
      "Epoch: 8357 Training Loss: 0.2172633173789215 Test Loss: 0.21377515190280874\n",
      "Epoch: 8358 Training Loss: 0.21726544589593236 Test Loss: 0.21379539973419082\n",
      "Epoch: 8359 Training Loss: 0.21726462806246627 Test Loss: 0.21371898426300176\n",
      "Epoch: 8360 Training Loss: 0.21726545132024194 Test Loss: 0.2136543908290857\n",
      "Epoch: 8361 Training Loss: 0.21726532521625144 Test Loss: 0.21364377432850828\n",
      "Epoch: 8362 Training Loss: 0.21726315562690637 Test Loss: 0.21786290647067802\n",
      "Epoch: 8363 Training Loss: 0.2172680987509149 Test Loss: 0.2138385916510699\n",
      "Epoch: 8364 Training Loss: 0.2172641665388961 Test Loss: 0.213716223195086\n",
      "Epoch: 8365 Training Loss: 0.21726442277252173 Test Loss: 0.2138405490278552\n",
      "Epoch: 8366 Training Loss: 0.2172646133137237 Test Loss: 0.21383863053935037\n",
      "Epoch: 8367 Training Loss: 0.2172621123214778 Test Loss: 0.2137386487701763\n",
      "Epoch: 8368 Training Loss: 0.2172627545238689 Test Loss: 0.21404853651474726\n",
      "Epoch: 8369 Training Loss: 0.21726354910004728 Test Loss: 0.21450741822468675\n",
      "Epoch: 8370 Training Loss: 0.21726260963652538 Test Loss: 0.21410152827831316\n",
      "Epoch: 8371 Training Loss: 0.2172623368520312 Test Loss: 0.21387763548469524\n",
      "Epoch: 8372 Training Loss: 0.2172655043170914 Test Loss: 0.2138504396138632\n",
      "Epoch: 8373 Training Loss: 0.21726416332913936 Test Loss: 0.21367396459693905\n",
      "Epoch: 8374 Training Loss: 0.21726498001498293 Test Loss: 0.2137723519466125\n",
      "Epoch: 8375 Training Loss: 0.2172641908362167 Test Loss: 0.21369307170542637\n",
      "Epoch: 8376 Training Loss: 0.2172639618496602 Test Loss: 0.21371531580187428\n",
      "Epoch: 8377 Training Loss: 0.217264421723523 Test Loss: 0.2137124899201577\n",
      "Epoch: 8378 Training Loss: 0.21726515760060253 Test Loss: 0.21372695636050493\n",
      "Epoch: 8379 Training Loss: 0.21726366928660912 Test Loss: 0.2137092233045954\n",
      "Epoch: 8380 Training Loss: 0.21726495284860603 Test Loss: 0.21373273775153978\n",
      "Epoch: 8381 Training Loss: 0.2172640544125893 Test Loss: 0.21365737226392428\n",
      "Epoch: 8382 Training Loss: 0.21726638479460877 Test Loss: 0.21371925648096526\n",
      "Epoch: 8383 Training Loss: 0.21726398723184273 Test Loss: 0.21376872237376554\n",
      "Epoch: 8384 Training Loss: 0.21726500941384427 Test Loss: 0.21380273665644578\n",
      "Epoch: 8385 Training Loss: 0.2172651012684751 Test Loss: 0.2137141750789795\n",
      "Epoch: 8386 Training Loss: 0.2172646115474609 Test Loss: 0.21378392769144233\n",
      "Epoch: 8387 Training Loss: 0.2172641355710196 Test Loss: 0.2137371450899968\n",
      "Epoch: 8388 Training Loss: 0.21726551079936549 Test Loss: 0.21386705787239832\n",
      "Epoch: 8389 Training Loss: 0.21726363214129576 Test Loss: 0.21376903348000956\n",
      "Epoch: 8390 Training Loss: 0.2172649162233091 Test Loss: 0.2137445857143331\n",
      "Epoch: 8391 Training Loss: 0.21726492317180485 Test Loss: 0.21368869029248966\n",
      "Epoch: 8392 Training Loss: 0.21726333271940693 Test Loss: 0.2136930587426662\n",
      "Epoch: 8393 Training Loss: 0.21726730240847372 Test Loss: 0.2137969811909313\n",
      "Epoch: 8394 Training Loss: 0.21726335923128037 Test Loss: 0.2137591169684812\n",
      "Epoch: 8395 Training Loss: 0.21726563961999368 Test Loss: 0.21378259252714504\n",
      "Epoch: 8396 Training Loss: 0.21726414164086685 Test Loss: 0.2138554302765278\n",
      "Epoch: 8397 Training Loss: 0.21726540409736828 Test Loss: 0.21379825154142773\n",
      "Epoch: 8398 Training Loss: 0.21726257551268693 Test Loss: 0.21428366802143065\n",
      "Epoch: 8399 Training Loss: 0.21726805129493043 Test Loss: 0.2137807647779614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8400 Training Loss: 0.21726509187231569 Test Loss: 0.2137690594055299\n",
      "Epoch: 8401 Training Loss: 0.21726497279751314 Test Loss: 0.2141169539629128\n",
      "Epoch: 8402 Training Loss: 0.21726619271128902 Test Loss: 0.21436026497126207\n",
      "Epoch: 8403 Training Loss: 0.21726421950884817 Test Loss: 0.2139787968650446\n",
      "Epoch: 8404 Training Loss: 0.21726464910520113 Test Loss: 0.2138235418865151\n",
      "Epoch: 8405 Training Loss: 0.21726338465829187 Test Loss: 0.21386752453176436\n",
      "Epoch: 8406 Training Loss: 0.21726467853095988 Test Loss: 0.2137519226365881\n",
      "Epoch: 8407 Training Loss: 0.21726627398627474 Test Loss: 0.21377813333764734\n",
      "Epoch: 8408 Training Loss: 0.21726381186974178 Test Loss: 0.21367046465169376\n",
      "Epoch: 8409 Training Loss: 0.21726455515257287 Test Loss: 0.21369483464080918\n",
      "Epoch: 8410 Training Loss: 0.21726317210604854 Test Loss: 0.2136489594325754\n",
      "Epoch: 8411 Training Loss: 0.21726552892821505 Test Loss: 0.21382499371565386\n",
      "Epoch: 8412 Training Loss: 0.21726398725874013 Test Loss: 0.2138210011855222\n",
      "Epoch: 8413 Training Loss: 0.21726576298941488 Test Loss: 0.21385011554485903\n",
      "Epoch: 8414 Training Loss: 0.21726501740237292 Test Loss: 0.21377285749425906\n",
      "Epoch: 8415 Training Loss: 0.2172640813458554 Test Loss: 0.2137745296903207\n",
      "Epoch: 8416 Training Loss: 0.21726315167298815 Test Loss: 0.21572857504629261\n",
      "Epoch: 8417 Training Loss: 0.21726094517144054 Test Loss: 0.21531524447558273\n",
      "Epoch: 8418 Training Loss: 0.21726323793295918 Test Loss: 0.21456690433109668\n",
      "Epoch: 8419 Training Loss: 0.2172645236736457 Test Loss: 0.21415944589074282\n",
      "Epoch: 8420 Training Loss: 0.21726400677728883 Test Loss: 0.21382879180438302\n",
      "Epoch: 8421 Training Loss: 0.21726632312782979 Test Loss: 0.2138900019578953\n",
      "Epoch: 8422 Training Loss: 0.21726471020713467 Test Loss: 0.21382360670031592\n",
      "Epoch: 8423 Training Loss: 0.21726455142279968 Test Loss: 0.21377530745593076\n",
      "Epoch: 8424 Training Loss: 0.2172650377637069 Test Loss: 0.2137805703365589\n",
      "Epoch: 8425 Training Loss: 0.2172633324145697 Test Loss: 0.2137386487701763\n",
      "Epoch: 8426 Training Loss: 0.21726367680891612 Test Loss: 0.21374054133316078\n",
      "Epoch: 8427 Training Loss: 0.21726574235014107 Test Loss: 0.21375430778445895\n",
      "Epoch: 8428 Training Loss: 0.21726541751020653 Test Loss: 0.21375809291042794\n",
      "Epoch: 8429 Training Loss: 0.21726424941876019 Test Loss: 0.21375656330472814\n",
      "Epoch: 8430 Training Loss: 0.21726525038767663 Test Loss: 0.21380199777911624\n",
      "Epoch: 8431 Training Loss: 0.21726346938511096 Test Loss: 0.2145321382083267\n",
      "Epoch: 8432 Training Loss: 0.21726571618793386 Test Loss: 0.2137902146301237\n",
      "Epoch: 8433 Training Loss: 0.2172644778135738 Test Loss: 0.2138376194440573\n",
      "Epoch: 8434 Training Loss: 0.21726405303185597 Test Loss: 0.21380567920300386\n",
      "Epoch: 8435 Training Loss: 0.2172632270215794 Test Loss: 0.2138617172152092\n",
      "Epoch: 8436 Training Loss: 0.21726259339945986 Test Loss: 0.21374522088958134\n",
      "Epoch: 8437 Training Loss: 0.21726499452164888 Test Loss: 0.21381060505186764\n",
      "Epoch: 8438 Training Loss: 0.21726491465429393 Test Loss: 0.21543573333134225\n",
      "Epoch: 8439 Training Loss: 0.21726523525340463 Test Loss: 0.213754852220386\n",
      "Epoch: 8440 Training Loss: 0.2172635680985795 Test Loss: 0.2137671279542649\n",
      "Epoch: 8441 Training Loss: 0.21726364505204915 Test Loss: 0.21378860724786292\n",
      "Epoch: 8442 Training Loss: 0.21726355889070192 Test Loss: 0.21375723736825689\n",
      "Epoch: 8443 Training Loss: 0.21726321959789624 Test Loss: 0.21375198745038893\n",
      "Epoch: 8444 Training Loss: 0.2172645768946402 Test Loss: 0.2137120362235518\n",
      "Epoch: 8445 Training Loss: 0.21726491006380383 Test Loss: 0.21374852639342412\n",
      "Epoch: 8446 Training Loss: 0.2172648273811874 Test Loss: 0.2137977848820617\n",
      "Epoch: 8447 Training Loss: 0.21726411052953754 Test Loss: 0.21381781234652092\n",
      "Epoch: 8448 Training Loss: 0.2172647795486393 Test Loss: 0.21373102666719762\n",
      "Epoch: 8449 Training Loss: 0.21726559795591663 Test Loss: 0.2137565762674883\n",
      "Epoch: 8450 Training Loss: 0.2172635671033756 Test Loss: 0.2137371191644765\n",
      "Epoch: 8451 Training Loss: 0.21726562972174943 Test Loss: 0.21390975720439098\n",
      "Epoch: 8452 Training Loss: 0.21726498753728993 Test Loss: 0.21372516749960177\n",
      "Epoch: 8453 Training Loss: 0.2172635139451417 Test Loss: 0.2141570348173516\n",
      "Epoch: 8454 Training Loss: 0.21726904265250827 Test Loss: 0.2137788333266964\n",
      "Epoch: 8455 Training Loss: 0.2172653910431621 Test Loss: 0.21378785540777318\n",
      "Epoch: 8456 Training Loss: 0.21726454926204164 Test Loss: 0.21375093746681534\n",
      "Epoch: 8457 Training Loss: 0.21726625830508886 Test Loss: 0.21386845785049644\n",
      "Epoch: 8458 Training Loss: 0.21726642661110446 Test Loss: 0.21378677949867925\n",
      "Epoch: 8459 Training Loss: 0.2172645403769329 Test Loss: 0.21382420298728363\n",
      "Epoch: 8460 Training Loss: 0.21726609084185855 Test Loss: 0.21381426055023495\n",
      "Epoch: 8461 Training Loss: 0.21726426502821966 Test Loss: 0.21376883903860705\n",
      "Epoch: 8462 Training Loss: 0.2172628658880826 Test Loss: 0.21446763551373182\n",
      "Epoch: 8463 Training Loss: 0.21726640378417522 Test Loss: 0.21391938853519565\n",
      "Epoch: 8464 Training Loss: 0.2172643077323296 Test Loss: 0.21397612653645004\n",
      "Epoch: 8465 Training Loss: 0.2172643073916292 Test Loss: 0.21378062218759955\n",
      "Epoch: 8466 Training Loss: 0.21726322828575736 Test Loss: 0.21373202479973055\n",
      "Epoch: 8467 Training Loss: 0.217263615913196 Test Loss: 0.21372011202313634\n",
      "Epoch: 8468 Training Loss: 0.21726462239608008 Test Loss: 0.21376649277901666\n",
      "Epoch: 8469 Training Loss: 0.21726414512856343 Test Loss: 0.2136635684632845\n",
      "Epoch: 8470 Training Loss: 0.217263757940449 Test Loss: 0.2135892140709621\n",
      "Epoch: 8471 Training Loss: 0.21726623201736045 Test Loss: 0.2137014197229744\n",
      "Epoch: 8472 Training Loss: 0.21726377253677298 Test Loss: 0.21369173654112908\n",
      "Epoch: 8473 Training Loss: 0.21726441115284367 Test Loss: 0.21404261253335058\n",
      "Epoch: 8474 Training Loss: 0.2172653857264421 Test Loss: 0.2136998512289941\n",
      "Epoch: 8475 Training Loss: 0.21726512077805799 Test Loss: 0.21377079641539237\n",
      "Epoch: 8476 Training Loss: 0.21726440146977863 Test Loss: 0.21374833195202161\n",
      "Epoch: 8477 Training Loss: 0.21726555595113914 Test Loss: 0.21374856528170463\n",
      "Epoch: 8478 Training Loss: 0.21726609302054817 Test Loss: 0.21370852331554635\n",
      "Epoch: 8479 Training Loss: 0.21726499536443417 Test Loss: 0.2136665498981231\n",
      "Epoch: 8480 Training Loss: 0.21726529471459657 Test Loss: 0.21373068963543326\n",
      "Epoch: 8481 Training Loss: 0.2172669795230838 Test Loss: 0.21373027482710788\n",
      "Epoch: 8482 Training Loss: 0.21726382577569905 Test Loss: 0.21374776159057424\n",
      "Epoch: 8483 Training Loss: 0.217265872443913 Test Loss: 0.21374168205605554\n",
      "Epoch: 8484 Training Loss: 0.21726332179009555 Test Loss: 0.2162259043028897\n",
      "Epoch: 8485 Training Loss: 0.217266158748835 Test Loss: 0.21372537490376448\n",
      "Epoch: 8486 Training Loss: 0.21726459415380706 Test Loss: 0.2136494001664211\n",
      "Epoch: 8487 Training Loss: 0.21726488886865036 Test Loss: 0.21372311938349528\n",
      "Epoch: 8488 Training Loss: 0.21726574573024804 Test Loss: 0.21367737380286317\n",
      "Epoch: 8489 Training Loss: 0.21726568341793137 Test Loss: 0.21377574818977646\n",
      "Epoch: 8490 Training Loss: 0.21726557872427357 Test Loss: 0.2137996774450462\n",
      "Epoch: 8491 Training Loss: 0.21726456959647822 Test Loss: 0.2137449227460975\n",
      "Epoch: 8492 Training Loss: 0.21726544338550807 Test Loss: 0.21382634184271132\n",
      "Epoch: 8493 Training Loss: 0.21726499405542724 Test Loss: 0.21547368829311353\n",
      "Epoch: 8494 Training Loss: 0.21726710276698377 Test Loss: 0.21383138435641658\n",
      "Epoch: 8495 Training Loss: 0.21726473907701374 Test Loss: 0.2138176567933989\n",
      "Epoch: 8496 Training Loss: 0.21726465619714969 Test Loss: 0.2137926386662751\n",
      "Epoch: 8497 Training Loss: 0.21726438456924382 Test Loss: 0.2138144809171578\n",
      "Epoch: 8498 Training Loss: 0.21726597775621104 Test Loss: 0.21390857759321572\n",
      "Epoch: 8499 Training Loss: 0.21726582800940342 Test Loss: 0.2138399527408875\n",
      "Epoch: 8500 Training Loss: 0.21726297176522613 Test Loss: 0.21371298250504406\n",
      "Epoch: 8501 Training Loss: 0.2172665673114189 Test Loss: 0.21375467074174365\n",
      "Epoch: 8502 Training Loss: 0.2172635513056343 Test Loss: 0.21378884057754594\n",
      "Epoch: 8503 Training Loss: 0.2172646284838589 Test Loss: 0.21371282695192206\n",
      "Epoch: 8504 Training Loss: 0.2172650872997572 Test Loss: 0.2137487337975868\n",
      "Epoch: 8505 Training Loss: 0.21726447854876949 Test Loss: 0.21363847255959967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8506 Training Loss: 0.2172646674492299 Test Loss: 0.2137186472312374\n",
      "Epoch: 8507 Training Loss: 0.21726484119748668 Test Loss: 0.2137660390824108\n",
      "Epoch: 8508 Training Loss: 0.21726364166297638 Test Loss: 0.2136977123735664\n",
      "Epoch: 8509 Training Loss: 0.21726589570120067 Test Loss: 0.21380661252173597\n",
      "Epoch: 8510 Training Loss: 0.21726423523486307 Test Loss: 0.21374619309659393\n",
      "Epoch: 8511 Training Loss: 0.217264187985092 Test Loss: 0.21370103084016936\n",
      "Epoch: 8512 Training Loss: 0.2172662270413409 Test Loss: 0.2136453428224886\n",
      "Epoch: 8513 Training Loss: 0.2172642073870853 Test Loss: 0.21373004149742486\n",
      "Epoch: 8514 Training Loss: 0.21726608003806838 Test Loss: 0.21368209224756424\n",
      "Epoch: 8515 Training Loss: 0.21726352953666958 Test Loss: 0.2137127621381212\n",
      "Epoch: 8516 Training Loss: 0.21726410384105 Test Loss: 0.21369772533632658\n",
      "Epoch: 8517 Training Loss: 0.2172640075662793 Test Loss: 0.2136582537316157\n",
      "Epoch: 8518 Training Loss: 0.21726413416338886 Test Loss: 0.21372680080738293\n",
      "Epoch: 8519 Training Loss: 0.21726432202381635 Test Loss: 0.21370293636591403\n",
      "Epoch: 8520 Training Loss: 0.21726409503663346 Test Loss: 0.21366253144247108\n",
      "Epoch: 8521 Training Loss: 0.21726617542522478 Test Loss: 0.21499079955134331\n",
      "Epoch: 8522 Training Loss: 0.2172652125250992 Test Loss: 0.21364837610836784\n",
      "Epoch: 8523 Training Loss: 0.21726428425089692 Test Loss: 0.21365472786085005\n",
      "Epoch: 8524 Training Loss: 0.21726452199704094 Test Loss: 0.21368180706684056\n",
      "Epoch: 8525 Training Loss: 0.21726549171117526 Test Loss: 0.213766518704537\n",
      "Epoch: 8526 Training Loss: 0.21726373099821714 Test Loss: 0.213714291743821\n",
      "Epoch: 8527 Training Loss: 0.2172642103637312 Test Loss: 0.2137769018754314\n",
      "Epoch: 8528 Training Loss: 0.21726232112601632 Test Loss: 0.21434673184964692\n",
      "Epoch: 8529 Training Loss: 0.21726532370103108 Test Loss: 0.2139007740115947\n",
      "Epoch: 8530 Training Loss: 0.21726460377411147 Test Loss: 0.21388429834342146\n",
      "Epoch: 8531 Training Loss: 0.21726518119859065 Test Loss: 0.21385769875955718\n",
      "Epoch: 8532 Training Loss: 0.21726490835133586 Test Loss: 0.21379306643736062\n",
      "Epoch: 8533 Training Loss: 0.21726457002683666 Test Loss: 0.21373562844705718\n",
      "Epoch: 8534 Training Loss: 0.21726670224672334 Test Loss: 0.21385228032580705\n",
      "Epoch: 8535 Training Loss: 0.21726467633433863 Test Loss: 0.21373746915900102\n",
      "Epoch: 8536 Training Loss: 0.21726435766287516 Test Loss: 0.21375692626201284\n",
      "Epoch: 8537 Training Loss: 0.21726402849245874 Test Loss: 0.21376662240661834\n",
      "Epoch: 8538 Training Loss: 0.21726408770260827 Test Loss: 0.21360549529773282\n",
      "Epoch: 8539 Training Loss: 0.21726527166352233 Test Loss: 0.21375874104843634\n",
      "Epoch: 8540 Training Loss: 0.21726266611210562 Test Loss: 0.218075677216072\n",
      "Epoch: 8541 Training Loss: 0.21726584008633731 Test Loss: 0.21387155595017654\n",
      "Epoch: 8542 Training Loss: 0.21726464987626 Test Loss: 0.2137662464865735\n",
      "Epoch: 8543 Training Loss: 0.2172647555471901 Test Loss: 0.213741098731848\n",
      "Epoch: 8544 Training Loss: 0.2172647534491927 Test Loss: 0.2136293208509212\n",
      "Epoch: 8545 Training Loss: 0.2172650762090614 Test Loss: 0.21374820232441993\n",
      "Epoch: 8546 Training Loss: 0.21726328442063716 Test Loss: 0.2137013419464134\n",
      "Epoch: 8547 Training Loss: 0.217264550508288 Test Loss: 0.2136661350897977\n",
      "Epoch: 8548 Training Loss: 0.21726631994497045 Test Loss: 0.21370156231333626\n",
      "Epoch: 8549 Training Loss: 0.2172654923118839 Test Loss: 0.21371020847436817\n",
      "Epoch: 8550 Training Loss: 0.21726402761381025 Test Loss: 0.21366476103721993\n",
      "Epoch: 8551 Training Loss: 0.21726542600978582 Test Loss: 0.21381269205625464\n",
      "Epoch: 8552 Training Loss: 0.21726359660982655 Test Loss: 0.21399506512905517\n",
      "Epoch: 8553 Training Loss: 0.2172629537708636 Test Loss: 0.21382692516691887\n",
      "Epoch: 8554 Training Loss: 0.21726440258153795 Test Loss: 0.21384941555580997\n",
      "Epoch: 8555 Training Loss: 0.21726237002549476 Test Loss: 0.21373757286108236\n",
      "Epoch: 8556 Training Loss: 0.21726348555941588 Test Loss: 0.2137310785182383\n",
      "Epoch: 8557 Training Loss: 0.21726370602846146 Test Loss: 0.21382142895660772\n",
      "Epoch: 8558 Training Loss: 0.21726457327245663 Test Loss: 0.21369042730235213\n",
      "Epoch: 8559 Training Loss: 0.217266931089827 Test Loss: 0.213723715670463\n",
      "Epoch: 8560 Training Loss: 0.2172664663027053 Test Loss: 0.21369102358931985\n",
      "Epoch: 8561 Training Loss: 0.21726543815844612 Test Loss: 0.21367979783901456\n",
      "Epoch: 8562 Training Loss: 0.2172649236649239 Test Loss: 0.21374707456428532\n",
      "Epoch: 8563 Training Loss: 0.21726503327184063 Test Loss: 0.21374907082935118\n",
      "Epoch: 8564 Training Loss: 0.2172640523683867 Test Loss: 0.21385614322833704\n",
      "Epoch: 8565 Training Loss: 0.21726567829845903 Test Loss: 0.21403847741285706\n",
      "Epoch: 8566 Training Loss: 0.21726287133928956 Test Loss: 0.21377958516678613\n",
      "Epoch: 8567 Training Loss: 0.21726575789683994 Test Loss: 0.21378150365529097\n",
      "Epoch: 8568 Training Loss: 0.2172655556552677 Test Loss: 0.21390607578050336\n",
      "Epoch: 8569 Training Loss: 0.217264767399979 Test Loss: 0.21376585760376846\n",
      "Epoch: 8570 Training Loss: 0.21726458476661345 Test Loss: 0.21378417398388552\n",
      "Epoch: 8571 Training Loss: 0.2172660763710558 Test Loss: 0.21370631964631784\n",
      "Epoch: 8572 Training Loss: 0.21726558873010743 Test Loss: 0.2137078233264973\n",
      "Epoch: 8573 Training Loss: 0.21726392626439622 Test Loss: 0.21473652204789212\n",
      "Epoch: 8574 Training Loss: 0.2172670477797265 Test Loss: 0.21375365964645057\n",
      "Epoch: 8575 Training Loss: 0.21726405498640058 Test Loss: 0.21367147574698683\n",
      "Epoch: 8576 Training Loss: 0.2172652249158361 Test Loss: 0.21369821792121296\n",
      "Epoch: 8577 Training Loss: 0.21726662200280475 Test Loss: 0.21374808565957842\n",
      "Epoch: 8578 Training Loss: 0.21726269986834623 Test Loss: 0.21389092231386722\n",
      "Epoch: 8579 Training Loss: 0.21726304745451783 Test Loss: 0.21367908488720533\n",
      "Epoch: 8580 Training Loss: 0.21726309336838454 Test Loss: 0.2137128917657229\n",
      "Epoch: 8581 Training Loss: 0.21726427714998253 Test Loss: 0.21380584771888606\n",
      "Epoch: 8582 Training Loss: 0.21726531505799895 Test Loss: 0.21369811421913162\n",
      "Epoch: 8583 Training Loss: 0.21726449139676227 Test Loss: 0.2137578725435051\n",
      "Epoch: 8584 Training Loss: 0.21726494868847437 Test Loss: 0.2137187638960789\n",
      "Epoch: 8585 Training Loss: 0.21726521573485594 Test Loss: 0.2136990086495832\n",
      "Epoch: 8586 Training Loss: 0.2172641270176455 Test Loss: 0.2137367562071918\n",
      "Epoch: 8587 Training Loss: 0.21726504658605506 Test Loss: 0.2138722559392256\n",
      "Epoch: 8588 Training Loss: 0.21726394341597344 Test Loss: 0.2137394394985465\n",
      "Epoch: 8589 Training Loss: 0.2172639495216839 Test Loss: 0.21373350255438966\n",
      "Epoch: 8590 Training Loss: 0.21726291311992205 Test Loss: 0.21370974181500213\n",
      "Epoch: 8591 Training Loss: 0.2172624823759466 Test Loss: 0.21362665052232666\n",
      "Epoch: 8592 Training Loss: 0.2172654453490185 Test Loss: 0.21371005292124615\n",
      "Epoch: 8593 Training Loss: 0.21726591907504378 Test Loss: 0.21372118793223027\n",
      "Epoch: 8594 Training Loss: 0.21726473255887646 Test Loss: 0.21582168655257777\n",
      "Epoch: 8595 Training Loss: 0.21726479531948317 Test Loss: 0.21374207093886058\n",
      "Epoch: 8596 Training Loss: 0.21726570139436227 Test Loss: 0.21373295811846263\n",
      "Epoch: 8597 Training Loss: 0.21726455863130364 Test Loss: 0.2137543725982598\n",
      "Epoch: 8598 Training Loss: 0.2172649665752473 Test Loss: 0.21402898867241424\n",
      "Epoch: 8599 Training Loss: 0.21726326744837596 Test Loss: 0.2143410671234536\n",
      "Epoch: 8600 Training Loss: 0.21726625713056893 Test Loss: 0.21394252706209516\n",
      "Epoch: 8601 Training Loss: 0.21726386414932716 Test Loss: 0.21383449541885685\n",
      "Epoch: 8602 Training Loss: 0.21726437481445238 Test Loss: 0.2137692149586519\n",
      "Epoch: 8603 Training Loss: 0.21726505569530882 Test Loss: 0.2137426672258283\n",
      "Epoch: 8604 Training Loss: 0.21726297014241613 Test Loss: 0.21372319716005628\n",
      "Epoch: 8605 Training Loss: 0.21726282162392327 Test Loss: 0.2136909976637995\n",
      "Epoch: 8606 Training Loss: 0.21726531700357776 Test Loss: 0.21383649168392269\n",
      "Epoch: 8607 Training Loss: 0.21726511087084793 Test Loss: 0.21374476719297547\n",
      "Epoch: 8608 Training Loss: 0.21726481590496216 Test Loss: 0.2136973234907614\n",
      "Epoch: 8609 Training Loss: 0.21726467577845898 Test Loss: 0.2137882572533384\n",
      "Epoch: 8610 Training Loss: 0.217264767489637 Test Loss: 0.21374914860591218\n",
      "Epoch: 8611 Training Loss: 0.21726556888878992 Test Loss: 0.2137184787153552\n",
      "Epoch: 8612 Training Loss: 0.21726630781424175 Test Loss: 0.21370888627283105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8613 Training Loss: 0.21726655761042227 Test Loss: 0.21372827856204205\n",
      "Epoch: 8614 Training Loss: 0.21726448382962624 Test Loss: 0.21396510819030742\n",
      "Epoch: 8615 Training Loss: 0.21726680575689541 Test Loss: 0.21379687748884996\n",
      "Epoch: 8616 Training Loss: 0.21726715116437736 Test Loss: 0.2137575484745009\n",
      "Epoch: 8617 Training Loss: 0.21726364133124174 Test Loss: 0.21368461998579696\n",
      "Epoch: 8618 Training Loss: 0.21726548022598421 Test Loss: 0.21371492691906924\n",
      "Epoch: 8619 Training Loss: 0.21726432589704237 Test Loss: 0.21369058285547415\n",
      "Epoch: 8620 Training Loss: 0.21726501756375735 Test Loss: 0.2137514689399822\n",
      "Epoch: 8621 Training Loss: 0.2172634409724877 Test Loss: 0.21382971216035496\n",
      "Epoch: 8622 Training Loss: 0.21726396282693253 Test Loss: 0.2137620595150393\n",
      "Epoch: 8623 Training Loss: 0.21726307722097699 Test Loss: 0.2137128917657229\n",
      "Epoch: 8624 Training Loss: 0.21726367140253816 Test Loss: 0.21368813289380242\n",
      "Epoch: 8625 Training Loss: 0.2172653517550223 Test Loss: 0.21371521209979294\n",
      "Epoch: 8626 Training Loss: 0.21726495403209176 Test Loss: 0.21391969964143967\n",
      "Epoch: 8627 Training Loss: 0.21726253515761682 Test Loss: 0.21417326419308166\n",
      "Epoch: 8628 Training Loss: 0.2172655409423883 Test Loss: 0.2139622563830705\n",
      "Epoch: 8629 Training Loss: 0.21726438486511526 Test Loss: 0.21385168403883933\n",
      "Epoch: 8630 Training Loss: 0.21726402641239292 Test Loss: 0.21379097943297362\n",
      "Epoch: 8631 Training Loss: 0.21726553492633588 Test Loss: 0.21375465777898348\n",
      "Epoch: 8632 Training Loss: 0.21726416806308227 Test Loss: 0.21372441565951206\n",
      "Epoch: 8633 Training Loss: 0.2172640300883713 Test Loss: 0.21368764030891604\n",
      "Epoch: 8634 Training Loss: 0.21726495387967312 Test Loss: 0.21368856066488798\n",
      "Epoch: 8635 Training Loss: 0.21726487284676407 Test Loss: 0.2136823515027676\n",
      "Epoch: 8636 Training Loss: 0.21726478648816921 Test Loss: 0.21368980508986407\n",
      "Epoch: 8637 Training Loss: 0.21726435966224877 Test Loss: 0.2137134491644101\n",
      "Epoch: 8638 Training Loss: 0.21726473645003408 Test Loss: 0.21364045586190533\n",
      "Epoch: 8639 Training Loss: 0.21726591704877274 Test Loss: 0.21370634557183815\n",
      "Epoch: 8640 Training Loss: 0.2172679500710376 Test Loss: 0.21365979630007567\n",
      "Epoch: 8641 Training Loss: 0.21726603645530992 Test Loss: 0.21394095856811485\n",
      "Epoch: 8642 Training Loss: 0.21726549627476793 Test Loss: 0.21377542412077227\n",
      "Epoch: 8643 Training Loss: 0.21726413064879488 Test Loss: 0.21372667117978125\n",
      "Epoch: 8644 Training Loss: 0.21726472240958977 Test Loss: 0.2137544114865403\n",
      "Epoch: 8645 Training Loss: 0.21726389094810625 Test Loss: 0.21375041895640862\n",
      "Epoch: 8646 Training Loss: 0.21726529514495502 Test Loss: 0.21369257912054\n",
      "Epoch: 8647 Training Loss: 0.21726751626972401 Test Loss: 0.21382070304203835\n",
      "Epoch: 8648 Training Loss: 0.2172651986281077 Test Loss: 0.21374288759275115\n",
      "Epoch: 8649 Training Loss: 0.21726455747471532 Test Loss: 0.2137392839454245\n",
      "Epoch: 8650 Training Loss: 0.2172644461732622 Test Loss: 0.2136896754622624\n",
      "Epoch: 8651 Training Loss: 0.21726516425322684 Test Loss: 0.21376470391811353\n",
      "Epoch: 8652 Training Loss: 0.21726506081478117 Test Loss: 0.21368228668896677\n",
      "Epoch: 8653 Training Loss: 0.21726532108301722 Test Loss: 0.21370362339220292\n",
      "Epoch: 8654 Training Loss: 0.21726516437874804 Test Loss: 0.21394370667327042\n",
      "Epoch: 8655 Training Loss: 0.21726592544076245 Test Loss: 0.21382531778465808\n",
      "Epoch: 8656 Training Loss: 0.2172643663328047 Test Loss: 0.21372801930683868\n",
      "Epoch: 8657 Training Loss: 0.21726633662136022 Test Loss: 0.21380159593355103\n",
      "Epoch: 8658 Training Loss: 0.2172655883625096 Test Loss: 0.21378375917556017\n",
      "Epoch: 8659 Training Loss: 0.2172647337871912 Test Loss: 0.21381608829941862\n",
      "Epoch: 8660 Training Loss: 0.2172647348630873 Test Loss: 0.21385762098299618\n",
      "Epoch: 8661 Training Loss: 0.21726457574701769 Test Loss: 0.21384382860617765\n",
      "Epoch: 8662 Training Loss: 0.21726504011274675 Test Loss: 0.21379687748884996\n",
      "Epoch: 8663 Training Loss: 0.21726404268532168 Test Loss: 0.2137534522422879\n",
      "Epoch: 8664 Training Loss: 0.21726338465829187 Test Loss: 0.2137628632061697\n",
      "Epoch: 8665 Training Loss: 0.21726613190522692 Test Loss: 0.21379825154142773\n",
      "Epoch: 8666 Training Loss: 0.21726301589489844 Test Loss: 0.21379381827745036\n",
      "Epoch: 8667 Training Loss: 0.21726452560129292 Test Loss: 0.21369406983795927\n",
      "Epoch: 8668 Training Loss: 0.21726505070135768 Test Loss: 0.21382870106506185\n",
      "Epoch: 8669 Training Loss: 0.21726201750813265 Test Loss: 0.21440756608311431\n",
      "Epoch: 8670 Training Loss: 0.21726763769356638 Test Loss: 0.2138735133269619\n",
      "Epoch: 8671 Training Loss: 0.2172648892721114 Test Loss: 0.2137389598764203\n",
      "Epoch: 8672 Training Loss: 0.21726344397603103 Test Loss: 0.21378676653591908\n",
      "Epoch: 8673 Training Loss: 0.2172641440347357 Test Loss: 0.21368555330452904\n",
      "Epoch: 8674 Training Loss: 0.21726629178338963 Test Loss: 0.21377149640444143\n",
      "Epoch: 8675 Training Loss: 0.21726485105090193 Test Loss: 0.21377834074181\n",
      "Epoch: 8676 Training Loss: 0.21726441448812164 Test Loss: 0.21374437831017043\n",
      "Epoch: 8677 Training Loss: 0.2172649264712196 Test Loss: 0.2138872279272194\n",
      "Epoch: 8678 Training Loss: 0.21726502616196047 Test Loss: 0.21367969413693322\n",
      "Epoch: 8679 Training Loss: 0.2172645272330687 Test Loss: 0.21369968271311193\n",
      "Epoch: 8680 Training Loss: 0.217265480055634 Test Loss: 0.21373380069787354\n",
      "Epoch: 8681 Training Loss: 0.21726467525844254 Test Loss: 0.2137033900625199\n",
      "Epoch: 8682 Training Loss: 0.21726621977007632 Test Loss: 0.2136756367930007\n",
      "Epoch: 8683 Training Loss: 0.2172656726948334 Test Loss: 0.2137657150134066\n",
      "Epoch: 8684 Training Loss: 0.21726492685674903 Test Loss: 0.2136491149856974\n",
      "Epoch: 8685 Training Loss: 0.21726496990155944 Test Loss: 0.2137741278447555\n",
      "Epoch: 8686 Training Loss: 0.21726515226595094 Test Loss: 0.21378094625660374\n",
      "Epoch: 8687 Training Loss: 0.21726374340688565 Test Loss: 0.21370204193546247\n",
      "Epoch: 8688 Training Loss: 0.21726668908492752 Test Loss: 0.21377675928506956\n",
      "Epoch: 8689 Training Loss: 0.21726410987503406 Test Loss: 0.21683384479199852\n",
      "Epoch: 8690 Training Loss: 0.21726649932375022 Test Loss: 0.21380879026544414\n",
      "Epoch: 8691 Training Loss: 0.21726625175108835 Test Loss: 0.21371054550613253\n",
      "Epoch: 8692 Training Loss: 0.21726520833807014 Test Loss: 0.21367249980504008\n",
      "Epoch: 8693 Training Loss: 0.21726402666343536 Test Loss: 0.2136682739452254\n",
      "Epoch: 8694 Training Loss: 0.21726482656529952 Test Loss: 0.213679292291368\n",
      "Epoch: 8695 Training Loss: 0.21726487718621174 Test Loss: 0.21373945246130668\n",
      "Epoch: 8696 Training Loss: 0.21726354519095806 Test Loss: 0.21369073840859615\n",
      "Epoch: 8697 Training Loss: 0.21726837768594853 Test Loss: 0.2137785870342532\n",
      "Epoch: 8698 Training Loss: 0.2172631559048462 Test Loss: 0.2139225903369571\n",
      "Epoch: 8699 Training Loss: 0.21726631062053745 Test Loss: 0.21393080872690348\n",
      "Epoch: 8700 Training Loss: 0.2172644292278984 Test Loss: 0.21384323231920993\n",
      "Epoch: 8701 Training Loss: 0.21726348245724875 Test Loss: 0.21373958208890836\n",
      "Epoch: 8702 Training Loss: 0.21726512802242515 Test Loss: 0.21373138962448232\n",
      "Epoch: 8703 Training Loss: 0.21726345070934755 Test Loss: 0.21369709016107838\n",
      "Epoch: 8704 Training Loss: 0.2172642543589165 Test Loss: 0.213791549794421\n",
      "Epoch: 8705 Training Loss: 0.21726374515521685 Test Loss: 0.21374035985451842\n",
      "Epoch: 8706 Training Loss: 0.21726523729760724 Test Loss: 0.21371321583472708\n",
      "Epoch: 8707 Training Loss: 0.21726623359534142 Test Loss: 0.21371560098259795\n",
      "Epoch: 8708 Training Loss: 0.2172658771330269 Test Loss: 0.21373185628384836\n",
      "Epoch: 8709 Training Loss: 0.217266510055814 Test Loss: 0.21371009180952666\n",
      "Epoch: 8710 Training Loss: 0.21726444994786442 Test Loss: 0.21375253188631596\n",
      "Epoch: 8711 Training Loss: 0.21726458005060215 Test Loss: 0.21374183760917756\n",
      "Epoch: 8712 Training Loss: 0.21726305085255637 Test Loss: 0.21368585144801291\n",
      "Epoch: 8713 Training Loss: 0.21726512028493894 Test Loss: 0.213755007773508\n",
      "Epoch: 8714 Training Loss: 0.21726323685706309 Test Loss: 0.21373141555000266\n",
      "Epoch: 8715 Training Loss: 0.21726514355119242 Test Loss: 0.21376939643729426\n",
      "Epoch: 8716 Training Loss: 0.2172635970312192 Test Loss: 0.21371355286649146\n",
      "Epoch: 8717 Training Loss: 0.21726356774891326 Test Loss: 0.21384038051197302\n",
      "Epoch: 8718 Training Loss: 0.21726497782732748 Test Loss: 0.21376618167277264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8719 Training Loss: 0.21726609635582614 Test Loss: 0.21373885617433896\n",
      "Epoch: 8720 Training Loss: 0.2172627509644459 Test Loss: 0.2137136565685728\n",
      "Epoch: 8721 Training Loss: 0.21726469505493104 Test Loss: 0.21377487968484524\n",
      "Epoch: 8722 Training Loss: 0.21726417912688065 Test Loss: 0.21371692318413507\n",
      "Epoch: 8723 Training Loss: 0.21726637623226885 Test Loss: 0.21376707610322424\n",
      "Epoch: 8724 Training Loss: 0.2172641765536958 Test Loss: 0.21376711499150472\n",
      "Epoch: 8725 Training Loss: 0.2172640233191916 Test Loss: 0.2139064257750279\n",
      "Epoch: 8726 Training Loss: 0.21726457722637485 Test Loss: 0.2137428616672308\n",
      "Epoch: 8727 Training Loss: 0.2172644113949203 Test Loss: 0.21374166909329537\n",
      "Epoch: 8728 Training Loss: 0.21726438365473213 Test Loss: 0.21371033810196985\n",
      "Epoch: 8729 Training Loss: 0.21726668405511318 Test Loss: 0.21370552891794758\n",
      "Epoch: 8730 Training Loss: 0.21726450580480441 Test Loss: 0.21371026032540882\n",
      "Epoch: 8731 Training Loss: 0.2172651098487466 Test Loss: 0.21374181168365722\n",
      "Epoch: 8732 Training Loss: 0.2172650398527385 Test Loss: 0.21372967854014016\n",
      "Epoch: 8733 Training Loss: 0.21726455398701874 Test Loss: 0.21376017991481497\n",
      "Epoch: 8734 Training Loss: 0.21726489887448425 Test Loss: 0.21381856418661066\n",
      "Epoch: 8735 Training Loss: 0.21726470769671039 Test Loss: 0.21576226525996867\n",
      "Epoch: 8736 Training Loss: 0.21726510354578854 Test Loss: 0.21377335007914544\n",
      "Epoch: 8737 Training Loss: 0.2172648788179875 Test Loss: 0.2137200601720957\n",
      "Epoch: 8738 Training Loss: 0.21726507112545226 Test Loss: 0.21369395317311776\n",
      "Epoch: 8739 Training Loss: 0.2172672272660959 Test Loss: 0.2137073307416109\n",
      "Epoch: 8740 Training Loss: 0.21726703804286665 Test Loss: 0.21373374884683286\n",
      "Epoch: 8741 Training Loss: 0.21726549279603719 Test Loss: 0.21372083793770574\n",
      "Epoch: 8742 Training Loss: 0.21726490910446314 Test Loss: 0.21371510839771157\n",
      "Epoch: 8743 Training Loss: 0.21726475623755678 Test Loss: 0.21367926636584766\n",
      "Epoch: 8744 Training Loss: 0.21726488392849405 Test Loss: 0.21370736962989142\n",
      "Epoch: 8745 Training Loss: 0.21726511570341464 Test Loss: 0.2152797654010035\n",
      "Epoch: 8746 Training Loss: 0.21726199586468914 Test Loss: 0.2149685165666149\n",
      "Epoch: 8747 Training Loss: 0.21726372216690318 Test Loss: 0.2143123805352023\n",
      "Epoch: 8748 Training Loss: 0.21726569449966135 Test Loss: 0.21405457716098544\n",
      "Epoch: 8749 Training Loss: 0.21726252073164307 Test Loss: 0.21386959857339122\n",
      "Epoch: 8750 Training Loss: 0.2172621067178522 Test Loss: 0.2138163734801423\n",
      "Epoch: 8751 Training Loss: 0.2172635838783892 Test Loss: 0.21385510620752363\n",
      "Epoch: 8752 Training Loss: 0.21726394560362888 Test Loss: 0.21387820584614262\n",
      "Epoch: 8753 Training Loss: 0.2172629976494935 Test Loss: 0.2137461671710736\n",
      "Epoch: 8754 Training Loss: 0.2172642514270996 Test Loss: 0.2137389598764203\n",
      "Epoch: 8755 Training Loss: 0.21726291404339954 Test Loss: 0.21577378915375783\n",
      "Epoch: 8756 Training Loss: 0.21726479904925636 Test Loss: 0.2137538151995726\n",
      "Epoch: 8757 Training Loss: 0.21726544667595704 Test Loss: 0.2137245971381544\n",
      "Epoch: 8758 Training Loss: 0.21726540661675836 Test Loss: 0.2137385191425746\n",
      "Epoch: 8759 Training Loss: 0.21726554772053386 Test Loss: 0.21371254177119836\n",
      "Epoch: 8760 Training Loss: 0.21726406093072662 Test Loss: 0.21374389868804422\n",
      "Epoch: 8761 Training Loss: 0.21726602923784016 Test Loss: 0.21374917453143252\n",
      "Epoch: 8762 Training Loss: 0.21726542254002085 Test Loss: 0.21375876697395668\n",
      "Epoch: 8763 Training Loss: 0.21726457129101462 Test Loss: 0.21377966294334713\n",
      "Epoch: 8764 Training Loss: 0.21726506333417123 Test Loss: 0.21375705588961452\n",
      "Epoch: 8765 Training Loss: 0.21726659489022265 Test Loss: 0.21374815047337925\n",
      "Epoch: 8766 Training Loss: 0.21726545914738618 Test Loss: 0.21374818936165976\n",
      "Epoch: 8767 Training Loss: 0.21726376470066294 Test Loss: 0.21373346366610915\n",
      "Epoch: 8768 Training Loss: 0.21726519223549162 Test Loss: 0.21378674061039873\n",
      "Epoch: 8769 Training Loss: 0.2172642834081116 Test Loss: 0.21369663646447248\n",
      "Epoch: 8770 Training Loss: 0.21726526972690932 Test Loss: 0.21378504248881677\n",
      "Epoch: 8771 Training Loss: 0.21726463059082216 Test Loss: 0.21383382135532814\n",
      "Epoch: 8772 Training Loss: 0.21726406242801538 Test Loss: 0.21382269930710418\n",
      "Epoch: 8773 Training Loss: 0.21726437373855628 Test Loss: 0.2138261862895893\n",
      "Epoch: 8774 Training Loss: 0.21726382469980296 Test Loss: 0.213888511240476\n",
      "Epoch: 8775 Training Loss: 0.21726308729853727 Test Loss: 0.21381225132240894\n",
      "Epoch: 8776 Training Loss: 0.21726470352761296 Test Loss: 0.213797266371655\n",
      "Epoch: 8777 Training Loss: 0.2172644889939276 Test Loss: 0.213789540566595\n",
      "Epoch: 8778 Training Loss: 0.21726716359994327 Test Loss: 0.21365907038550627\n",
      "Epoch: 8779 Training Loss: 0.21726522850215652 Test Loss: 0.21365677597695656\n",
      "Epoch: 8780 Training Loss: 0.21726458079476363 Test Loss: 0.21383991385260698\n",
      "Epoch: 8781 Training Loss: 0.2172665897976477 Test Loss: 0.21390214806417251\n",
      "Epoch: 8782 Training Loss: 0.21726600378393124 Test Loss: 0.21385562471793035\n",
      "Epoch: 8783 Training Loss: 0.21726637225145323 Test Loss: 0.21371041587853085\n",
      "Epoch: 8784 Training Loss: 0.21726482411763584 Test Loss: 0.21367033502409208\n",
      "Epoch: 8785 Training Loss: 0.21726517468941914 Test Loss: 0.21377651299262637\n",
      "Epoch: 8786 Training Loss: 0.2172644122735688 Test Loss: 0.21369238467913745\n",
      "Epoch: 8787 Training Loss: 0.2172660662755639 Test Loss: 0.21373648398922826\n",
      "Epoch: 8788 Training Loss: 0.21726347152793737 Test Loss: 0.21369081618515717\n",
      "Epoch: 8789 Training Loss: 0.21726497848183096 Test Loss: 0.21370853627830652\n",
      "Epoch: 8790 Training Loss: 0.21726315780559602 Test Loss: 0.21369685683139536\n",
      "Epoch: 8791 Training Loss: 0.2172651125653843 Test Loss: 0.21384148234658729\n",
      "Epoch: 8792 Training Loss: 0.21726455039173256 Test Loss: 0.21378593691926834\n",
      "Epoch: 8793 Training Loss: 0.21726361496282112 Test Loss: 0.21371674170549274\n",
      "Epoch: 8794 Training Loss: 0.21726558343131908 Test Loss: 0.2138772854901707\n",
      "Epoch: 8795 Training Loss: 0.21726337037577095 Test Loss: 0.21382902513406604\n",
      "Epoch: 8796 Training Loss: 0.2172643339214342 Test Loss: 0.2139198940828422\n",
      "Epoch: 8797 Training Loss: 0.21726429136077707 Test Loss: 0.21381969194674524\n",
      "Epoch: 8798 Training Loss: 0.21726379291603856 Test Loss: 0.21382665294895534\n",
      "Epoch: 8799 Training Loss: 0.2172638990980193 Test Loss: 0.21635435229339228\n",
      "Epoch: 8800 Training Loss: 0.21726521584244554 Test Loss: 0.21387819288338245\n",
      "Epoch: 8801 Training Loss: 0.21726342515681482 Test Loss: 0.21375967436716842\n",
      "Epoch: 8802 Training Loss: 0.21726519695150293 Test Loss: 0.21384529339807662\n",
      "Epoch: 8803 Training Loss: 0.2172637617329828 Test Loss: 0.21385806171684188\n",
      "Epoch: 8804 Training Loss: 0.21726455575328155 Test Loss: 0.21378802392365537\n",
      "Epoch: 8805 Training Loss: 0.21726507355518432 Test Loss: 0.21376112619630722\n",
      "Epoch: 8806 Training Loss: 0.21726479826923167 Test Loss: 0.21378681838695976\n",
      "Epoch: 8807 Training Loss: 0.21726395274937224 Test Loss: 0.21372418232982904\n",
      "Epoch: 8808 Training Loss: 0.21726501952726776 Test Loss: 0.21612898174511522\n",
      "Epoch: 8809 Training Loss: 0.21726442375875982 Test Loss: 0.21388161505206674\n",
      "Epoch: 8810 Training Loss: 0.2172653323799264 Test Loss: 0.21372742301987097\n",
      "Epoch: 8811 Training Loss: 0.21726649283251034 Test Loss: 0.21368205335928375\n",
      "Epoch: 8812 Training Loss: 0.21726540919890902 Test Loss: 0.21367130723110464\n",
      "Epoch: 8813 Training Loss: 0.21726448226061107 Test Loss: 0.2136584481730182\n",
      "Epoch: 8814 Training Loss: 0.21726516644984806 Test Loss: 0.21387895768623236\n",
      "Epoch: 8815 Training Loss: 0.2172648870934218 Test Loss: 0.21417494935190348\n",
      "Epoch: 8816 Training Loss: 0.21726556948053277 Test Loss: 0.21393226055604225\n",
      "Epoch: 8817 Training Loss: 0.2172632051988199 Test Loss: 0.21618140314723372\n",
      "Epoch: 8818 Training Loss: 0.2172654125072896 Test Loss: 0.21380700140454098\n",
      "Epoch: 8819 Training Loss: 0.21726434729840924 Test Loss: 0.21377836666733036\n",
      "Epoch: 8820 Training Loss: 0.21726379949693644 Test Loss: 0.21366349068672347\n",
      "Epoch: 8821 Training Loss: 0.21726542132067192 Test Loss: 0.21374080058836412\n",
      "Epoch: 8822 Training Loss: 0.21726391714617663 Test Loss: 0.21379394790505205\n",
      "Epoch: 8823 Training Loss: 0.21726423875842285 Test Loss: 0.21377209269140915\n",
      "Epoch: 8824 Training Loss: 0.2172648207464947 Test Loss: 0.21372961372633933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8825 Training Loss: 0.2172636801979889 Test Loss: 0.21374178575813688\n",
      "Epoch: 8826 Training Loss: 0.21726205325478107 Test Loss: 0.2137919127517057\n",
      "Epoch: 8827 Training Loss: 0.21726500220534029 Test Loss: 0.21376611685897182\n",
      "Epoch: 8828 Training Loss: 0.21726408371282685 Test Loss: 0.2138297640113956\n",
      "Epoch: 8829 Training Loss: 0.21726271451846502 Test Loss: 0.21375901326639987\n",
      "Epoch: 8830 Training Loss: 0.2172634483782393 Test Loss: 0.21371029921368934\n",
      "Epoch: 8831 Training Loss: 0.2172638139856708 Test Loss: 0.2136871477240297\n",
      "Epoch: 8832 Training Loss: 0.21726501635337422 Test Loss: 0.2136850477568825\n",
      "Epoch: 8833 Training Loss: 0.21726407636087006 Test Loss: 0.21387296889103483\n",
      "Epoch: 8834 Training Loss: 0.217263112895899 Test Loss: 0.21379488122378412\n",
      "Epoch: 8835 Training Loss: 0.2172649713360876 Test Loss: 0.21375213004075078\n",
      "Epoch: 8836 Training Loss: 0.2172644739851768 Test Loss: 0.21379244422487256\n",
      "Epoch: 8837 Training Loss: 0.21726514752304224 Test Loss: 0.21380564031472338\n",
      "Epoch: 8838 Training Loss: 0.2172644299989573 Test Loss: 0.21389689814630455\n",
      "Epoch: 8839 Training Loss: 0.2172619525509047 Test Loss: 0.2140702102497478\n",
      "Epoch: 8840 Training Loss: 0.21726281114290194 Test Loss: 0.21387851695238663\n",
      "Epoch: 8841 Training Loss: 0.21726415514336309 Test Loss: 0.21394874918697568\n",
      "Epoch: 8842 Training Loss: 0.2172643657051986 Test Loss: 0.2137696686552578\n",
      "Epoch: 8843 Training Loss: 0.2172650181734318 Test Loss: 0.21376010213825394\n",
      "Epoch: 8844 Training Loss: 0.21726425986391829 Test Loss: 0.21375562998599607\n",
      "Epoch: 8845 Training Loss: 0.21726378560891077 Test Loss: 0.21370011048419746\n",
      "Epoch: 8846 Training Loss: 0.21726417165836845 Test Loss: 0.2137323229432144\n",
      "Epoch: 8847 Training Loss: 0.21726310192175863 Test Loss: 0.21478738791879048\n",
      "Epoch: 8848 Training Loss: 0.21726516730159914 Test Loss: 0.21377074456435172\n",
      "Epoch: 8849 Training Loss: 0.2172649124845701 Test Loss: 0.21374813751061908\n",
      "Epoch: 8850 Training Loss: 0.21726562966795462 Test Loss: 0.2137534781678082\n",
      "Epoch: 8851 Training Loss: 0.21726564856786304 Test Loss: 0.21368538478864685\n",
      "Epoch: 8852 Training Loss: 0.21726580311137417 Test Loss: 0.21370244378102765\n",
      "Epoch: 8853 Training Loss: 0.217265220101201 Test Loss: 0.21370956033635977\n",
      "Epoch: 8854 Training Loss: 0.2172628966945747 Test Loss: 0.21444600066701178\n",
      "Epoch: 8855 Training Loss: 0.21726699708708788 Test Loss: 0.21376288913169003\n",
      "Epoch: 8856 Training Loss: 0.21726507389588476 Test Loss: 0.21370381783360545\n",
      "Epoch: 8857 Training Loss: 0.2172641765536958 Test Loss: 0.2136727720230036\n",
      "Epoch: 8858 Training Loss: 0.21726610513334527 Test Loss: 0.21371999535829483\n",
      "Epoch: 8859 Training Loss: 0.21726556355413834 Test Loss: 0.21373877839777797\n",
      "Epoch: 8860 Training Loss: 0.21726510099053525 Test Loss: 0.21371692318413507\n",
      "Epoch: 8861 Training Loss: 0.21726513078389184 Test Loss: 0.2139769043020601\n",
      "Epoch: 8862 Training Loss: 0.21726559628827763 Test Loss: 0.21367147574698683\n",
      "Epoch: 8863 Training Loss: 0.21726473156367254 Test Loss: 0.2139190515034313\n",
      "Epoch: 8864 Training Loss: 0.21726430505155514 Test Loss: 0.21421392837172798\n",
      "Epoch: 8865 Training Loss: 0.21726596575100357 Test Loss: 0.21395930087375226\n",
      "Epoch: 8866 Training Loss: 0.21726408266382813 Test Loss: 0.2138547951012796\n",
      "Epoch: 8867 Training Loss: 0.21726539911238296 Test Loss: 0.21387976137736275\n",
      "Epoch: 8868 Training Loss: 0.2172648282329385 Test Loss: 0.2138242159500438\n",
      "Epoch: 8869 Training Loss: 0.21726414251054954 Test Loss: 0.2137705890112297\n",
      "Epoch: 8870 Training Loss: 0.21726440372019468 Test Loss: 0.21372314530901562\n",
      "Epoch: 8871 Training Loss: 0.21726512053598135 Test Loss: 0.21371711762553758\n",
      "Epoch: 8872 Training Loss: 0.21726508610730566 Test Loss: 0.2137172342903791\n",
      "Epoch: 8873 Training Loss: 0.21726448828562933 Test Loss: 0.2137735704460683\n",
      "Epoch: 8874 Training Loss: 0.21726363993257677 Test Loss: 0.21371657318961054\n",
      "Epoch: 8875 Training Loss: 0.21726390629755749 Test Loss: 0.2136979327404893\n",
      "Epoch: 8876 Training Loss: 0.21726379829551912 Test Loss: 0.2137944793782189\n",
      "Epoch: 8877 Training Loss: 0.2172650985787348 Test Loss: 0.2137350191973293\n",
      "Epoch: 8878 Training Loss: 0.21726450886214252 Test Loss: 0.21371255473395853\n",
      "Epoch: 8879 Training Loss: 0.2172671666572814 Test Loss: 0.2137996385567657\n",
      "Epoch: 8880 Training Loss: 0.21726437621311734 Test Loss: 0.21377226120729134\n",
      "Epoch: 8881 Training Loss: 0.2172643992641916 Test Loss: 0.2137532059498447\n",
      "Epoch: 8882 Training Loss: 0.21726385178548765 Test Loss: 0.21363849848511998\n",
      "Epoch: 8883 Training Loss: 0.21726540662572416 Test Loss: 0.21375940214920489\n",
      "Epoch: 8884 Training Loss: 0.2172644317831517 Test Loss: 0.21380013114165206\n",
      "Epoch: 8885 Training Loss: 0.21726412113608007 Test Loss: 0.21370027900007965\n",
      "Epoch: 8886 Training Loss: 0.217264288886216 Test Loss: 0.2137791185074201\n",
      "Epoch: 8887 Training Loss: 0.21726540103106434 Test Loss: 0.21364720945995275\n",
      "Epoch: 8888 Training Loss: 0.21726393323978935 Test Loss: 0.21364510949280557\n",
      "Epoch: 8889 Training Loss: 0.21726585987386005 Test Loss: 0.21370644927391952\n",
      "Epoch: 8890 Training Loss: 0.2172614646140849 Test Loss: 0.2145930891066356\n",
      "Epoch: 8891 Training Loss: 0.21726668334681493 Test Loss: 0.21376307061033237\n",
      "Epoch: 8892 Training Loss: 0.2172669821769609 Test Loss: 0.2137995996684852\n",
      "Epoch: 8893 Training Loss: 0.2172652003495415 Test Loss: 0.21387553551754804\n",
      "Epoch: 8894 Training Loss: 0.21726352772557778 Test Loss: 0.2138465378230527\n",
      "Epoch: 8895 Training Loss: 0.2172648649299618 Test Loss: 0.2137894109389933\n",
      "Epoch: 8896 Training Loss: 0.2172665025514386 Test Loss: 0.21378396657972285\n",
      "Epoch: 8897 Training Loss: 0.21726416795549267 Test Loss: 0.21378996833768052\n",
      "Epoch: 8898 Training Loss: 0.21726474828489134 Test Loss: 0.2137480467712979\n",
      "Epoch: 8899 Training Loss: 0.21726526724338244 Test Loss: 0.21374803380853774\n",
      "Epoch: 8900 Training Loss: 0.21726206274956428 Test Loss: 0.21445804307120767\n",
      "Epoch: 8901 Training Loss: 0.21726652319071238 Test Loss: 0.2138387342414317\n",
      "Epoch: 8902 Training Loss: 0.21726571420649185 Test Loss: 0.21382823440569582\n",
      "Epoch: 8903 Training Loss: 0.21726625440496544 Test Loss: 0.21377922220950143\n",
      "Epoch: 8904 Training Loss: 0.217264184156695 Test Loss: 0.21374240797062494\n",
      "Epoch: 8905 Training Loss: 0.21726683262740087 Test Loss: 0.21376851496960284\n",
      "Epoch: 8906 Training Loss: 0.21726616032681598 Test Loss: 0.21383522133342625\n",
      "Epoch: 8907 Training Loss: 0.21726339920978685 Test Loss: 0.21371750650834262\n",
      "Epoch: 8908 Training Loss: 0.21726557608832808 Test Loss: 0.21378549618542264\n",
      "Epoch: 8909 Training Loss: 0.21726512106496362 Test Loss: 0.21389184266983913\n",
      "Epoch: 8910 Training Loss: 0.21726060713384698 Test Loss: 0.21483479273272407\n",
      "Epoch: 8911 Training Loss: 0.21726751037919279 Test Loss: 0.2139203348166879\n",
      "Epoch: 8912 Training Loss: 0.2172641469127578 Test Loss: 0.21384340083509212\n",
      "Epoch: 8913 Training Loss: 0.21726464849552665 Test Loss: 0.2138658523357027\n",
      "Epoch: 8914 Training Loss: 0.2172635390314528 Test Loss: 0.21381257539141313\n",
      "Epoch: 8915 Training Loss: 0.21726493548184955 Test Loss: 0.21379985892368855\n",
      "Epoch: 8916 Training Loss: 0.21726514040419628 Test Loss: 0.2137888146520256\n",
      "Epoch: 8917 Training Loss: 0.21726431736159985 Test Loss: 0.21377576115253663\n",
      "Epoch: 8918 Training Loss: 0.21726559626138023 Test Loss: 0.21544724426237125\n",
      "Epoch: 8919 Training Loss: 0.21726369976136659 Test Loss: 0.2139450807258482\n",
      "Epoch: 8920 Training Loss: 0.2172618001771174 Test Loss: 0.21494196883379127\n",
      "Epoch: 8921 Training Loss: 0.2172663896630387 Test Loss: 0.21399100778512264\n",
      "Epoch: 8922 Training Loss: 0.21726302373997428 Test Loss: 0.2138743818318931\n",
      "Epoch: 8923 Training Loss: 0.2172624330012807 Test Loss: 0.21372678784462276\n",
      "Epoch: 8924 Training Loss: 0.21726288511972566 Test Loss: 0.21367839786091644\n",
      "Epoch: 8925 Training Loss: 0.21726197092183086 Test Loss: 0.21371569172191915\n",
      "Epoch: 8926 Training Loss: 0.21726550404811737 Test Loss: 0.21363022824413294\n",
      "Epoch: 8927 Training Loss: 0.21726268143465946 Test Loss: 0.21369139950936472\n",
      "Epoch: 8928 Training Loss: 0.2172637869896441 Test Loss: 0.21369836051157481\n",
      "Epoch: 8929 Training Loss: 0.21726573186015394 Test Loss: 0.2137286155938064\n",
      "Epoch: 8930 Training Loss: 0.2172652685613552 Test Loss: 0.21375378927405225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8931 Training Loss: 0.21726477838308517 Test Loss: 0.21378303326099077\n",
      "Epoch: 8932 Training Loss: 0.21726479999963128 Test Loss: 0.21375669293232982\n",
      "Epoch: 8933 Training Loss: 0.2172645085842027 Test Loss: 0.21371512136047174\n",
      "Epoch: 8934 Training Loss: 0.21726521795837458 Test Loss: 0.21370180860577945\n",
      "Epoch: 8935 Training Loss: 0.21726673585951112 Test Loss: 0.2137042326419308\n",
      "Epoch: 8936 Training Loss: 0.21726435640766303 Test Loss: 0.21369874939437986\n",
      "Epoch: 8937 Training Loss: 0.2172665355186887 Test Loss: 0.21377980553370898\n",
      "Epoch: 8938 Training Loss: 0.21726402220743227 Test Loss: 0.2137652353912804\n",
      "Epoch: 8939 Training Loss: 0.21726323290314484 Test Loss: 0.21381238095001062\n",
      "Epoch: 8940 Training Loss: 0.2172638718330186 Test Loss: 0.21377302601014123\n",
      "Epoch: 8941 Training Loss: 0.21726519155409074 Test Loss: 0.21381989935090792\n",
      "Epoch: 8942 Training Loss: 0.2172644917015995 Test Loss: 0.21465633441349422\n",
      "Epoch: 8943 Training Loss: 0.2172595011574697 Test Loss: 0.21470307812665923\n",
      "Epoch: 8944 Training Loss: 0.21726489593370152 Test Loss: 0.2141988786071732\n",
      "Epoch: 8945 Training Loss: 0.21726552301975222 Test Loss: 0.21397623023853138\n",
      "Epoch: 8946 Training Loss: 0.21726291967392256 Test Loss: 0.21381244576381145\n",
      "Epoch: 8947 Training Loss: 0.21726600185628403 Test Loss: 0.21378007775167251\n",
      "Epoch: 8948 Training Loss: 0.21726550344740872 Test Loss: 0.21375013377568494\n",
      "Epoch: 8949 Training Loss: 0.2172627960534589 Test Loss: 0.21369282541298318\n",
      "Epoch: 8950 Training Loss: 0.21726499538236577 Test Loss: 0.21718774110733913\n",
      "Epoch: 8951 Training Loss: 0.21726536962386359 Test Loss: 0.21377924813502178\n",
      "Epoch: 8952 Training Loss: 0.21726510358165174 Test Loss: 0.2137591558567617\n",
      "Epoch: 8953 Training Loss: 0.2172644047064328 Test Loss: 0.21374575236274823\n",
      "Epoch: 8954 Training Loss: 0.21726551248493609 Test Loss: 0.21371267139880004\n",
      "Epoch: 8955 Training Loss: 0.2172633267302519 Test Loss: 0.2137290304021318\n",
      "Epoch: 8956 Training Loss: 0.21726655110125076 Test Loss: 0.21383050288872518\n",
      "Epoch: 8957 Training Loss: 0.21726571424235505 Test Loss: 0.21370725296504992\n",
      "Epoch: 8958 Training Loss: 0.21726470519525193 Test Loss: 0.21372527120168314\n",
      "Epoch: 8959 Training Loss: 0.21726127389356692 Test Loss: 0.21454865276478044\n",
      "Epoch: 8960 Training Loss: 0.21726741820179313 Test Loss: 0.21384826187015504\n",
      "Epoch: 8961 Training Loss: 0.21726328607034454 Test Loss: 0.2137103640274902\n",
      "Epoch: 8962 Training Loss: 0.2172638428017551 Test Loss: 0.21374166909329537\n",
      "Epoch: 8963 Training Loss: 0.21726586215117347 Test Loss: 0.21378843873198072\n",
      "Epoch: 8964 Training Loss: 0.21726363511794167 Test Loss: 0.21377293527082006\n",
      "Epoch: 8965 Training Loss: 0.21726484161887932 Test Loss: 0.2140405125662034\n",
      "Epoch: 8966 Training Loss: 0.21726221293569614 Test Loss: 0.21432245259985266\n",
      "Epoch: 8967 Training Loss: 0.21726328897526404 Test Loss: 0.21397753947730833\n",
      "Epoch: 8968 Training Loss: 0.21726104584841952 Test Loss: 0.21391018497547654\n",
      "Epoch: 8969 Training Loss: 0.2172635540043404 Test Loss: 0.213747048638765\n",
      "Epoch: 8970 Training Loss: 0.2172675714990579 Test Loss: 0.2138598246522247\n",
      "Epoch: 8971 Training Loss: 0.217264219652301 Test Loss: 0.21371560098259795\n",
      "Epoch: 8972 Training Loss: 0.2172653911776491 Test Loss: 0.21369789385220878\n",
      "Epoch: 8973 Training Loss: 0.21726562046007702 Test Loss: 0.21369580684782175\n",
      "Epoch: 8974 Training Loss: 0.21726608456579788 Test Loss: 0.21365730745012346\n",
      "Epoch: 8975 Training Loss: 0.21726496348204596 Test Loss: 0.2137717945479253\n",
      "Epoch: 8976 Training Loss: 0.21726444711467133 Test Loss: 0.21373237479425508\n",
      "Epoch: 8977 Training Loss: 0.21726507824429822 Test Loss: 0.2136753127239965\n",
      "Epoch: 8978 Training Loss: 0.2172664857405618 Test Loss: 0.21375756143726107\n",
      "Epoch: 8979 Training Loss: 0.21726541689156625 Test Loss: 0.21371041587853085\n",
      "Epoch: 8980 Training Loss: 0.2172648237141748 Test Loss: 0.21500978999498913\n",
      "Epoch: 8981 Training Loss: 0.21726105424937503 Test Loss: 0.2151327936262212\n",
      "Epoch: 8982 Training Loss: 0.21726350039781647 Test Loss: 0.2143781535802936\n",
      "Epoch: 8983 Training Loss: 0.2172617084031788 Test Loss: 0.2140539549484974\n",
      "Epoch: 8984 Training Loss: 0.21726445701291558 Test Loss: 0.2138419101176728\n",
      "Epoch: 8985 Training Loss: 0.2172661446276985 Test Loss: 0.21387327999727887\n",
      "Epoch: 8986 Training Loss: 0.21726669138017257 Test Loss: 0.2137531670615642\n",
      "Epoch: 8987 Training Loss: 0.21726445226104107 Test Loss: 0.21378645542967506\n",
      "Epoch: 8988 Training Loss: 0.21726513081078924 Test Loss: 0.2137175972476638\n",
      "Epoch: 8989 Training Loss: 0.21726433021855843 Test Loss: 0.21372923780629446\n",
      "Epoch: 8990 Training Loss: 0.21726548619720767 Test Loss: 0.21372529712720345\n",
      "Epoch: 8991 Training Loss: 0.21726420786227274 Test Loss: 0.21381217354584794\n",
      "Epoch: 8992 Training Loss: 0.21726445676187314 Test Loss: 0.21374380794872305\n",
      "Epoch: 8993 Training Loss: 0.21726452510817387 Test Loss: 0.21375109301993736\n",
      "Epoch: 8994 Training Loss: 0.21726541624602858 Test Loss: 0.2137841091700847\n",
      "Epoch: 8995 Training Loss: 0.2172652032006662 Test Loss: 0.2137288229979691\n",
      "Epoch: 8996 Training Loss: 0.2172661813067902 Test Loss: 0.2137636280090196\n",
      "Epoch: 8997 Training Loss: 0.21726402672619596 Test Loss: 0.21405050685429278\n",
      "Epoch: 8998 Training Loss: 0.21726127776679294 Test Loss: 0.21439023487276998\n",
      "Epoch: 8999 Training Loss: 0.2172644654138711 Test Loss: 0.21551098215411624\n",
      "Epoch: 9000 Training Loss: 0.2172647012682311 Test Loss: 0.21387731141569102\n",
      "Epoch: 9001 Training Loss: 0.2172638645976172 Test Loss: 0.21380890693028565\n",
      "Epoch: 9002 Training Loss: 0.21726374796151254 Test Loss: 0.2137632261634544\n",
      "Epoch: 9003 Training Loss: 0.2172660773303965 Test Loss: 0.2137925479269539\n",
      "Epoch: 9004 Training Loss: 0.21726503925202986 Test Loss: 0.2136808218970678\n",
      "Epoch: 9005 Training Loss: 0.21725986325927302 Test Loss: 0.21462063497199213\n",
      "Epoch: 9006 Training Loss: 0.2172664709380244 Test Loss: 0.21370787517753798\n",
      "Epoch: 9007 Training Loss: 0.21726671286223168 Test Loss: 0.21384271380880324\n",
      "Epoch: 9008 Training Loss: 0.21726529833678018 Test Loss: 0.21379269051731575\n",
      "Epoch: 9009 Training Loss: 0.21726455944719153 Test Loss: 0.21378817947677736\n",
      "Epoch: 9010 Training Loss: 0.21726449026707134 Test Loss: 0.21374199316229955\n",
      "Epoch: 9011 Training Loss: 0.21726564149384608 Test Loss: 0.213817047543671\n",
      "Epoch: 9012 Training Loss: 0.2172651976598012 Test Loss: 0.2136899087919454\n",
      "Epoch: 9013 Training Loss: 0.21726501779686816 Test Loss: 0.21386786156352872\n",
      "Epoch: 9014 Training Loss: 0.21726690036402713 Test Loss: 0.21378771281741132\n",
      "Epoch: 9015 Training Loss: 0.21726535046394696 Test Loss: 0.21384770447146784\n",
      "Epoch: 9016 Training Loss: 0.21726453410983804 Test Loss: 0.21382632887995115\n",
      "Epoch: 9017 Training Loss: 0.21726462135604716 Test Loss: 0.21381803271344377\n",
      "Epoch: 9018 Training Loss: 0.21726403030355054 Test Loss: 0.21379832931798876\n",
      "Epoch: 9019 Training Loss: 0.21726476235223305 Test Loss: 0.21371363064305246\n",
      "Epoch: 9020 Training Loss: 0.21726547497202486 Test Loss: 0.2137988737539158\n",
      "Epoch: 9021 Training Loss: 0.2172648835967594 Test Loss: 0.21394651959222682\n",
      "Epoch: 9022 Training Loss: 0.21726390583133584 Test Loss: 0.2138007144658596\n",
      "Epoch: 9023 Training Loss: 0.21726344601126785 Test Loss: 0.2137442746080891\n",
      "Epoch: 9024 Training Loss: 0.21726435381654655 Test Loss: 0.21367614234064725\n",
      "Epoch: 9025 Training Loss: 0.21726531343518898 Test Loss: 0.213727578572993\n",
      "Epoch: 9026 Training Loss: 0.2172662279199894 Test Loss: 0.21377608522154082\n",
      "Epoch: 9027 Training Loss: 0.21726338350170357 Test Loss: 0.21376397800354413\n",
      "Epoch: 9028 Training Loss: 0.21726517395422346 Test Loss: 0.2137802333047945\n",
      "Epoch: 9029 Training Loss: 0.21726432512598348 Test Loss: 0.2137498226694409\n",
      "Epoch: 9030 Training Loss: 0.21726475656032562 Test Loss: 0.21377841851837104\n",
      "Epoch: 9031 Training Loss: 0.217264728165634 Test Loss: 0.21376511872643889\n",
      "Epoch: 9032 Training Loss: 0.21726422415313307 Test Loss: 0.21372123978327096\n",
      "Epoch: 9033 Training Loss: 0.21726427266708206 Test Loss: 0.2137364191754274\n",
      "Epoch: 9034 Training Loss: 0.21726453074766267 Test Loss: 0.21374895416450967\n",
      "Epoch: 9035 Training Loss: 0.21726446435590657 Test Loss: 0.21372477861679676\n",
      "Epoch: 9036 Training Loss: 0.2172645937413802 Test Loss: 0.21378850354578158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9037 Training Loss: 0.21726518888228205 Test Loss: 0.2138267177627562\n",
      "Epoch: 9038 Training Loss: 0.21726506030373052 Test Loss: 0.21378225549538069\n",
      "Epoch: 9039 Training Loss: 0.21726735213280585 Test Loss: 0.21387413553944992\n",
      "Epoch: 9040 Training Loss: 0.21726377579135872 Test Loss: 0.2136998901172746\n",
      "Epoch: 9041 Training Loss: 0.21726594724559037 Test Loss: 0.21360268237877642\n",
      "Epoch: 9042 Training Loss: 0.2172677292971548 Test Loss: 0.2136783719353961\n",
      "Epoch: 9043 Training Loss: 0.21726494749602285 Test Loss: 0.21366857208870924\n",
      "Epoch: 9044 Training Loss: 0.21726418858580068 Test Loss: 0.21362802457490443\n",
      "Epoch: 9045 Training Loss: 0.21726512301950823 Test Loss: 0.21373289330466178\n",
      "Epoch: 9046 Training Loss: 0.21726527376151974 Test Loss: 0.2136896754622624\n",
      "Epoch: 9047 Training Loss: 0.21726514142629758 Test Loss: 0.21373710620171632\n",
      "Epoch: 9048 Training Loss: 0.21726344731130898 Test Loss: 0.21410611709541255\n",
      "Epoch: 9049 Training Loss: 0.2172657473351264 Test Loss: 0.21374571347446772\n",
      "Epoch: 9050 Training Loss: 0.21726633504337925 Test Loss: 0.21372179718195816\n",
      "Epoch: 9051 Training Loss: 0.21726515613021116 Test Loss: 0.21376791868263512\n",
      "Epoch: 9052 Training Loss: 0.2172648993586375 Test Loss: 0.21487487358716284\n",
      "Epoch: 9053 Training Loss: 0.21726638812988674 Test Loss: 0.21373527845253265\n",
      "Epoch: 9054 Training Loss: 0.21726534008154944 Test Loss: 0.21365573895614315\n",
      "Epoch: 9055 Training Loss: 0.21726534674313955 Test Loss: 0.21370985847984364\n",
      "Epoch: 9056 Training Loss: 0.21726549646304977 Test Loss: 0.21370139379745406\n",
      "Epoch: 9057 Training Loss: 0.21726662205659955 Test Loss: 0.21373364514475152\n",
      "Epoch: 9058 Training Loss: 0.21726435919602713 Test Loss: 0.21376320023793405\n",
      "Epoch: 9059 Training Loss: 0.21726654149887795 Test Loss: 0.21374116354564882\n",
      "Epoch: 9060 Training Loss: 0.21726520537935584 Test Loss: 0.2138193938032614\n",
      "Epoch: 9061 Training Loss: 0.21726475687412866 Test Loss: 0.2137167935565334\n",
      "Epoch: 9062 Training Loss: 0.21726543084235256 Test Loss: 0.21385014147037937\n",
      "Epoch: 9063 Training Loss: 0.21726450974975683 Test Loss: 0.21381256242865296\n",
      "Epoch: 9064 Training Loss: 0.21726441804754462 Test Loss: 0.21379936633880217\n",
      "Epoch: 9065 Training Loss: 0.2172663315915459 Test Loss: 0.21377749816239913\n",
      "Epoch: 9066 Training Loss: 0.21726448094263834 Test Loss: 0.21378360362243815\n",
      "Epoch: 9067 Training Loss: 0.21726401552791055 Test Loss: 0.21372996372086386\n",
      "Epoch: 9068 Training Loss: 0.21726488022561824 Test Loss: 0.21385683025462593\n",
      "Epoch: 9069 Training Loss: 0.21726239052131574 Test Loss: 0.2139236273577705\n",
      "Epoch: 9070 Training Loss: 0.21726568304136773 Test Loss: 0.21384700448241875\n",
      "Epoch: 9071 Training Loss: 0.21726474160536963 Test Loss: 0.21380589956992674\n",
      "Epoch: 9072 Training Loss: 0.21726269925867178 Test Loss: 0.2143261729120208\n",
      "Epoch: 9073 Training Loss: 0.21726538214012173 Test Loss: 0.21375795032006611\n",
      "Epoch: 9074 Training Loss: 0.21726407491737612 Test Loss: 0.21373653584026892\n",
      "Epoch: 9075 Training Loss: 0.21726484818184563 Test Loss: 0.21376522242852022\n",
      "Epoch: 9076 Training Loss: 0.21726471082577492 Test Loss: 0.2137646779925932\n",
      "Epoch: 9077 Training Loss: 0.21726481115308766 Test Loss: 0.213679058961685\n",
      "Epoch: 9078 Training Loss: 0.21726611374948 Test Loss: 0.21365243345230037\n",
      "Epoch: 9079 Training Loss: 0.21726398559110116 Test Loss: 0.2147755918070378\n",
      "Epoch: 9080 Training Loss: 0.21726707921382465 Test Loss: 0.21373422846895906\n",
      "Epoch: 9081 Training Loss: 0.21726232509786614 Test Loss: 0.2137642631842678\n",
      "Epoch: 9082 Training Loss: 0.2172652276324738 Test Loss: 0.2137690594055299\n",
      "Epoch: 9083 Training Loss: 0.21726317856142524 Test Loss: 0.21370917145355475\n",
      "Epoch: 9084 Training Loss: 0.21726412005121815 Test Loss: 0.213667548030656\n",
      "Epoch: 9085 Training Loss: 0.21726532284928 Test Loss: 0.2139307568758628\n",
      "Epoch: 9086 Training Loss: 0.2172646169986679 Test Loss: 0.21423702801034697\n",
      "Epoch: 9087 Training Loss: 0.21726311357729988 Test Loss: 0.21382046971235533\n",
      "Epoch: 9088 Training Loss: 0.21726543343346902 Test Loss: 0.21376855385788335\n",
      "Epoch: 9089 Training Loss: 0.21726451593615947 Test Loss: 0.21369238467913745\n",
      "Epoch: 9090 Training Loss: 0.2172625462662442 Test Loss: 0.21370390857292662\n",
      "Epoch: 9091 Training Loss: 0.2172650469805503 Test Loss: 0.21381310686458002\n",
      "Epoch: 9092 Training Loss: 0.2172634213104862 Test Loss: 0.21376069842522166\n",
      "Epoch: 9093 Training Loss: 0.21726456636878988 Test Loss: 0.2137493041590342\n",
      "Epoch: 9094 Training Loss: 0.2172623750284117 Test Loss: 0.21370834183690401\n",
      "Epoch: 9095 Training Loss: 0.21726502312255394 Test Loss: 0.2137988737539158\n",
      "Epoch: 9096 Training Loss: 0.21726277834600202 Test Loss: 0.2138096976586559\n",
      "Epoch: 9097 Training Loss: 0.21726297788886817 Test Loss: 0.21378007775167251\n",
      "Epoch: 9098 Training Loss: 0.21726382209972067 Test Loss: 0.21369963086207125\n",
      "Epoch: 9099 Training Loss: 0.21726597666238331 Test Loss: 0.2139257662131982\n",
      "Epoch: 9100 Training Loss: 0.2172657950780165 Test Loss: 0.21378604062134968\n",
      "Epoch: 9101 Training Loss: 0.2172660250149479 Test Loss: 0.213735615484297\n",
      "Epoch: 9102 Training Loss: 0.2172655240328877 Test Loss: 0.2136952624118947\n",
      "Epoch: 9103 Training Loss: 0.21726651462837246 Test Loss: 0.21371388989825582\n",
      "Epoch: 9104 Training Loss: 0.21726533523105113 Test Loss: 0.2137244675105527\n",
      "Epoch: 9105 Training Loss: 0.2172643626657921 Test Loss: 0.21371189363319\n",
      "Epoch: 9106 Training Loss: 0.21726477715477044 Test Loss: 0.2137128917657229\n",
      "Epoch: 9107 Training Loss: 0.2172659521229861 Test Loss: 0.2135853900567126\n",
      "Epoch: 9108 Training Loss: 0.21726592486695118 Test Loss: 0.21368754956959488\n",
      "Epoch: 9109 Training Loss: 0.21726609110186676 Test Loss: 0.21376043917001833\n",
      "Epoch: 9110 Training Loss: 0.2172655024342732 Test Loss: 0.21375286891808035\n",
      "Epoch: 9111 Training Loss: 0.21726384332177154 Test Loss: 0.21440407910062917\n",
      "Epoch: 9112 Training Loss: 0.21726766636619785 Test Loss: 0.21377769260380164\n",
      "Epoch: 9113 Training Loss: 0.21726402817865573 Test Loss: 0.2137208120121854\n",
      "Epoch: 9114 Training Loss: 0.21726583215160347 Test Loss: 0.21380741621286636\n",
      "Epoch: 9115 Training Loss: 0.21726522781178984 Test Loss: 0.21382267338158384\n",
      "Epoch: 9116 Training Loss: 0.21726683475229572 Test Loss: 0.21375822253802962\n",
      "Epoch: 9117 Training Loss: 0.21726578642601857 Test Loss: 0.2136718127787512\n",
      "Epoch: 9118 Training Loss: 0.21726462727347579 Test Loss: 0.21371215288839332\n",
      "Epoch: 9119 Training Loss: 0.21726493798330804 Test Loss: 0.21373813025976957\n",
      "Epoch: 9120 Training Loss: 0.21726445882400738 Test Loss: 0.21376260395096633\n",
      "Epoch: 9121 Training Loss: 0.2172654146859792 Test Loss: 0.21375259670011681\n",
      "Epoch: 9122 Training Loss: 0.2172650376830147 Test Loss: 0.2137907461032906\n",
      "Epoch: 9123 Training Loss: 0.21726465228806047 Test Loss: 0.21488441417864634\n",
      "Epoch: 9124 Training Loss: 0.21726486849835058 Test Loss: 0.21383166953714028\n",
      "Epoch: 9125 Training Loss: 0.21726340424856697 Test Loss: 0.21377377785023097\n",
      "Epoch: 9126 Training Loss: 0.2172650787284515 Test Loss: 0.21376566316236592\n",
      "Epoch: 9127 Training Loss: 0.21726373150030198 Test Loss: 0.21380701436730115\n",
      "Epoch: 9128 Training Loss: 0.21726428828550734 Test Loss: 0.21380902359512716\n",
      "Epoch: 9129 Training Loss: 0.217266188569089 Test Loss: 0.21372288605381226\n",
      "Epoch: 9130 Training Loss: 0.21726616315104327 Test Loss: 0.2137553448052724\n",
      "Epoch: 9131 Training Loss: 0.21726552571845828 Test Loss: 0.2137710297450754\n",
      "Epoch: 9132 Training Loss: 0.21726581620144356 Test Loss: 0.21372520638788228\n",
      "Epoch: 9133 Training Loss: 0.21726360504664524 Test Loss: 0.213801440380429\n",
      "Epoch: 9134 Training Loss: 0.21726376088123173 Test Loss: 0.21368220891240575\n",
      "Epoch: 9135 Training Loss: 0.21726505079998149 Test Loss: 0.2138276381187281\n",
      "Epoch: 9136 Training Loss: 0.21726305810588936 Test Loss: 0.21378767392913084\n",
      "Epoch: 9137 Training Loss: 0.21726300746704555 Test Loss: 0.21371626208336653\n",
      "Epoch: 9138 Training Loss: 0.21726474017084146 Test Loss: 0.21378697394008175\n",
      "Epoch: 9139 Training Loss: 0.21726606174783442 Test Loss: 0.21370106972844988\n",
      "Epoch: 9140 Training Loss: 0.21726531513869116 Test Loss: 0.21378469249429224\n",
      "Epoch: 9141 Training Loss: 0.21726611999864326 Test Loss: 0.21372174533091748\n",
      "Epoch: 9142 Training Loss: 0.2172654704442954 Test Loss: 0.2136160469845094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9143 Training Loss: 0.21726636387739515 Test Loss: 0.21536941585032388\n",
      "Epoch: 9144 Training Loss: 0.21726521178990352 Test Loss: 0.21373836358945258\n",
      "Epoch: 9145 Training Loss: 0.21726563653575814 Test Loss: 0.21373776730248487\n",
      "Epoch: 9146 Training Loss: 0.21726576196731356 Test Loss: 0.21371479729146756\n",
      "Epoch: 9147 Training Loss: 0.21726606565692363 Test Loss: 0.2137692149586519\n",
      "Epoch: 9148 Training Loss: 0.21726476088184168 Test Loss: 0.21376955199041628\n",
      "Epoch: 9149 Training Loss: 0.21726451625892831 Test Loss: 0.21379755155237867\n",
      "Epoch: 9150 Training Loss: 0.21726410419071623 Test Loss: 0.21374181168365722\n",
      "Epoch: 9151 Training Loss: 0.21726553783125538 Test Loss: 0.21374856528170463\n",
      "Epoch: 9152 Training Loss: 0.2172660390464264 Test Loss: 0.21368423110299192\n",
      "Epoch: 9153 Training Loss: 0.21726432291143064 Test Loss: 0.213634868912273\n",
      "Epoch: 9154 Training Loss: 0.21726464702513532 Test Loss: 0.21404516619710365\n",
      "Epoch: 9155 Training Loss: 0.21726509704558283 Test Loss: 0.2140351589462541\n",
      "Epoch: 9156 Training Loss: 0.21726671322086372 Test Loss: 0.2138693911692285\n",
      "Epoch: 9157 Training Loss: 0.21726331897483406 Test Loss: 0.21378453694117022\n",
      "Epoch: 9158 Training Loss: 0.21726494814156053 Test Loss: 0.21375920770780238\n",
      "Epoch: 9159 Training Loss: 0.21726628890536753 Test Loss: 0.2137062418697568\n",
      "Epoch: 9160 Training Loss: 0.2172629108426086 Test Loss: 0.21756357041288363\n",
      "Epoch: 9161 Training Loss: 0.2172663662085034 Test Loss: 0.2137804018206767\n",
      "Epoch: 9162 Training Loss: 0.21726584580651834 Test Loss: 0.21369798459152994\n",
      "Epoch: 9163 Training Loss: 0.21726459689734215 Test Loss: 0.21371338435060927\n",
      "Epoch: 9164 Training Loss: 0.2172662597037538 Test Loss: 0.2138408730968594\n",
      "Epoch: 9165 Training Loss: 0.21726512798656195 Test Loss: 0.2138257066674631\n",
      "Epoch: 9166 Training Loss: 0.21726465405432327 Test Loss: 0.21370663075256185\n",
      "Epoch: 9167 Training Loss: 0.2172644440125042 Test Loss: 0.21372322308557662\n",
      "Epoch: 9168 Training Loss: 0.2172655323352194 Test Loss: 0.21379486826102395\n",
      "Epoch: 9169 Training Loss: 0.21726385302276818 Test Loss: 0.21370700667260673\n",
      "Epoch: 9170 Training Loss: 0.21726537775584506 Test Loss: 0.21381770864443958\n",
      "Epoch: 9171 Training Loss: 0.21726486153192326 Test Loss: 0.21377990923579032\n",
      "Epoch: 9172 Training Loss: 0.2172659605418732 Test Loss: 0.21382512334325554\n",
      "Epoch: 9173 Training Loss: 0.2172648339800169 Test Loss: 0.21375995954789212\n",
      "Epoch: 9174 Training Loss: 0.2172641465989548 Test Loss: 0.21372634711077707\n",
      "Epoch: 9175 Training Loss: 0.21726596609170398 Test Loss: 0.21377769260380164\n",
      "Epoch: 9176 Training Loss: 0.21726460445551235 Test Loss: 0.21369869754333917\n",
      "Epoch: 9177 Training Loss: 0.21726366888314808 Test Loss: 0.21505330598087236\n",
      "Epoch: 9178 Training Loss: 0.21726627218414873 Test Loss: 0.21377192417552698\n",
      "Epoch: 9179 Training Loss: 0.21726384355488237 Test Loss: 0.21378304622375094\n",
      "Epoch: 9180 Training Loss: 0.21726451000079924 Test Loss: 0.21376638907693532\n",
      "Epoch: 9181 Training Loss: 0.21726324498904453 Test Loss: 0.21376332986553573\n",
      "Epoch: 9182 Training Loss: 0.21726573844105185 Test Loss: 0.21378093329384357\n",
      "Epoch: 9183 Training Loss: 0.21726451658169715 Test Loss: 0.21380255517780344\n",
      "Epoch: 9184 Training Loss: 0.21726426863247164 Test Loss: 0.2137464005007566\n",
      "Epoch: 9185 Training Loss: 0.2172657978394832 Test Loss: 0.21374510422473983\n",
      "Epoch: 9186 Training Loss: 0.21726444281108687 Test Loss: 0.21368607181493576\n",
      "Epoch: 9187 Training Loss: 0.2172640804313437 Test Loss: 0.2137243508457112\n",
      "Epoch: 9188 Training Loss: 0.21726531778360245 Test Loss: 0.21360921560990098\n",
      "Epoch: 9189 Training Loss: 0.21726697188422137 Test Loss: 0.21509671826467425\n",
      "Epoch: 9190 Training Loss: 0.21726726112992611 Test Loss: 0.21378986463559918\n",
      "Epoch: 9191 Training Loss: 0.21726465235978687 Test Loss: 0.2137379228556069\n",
      "Epoch: 9192 Training Loss: 0.21726517259142172 Test Loss: 0.21376148915359192\n",
      "Epoch: 9193 Training Loss: 0.21726441568953897 Test Loss: 0.2137167935565334\n",
      "Epoch: 9194 Training Loss: 0.21726382903925062 Test Loss: 0.21367198129463338\n",
      "Epoch: 9195 Training Loss: 0.21726731463782623 Test Loss: 0.21375833920287113\n",
      "Epoch: 9196 Training Loss: 0.2172640261792821 Test Loss: 0.2137449227460975\n",
      "Epoch: 9197 Training Loss: 0.21726491867993855 Test Loss: 0.21375853364427366\n",
      "Epoch: 9198 Training Loss: 0.2172632895759727 Test Loss: 0.2137652742795609\n",
      "Epoch: 9199 Training Loss: 0.21726393021831444 Test Loss: 0.21378202216569767\n",
      "Epoch: 9200 Training Loss: 0.21726465682475576 Test Loss: 0.2153275202094616\n",
      "Epoch: 9201 Training Loss: 0.2172646001070989 Test Loss: 0.21370401227500796\n",
      "Epoch: 9202 Training Loss: 0.2172645196838643 Test Loss: 0.2137513522751407\n",
      "Epoch: 9203 Training Loss: 0.21726461201368255 Test Loss: 0.21376038731897765\n",
      "Epoch: 9204 Training Loss: 0.2172658484155664 Test Loss: 0.21371124549518158\n",
      "Epoch: 9205 Training Loss: 0.21726446770911612 Test Loss: 0.2136637240164065\n",
      "Epoch: 9206 Training Loss: 0.21726336674462157 Test Loss: 0.2136881069682821\n",
      "Epoch: 9207 Training Loss: 0.2172668410373222 Test Loss: 0.2139759061695272\n",
      "Epoch: 9208 Training Loss: 0.21726339907529982 Test Loss: 0.21429976776955903\n",
      "Epoch: 9209 Training Loss: 0.21726477197253746 Test Loss: 0.2138986092306467\n",
      "Epoch: 9210 Training Loss: 0.21726483880361783 Test Loss: 0.21381566052833306\n",
      "Epoch: 9211 Training Loss: 0.21726315357373796 Test Loss: 0.21385279883621378\n",
      "Epoch: 9212 Training Loss: 0.21726452032043617 Test Loss: 0.2137699408732213\n",
      "Epoch: 9213 Training Loss: 0.21726390454922628 Test Loss: 0.2137362247340249\n",
      "Epoch: 9214 Training Loss: 0.21726565197486739 Test Loss: 0.2138384231351877\n",
      "Epoch: 9215 Training Loss: 0.21726546023224808 Test Loss: 0.2136829607524955\n",
      "Epoch: 9216 Training Loss: 0.2172649396419812 Test Loss: 0.2136869792081475\n",
      "Epoch: 9217 Training Loss: 0.21726609510957978 Test Loss: 0.21369992900555512\n",
      "Epoch: 9218 Training Loss: 0.21726432000651114 Test Loss: 0.2137826573409459\n",
      "Epoch: 9219 Training Loss: 0.21726452763652973 Test Loss: 0.21375222078007194\n",
      "Epoch: 9220 Training Loss: 0.21726482066580247 Test Loss: 0.2137423042685436\n",
      "Epoch: 9221 Training Loss: 0.21726390903212678 Test Loss: 0.2136976864480461\n",
      "Epoch: 9222 Training Loss: 0.21726112639717535 Test Loss: 0.213687018096428\n",
      "Epoch: 9223 Training Loss: 0.2172642177515512 Test Loss: 0.213708108507221\n",
      "Epoch: 9224 Training Loss: 0.2172645311062947 Test Loss: 0.2137223027296047\n",
      "Epoch: 9225 Training Loss: 0.21726475366437192 Test Loss: 0.21374875972310714\n",
      "Epoch: 9226 Training Loss: 0.21726354972765333 Test Loss: 0.21370254748310902\n",
      "Epoch: 9227 Training Loss: 0.21726505957750064 Test Loss: 0.21380882915372465\n",
      "Epoch: 9228 Training Loss: 0.21726514638438552 Test Loss: 0.2136629203252761\n",
      "Epoch: 9229 Training Loss: 0.21726265411586393 Test Loss: 0.21380955506829405\n",
      "Epoch: 9230 Training Loss: 0.2172646273452022 Test Loss: 0.2139196089021185\n",
      "Epoch: 9231 Training Loss: 0.21726479767748882 Test Loss: 0.21372776005163535\n",
      "Epoch: 9232 Training Loss: 0.2172640571202612 Test Loss: 0.2136756367930007\n",
      "Epoch: 9233 Training Loss: 0.21726395268661164 Test Loss: 0.21374878564862748\n",
      "Epoch: 9234 Training Loss: 0.21726470013854018 Test Loss: 0.2164219660504274\n",
      "Epoch: 9235 Training Loss: 0.21726430144730313 Test Loss: 0.2137448320067763\n",
      "Epoch: 9236 Training Loss: 0.2172632706312353 Test Loss: 0.21369794570324943\n",
      "Epoch: 9237 Training Loss: 0.2172654014614228 Test Loss: 0.21369352540203224\n",
      "Epoch: 9238 Training Loss: 0.21726556714942452 Test Loss: 0.21373496734628863\n",
      "Epoch: 9239 Training Loss: 0.21726529258073596 Test Loss: 0.21373737841967982\n",
      "Epoch: 9240 Training Loss: 0.21726504192383855 Test Loss: 0.21370997514468515\n",
      "Epoch: 9241 Training Loss: 0.21726670827174158 Test Loss: 0.21374126724773018\n",
      "Epoch: 9242 Training Loss: 0.21726467377908537 Test Loss: 0.21422915961492514\n",
      "Epoch: 9243 Training Loss: 0.21726247908549762 Test Loss: 0.21623571711233674\n",
      "Epoch: 9244 Training Loss: 0.21726501482918806 Test Loss: 0.21407301020594405\n",
      "Epoch: 9245 Training Loss: 0.21726483263514676 Test Loss: 0.21395875643782522\n",
      "Epoch: 9246 Training Loss: 0.21726383484012382 Test Loss: 0.21391087200176542\n",
      "Epoch: 9247 Training Loss: 0.21726331142562966 Test Loss: 0.2137951664045078\n",
      "Epoch: 9248 Training Loss: 0.21726285140831406 Test Loss: 0.21368787363859906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9249 Training Loss: 0.217264335687697 Test Loss: 0.21378344806931612\n",
      "Epoch: 9250 Training Loss: 0.21726454997930572 Test Loss: 0.2137148621052684\n",
      "Epoch: 9251 Training Loss: 0.217263168932155 Test Loss: 0.21445744678423995\n",
      "Epoch: 9252 Training Loss: 0.21726592059026412 Test Loss: 0.21375254484907613\n",
      "Epoch: 9253 Training Loss: 0.21726575612161134 Test Loss: 0.21379362383604783\n",
      "Epoch: 9254 Training Loss: 0.21726425498652258 Test Loss: 0.21373575807465886\n",
      "Epoch: 9255 Training Loss: 0.21726473215541542 Test Loss: 0.21380937358965169\n",
      "Epoch: 9256 Training Loss: 0.21726406132522186 Test Loss: 0.21371815464635102\n",
      "Epoch: 9257 Training Loss: 0.21726427177946778 Test Loss: 0.21379025351840422\n",
      "Epoch: 9258 Training Loss: 0.21726331568438512 Test Loss: 0.2137474893726107\n",
      "Epoch: 9259 Training Loss: 0.2172633821568334 Test Loss: 0.21380818101571625\n",
      "Epoch: 9260 Training Loss: 0.21726430886202053 Test Loss: 0.21378067403864023\n",
      "Epoch: 9261 Training Loss: 0.2172641877968102 Test Loss: 0.2137459597669109\n",
      "Epoch: 9262 Training Loss: 0.2172645729048588 Test Loss: 0.21385803579132154\n",
      "Epoch: 9263 Training Loss: 0.21726300641804683 Test Loss: 0.214086297035116\n",
      "Epoch: 9264 Training Loss: 0.2172637137749135 Test Loss: 0.21373426735723958\n",
      "Epoch: 9265 Training Loss: 0.2172649517009835 Test Loss: 0.21373612103194356\n",
      "Epoch: 9266 Training Loss: 0.21726449241886356 Test Loss: 0.21376511872643889\n",
      "Epoch: 9267 Training Loss: 0.21726597531751318 Test Loss: 0.2137267748818626\n",
      "Epoch: 9268 Training Loss: 0.21726588633193866 Test Loss: 0.21369413465176013\n",
      "Epoch: 9269 Training Loss: 0.21726465123009595 Test Loss: 0.21374476719297547\n",
      "Epoch: 9270 Training Loss: 0.21726436334719296 Test Loss: 0.21376130767494955\n",
      "Epoch: 9271 Training Loss: 0.2172658763709338 Test Loss: 0.2137345006869226\n",
      "Epoch: 9272 Training Loss: 0.2172631081619561 Test Loss: 0.21775619702897686\n",
      "Epoch: 9273 Training Loss: 0.21726624968895414 Test Loss: 0.21390541467973478\n",
      "Epoch: 9274 Training Loss: 0.21726452419366216 Test Loss: 0.2137206564590634\n",
      "Epoch: 9275 Training Loss: 0.21726598241842754 Test Loss: 0.21377592966841882\n",
      "Epoch: 9276 Training Loss: 0.217264881722907 Test Loss: 0.21373558955877667\n",
      "Epoch: 9277 Training Loss: 0.21726544736632372 Test Loss: 0.21376076323902252\n",
      "Epoch: 9278 Training Loss: 0.2172643469577088 Test Loss: 0.21376698536390304\n",
      "Epoch: 9279 Training Loss: 0.2172652061414489 Test Loss: 0.21375941511196506\n",
      "Epoch: 9280 Training Loss: 0.2172664852115795 Test Loss: 0.21367179981599102\n",
      "Epoch: 9281 Training Loss: 0.21726509206956332 Test Loss: 0.2137305081567909\n",
      "Epoch: 9282 Training Loss: 0.2172651939658912 Test Loss: 0.2138277547835696\n",
      "Epoch: 9283 Training Loss: 0.2172657481061853 Test Loss: 0.21379196460274635\n",
      "Epoch: 9284 Training Loss: 0.21726491483360996 Test Loss: 0.21377465931792236\n",
      "Epoch: 9285 Training Loss: 0.21726328731659086 Test Loss: 0.21374152650293352\n",
      "Epoch: 9286 Training Loss: 0.21726460790734572 Test Loss: 0.21371831019947302\n",
      "Epoch: 9287 Training Loss: 0.2172650611016868 Test Loss: 0.2136736275651747\n",
      "Epoch: 9288 Training Loss: 0.21726511140879598 Test Loss: 0.21374772270229372\n",
      "Epoch: 9289 Training Loss: 0.2172650256598756 Test Loss: 0.21372983409326218\n",
      "Epoch: 9290 Training Loss: 0.2172650496882222 Test Loss: 0.2137928331076776\n",
      "Epoch: 9291 Training Loss: 0.21726631434134486 Test Loss: 0.21375931140988372\n",
      "Epoch: 9292 Training Loss: 0.21726495956399094 Test Loss: 0.21388746125690242\n",
      "Epoch: 9293 Training Loss: 0.2172648152952877 Test Loss: 0.21384587672228417\n",
      "Epoch: 9294 Training Loss: 0.2172635363596441 Test Loss: 0.21385935799285866\n",
      "Epoch: 9295 Training Loss: 0.21726548417990246 Test Loss: 0.21366048332636456\n",
      "Epoch: 9296 Training Loss: 0.21726576753507595 Test Loss: 0.2137218231074785\n",
      "Epoch: 9297 Training Loss: 0.21726497658108115 Test Loss: 0.21368524219828502\n",
      "Epoch: 9298 Training Loss: 0.21726603662566013 Test Loss: 0.21395165284525328\n",
      "Epoch: 9299 Training Loss: 0.21726535022187032 Test Loss: 0.2138445804462674\n",
      "Epoch: 9300 Training Loss: 0.21726541582463593 Test Loss: 0.21375373742301157\n",
      "Epoch: 9301 Training Loss: 0.21726468378491923 Test Loss: 0.2137508596902543\n",
      "Epoch: 9302 Training Loss: 0.217264143030566 Test Loss: 0.21378311103755177\n",
      "Epoch: 9303 Training Loss: 0.21726382769438046 Test Loss: 0.21375530591699188\n",
      "Epoch: 9304 Training Loss: 0.21726463141567584 Test Loss: 0.2138618727683312\n",
      "Epoch: 9305 Training Loss: 0.21726509278682737 Test Loss: 0.21372454528711374\n",
      "Epoch: 9306 Training Loss: 0.21726693318782445 Test Loss: 0.21372941928493683\n",
      "Epoch: 9307 Training Loss: 0.21726500720825723 Test Loss: 0.21369955308551025\n",
      "Epoch: 9308 Training Loss: 0.21726428559576705 Test Loss: 0.21372877114692843\n",
      "Epoch: 9309 Training Loss: 0.217265558524324 Test Loss: 0.2137559410922401\n",
      "Epoch: 9310 Training Loss: 0.21726480710054563 Test Loss: 0.2137208120121854\n",
      "Epoch: 9311 Training Loss: 0.21726373378658123 Test Loss: 0.21375390593889376\n",
      "Epoch: 9312 Training Loss: 0.21726419113208814 Test Loss: 0.21379829042970824\n",
      "Epoch: 9313 Training Loss: 0.2172645562105374 Test Loss: 0.21377023901670517\n",
      "Epoch: 9314 Training Loss: 0.21726403288570123 Test Loss: 0.21453642888194224\n",
      "Epoch: 9315 Training Loss: 0.21726712524424677 Test Loss: 0.2167821493044494\n",
      "Epoch: 9316 Training Loss: 0.21726706345194657 Test Loss: 0.21401886475672321\n",
      "Epoch: 9317 Training Loss: 0.21726518257932398 Test Loss: 0.21380311257649065\n",
      "Epoch: 9318 Training Loss: 0.21726420032203414 Test Loss: 0.2137921849696692\n",
      "Epoch: 9319 Training Loss: 0.21726616042543978 Test Loss: 0.21418791211207125\n",
      "Epoch: 9320 Training Loss: 0.21726400335235285 Test Loss: 0.21446283929246973\n",
      "Epoch: 9321 Training Loss: 0.21726451807002012 Test Loss: 0.21411708359051448\n",
      "Epoch: 9322 Training Loss: 0.21726271314669746 Test Loss: 0.21381099393467268\n",
      "Epoch: 9323 Training Loss: 0.21726377690311804 Test Loss: 0.2137635372696984\n",
      "Epoch: 9324 Training Loss: 0.21726402680688817 Test Loss: 0.21411677248427047\n",
      "Epoch: 9325 Training Loss: 0.2172635184459738 Test Loss: 0.21368458109751645\n",
      "Epoch: 9326 Training Loss: 0.21726491295079176 Test Loss: 0.21369278652470267\n",
      "Epoch: 9327 Training Loss: 0.2172650732772445 Test Loss: 0.213700616031844\n",
      "Epoch: 9328 Training Loss: 0.21726640162341718 Test Loss: 0.21372739709435065\n",
      "Epoch: 9329 Training Loss: 0.21726475283951824 Test Loss: 0.21366743136581448\n",
      "Epoch: 9330 Training Loss: 0.21726594644763408 Test Loss: 0.21371146586210443\n",
      "Epoch: 9331 Training Loss: 0.21726556727494573 Test Loss: 0.21374275796514947\n",
      "Epoch: 9332 Training Loss: 0.21726407740986878 Test Loss: 0.2136939790986381\n",
      "Epoch: 9333 Training Loss: 0.21726410868258253 Test Loss: 0.21375398371545476\n",
      "Epoch: 9334 Training Loss: 0.21726540001792885 Test Loss: 0.21372445454779257\n",
      "Epoch: 9335 Training Loss: 0.21726430710472355 Test Loss: 0.21401475556175004\n",
      "Epoch: 9336 Training Loss: 0.21726092284659618 Test Loss: 0.21429967703023786\n",
      "Epoch: 9337 Training Loss: 0.2172644889849618 Test Loss: 0.21441540855301583\n",
      "Epoch: 9338 Training Loss: 0.21726369057142061 Test Loss: 0.21389005380893597\n",
      "Epoch: 9339 Training Loss: 0.2172651788316192 Test Loss: 0.2138092180365297\n",
      "Epoch: 9340 Training Loss: 0.21726464086563005 Test Loss: 0.21372953594977834\n",
      "Epoch: 9341 Training Loss: 0.21726394926167567 Test Loss: 0.213789773896278\n",
      "Epoch: 9342 Training Loss: 0.21726383854299963 Test Loss: 0.21374226538026309\n",
      "Epoch: 9343 Training Loss: 0.21726493470182487 Test Loss: 0.2137123732553162\n",
      "Epoch: 9344 Training Loss: 0.21726449905355627 Test Loss: 0.21379177016134385\n",
      "Epoch: 9345 Training Loss: 0.21726360192654653 Test Loss: 0.216961592793452\n",
      "Epoch: 9346 Training Loss: 0.21726633082945282 Test Loss: 0.21378103699592493\n",
      "Epoch: 9347 Training Loss: 0.21726535230193614 Test Loss: 0.21369814014465197\n",
      "Epoch: 9348 Training Loss: 0.21726529992372695 Test Loss: 0.2136745868094271\n",
      "Epoch: 9349 Training Loss: 0.2172639935168692 Test Loss: 0.2136779700898309\n",
      "Epoch: 9350 Training Loss: 0.2172659185908905 Test Loss: 0.21376618167277264\n",
      "Epoch: 9351 Training Loss: 0.2172636083370942 Test Loss: 0.21373841544049327\n",
      "Epoch: 9352 Training Loss: 0.2172655995876924 Test Loss: 0.21368353111394287\n",
      "Epoch: 9353 Training Loss: 0.21726450182398876 Test Loss: 0.21369379761999574\n",
      "Epoch: 9354 Training Loss: 0.21726705829661103 Test Loss: 0.2137657150134066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9355 Training Loss: 0.21726416895069658 Test Loss: 0.21372217310200303\n",
      "Epoch: 9356 Training Loss: 0.21726505982854305 Test Loss: 0.2137018604568201\n",
      "Epoch: 9357 Training Loss: 0.21726557034124966 Test Loss: 0.21613150948334794\n",
      "Epoch: 9358 Training Loss: 0.2172619516722562 Test Loss: 0.21493075604624615\n",
      "Epoch: 9359 Training Loss: 0.21726350987466808 Test Loss: 0.21440896606121243\n",
      "Epoch: 9360 Training Loss: 0.21726315793111722 Test Loss: 0.2139431492745832\n",
      "Epoch: 9361 Training Loss: 0.2172646179938718 Test Loss: 0.2137932738415233\n",
      "Epoch: 9362 Training Loss: 0.21726543382796426 Test Loss: 0.21376835941648084\n",
      "Epoch: 9363 Training Loss: 0.21726279708452603 Test Loss: 0.21398158385848068\n",
      "Epoch: 9364 Training Loss: 0.21726603418696228 Test Loss: 0.2137159380143623\n",
      "Epoch: 9365 Training Loss: 0.21726432979716578 Test Loss: 0.21381507720412551\n",
      "Epoch: 9366 Training Loss: 0.2172655755234826 Test Loss: 0.21372527120168314\n",
      "Epoch: 9367 Training Loss: 0.21726443257214217 Test Loss: 0.21374083947664463\n",
      "Epoch: 9368 Training Loss: 0.21726431739746305 Test Loss: 0.21380311257649065\n",
      "Epoch: 9369 Training Loss: 0.21726475042771776 Test Loss: 0.213843776755137\n",
      "Epoch: 9370 Training Loss: 0.21726371500322822 Test Loss: 0.21377084826643306\n",
      "Epoch: 9371 Training Loss: 0.21726232012184662 Test Loss: 0.21411428363431825\n",
      "Epoch: 9372 Training Loss: 0.21726493884402492 Test Loss: 0.2137464005007566\n",
      "Epoch: 9373 Training Loss: 0.21726593380585474 Test Loss: 0.21377390747783265\n",
      "Epoch: 9374 Training Loss: 0.21726468076344432 Test Loss: 0.21373338588954816\n",
      "Epoch: 9375 Training Loss: 0.21726538252565117 Test Loss: 0.2137417727953767\n",
      "Epoch: 9376 Training Loss: 0.21726369102867646 Test Loss: 0.21381546608693056\n",
      "Epoch: 9377 Training Loss: 0.21726464640649504 Test Loss: 0.21384095087342042\n",
      "Epoch: 9378 Training Loss: 0.21726348264553055 Test Loss: 0.21373058593335192\n",
      "Epoch: 9379 Training Loss: 0.21726451312986378 Test Loss: 0.2137870387538826\n",
      "Epoch: 9380 Training Loss: 0.21726555026682132 Test Loss: 0.21370043455320165\n",
      "Epoch: 9381 Training Loss: 0.2172656410186586 Test Loss: 0.21377390747783265\n",
      "Epoch: 9382 Training Loss: 0.21726461454203844 Test Loss: 0.21375254484907613\n",
      "Epoch: 9383 Training Loss: 0.2172660037211706 Test Loss: 0.21369429020488212\n",
      "Epoch: 9384 Training Loss: 0.21726545601832165 Test Loss: 0.2137128139891619\n",
      "Epoch: 9385 Training Loss: 0.21726517746881743 Test Loss: 0.21381857714937083\n",
      "Epoch: 9386 Training Loss: 0.2172643309089251 Test Loss: 0.21373872654673728\n",
      "Epoch: 9387 Training Loss: 0.21726390155464878 Test Loss: 0.21380553661264204\n",
      "Epoch: 9388 Training Loss: 0.21726588218973863 Test Loss: 0.21377377785023097\n",
      "Epoch: 9389 Training Loss: 0.21726489958278253 Test Loss: 0.2137934423574055\n",
      "Epoch: 9390 Training Loss: 0.2172644533548688 Test Loss: 0.2137122565904747\n",
      "Epoch: 9391 Training Loss: 0.217265010686988 Test Loss: 0.213719697214811\n",
      "Epoch: 9392 Training Loss: 0.21726587342118528 Test Loss: 0.2137633817165764\n",
      "Epoch: 9393 Training Loss: 0.21726525515748274 Test Loss: 0.21466630277606324\n",
      "Epoch: 9394 Training Loss: 0.21726660597195263 Test Loss: 0.21375447630034114\n",
      "Epoch: 9395 Training Loss: 0.21726538526918626 Test Loss: 0.21378842576922055\n",
      "Epoch: 9396 Training Loss: 0.21726317650825683 Test Loss: 0.21368788660135923\n",
      "Epoch: 9397 Training Loss: 0.21726458004163635 Test Loss: 0.21382376225343794\n",
      "Epoch: 9398 Training Loss: 0.2172654761286132 Test Loss: 0.21386708379791866\n",
      "Epoch: 9399 Training Loss: 0.21725921060275802 Test Loss: 0.21583776037518584\n",
      "Epoch: 9400 Training Loss: 0.21726575224838532 Test Loss: 0.21848654486235\n",
      "Epoch: 9401 Training Loss: 0.21726660163250497 Test Loss: 0.21401822958147498\n",
      "Epoch: 9402 Training Loss: 0.21726394926167567 Test Loss: 0.21381284760937666\n",
      "Epoch: 9403 Training Loss: 0.21726688261174124 Test Loss: 0.2137878035567325\n",
      "Epoch: 9404 Training Loss: 0.2172642701297604 Test Loss: 0.21374925230799352\n",
      "Epoch: 9405 Training Loss: 0.217264945326299 Test Loss: 0.2137420190878199\n",
      "Epoch: 9406 Training Loss: 0.21726541621913117 Test Loss: 0.21366101479953142\n",
      "Epoch: 9407 Training Loss: 0.217264543434271 Test Loss: 0.2137301451995062\n",
      "Epoch: 9408 Training Loss: 0.21726535995873017 Test Loss: 0.21374317277347482\n",
      "Epoch: 9409 Training Loss: 0.2172647983588897 Test Loss: 0.21373468216556493\n",
      "Epoch: 9410 Training Loss: 0.2172657509483442 Test Loss: 0.21383007511763963\n",
      "Epoch: 9411 Training Loss: 0.21726471320171217 Test Loss: 0.21374914860591218\n",
      "Epoch: 9412 Training Loss: 0.21726303323475749 Test Loss: 0.21373299700674311\n",
      "Epoch: 9413 Training Loss: 0.21726673319666825 Test Loss: 0.2138021662949984\n",
      "Epoch: 9414 Training Loss: 0.21726421463145246 Test Loss: 0.21376176137155542\n",
      "Epoch: 9415 Training Loss: 0.21726404049766623 Test Loss: 0.21380615882513007\n",
      "Epoch: 9416 Training Loss: 0.2172645555560339 Test Loss: 0.21374875972310714\n",
      "Epoch: 9417 Training Loss: 0.21726401057878844 Test Loss: 0.21374474126745513\n",
      "Epoch: 9418 Training Loss: 0.21726613489980445 Test Loss: 0.2137764222533052\n",
      "Epoch: 9419 Training Loss: 0.21726539739991496 Test Loss: 0.2137193342575263\n",
      "Epoch: 9420 Training Loss: 0.21726466228492855 Test Loss: 0.21443600637892243\n",
      "Epoch: 9421 Training Loss: 0.21726308154249305 Test Loss: 0.21418373810329722\n",
      "Epoch: 9422 Training Loss: 0.21726448399997647 Test Loss: 0.21393871601060582\n",
      "Epoch: 9423 Training Loss: 0.21726372944713357 Test Loss: 0.2138696115361514\n",
      "Epoch: 9424 Training Loss: 0.21726389942078814 Test Loss: 0.21382520111981657\n",
      "Epoch: 9425 Training Loss: 0.21726339204611186 Test Loss: 0.21375266151391764\n",
      "Epoch: 9426 Training Loss: 0.21726458545698013 Test Loss: 0.21367962932313236\n",
      "Epoch: 9427 Training Loss: 0.2172640915399711 Test Loss: 0.21410479489387543\n",
      "Epoch: 9428 Training Loss: 0.21726478538537572 Test Loss: 0.21378360362243815\n",
      "Epoch: 9429 Training Loss: 0.2172658773302745 Test Loss: 0.21376379652490177\n",
      "Epoch: 9430 Training Loss: 0.21726474640207313 Test Loss: 0.21364278915873552\n",
      "Epoch: 9431 Training Loss: 0.21726344390430463 Test Loss: 0.21362259317839413\n",
      "Epoch: 9432 Training Loss: 0.21726611924551598 Test Loss: 0.21366367216536583\n",
      "Epoch: 9433 Training Loss: 0.21726578618394196 Test Loss: 0.21363398744458162\n",
      "Epoch: 9434 Training Loss: 0.21726432145000507 Test Loss: 0.2137259193396915\n",
      "Epoch: 9435 Training Loss: 0.2172642177336196 Test Loss: 0.21373504512284963\n",
      "Epoch: 9436 Training Loss: 0.21726460544175047 Test Loss: 0.21372344345249947\n",
      "Epoch: 9437 Training Loss: 0.21726517271694293 Test Loss: 0.2137686575599647\n",
      "Epoch: 9438 Training Loss: 0.21726501413882138 Test Loss: 0.2137717556596448\n",
      "Epoch: 9439 Training Loss: 0.21726438880110188 Test Loss: 0.21372506379752043\n",
      "Epoch: 9440 Training Loss: 0.21726462601826366 Test Loss: 0.2136892736166972\n",
      "Epoch: 9441 Training Loss: 0.21726500645512994 Test Loss: 0.21369265689710099\n",
      "Epoch: 9442 Training Loss: 0.21726590109861285 Test Loss: 0.2137332432991863\n",
      "Epoch: 9443 Training Loss: 0.2172645505172538 Test Loss: 0.21371175104282814\n",
      "Epoch: 9444 Training Loss: 0.21726406000724913 Test Loss: 0.21362732458585537\n",
      "Epoch: 9445 Training Loss: 0.2172662670288132 Test Loss: 0.21384701744517892\n",
      "Epoch: 9446 Training Loss: 0.21726362650180694 Test Loss: 0.21376962976697728\n",
      "Epoch: 9447 Training Loss: 0.217264453525219 Test Loss: 0.2137899813004407\n",
      "Epoch: 9448 Training Loss: 0.21726437710969743 Test Loss: 0.2137120362235518\n",
      "Epoch: 9449 Training Loss: 0.21726329748380915 Test Loss: 0.21376348541865775\n",
      "Epoch: 9450 Training Loss: 0.21726421604804902 Test Loss: 0.21377703150303307\n",
      "Epoch: 9451 Training Loss: 0.21726316261126533 Test Loss: 0.21372068238458372\n",
      "Epoch: 9452 Training Loss: 0.21726252704356694 Test Loss: 0.21374087836492514\n",
      "Epoch: 9453 Training Loss: 0.21726450449579746 Test Loss: 0.2138530321658968\n",
      "Epoch: 9454 Training Loss: 0.21726382401840208 Test Loss: 0.21377096493127457\n",
      "Epoch: 9455 Training Loss: 0.21726342257466413 Test Loss: 0.21371793427942817\n",
      "Epoch: 9456 Training Loss: 0.21726537292327835 Test Loss: 0.21379473863342227\n",
      "Epoch: 9457 Training Loss: 0.21726266885564072 Test Loss: 0.21387750585709356\n",
      "Epoch: 9458 Training Loss: 0.21726410513212535 Test Loss: 0.21376151507911226\n",
      "Epoch: 9459 Training Loss: 0.2172630379507688 Test Loss: 0.2136730183154468\n",
      "Epoch: 9460 Training Loss: 0.21726522627863787 Test Loss: 0.2137672446191064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9461 Training Loss: 0.2172643993090206 Test Loss: 0.21384948036961082\n",
      "Epoch: 9462 Training Loss: 0.21726434537972783 Test Loss: 0.21373929690818466\n",
      "Epoch: 9463 Training Loss: 0.2172628837927871 Test Loss: 0.2137092233045954\n",
      "Epoch: 9464 Training Loss: 0.2172623603513955 Test Loss: 0.21366934985431932\n",
      "Epoch: 9465 Training Loss: 0.2172645140712729 Test Loss: 0.21375106709441702\n",
      "Epoch: 9466 Training Loss: 0.21726438466786765 Test Loss: 0.21375023747776628\n",
      "Epoch: 9467 Training Loss: 0.21726419041482406 Test Loss: 0.21529475035175746\n",
      "Epoch: 9468 Training Loss: 0.2172658016499486 Test Loss: 0.21369460131112616\n",
      "Epoch: 9469 Training Loss: 0.2172643310703095 Test Loss: 0.21371138808554344\n",
      "Epoch: 9470 Training Loss: 0.21726361725806617 Test Loss: 0.21373442291036157\n",
      "Epoch: 9471 Training Loss: 0.21726320404223157 Test Loss: 0.2138047069959913\n",
      "Epoch: 9472 Training Loss: 0.2172639447787752 Test Loss: 0.214043416224481\n",
      "Epoch: 9473 Training Loss: 0.21726419924613802 Test Loss: 0.21385191736852235\n",
      "Epoch: 9474 Training Loss: 0.2172592927025974 Test Loss: 0.21480418765596793\n",
      "Epoch: 9475 Training Loss: 0.21726679499793425 Test Loss: 0.21393738084630853\n",
      "Epoch: 9476 Training Loss: 0.21726402844762974 Test Loss: 0.21396304711144074\n",
      "Epoch: 9477 Training Loss: 0.21726450532065114 Test Loss: 0.2138298158624363\n",
      "Epoch: 9478 Training Loss: 0.21726441114387787 Test Loss: 0.21383606391283716\n",
      "Epoch: 9479 Training Loss: 0.21726465594610725 Test Loss: 0.2137512744985797\n",
      "Epoch: 9480 Training Loss: 0.21726547663966383 Test Loss: 0.21381212169480726\n",
      "Epoch: 9481 Training Loss: 0.21726455701745948 Test Loss: 0.21373210257629155\n",
      "Epoch: 9482 Training Loss: 0.21726616220963418 Test Loss: 0.21376439281186949\n",
      "Epoch: 9483 Training Loss: 0.21726449609484197 Test Loss: 0.2183827390789264\n",
      "Epoch: 9484 Training Loss: 0.21726764110953656 Test Loss: 0.21391273863922958\n",
      "Epoch: 9485 Training Loss: 0.21726519906743194 Test Loss: 0.2137492652707537\n",
      "Epoch: 9486 Training Loss: 0.2172648263590861 Test Loss: 0.2140197462244146\n",
      "Epoch: 9487 Training Loss: 0.21726541887300826 Test Loss: 0.21419548236400923\n",
      "Epoch: 9488 Training Loss: 0.21726484191475076 Test Loss: 0.2139349827356775\n",
      "Epoch: 9489 Training Loss: 0.21726576821647683 Test Loss: 0.2139077090882845\n",
      "Epoch: 9490 Training Loss: 0.21726382685159518 Test Loss: 0.21381871973973265\n",
      "Epoch: 9491 Training Loss: 0.2172640834438528 Test Loss: 0.21373205072525087\n",
      "Epoch: 9492 Training Loss: 0.2172647405205077 Test Loss: 0.2137301451995062\n",
      "Epoch: 9493 Training Loss: 0.21726475411266197 Test Loss: 0.21371460285006505\n",
      "Epoch: 9494 Training Loss: 0.21726385091580494 Test Loss: 0.213735226601492\n",
      "Epoch: 9495 Training Loss: 0.2172647620922248 Test Loss: 0.21379896449323696\n",
      "Epoch: 9496 Training Loss: 0.21726378013080638 Test Loss: 0.21376452243947117\n",
      "Epoch: 9497 Training Loss: 0.21726408759501867 Test Loss: 0.21370245674378782\n",
      "Epoch: 9498 Training Loss: 0.21726406853372582 Test Loss: 0.21380364404965754\n",
      "Epoch: 9499 Training Loss: 0.21726456977579425 Test Loss: 0.213731597028645\n",
      "Epoch: 9500 Training Loss: 0.21726521596796677 Test Loss: 0.2137193342575263\n",
      "Epoch: 9501 Training Loss: 0.21726496103438228 Test Loss: 0.21391828670058138\n",
      "Epoch: 9502 Training Loss: 0.2172640205487591 Test Loss: 0.21418804173967293\n",
      "Epoch: 9503 Training Loss: 0.21726530259553561 Test Loss: 0.2139690618321586\n",
      "Epoch: 9504 Training Loss: 0.21726301041679405 Test Loss: 0.21385321364453913\n",
      "Epoch: 9505 Training Loss: 0.2172623395866005 Test Loss: 0.21381324945494187\n",
      "Epoch: 9506 Training Loss: 0.21726344492640592 Test Loss: 0.21377627966294335\n",
      "Epoch: 9507 Training Loss: 0.21726485673521972 Test Loss: 0.21376530020508125\n",
      "Epoch: 9508 Training Loss: 0.21726355174495857 Test Loss: 0.21367860526507912\n",
      "Epoch: 9509 Training Loss: 0.21726608291609048 Test Loss: 0.21374828010098093\n",
      "Epoch: 9510 Training Loss: 0.21726628002922457 Test Loss: 0.21376755572535042\n",
      "Epoch: 9511 Training Loss: 0.21726352549309336 Test Loss: 0.21375640775160615\n",
      "Epoch: 9512 Training Loss: 0.21726470506973072 Test Loss: 0.2139014351123633\n",
      "Epoch: 9513 Training Loss: 0.21726389559239112 Test Loss: 0.2138828465142827\n",
      "Epoch: 9514 Training Loss: 0.21726272236354086 Test Loss: 0.21378062218759955\n",
      "Epoch: 9515 Training Loss: 0.21726482252172327 Test Loss: 0.21379609972323987\n",
      "Epoch: 9516 Training Loss: 0.2172640325181034 Test Loss: 0.2137151861742726\n",
      "Epoch: 9517 Training Loss: 0.2172656558122302 Test Loss: 0.21377434821167834\n",
      "Epoch: 9518 Training Loss: 0.2172644282237287 Test Loss: 0.21376365393453992\n",
      "Epoch: 9519 Training Loss: 0.21726356493365176 Test Loss: 0.21374432645912977\n",
      "Epoch: 9520 Training Loss: 0.2172650160754344 Test Loss: 0.21380359219861686\n",
      "Epoch: 9521 Training Loss: 0.21726471549695722 Test Loss: 0.21370650112496017\n",
      "Epoch: 9522 Training Loss: 0.2172627470643225 Test Loss: 0.21374032096623793\n",
      "Epoch: 9523 Training Loss: 0.21726467392253818 Test Loss: 0.21377760186448047\n",
      "Epoch: 9524 Training Loss: 0.21726360216862314 Test Loss: 0.21377445191375968\n",
      "Epoch: 9525 Training Loss: 0.21726596492614986 Test Loss: 0.21381076060498966\n",
      "Epoch: 9526 Training Loss: 0.21726392890034169 Test Loss: 0.2137441449804874\n",
      "Epoch: 9527 Training Loss: 0.21726568773944743 Test Loss: 0.2141298778348001\n",
      "Epoch: 9528 Training Loss: 0.2172635922524473 Test Loss: 0.21375089857853483\n",
      "Epoch: 9529 Training Loss: 0.2172651808399586 Test Loss: 0.21370613816767547\n",
      "Epoch: 9530 Training Loss: 0.21726627419248815 Test Loss: 0.2138133920453037\n",
      "Epoch: 9531 Training Loss: 0.21726384594875123 Test Loss: 0.21377309082394208\n",
      "Epoch: 9532 Training Loss: 0.2172643543724262 Test Loss: 0.21373376180959303\n",
      "Epoch: 9533 Training Loss: 0.21726502518468815 Test Loss: 0.21383764536957764\n",
      "Epoch: 9534 Training Loss: 0.21726622145564692 Test Loss: 0.21377808148660668\n",
      "Epoch: 9535 Training Loss: 0.21726425191125284 Test Loss: 0.21376078916454286\n",
      "Epoch: 9536 Training Loss: 0.21726483972709532 Test Loss: 0.21370051232976267\n",
      "Epoch: 9537 Training Loss: 0.21726482590183024 Test Loss: 0.21369348651375172\n",
      "Epoch: 9538 Training Loss: 0.21726459822428068 Test Loss: 0.21372573786104918\n",
      "Epoch: 9539 Training Loss: 0.21726492894578067 Test Loss: 0.21372322308557662\n",
      "Epoch: 9540 Training Loss: 0.21726464632580283 Test Loss: 0.21370836776242433\n",
      "Epoch: 9541 Training Loss: 0.21726424724007054 Test Loss: 0.2138026070288441\n",
      "Epoch: 9542 Training Loss: 0.21726648423430722 Test Loss: 0.2136710090876208\n",
      "Epoch: 9543 Training Loss: 0.21726328352405708 Test Loss: 0.21374212278990123\n",
      "Epoch: 9544 Training Loss: 0.21726213161588145 Test Loss: 0.21366905171083544\n",
      "Epoch: 9545 Training Loss: 0.217264543389442 Test Loss: 0.213731597028645\n",
      "Epoch: 9546 Training Loss: 0.21726465382121243 Test Loss: 0.21376415948218647\n",
      "Epoch: 9547 Training Loss: 0.2172628911088807 Test Loss: 0.21364026142050282\n",
      "Epoch: 9548 Training Loss: 0.21726569327134662 Test Loss: 0.21388779828866678\n",
      "Epoch: 9549 Training Loss: 0.21726537952210787 Test Loss: 0.21384089902237974\n",
      "Epoch: 9550 Training Loss: 0.2172638038274183 Test Loss: 0.21380937358965169\n",
      "Epoch: 9551 Training Loss: 0.21726294508300245 Test Loss: 0.2137474893726107\n",
      "Epoch: 9552 Training Loss: 0.21726632541410903 Test Loss: 0.21381808456448445\n",
      "Epoch: 9553 Training Loss: 0.21726255481065251 Test Loss: 0.21384496932907243\n",
      "Epoch: 9554 Training Loss: 0.2172624752122716 Test Loss: 0.21369355132755258\n",
      "Epoch: 9555 Training Loss: 0.21726428078113194 Test Loss: 0.21384460637178773\n",
      "Epoch: 9556 Training Loss: 0.21726457918988526 Test Loss: 0.21375661515576883\n",
      "Epoch: 9557 Training Loss: 0.21726412929495892 Test Loss: 0.21372311938349528\n",
      "Epoch: 9558 Training Loss: 0.217264472084427 Test Loss: 0.21377660373194754\n",
      "Epoch: 9559 Training Loss: 0.21726515525156267 Test Loss: 0.2137062418697568\n",
      "Epoch: 9560 Training Loss: 0.2172654042946159 Test Loss: 0.21362291724739832\n",
      "Epoch: 9561 Training Loss: 0.217264473976211 Test Loss: 0.21369025878646994\n",
      "Epoch: 9562 Training Loss: 0.21726489730546908 Test Loss: 0.2136688313439126\n",
      "Epoch: 9563 Training Loss: 0.21726595828249137 Test Loss: 0.21374605050623208\n",
      "Epoch: 9564 Training Loss: 0.21726528532740297 Test Loss: 0.21401949993197145\n",
      "Epoch: 9565 Training Loss: 0.21726313736356984 Test Loss: 0.21425846841566448\n",
      "Epoch: 9566 Training Loss: 0.2172650677094821 Test Loss: 0.21410627264853457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9567 Training Loss: 0.21726614537185998 Test Loss: 0.21384955814617182\n",
      "Epoch: 9568 Training Loss: 0.2172639711471958 Test Loss: 0.21372630822249655\n",
      "Epoch: 9569 Training Loss: 0.21726381480155868 Test Loss: 0.21371831019947302\n",
      "Epoch: 9570 Training Loss: 0.2172640176797028 Test Loss: 0.21370389561016645\n",
      "Epoch: 9571 Training Loss: 0.217264016003098 Test Loss: 0.21376572797616678\n",
      "Epoch: 9572 Training Loss: 0.217263208121671 Test Loss: 0.21372840818964373\n",
      "Epoch: 9573 Training Loss: 0.21726524672962985 Test Loss: 0.21376099656870554\n",
      "Epoch: 9574 Training Loss: 0.21726512136980083 Test Loss: 0.2138468748548171\n",
      "Epoch: 9575 Training Loss: 0.2172644187648087 Test Loss: 0.21382765108148827\n",
      "Epoch: 9576 Training Loss: 0.2172635476027585 Test Loss: 0.21376129471218938\n",
      "Epoch: 9577 Training Loss: 0.2172660232845483 Test Loss: 0.21380833656883827\n",
      "Epoch: 9578 Training Loss: 0.2172637090857996 Test Loss: 0.21373289330466178\n",
      "Epoch: 9579 Training Loss: 0.217264307759227 Test Loss: 0.21377857407149303\n",
      "Epoch: 9580 Training Loss: 0.2172646629663294 Test Loss: 0.21377832777904987\n",
      "Epoch: 9581 Training Loss: 0.21726530589495038 Test Loss: 0.21385706358430898\n",
      "Epoch: 9582 Training Loss: 0.21726514886791237 Test Loss: 0.2138997758790618\n",
      "Epoch: 9583 Training Loss: 0.21726375187060176 Test Loss: 0.21380109038590447\n",
      "Epoch: 9584 Training Loss: 0.21726384894332873 Test Loss: 0.2137059307635128\n",
      "Epoch: 9585 Training Loss: 0.21726544473934403 Test Loss: 0.2136859292245739\n",
      "Epoch: 9586 Training Loss: 0.21726574061974147 Test Loss: 0.2137285896682861\n",
      "Epoch: 9587 Training Loss: 0.2172626751854962 Test Loss: 0.21398504491544548\n",
      "Epoch: 9588 Training Loss: 0.2172644953506805 Test Loss: 0.21424704822395668\n",
      "Epoch: 9589 Training Loss: 0.21726578621980516 Test Loss: 0.2139225125603961\n",
      "Epoch: 9590 Training Loss: 0.21726613826197982 Test Loss: 0.2138587357803706\n",
      "Epoch: 9591 Training Loss: 0.21726253000228127 Test Loss: 0.21381358648670623\n",
      "Epoch: 9592 Training Loss: 0.21726545007399561 Test Loss: 0.21385360252734417\n",
      "Epoch: 9593 Training Loss: 0.21726295589575842 Test Loss: 0.21372196569784035\n",
      "Epoch: 9594 Training Loss: 0.21726483882154943 Test Loss: 0.21365244641506054\n",
      "Epoch: 9595 Training Loss: 0.21726567684599926 Test Loss: 0.21368073115774663\n",
      "Epoch: 9596 Training Loss: 0.2172637139900927 Test Loss: 0.2141396128676861\n",
      "Epoch: 9597 Training Loss: 0.2172670742109077 Test Loss: 0.2137598428830506\n",
      "Epoch: 9598 Training Loss: 0.21726528107761334 Test Loss: 0.21367564975576087\n",
      "Epoch: 9599 Training Loss: 0.2172662924737563 Test Loss: 0.21376234469576297\n",
      "Epoch: 9600 Training Loss: 0.2172635595810686 Test Loss: 0.21381293834869783\n",
      "Epoch: 9601 Training Loss: 0.2172663961901418 Test Loss: 0.2138444119303852\n",
      "Epoch: 9602 Training Loss: 0.21726327922943842 Test Loss: 0.21383038622388367\n",
      "Epoch: 9603 Training Loss: 0.21726433257656408 Test Loss: 0.2137594799257659\n",
      "Epoch: 9604 Training Loss: 0.2172675798641502 Test Loss: 0.21385309697969762\n",
      "Epoch: 9605 Training Loss: 0.21726374446485017 Test Loss: 0.21374833195202161\n",
      "Epoch: 9606 Training Loss: 0.2172629902347761 Test Loss: 0.21378991648663986\n",
      "Epoch: 9607 Training Loss: 0.21726434787222051 Test Loss: 0.21370077158496603\n",
      "Epoch: 9608 Training Loss: 0.21726408099618916 Test Loss: 0.21367658307449294\n",
      "Epoch: 9609 Training Loss: 0.21726384524045295 Test Loss: 0.2137143824831422\n",
      "Epoch: 9610 Training Loss: 0.21726653480142463 Test Loss: 0.21373942653578634\n",
      "Epoch: 9611 Training Loss: 0.21726595994116454 Test Loss: 0.21376703721494372\n",
      "Epoch: 9612 Training Loss: 0.21726503275182416 Test Loss: 0.2137969811909313\n",
      "Epoch: 9613 Training Loss: 0.21726482719290557 Test Loss: 0.21374624494763458\n",
      "Epoch: 9614 Training Loss: 0.21726416235186707 Test Loss: 0.2136628425487151\n",
      "Epoch: 9615 Training Loss: 0.21726506720739724 Test Loss: 0.21387779103781723\n",
      "Epoch: 9616 Training Loss: 0.21726441530400953 Test Loss: 0.21386639677162977\n",
      "Epoch: 9617 Training Loss: 0.21726401001394297 Test Loss: 0.213820677116518\n",
      "Epoch: 9618 Training Loss: 0.21726341700690174 Test Loss: 0.21381577719317457\n",
      "Epoch: 9619 Training Loss: 0.21726653671114024 Test Loss: 0.21381509016688569\n",
      "Epoch: 9620 Training Loss: 0.21726599391258436 Test Loss: 0.21372332678765796\n",
      "Epoch: 9621 Training Loss: 0.2172652687406712 Test Loss: 0.21364689835370873\n",
      "Epoch: 9622 Training Loss: 0.21726571211746024 Test Loss: 0.21365722967356243\n",
      "Epoch: 9623 Training Loss: 0.21726333858304075 Test Loss: 0.21371754539662313\n",
      "Epoch: 9624 Training Loss: 0.21726650708813386 Test Loss: 0.21375796328282629\n",
      "Epoch: 9625 Training Loss: 0.2172646601779653 Test Loss: 0.21384579894572317\n",
      "Epoch: 9626 Training Loss: 0.21726562282704848 Test Loss: 0.21375697811305353\n",
      "Epoch: 9627 Training Loss: 0.21726379017250347 Test Loss: 0.21374637457523626\n",
      "Epoch: 9628 Training Loss: 0.21726716585932512 Test Loss: 0.21390604985498302\n",
      "Epoch: 9629 Training Loss: 0.21726504782333558 Test Loss: 0.21375295965740151\n",
      "Epoch: 9630 Training Loss: 0.21726634332777933 Test Loss: 0.21369954012275008\n",
      "Epoch: 9631 Training Loss: 0.2172650936744417 Test Loss: 0.2138059903092479\n",
      "Epoch: 9632 Training Loss: 0.2172657508855836 Test Loss: 0.2137289007745301\n",
      "Epoch: 9633 Training Loss: 0.2172648619802133 Test Loss: 0.21369887902198154\n",
      "Epoch: 9634 Training Loss: 0.21726586109320897 Test Loss: 0.2137783148162897\n",
      "Epoch: 9635 Training Loss: 0.2172648891645218 Test Loss: 0.21371197140975098\n",
      "Epoch: 9636 Training Loss: 0.2172652443626584 Test Loss: 0.21366100183677125\n",
      "Epoch: 9637 Training Loss: 0.21726479574984162 Test Loss: 0.2137021067492633\n",
      "Epoch: 9638 Training Loss: 0.21726471198236325 Test Loss: 0.21365868150270123\n",
      "Epoch: 9639 Training Loss: 0.21726582665556748 Test Loss: 0.21426640162488717\n",
      "Epoch: 9640 Training Loss: 0.21726627483802582 Test Loss: 0.21377659076918737\n",
      "Epoch: 9641 Training Loss: 0.21726448184818423 Test Loss: 0.21371263251051953\n",
      "Epoch: 9642 Training Loss: 0.217265259111401 Test Loss: 0.21376268172752735\n",
      "Epoch: 9643 Training Loss: 0.21726485488826475 Test Loss: 0.21376110027078687\n",
      "Epoch: 9644 Training Loss: 0.21726538221184813 Test Loss: 0.21379125165093713\n",
      "Epoch: 9645 Training Loss: 0.21726510152848333 Test Loss: 0.2138632857091895\n",
      "Epoch: 9646 Training Loss: 0.21726252783255742 Test Loss: 0.2140904062300892\n",
      "Epoch: 9647 Training Loss: 0.21726538152148148 Test Loss: 0.21396308599972125\n",
      "Epoch: 9648 Training Loss: 0.2172631145994012 Test Loss: 0.21382868810230168\n",
      "Epoch: 9649 Training Loss: 0.21726630995706817 Test Loss: 0.21376695943838273\n",
      "Epoch: 9650 Training Loss: 0.21726362947845287 Test Loss: 0.2137961386115204\n",
      "Epoch: 9651 Training Loss: 0.21726474349715363 Test Loss: 0.21385489880336095\n",
      "Epoch: 9652 Training Loss: 0.21726248749541893 Test Loss: 0.21382967327207444\n",
      "Epoch: 9653 Training Loss: 0.21726222963001754 Test Loss: 0.21374724308016751\n",
      "Epoch: 9654 Training Loss: 0.21726263840778065 Test Loss: 0.21375837809115164\n",
      "Epoch: 9655 Training Loss: 0.21726360897366606 Test Loss: 0.21377598151945948\n",
      "Epoch: 9656 Training Loss: 0.2172645067910425 Test Loss: 0.21379683860056944\n",
      "Epoch: 9657 Training Loss: 0.2172639159806225 Test Loss: 0.2137690594055299\n",
      "Epoch: 9658 Training Loss: 0.21726454766612907 Test Loss: 0.21363847255959967\n",
      "Epoch: 9659 Training Loss: 0.21726571070086367 Test Loss: 0.21385878763141128\n",
      "Epoch: 9660 Training Loss: 0.2172649937416242 Test Loss: 0.2137336581075117\n",
      "Epoch: 9661 Training Loss: 0.21726521228302256 Test Loss: 0.2138293621658304\n",
      "Epoch: 9662 Training Loss: 0.21726439652962232 Test Loss: 0.21372695636050493\n",
      "Epoch: 9663 Training Loss: 0.21726568394691362 Test Loss: 0.21374277092790964\n",
      "Epoch: 9664 Training Loss: 0.2172645272599661 Test Loss: 0.21373326922470665\n",
      "Epoch: 9665 Training Loss: 0.21726480001756288 Test Loss: 0.2137752167166096\n",
      "Epoch: 9666 Training Loss: 0.21726520626697013 Test Loss: 0.21390213510141234\n",
      "Epoch: 9667 Training Loss: 0.217264887084456 Test Loss: 0.21379764229169984\n",
      "Epoch: 9668 Training Loss: 0.21726510090087725 Test Loss: 0.21377909258189975\n",
      "Epoch: 9669 Training Loss: 0.21726217239234422 Test Loss: 0.2137880887374562\n",
      "Epoch: 9670 Training Loss: 0.21726517759433867 Test Loss: 0.21377507412624774\n",
      "Epoch: 9671 Training Loss: 0.21726410597491064 Test Loss: 0.21376099656870554\n",
      "Epoch: 9672 Training Loss: 0.2172637408516324 Test Loss: 0.21384592857332485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9673 Training Loss: 0.2172646140489194 Test Loss: 0.21374682827184216\n",
      "Epoch: 9674 Training Loss: 0.21726395083965663 Test Loss: 0.21410238382048424\n",
      "Epoch: 9675 Training Loss: 0.21726375629970743 Test Loss: 0.21427465890311406\n",
      "Epoch: 9676 Training Loss: 0.2172645067641451 Test Loss: 0.21397355990993683\n",
      "Epoch: 9677 Training Loss: 0.21726183802176324 Test Loss: 0.21379909412083864\n",
      "Epoch: 9678 Training Loss: 0.2172634333156937 Test Loss: 0.21381219947136826\n",
      "Epoch: 9679 Training Loss: 0.21726450698829014 Test Loss: 0.2136276745803799\n",
      "Epoch: 9680 Training Loss: 0.21726603572011424 Test Loss: 0.21369212542393412\n",
      "Epoch: 9681 Training Loss: 0.21726346377251954 Test Loss: 0.2136960401775048\n",
      "Epoch: 9682 Training Loss: 0.2172662792223025 Test Loss: 0.21376717980530557\n",
      "Epoch: 9683 Training Loss: 0.2172628960580028 Test Loss: 0.21372674895634225\n",
      "Epoch: 9684 Training Loss: 0.2172646235616342 Test Loss: 0.21363551705028141\n",
      "Epoch: 9685 Training Loss: 0.21726357108419123 Test Loss: 0.213665772132513\n",
      "Epoch: 9686 Training Loss: 0.217267222711469 Test Loss: 0.21373965986546936\n",
      "Epoch: 9687 Training Loss: 0.2172649442504029 Test Loss: 0.21379031833220505\n",
      "Epoch: 9688 Training Loss: 0.21726470039854842 Test Loss: 0.21368883288285148\n",
      "Epoch: 9689 Training Loss: 0.217264326452922 Test Loss: 0.2141599644011495\n",
      "Epoch: 9690 Training Loss: 0.21726138903238282 Test Loss: 0.21428830868957072\n",
      "Epoch: 9691 Training Loss: 0.21726401186089797 Test Loss: 0.21397108402274476\n",
      "Epoch: 9692 Training Loss: 0.21726330887037637 Test Loss: 0.2139082275986912\n",
      "Epoch: 9693 Training Loss: 0.21726299271830296 Test Loss: 0.21375220781731177\n",
      "Epoch: 9694 Training Loss: 0.21726429188079352 Test Loss: 0.21373854506809495\n",
      "Epoch: 9695 Training Loss: 0.21726617964811704 Test Loss: 0.21378417398388552\n",
      "Epoch: 9696 Training Loss: 0.21726560302159417 Test Loss: 0.2137181676091112\n",
      "Epoch: 9697 Training Loss: 0.21726407780436402 Test Loss: 0.21372498602095943\n",
      "Epoch: 9698 Training Loss: 0.21726502023556601 Test Loss: 0.2137530892850032\n",
      "Epoch: 9699 Training Loss: 0.21726446432900917 Test Loss: 0.2136857347831714\n",
      "Epoch: 9700 Training Loss: 0.21726636185112413 Test Loss: 0.2136106933645601\n",
      "Epoch: 9701 Training Loss: 0.21726717821419886 Test Loss: 0.21370284562659286\n",
      "Epoch: 9702 Training Loss: 0.2172657641908322 Test Loss: 0.21369928086754672\n",
      "Epoch: 9703 Training Loss: 0.2172647179446209 Test Loss: 0.21382975104863544\n",
      "Epoch: 9704 Training Loss: 0.2172646357192603 Test Loss: 0.21375008192464426\n",
      "Epoch: 9705 Training Loss: 0.2172652442281714 Test Loss: 0.213789696119717\n",
      "Epoch: 9706 Training Loss: 0.21726501404916337 Test Loss: 0.21373680805823245\n",
      "Epoch: 9707 Training Loss: 0.21726370705056278 Test Loss: 0.21432298407301953\n",
      "Epoch: 9708 Training Loss: 0.21726565637707568 Test Loss: 0.2137736871109098\n",
      "Epoch: 9709 Training Loss: 0.2172640550939902 Test Loss: 0.21374051540764044\n",
      "Epoch: 9710 Training Loss: 0.2172641608904415 Test Loss: 0.21374111169460816\n",
      "Epoch: 9711 Training Loss: 0.21726371445631434 Test Loss: 0.2136695183702015\n",
      "Epoch: 9712 Training Loss: 0.21726492045516715 Test Loss: 0.21370848442726584\n",
      "Epoch: 9713 Training Loss: 0.21726651715672834 Test Loss: 0.21370860109210735\n",
      "Epoch: 9714 Training Loss: 0.21726497443825474 Test Loss: 0.21718910219715673\n",
      "Epoch: 9715 Training Loss: 0.21726615262519294 Test Loss: 0.21378859428510275\n",
      "Epoch: 9716 Training Loss: 0.2172656821358218 Test Loss: 0.21381873270249283\n",
      "Epoch: 9717 Training Loss: 0.21726572082325296 Test Loss: 0.2137346562400446\n",
      "Epoch: 9718 Training Loss: 0.21726398746495357 Test Loss: 0.21373632843610624\n",
      "Epoch: 9719 Training Loss: 0.21726484657696726 Test Loss: 0.2136959105499031\n",
      "Epoch: 9720 Training Loss: 0.21726461971530558 Test Loss: 0.21357237544550414\n",
      "Epoch: 9721 Training Loss: 0.2172663315825801 Test Loss: 0.21375028932880694\n",
      "Epoch: 9722 Training Loss: 0.21726536176982195 Test Loss: 0.2136633221708413\n",
      "Epoch: 9723 Training Loss: 0.2172665466721451 Test Loss: 0.21367790527603006\n",
      "Epoch: 9724 Training Loss: 0.21726569522589123 Test Loss: 0.21375713366617555\n",
      "Epoch: 9725 Training Loss: 0.21726486920664886 Test Loss: 0.21364147991995858\n",
      "Epoch: 9726 Training Loss: 0.2172654192316403 Test Loss: 0.21370431041849183\n",
      "Epoch: 9727 Training Loss: 0.21726287679049655 Test Loss: 0.21373302293226346\n",
      "Epoch: 9728 Training Loss: 0.21726556193132837 Test Loss: 0.2136581629922945\n",
      "Epoch: 9729 Training Loss: 0.21726592228480052 Test Loss: 0.2137530374339625\n",
      "Epoch: 9730 Training Loss: 0.21726364157331837 Test Loss: 0.21373954320062785\n",
      "Epoch: 9731 Training Loss: 0.21726474153364322 Test Loss: 0.21373097481615694\n",
      "Epoch: 9732 Training Loss: 0.21726365547927565 Test Loss: 0.21374751529813105\n",
      "Epoch: 9733 Training Loss: 0.21726524751862034 Test Loss: 0.21371197140975098\n",
      "Epoch: 9734 Training Loss: 0.21726467424530702 Test Loss: 0.21367913673824598\n",
      "Epoch: 9735 Training Loss: 0.21726495125269343 Test Loss: 0.2136670554457696\n",
      "Epoch: 9736 Training Loss: 0.21726457866090298 Test Loss: 0.21375132634962038\n",
      "Epoch: 9737 Training Loss: 0.21726431455530415 Test Loss: 0.2136800570942179\n",
      "Epoch: 9738 Training Loss: 0.21726453678164673 Test Loss: 0.21375775587866358\n",
      "Epoch: 9739 Training Loss: 0.2172648704259978 Test Loss: 0.213681301519194\n",
      "Epoch: 9740 Training Loss: 0.21726369009623314 Test Loss: 0.21397498581355529\n",
      "Epoch: 9741 Training Loss: 0.2172651891333245 Test Loss: 0.21371539357843528\n",
      "Epoch: 9742 Training Loss: 0.21726547706105648 Test Loss: 0.21376222803092146\n",
      "Epoch: 9743 Training Loss: 0.21726479791956543 Test Loss: 0.21369650683687083\n",
      "Epoch: 9744 Training Loss: 0.217265074245551 Test Loss: 0.21371422693002018\n",
      "Epoch: 9745 Training Loss: 0.21726527318770847 Test Loss: 0.2137286155938064\n",
      "Epoch: 9746 Training Loss: 0.2172641934631964 Test Loss: 0.21376568908788626\n",
      "Epoch: 9747 Training Loss: 0.2172641297522148 Test Loss: 0.2137169491096554\n",
      "Epoch: 9748 Training Loss: 0.21726413567860922 Test Loss: 0.21364311322773974\n",
      "Epoch: 9749 Training Loss: 0.21726362435001473 Test Loss: 0.21381328834322236\n",
      "Epoch: 9750 Training Loss: 0.2172628667219021 Test Loss: 0.2137528818808405\n",
      "Epoch: 9751 Training Loss: 0.21726532401483412 Test Loss: 0.21376912421933073\n",
      "Epoch: 9752 Training Loss: 0.21726407339318995 Test Loss: 0.2136976864480461\n",
      "Epoch: 9753 Training Loss: 0.217265197579109 Test Loss: 0.21372953594977834\n",
      "Epoch: 9754 Training Loss: 0.21726428935243766 Test Loss: 0.21718157083349926\n",
      "Epoch: 9755 Training Loss: 0.21726574330051596 Test Loss: 0.21385290253829511\n",
      "Epoch: 9756 Training Loss: 0.2172656931458254 Test Loss: 0.21381126615263618\n",
      "Epoch: 9757 Training Loss: 0.2172644657276741 Test Loss: 0.21372479157955693\n",
      "Epoch: 9758 Training Loss: 0.21726514490502835 Test Loss: 0.21384546191395878\n",
      "Epoch: 9759 Training Loss: 0.21726535814763837 Test Loss: 0.21371955462444914\n",
      "Epoch: 9760 Training Loss: 0.21726683312948575 Test Loss: 0.2137801555282335\n",
      "Epoch: 9761 Training Loss: 0.2172658024748023 Test Loss: 0.2137265674776999\n",
      "Epoch: 9762 Training Loss: 0.2172655267046964 Test Loss: 0.21375970029268876\n",
      "Epoch: 9763 Training Loss: 0.21726455244490098 Test Loss: 0.21373495438352846\n",
      "Epoch: 9764 Training Loss: 0.21726525695064294 Test Loss: 0.21378364251071866\n",
      "Epoch: 9765 Training Loss: 0.21726391334467704 Test Loss: 0.21374813751061908\n",
      "Epoch: 9766 Training Loss: 0.2172631250086961 Test Loss: 0.21374630976143544\n",
      "Epoch: 9767 Training Loss: 0.21726519403761763 Test Loss: 0.2137482930637411\n",
      "Epoch: 9768 Training Loss: 0.21726560746863144 Test Loss: 0.21373250442185676\n",
      "Epoch: 9769 Training Loss: 0.21726361720427134 Test Loss: 0.21562926734064725\n",
      "Epoch: 9770 Training Loss: 0.21726577648294532 Test Loss: 0.21382896032026522\n",
      "Epoch: 9771 Training Loss: 0.21726392059801 Test Loss: 0.21372913410421313\n",
      "Epoch: 9772 Training Loss: 0.21726383418562037 Test Loss: 0.21376185211087662\n",
      "Epoch: 9773 Training Loss: 0.21726426715311448 Test Loss: 0.2138130679762995\n",
      "Epoch: 9774 Training Loss: 0.21726383978924596 Test Loss: 0.21376445762567034\n",
      "Epoch: 9775 Training Loss: 0.21726479149108616 Test Loss: 0.21385686914290644\n",
      "Epoch: 9776 Training Loss: 0.21726448764009165 Test Loss: 0.21379289792147843\n",
      "Epoch: 9777 Training Loss: 0.21726404256876625 Test Loss: 0.2137678020177936\n",
      "Epoch: 9778 Training Loss: 0.21726497861631797 Test Loss: 0.21386845785049644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9779 Training Loss: 0.2172649715243694 Test Loss: 0.21371360471753212\n",
      "Epoch: 9780 Training Loss: 0.2172651218898173 Test Loss: 0.21364757241723745\n",
      "Epoch: 9781 Training Loss: 0.2172648058005045 Test Loss: 0.21368412740091058\n",
      "Epoch: 9782 Training Loss: 0.21726505709397376 Test Loss: 0.21366829987074573\n",
      "Epoch: 9783 Training Loss: 0.21726260836338165 Test Loss: 0.21367598678752522\n",
      "Epoch: 9784 Training Loss: 0.2172658848436157 Test Loss: 0.21372418232982904\n",
      "Epoch: 9785 Training Loss: 0.21726521360099532 Test Loss: 0.2137528041042795\n",
      "Epoch: 9786 Training Loss: 0.21726477806928213 Test Loss: 0.2137756574504553\n",
      "Epoch: 9787 Training Loss: 0.217264477580463 Test Loss: 0.21373715805275698\n",
      "Epoch: 9788 Training Loss: 0.21726369596883277 Test Loss: 0.21774898973432355\n",
      "Epoch: 9789 Training Loss: 0.21726552733230248 Test Loss: 0.21391616080791387\n",
      "Epoch: 9790 Training Loss: 0.2172648329041208 Test Loss: 0.21385860615276892\n",
      "Epoch: 9791 Training Loss: 0.21726479626089226 Test Loss: 0.2137594410374854\n",
      "Epoch: 9792 Training Loss: 0.21726425319336237 Test Loss: 0.21379309236288097\n",
      "Epoch: 9793 Training Loss: 0.2172645497461949 Test Loss: 0.21370839368794467\n",
      "Epoch: 9794 Training Loss: 0.21726714374965997 Test Loss: 0.21375233744491345\n",
      "Epoch: 9795 Training Loss: 0.21726515781578173 Test Loss: 0.21372221199028352\n",
      "Epoch: 9796 Training Loss: 0.21726660540710718 Test Loss: 0.21381545312417038\n",
      "Epoch: 9797 Training Loss: 0.2172644364453682 Test Loss: 0.21374590791587023\n",
      "Epoch: 9798 Training Loss: 0.21726327575070764 Test Loss: 0.21374139687533183\n",
      "Epoch: 9799 Training Loss: 0.2172652275248842 Test Loss: 0.21381599756009742\n",
      "Epoch: 9800 Training Loss: 0.21726425118502296 Test Loss: 0.21375467074174365\n",
      "Epoch: 9801 Training Loss: 0.21726429571815634 Test Loss: 0.21374139687533183\n",
      "Epoch: 9802 Training Loss: 0.21726510394028378 Test Loss: 0.21376985013390012\n",
      "Epoch: 9803 Training Loss: 0.21726589579085867 Test Loss: 0.21374799492025726\n",
      "Epoch: 9804 Training Loss: 0.21726596489925246 Test Loss: 0.213700693808405\n",
      "Epoch: 9805 Training Loss: 0.2172664944463545 Test Loss: 0.21374863009550546\n",
      "Epoch: 9806 Training Loss: 0.21726389476753744 Test Loss: 0.2136858903362934\n",
      "Epoch: 9807 Training Loss: 0.2172655164926491 Test Loss: 0.21374629679867527\n",
      "Epoch: 9808 Training Loss: 0.2172642627060772 Test Loss: 0.21372076016114475\n",
      "Epoch: 9809 Training Loss: 0.21726497298579497 Test Loss: 0.21663611084839918\n",
      "Epoch: 9810 Training Loss: 0.21726381446085827 Test Loss: 0.21377805556108634\n",
      "Epoch: 9811 Training Loss: 0.21726304946285724 Test Loss: 0.21395958605447596\n",
      "Epoch: 9812 Training Loss: 0.21726048155883876 Test Loss: 0.21432451367871933\n",
      "Epoch: 9813 Training Loss: 0.21726301293618414 Test Loss: 0.21402521650920542\n",
      "Epoch: 9814 Training Loss: 0.21726347615429067 Test Loss: 0.2137344099476014\n",
      "Epoch: 9815 Training Loss: 0.2172640242785323 Test Loss: 0.21372341752697913\n",
      "Epoch: 9816 Training Loss: 0.21726346734090832 Test Loss: 0.21373167480520602\n",
      "Epoch: 9817 Training Loss: 0.2172640954669919 Test Loss: 0.21363236709956063\n",
      "Epoch: 9818 Training Loss: 0.21726569339686783 Test Loss: 0.2137616187811936\n",
      "Epoch: 9819 Training Loss: 0.21726416760582642 Test Loss: 0.21369370688067457\n",
      "Epoch: 9820 Training Loss: 0.2172649675704512 Test Loss: 0.2136912050679622\n",
      "Epoch: 9821 Training Loss: 0.217265713614749 Test Loss: 0.21368153484887703\n",
      "Epoch: 9822 Training Loss: 0.21726500543302865 Test Loss: 0.2137186083429569\n",
      "Epoch: 9823 Training Loss: 0.21726547755417555 Test Loss: 0.21375792439454577\n",
      "Epoch: 9824 Training Loss: 0.21726541322455367 Test Loss: 0.21373247849633642\n",
      "Epoch: 9825 Training Loss: 0.2172653053390707 Test Loss: 0.213704401157813\n",
      "Epoch: 9826 Training Loss: 0.21726462519340997 Test Loss: 0.21378810170021637\n",
      "Epoch: 9827 Training Loss: 0.21726581792287736 Test Loss: 0.21371471951490656\n",
      "Epoch: 9828 Training Loss: 0.21726458744738794 Test Loss: 0.21374968007907905\n",
      "Epoch: 9829 Training Loss: 0.21726451546097203 Test Loss: 0.21363613926276945\n",
      "Epoch: 9830 Training Loss: 0.2172638204231159 Test Loss: 0.21366302402735743\n",
      "Epoch: 9831 Training Loss: 0.21726536439680164 Test Loss: 0.21369733645352157\n",
      "Epoch: 9832 Training Loss: 0.217265680145414 Test Loss: 0.21371873797055857\n",
      "Epoch: 9833 Training Loss: 0.21726458782395158 Test Loss: 0.21366238885210923\n",
      "Epoch: 9834 Training Loss: 0.21726610671132623 Test Loss: 0.2137723519466125\n",
      "Epoch: 9835 Training Loss: 0.21726479323045156 Test Loss: 0.21369134765832404\n",
      "Epoch: 9836 Training Loss: 0.21726521571692434 Test Loss: 0.213778029635566\n",
      "Epoch: 9837 Training Loss: 0.21726539939032277 Test Loss: 0.21380072742861977\n",
      "Epoch: 9838 Training Loss: 0.21726586472435835 Test Loss: 0.2137325692356576\n",
      "Epoch: 9839 Training Loss: 0.21726347877230454 Test Loss: 0.21371728614141977\n",
      "Epoch: 9840 Training Loss: 0.21726565672674192 Test Loss: 0.21373089703959594\n",
      "Epoch: 9841 Training Loss: 0.2172628538828751 Test Loss: 0.2177452305338749\n",
      "Epoch: 9842 Training Loss: 0.2172646974846631 Test Loss: 0.21390741094480062\n",
      "Epoch: 9843 Training Loss: 0.21726397298518502 Test Loss: 0.21379379235193002\n",
      "Epoch: 9844 Training Loss: 0.21726380618542396 Test Loss: 0.21371514728599209\n",
      "Epoch: 9845 Training Loss: 0.21726445669014674 Test Loss: 0.21361983211047839\n",
      "Epoch: 9846 Training Loss: 0.21726483192684848 Test Loss: 0.21367987561557555\n",
      "Epoch: 9847 Training Loss: 0.21726610606578856 Test Loss: 0.2136629592135566\n",
      "Epoch: 9848 Training Loss: 0.21726415668548085 Test Loss: 0.21363425966254512\n",
      "Epoch: 9849 Training Loss: 0.21726611469985488 Test Loss: 0.21369066063203515\n",
      "Epoch: 9850 Training Loss: 0.21726500175705024 Test Loss: 0.21369403094967876\n",
      "Epoch: 9851 Training Loss: 0.21726691649350305 Test Loss: 0.21419553421504992\n",
      "Epoch: 9852 Training Loss: 0.2172647649164521 Test Loss: 0.21384073050649754\n",
      "Epoch: 9853 Training Loss: 0.21726395333214932 Test Loss: 0.21408177303181747\n",
      "Epoch: 9854 Training Loss: 0.21726528397356704 Test Loss: 0.21388568535875943\n",
      "Epoch: 9855 Training Loss: 0.21726393298874694 Test Loss: 0.21383848794898852\n",
      "Epoch: 9856 Training Loss: 0.21726542916574776 Test Loss: 0.21379576269147552\n",
      "Epoch: 9857 Training Loss: 0.21726563835581575 Test Loss: 0.21380316442753133\n",
      "Epoch: 9858 Training Loss: 0.2172644432773085 Test Loss: 0.21373877839777797\n",
      "Epoch: 9859 Training Loss: 0.2172634531839086 Test Loss: 0.21372138237363278\n",
      "Epoch: 9860 Training Loss: 0.21726336972126747 Test Loss: 0.21378946279003397\n",
      "Epoch: 9861 Training Loss: 0.2172653836105131 Test Loss: 0.21370433634401215\n",
      "Epoch: 9862 Training Loss: 0.2172664539388658 Test Loss: 0.21362491351246415\n",
      "Epoch: 9863 Training Loss: 0.21726526485847938 Test Loss: 0.21360890450365694\n",
      "Epoch: 9864 Training Loss: 0.21726601805748635 Test Loss: 0.21374518200130083\n",
      "Epoch: 9865 Training Loss: 0.21726533339306192 Test Loss: 0.2137189713002416\n",
      "Epoch: 9866 Training Loss: 0.21726326778907637 Test Loss: 0.21382714553384172\n",
      "Epoch: 9867 Training Loss: 0.21726405056626072 Test Loss: 0.21382959549551345\n",
      "Epoch: 9868 Training Loss: 0.21726287213724585 Test Loss: 0.21377324637706407\n",
      "Epoch: 9869 Training Loss: 0.21726241736492383 Test Loss: 0.21412654640543696\n",
      "Epoch: 9870 Training Loss: 0.2172657454881714 Test Loss: 0.21373435809656074\n",
      "Epoch: 9871 Training Loss: 0.21726464349260974 Test Loss: 0.21373491549524795\n",
      "Epoch: 9872 Training Loss: 0.21726423543211068 Test Loss: 0.21427880698636773\n",
      "Epoch: 9873 Training Loss: 0.21726432117206526 Test Loss: 0.21374418386876792\n",
      "Epoch: 9874 Training Loss: 0.2172634706672205 Test Loss: 0.21369231986533663\n",
      "Epoch: 9875 Training Loss: 0.2172645459088321 Test Loss: 0.2137446764536543\n",
      "Epoch: 9876 Training Loss: 0.21726359176829405 Test Loss: 0.21382360670031592\n",
      "Epoch: 9877 Training Loss: 0.21726336394729168 Test Loss: 0.21370489374269938\n",
      "Epoch: 9878 Training Loss: 0.21726117907125597 Test Loss: 0.21492283579978363\n",
      "Epoch: 9879 Training Loss: 0.21726621160223167 Test Loss: 0.21382951771895242\n",
      "Epoch: 9880 Training Loss: 0.21726608734519617 Test Loss: 0.21382529185913773\n",
      "Epoch: 9881 Training Loss: 0.21726513047905463 Test Loss: 0.21379485529826378\n",
      "Epoch: 9882 Training Loss: 0.21726439119497074 Test Loss: 0.21378859428510275\n",
      "Epoch: 9883 Training Loss: 0.21726585705859852 Test Loss: 0.21378689616352076\n",
      "Epoch: 9884 Training Loss: 0.21726390725689818 Test Loss: 0.21456573768268158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9885 Training Loss: 0.21726642298892088 Test Loss: 0.21385181366644102\n",
      "Epoch: 9886 Training Loss: 0.21726216440381557 Test Loss: 0.21387457627329565\n",
      "Epoch: 9887 Training Loss: 0.21726475568167714 Test Loss: 0.21374452090053228\n",
      "Epoch: 9888 Training Loss: 0.2172650930199382 Test Loss: 0.21379542565971116\n",
      "Epoch: 9889 Training Loss: 0.21726346871267588 Test Loss: 0.21379192571446587\n",
      "Epoch: 9890 Training Loss: 0.21726388063743513 Test Loss: 0.21379459604306042\n",
      "Epoch: 9891 Training Loss: 0.2172649836013033 Test Loss: 0.2137180379815095\n",
      "Epoch: 9892 Training Loss: 0.2172642730884747 Test Loss: 0.21388409093925878\n",
      "Epoch: 9893 Training Loss: 0.21726473743627217 Test Loss: 0.21377149640444143\n",
      "Epoch: 9894 Training Loss: 0.21726582971290562 Test Loss: 0.21378718134424446\n",
      "Epoch: 9895 Training Loss: 0.21726386846187742 Test Loss: 0.2137531152105235\n",
      "Epoch: 9896 Training Loss: 0.2172652733132297 Test Loss: 0.21378723319528511\n",
      "Epoch: 9897 Training Loss: 0.2172624368117461 Test Loss: 0.21397905612024795\n",
      "Epoch: 9898 Training Loss: 0.21726353303333196 Test Loss: 0.21426330352520706\n",
      "Epoch: 9899 Training Loss: 0.21726753132330381 Test Loss: 0.21395097878172453\n",
      "Epoch: 9900 Training Loss: 0.2172640908137412 Test Loss: 0.21391533119126313\n",
      "Epoch: 9901 Training Loss: 0.21726444842367826 Test Loss: 0.2137970978557728\n",
      "Epoch: 9902 Training Loss: 0.21726452607648036 Test Loss: 0.2137328544163813\n",
      "Epoch: 9903 Training Loss: 0.217264105508689 Test Loss: 0.21371009180952666\n",
      "Epoch: 9904 Training Loss: 0.21726600566674942 Test Loss: 0.21539425249880534\n",
      "Epoch: 9905 Training Loss: 0.21726612631056713 Test Loss: 0.21379724044613466\n",
      "Epoch: 9906 Training Loss: 0.21726571206366543 Test Loss: 0.2137289007745301\n",
      "Epoch: 9907 Training Loss: 0.21726528781092985 Test Loss: 0.2137345006869226\n",
      "Epoch: 9908 Training Loss: 0.2172650444611602 Test Loss: 0.2136686369025101\n",
      "Epoch: 9909 Training Loss: 0.21726704388856888 Test Loss: 0.21368664217638314\n",
      "Epoch: 9910 Training Loss: 0.21726598243635914 Test Loss: 0.21366700359472895\n",
      "Epoch: 9911 Training Loss: 0.21726506712670504 Test Loss: 0.21364780574692047\n",
      "Epoch: 9912 Training Loss: 0.2172653425112815 Test Loss: 0.2137007456594457\n",
      "Epoch: 9913 Training Loss: 0.21726513114252388 Test Loss: 0.2136880680800016\n",
      "Epoch: 9914 Training Loss: 0.21726522782972144 Test Loss: 0.2136088396898561\n",
      "Epoch: 9915 Training Loss: 0.21726621216707712 Test Loss: 0.2136624795914304\n",
      "Epoch: 9916 Training Loss: 0.21726482721083717 Test Loss: 0.21365842224749787\n",
      "Epoch: 9917 Training Loss: 0.21726502598264444 Test Loss: 0.21364619836465965\n",
      "Epoch: 9918 Training Loss: 0.21726650161002947 Test Loss: 0.21380311257649065\n",
      "Epoch: 9919 Training Loss: 0.2172656343391369 Test Loss: 0.2137088603473107\n",
      "Epoch: 9920 Training Loss: 0.21726399182233283 Test Loss: 0.21378827021609856\n",
      "Epoch: 9921 Training Loss: 0.21726349300103068 Test Loss: 0.21376886496412736\n",
      "Epoch: 9922 Training Loss: 0.21726516316836492 Test Loss: 0.213714291743821\n",
      "Epoch: 9923 Training Loss: 0.2172661717133832 Test Loss: 0.21373074148647392\n",
      "Epoch: 9924 Training Loss: 0.21726568645733788 Test Loss: 0.21372827856204205\n",
      "Epoch: 9925 Training Loss: 0.21726457628496573 Test Loss: 0.2137809592193639\n",
      "Epoch: 9926 Training Loss: 0.21726627517872626 Test Loss: 0.2137672446191064\n",
      "Epoch: 9927 Training Loss: 0.21726523257263014 Test Loss: 0.213754929996947\n",
      "Epoch: 9928 Training Loss: 0.21726426241020577 Test Loss: 0.21381962713294442\n",
      "Epoch: 9929 Training Loss: 0.21726418775198117 Test Loss: 0.2138038644165804\n",
      "Epoch: 9930 Training Loss: 0.21726545636798789 Test Loss: 0.21377463339240205\n",
      "Epoch: 9931 Training Loss: 0.21726413574136982 Test Loss: 0.21374863009550546\n",
      "Epoch: 9932 Training Loss: 0.2172663793971966 Test Loss: 0.21387383739596608\n",
      "Epoch: 9933 Training Loss: 0.21726517782744947 Test Loss: 0.21371241214359668\n",
      "Epoch: 9934 Training Loss: 0.21726489713511885 Test Loss: 0.21373526548977248\n",
      "Epoch: 9935 Training Loss: 0.21726484208510097 Test Loss: 0.2137408135511243\n",
      "Epoch: 9936 Training Loss: 0.21726556545488815 Test Loss: 0.21376303172205188\n",
      "Epoch: 9937 Training Loss: 0.21726584876523264 Test Loss: 0.21368224780068626\n",
      "Epoch: 9938 Training Loss: 0.21726575457949357 Test Loss: 0.2137279415302777\n",
      "Epoch: 9939 Training Loss: 0.21726766416061083 Test Loss: 0.213729510024258\n",
      "Epoch: 9940 Training Loss: 0.21726543916261584 Test Loss: 0.21371316398368642\n",
      "Epoch: 9941 Training Loss: 0.2172660006190035 Test Loss: 0.21379257385247424\n",
      "Epoch: 9942 Training Loss: 0.21726458461419484 Test Loss: 0.21839145005375915\n",
      "Epoch: 9943 Training Loss: 0.21726764846149335 Test Loss: 0.21400464460881916\n",
      "Epoch: 9944 Training Loss: 0.21726522105157592 Test Loss: 0.21378407028180418\n",
      "Epoch: 9945 Training Loss: 0.217265268839295 Test Loss: 0.2137540485292556\n",
      "Epoch: 9946 Training Loss: 0.21726556344654874 Test Loss: 0.21369727163972072\n",
      "Epoch: 9947 Training Loss: 0.21726625481739228 Test Loss: 0.21365641301967186\n",
      "Epoch: 9948 Training Loss: 0.21726533248751603 Test Loss: 0.21365112421352342\n",
      "Epoch: 9949 Training Loss: 0.2172666095941362 Test Loss: 0.2137013419464134\n",
      "Epoch: 9950 Training Loss: 0.21726486324439123 Test Loss: 0.21370103084016936\n",
      "Epoch: 9951 Training Loss: 0.21726598967176053 Test Loss: 0.21375592812947994\n",
      "Epoch: 9952 Training Loss: 0.21726597848244092 Test Loss: 0.21373584881398003\n",
      "Epoch: 9953 Training Loss: 0.21726571727279578 Test Loss: 0.2136920865356536\n",
      "Epoch: 9954 Training Loss: 0.21726415809311161 Test Loss: 0.21403972183783318\n",
      "Epoch: 9955 Training Loss: 0.21726794564193194 Test Loss: 0.21379733118545582\n",
      "Epoch: 9956 Training Loss: 0.21726350415448706 Test Loss: 0.21383450838161702\n",
      "Epoch: 9957 Training Loss: 0.2172627209469443 Test Loss: 0.2137985885731921\n",
      "Epoch: 9958 Training Loss: 0.21726371049343032 Test Loss: 0.21379777191930152\n",
      "Epoch: 9959 Training Loss: 0.21726580022438624 Test Loss: 0.21378208697949852\n",
      "Epoch: 9960 Training Loss: 0.21726420811331515 Test Loss: 0.21392209775207072\n",
      "Epoch: 9961 Training Loss: 0.21726288764808152 Test Loss: 0.21370908071423356\n",
      "Epoch: 9962 Training Loss: 0.21726385269999934 Test Loss: 0.2139928744225868\n",
      "Epoch: 9963 Training Loss: 0.21726638844368978 Test Loss: 0.21373902469022113\n",
      "Epoch: 9964 Training Loss: 0.2172656829517097 Test Loss: 0.21379324791600296\n",
      "Epoch: 9965 Training Loss: 0.21726458656873943 Test Loss: 0.21375688737373236\n",
      "Epoch: 9966 Training Loss: 0.21726512144152724 Test Loss: 0.21385256550653073\n",
      "Epoch: 9967 Training Loss: 0.21726478385222375 Test Loss: 0.21379970337056653\n",
      "Epoch: 9968 Training Loss: 0.21726411326410683 Test Loss: 0.2137344488358819\n",
      "Epoch: 9969 Training Loss: 0.21726435581592016 Test Loss: 0.21371726021589943\n",
      "Epoch: 9970 Training Loss: 0.2172661127811735 Test Loss: 0.21370024011179914\n",
      "Epoch: 9971 Training Loss: 0.21726562992796283 Test Loss: 0.2136807441205068\n",
      "Epoch: 9972 Training Loss: 0.21726593356377813 Test Loss: 0.21363940587833175\n",
      "Epoch: 9973 Training Loss: 0.21726498092052882 Test Loss: 0.21379747377581768\n",
      "Epoch: 9974 Training Loss: 0.21726389230194218 Test Loss: 0.21376987605942047\n",
      "Epoch: 9975 Training Loss: 0.21726520738769525 Test Loss: 0.21379555528731284\n",
      "Epoch: 9976 Training Loss: 0.21726439235155906 Test Loss: 0.21473579613332272\n",
      "Epoch: 9977 Training Loss: 0.2172652621418417 Test Loss: 0.2137039604239673\n",
      "Epoch: 9978 Training Loss: 0.21726452284879202 Test Loss: 0.21366345179844298\n",
      "Epoch: 9979 Training Loss: 0.21726412380788873 Test Loss: 0.213724104553268\n",
      "Epoch: 9980 Training Loss: 0.21726355450642526 Test Loss: 0.21371762317318413\n",
      "Epoch: 9981 Training Loss: 0.217263688661705 Test Loss: 0.21370742148093208\n",
      "Epoch: 9982 Training Loss: 0.21726440909070946 Test Loss: 0.2136825459441701\n",
      "Epoch: 9983 Training Loss: 0.21726450435234465 Test Loss: 0.21376960384145693\n",
      "Epoch: 9984 Training Loss: 0.2172645002639394 Test Loss: 0.21384292121296591\n",
      "Epoch: 9985 Training Loss: 0.2172643683680415 Test Loss: 0.2137433024010765\n",
      "Epoch: 9986 Training Loss: 0.21726566685809698 Test Loss: 0.21372127867155144\n",
      "Epoch: 9987 Training Loss: 0.2172645748414718 Test Loss: 0.213715912088842\n",
      "Epoch: 9988 Training Loss: 0.21726568585662923 Test Loss: 0.2137256082334475\n",
      "Epoch: 9989 Training Loss: 0.21726524754551774 Test Loss: 0.21384139160726612\n",
      "Epoch: 9990 Training Loss: 0.21726352030189458 Test Loss: 0.21363900403276653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9991 Training Loss: 0.2172653948356959 Test Loss: 0.213716223195086\n",
      "Epoch: 9992 Training Loss: 0.2172648330924026 Test Loss: 0.21361690252668047\n",
      "Epoch: 9993 Training Loss: 0.2172656473485141 Test Loss: 0.21370852331554635\n",
      "Epoch: 9994 Training Loss: 0.2172663131219959 Test Loss: 0.21377390747783265\n",
      "Epoch: 9995 Training Loss: 0.2172645412107524 Test Loss: 0.21375050969572978\n",
      "Epoch: 9996 Training Loss: 0.21726436666453933 Test Loss: 0.21385545620204816\n",
      "Epoch: 9997 Training Loss: 0.21726430607365643 Test Loss: 0.21378050552275804\n",
      "Epoch: 9998 Training Loss: 0.21726505968509024 Test Loss: 0.2137905386991279\n",
      "Epoch: 9999 Training Loss: 0.2172652050386554 Test Loss: 0.2137307155609536\n",
      "Epoch: 10000 Training Loss: 0.21726541219348655 Test Loss: 0.21373380069787354\n",
      "Epoch: 10001 Training Loss: 0.21726470636977185 Test Loss: 0.21373600436710205\n",
      "Epoch: 10002 Training Loss: 0.217263436274408 Test Loss: 0.21457526531140492\n",
      "Epoch: 10003 Training Loss: 0.21726495520661168 Test Loss: 0.2137661557472523\n",
      "Epoch: 10004 Training Loss: 0.21726413106122172 Test Loss: 0.21377874258737523\n",
      "Epoch: 10005 Training Loss: 0.21726606480517255 Test Loss: 0.2137184787153552\n",
      "Epoch: 10006 Training Loss: 0.21726485896770417 Test Loss: 0.21370320858387756\n",
      "Epoch: 10007 Training Loss: 0.2172636233996398 Test Loss: 0.21368261075797096\n",
      "Epoch: 10008 Training Loss: 0.21726540641054493 Test Loss: 0.213681145966072\n",
      "Epoch: 10009 Training Loss: 0.21726618803114092 Test Loss: 0.21367758120702585\n",
      "Epoch: 10010 Training Loss: 0.21726586113803797 Test Loss: 0.21388157616378622\n",
      "Epoch: 10011 Training Loss: 0.21726497852665996 Test Loss: 0.21375959659060742\n",
      "Epoch: 10012 Training Loss: 0.21726471311205417 Test Loss: 0.21369569018298024\n",
      "Epoch: 10013 Training Loss: 0.21726538981484736 Test Loss: 0.21370490670545955\n",
      "Epoch: 10014 Training Loss: 0.21726441525021473 Test Loss: 0.21372059164526255\n",
      "Epoch: 10015 Training Loss: 0.21726505860919412 Test Loss: 0.21372965261461985\n",
      "Epoch: 10016 Training Loss: 0.21726430126798713 Test Loss: 0.21372428603191038\n",
      "Epoch: 10017 Training Loss: 0.2172646654588221 Test Loss: 0.21371888056092042\n",
      "Epoch: 10018 Training Loss: 0.2172651611510597 Test Loss: 0.2138653727135765\n",
      "Epoch: 10019 Training Loss: 0.21726405256563433 Test Loss: 0.21381982157434692\n",
      "Epoch: 10020 Training Loss: 0.21726484901566512 Test Loss: 0.21380628845273175\n",
      "Epoch: 10021 Training Loss: 0.21726603828433333 Test Loss: 0.21378352584587715\n",
      "Epoch: 10022 Training Loss: 0.21726345659091298 Test Loss: 0.21384455452074705\n",
      "Epoch: 10023 Training Loss: 0.2172648508088253 Test Loss: 0.21381406610883244\n",
      "Epoch: 10024 Training Loss: 0.21726505633188067 Test Loss: 0.21379148498062014\n",
      "Epoch: 10025 Training Loss: 0.21726452239153618 Test Loss: 0.2138015311197502\n",
      "Epoch: 10026 Training Loss: 0.21726615809433153 Test Loss: 0.21387532811338536\n",
      "Epoch: 10027 Training Loss: 0.2172641252603485 Test Loss: 0.21373789693008655\n",
      "Epoch: 10028 Training Loss: 0.21726601953684352 Test Loss: 0.21376991494770098\n",
      "Epoch: 10029 Training Loss: 0.21726460986189033 Test Loss: 0.2136827144600523\n",
      "Epoch: 10030 Training Loss: 0.21726676666600322 Test Loss: 0.21376727054462674\n",
      "Epoch: 10031 Training Loss: 0.21726481229174438 Test Loss: 0.21373247849633642\n",
      "Epoch: 10032 Training Loss: 0.21726431558637124 Test Loss: 0.213739245057144\n",
      "Epoch: 10033 Training Loss: 0.21726431902027302 Test Loss: 0.21372433788295106\n",
      "Epoch: 10034 Training Loss: 0.21726420321798784 Test Loss: 0.21384740632798396\n",
      "Epoch: 10035 Training Loss: 0.21726300164824072 Test Loss: 0.2137657927899676\n",
      "Epoch: 10036 Training Loss: 0.21726509487585902 Test Loss: 0.21390573874873897\n",
      "Epoch: 10037 Training Loss: 0.21726335478424308 Test Loss: 0.21411786135612457\n",
      "Epoch: 10038 Training Loss: 0.2172624284914828 Test Loss: 0.21386093944959914\n",
      "Epoch: 10039 Training Loss: 0.21726555890985344 Test Loss: 0.2137078622147778\n",
      "Epoch: 10040 Training Loss: 0.21726437914493427 Test Loss: 0.21374746344709036\n",
      "Epoch: 10041 Training Loss: 0.2172645198183513 Test Loss: 0.21370679926844405\n",
      "Epoch: 10042 Training Loss: 0.21726570135849907 Test Loss: 0.2136955864808989\n",
      "Epoch: 10043 Training Loss: 0.2172654417178691 Test Loss: 0.21369991604279495\n",
      "Epoch: 10044 Training Loss: 0.2172654604384615 Test Loss: 0.21369549574157773\n",
      "Epoch: 10045 Training Loss: 0.2172652705696946 Test Loss: 0.21366478696274024\n",
      "Epoch: 10046 Training Loss: 0.21726500292260437 Test Loss: 0.21374891527622916\n",
      "Epoch: 10047 Training Loss: 0.2172644924726584 Test Loss: 0.21366188330446267\n",
      "Epoch: 10048 Training Loss: 0.2172656818758136 Test Loss: 0.21683970395959434\n",
      "Epoch: 10049 Training Loss: 0.21726486580861032 Test Loss: 0.21382464372112933\n",
      "Epoch: 10050 Training Loss: 0.21726442693265335 Test Loss: 0.21382137710556706\n",
      "Epoch: 10051 Training Loss: 0.2172656751783603 Test Loss: 0.21377612410982133\n",
      "Epoch: 10052 Training Loss: 0.21726476818000368 Test Loss: 0.21382142895660772\n",
      "Epoch: 10053 Training Loss: 0.21726366767276495 Test Loss: 0.2137828517823484\n",
      "Epoch: 10054 Training Loss: 0.21726428074526874 Test Loss: 0.21373584881398003\n",
      "Epoch: 10055 Training Loss: 0.2172650134843179 Test Loss: 0.21379035722048556\n",
      "Epoch: 10056 Training Loss: 0.21726496040677623 Test Loss: 0.2137673223956674\n",
      "Epoch: 10057 Training Loss: 0.21726555820155516 Test Loss: 0.21366237588934905\n",
      "Epoch: 10058 Training Loss: 0.21726703539795536 Test Loss: 0.21384463229730807\n",
      "Epoch: 10059 Training Loss: 0.2172654529520177 Test Loss: 0.2137818536498155\n",
      "Epoch: 10060 Training Loss: 0.2172652418163709 Test Loss: 0.21369637720926915\n",
      "Epoch: 10061 Training Loss: 0.21726487336678052 Test Loss: 0.21373562844705718\n",
      "Epoch: 10062 Training Loss: 0.21726486321749383 Test Loss: 0.21374059318420144\n",
      "Epoch: 10063 Training Loss: 0.21726570202196835 Test Loss: 0.21405216608759425\n",
      "Epoch: 10064 Training Loss: 0.21726412856872906 Test Loss: 0.21440882347085058\n",
      "Epoch: 10065 Training Loss: 0.21726240286722368 Test Loss: 0.2140669565969457\n",
      "Epoch: 10066 Training Loss: 0.21726650504393125 Test Loss: 0.21388703348581686\n",
      "Epoch: 10067 Training Loss: 0.21726422047715468 Test Loss: 0.21382189561597378\n",
      "Epoch: 10068 Training Loss: 0.21726292092016888 Test Loss: 0.21386850970153712\n",
      "Epoch: 10069 Training Loss: 0.21726306000663917 Test Loss: 0.21372787671647686\n",
      "Epoch: 10070 Training Loss: 0.21726341319643633 Test Loss: 0.21456696914489753\n",
      "Epoch: 10071 Training Loss: 0.21726502060316386 Test Loss: 0.21382527889637756\n",
      "Epoch: 10072 Training Loss: 0.21726517592669967 Test Loss: 0.21375998547341243\n",
      "Epoch: 10073 Training Loss: 0.2172648507370989 Test Loss: 0.2137149009935489\n",
      "Epoch: 10074 Training Loss: 0.21726330401987806 Test Loss: 0.21371841390155438\n",
      "Epoch: 10075 Training Loss: 0.21726543803292492 Test Loss: 0.2137821777188197\n",
      "Epoch: 10076 Training Loss: 0.2172639792881431 Test Loss: 0.21378606654687002\n",
      "Epoch: 10077 Training Loss: 0.2172651293762611 Test Loss: 0.21375671885785016\n",
      "Epoch: 10078 Training Loss: 0.21726522323923134 Test Loss: 0.2137511967220187\n",
      "Epoch: 10079 Training Loss: 0.21726375456930785 Test Loss: 0.21382613443854864\n",
      "Epoch: 10080 Training Loss: 0.21726448559588904 Test Loss: 0.21393267536436764\n",
      "Epoch: 10081 Training Loss: 0.21726435248064221 Test Loss: 0.21379646268052457\n",
      "Epoch: 10082 Training Loss: 0.2172630631536353 Test Loss: 0.21377382970127162\n",
      "Epoch: 10083 Training Loss: 0.21726375476655546 Test Loss: 0.2136668221160866\n",
      "Epoch: 10084 Training Loss: 0.21726595249058395 Test Loss: 0.21377117233543724\n",
      "Epoch: 10085 Training Loss: 0.21726429873066544 Test Loss: 0.21380129779006718\n",
      "Epoch: 10086 Training Loss: 0.2172647769664886 Test Loss: 0.21373973764203036\n",
      "Epoch: 10087 Training Loss: 0.21726460000847508 Test Loss: 0.21376675203422002\n",
      "Epoch: 10088 Training Loss: 0.21726562458434548 Test Loss: 0.21375957066508708\n",
      "Epoch: 10089 Training Loss: 0.2172660943205893 Test Loss: 0.21380287924680763\n",
      "Epoch: 10090 Training Loss: 0.21726407487254712 Test Loss: 0.21376335579105607\n",
      "Epoch: 10091 Training Loss: 0.2172640709724237 Test Loss: 0.21380535513399967\n",
      "Epoch: 10092 Training Loss: 0.21726545775768702 Test Loss: 0.21383771018337847\n",
      "Epoch: 10093 Training Loss: 0.21726419617983408 Test Loss: 0.21445605976890197\n",
      "Epoch: 10094 Training Loss: 0.21726107883360124 Test Loss: 0.21475046997783265\n",
      "Epoch: 10095 Training Loss: 0.21726305803416296 Test Loss: 0.21419395275830944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10096 Training Loss: 0.2172647217281889 Test Loss: 0.21390454617480353\n",
      "Epoch: 10097 Training Loss: 0.2172647891868753 Test Loss: 0.21385003776829803\n",
      "Epoch: 10098 Training Loss: 0.2172624255148369 Test Loss: 0.21372123978327096\n",
      "Epoch: 10099 Training Loss: 0.21726238151068578 Test Loss: 0.21385281179897392\n",
      "Epoch: 10100 Training Loss: 0.21726605572281618 Test Loss: 0.21385767283403684\n",
      "Epoch: 10101 Training Loss: 0.2172651348633313 Test Loss: 0.21376675203422002\n",
      "Epoch: 10102 Training Loss: 0.21726614807953187 Test Loss: 0.2136686757907906\n",
      "Epoch: 10103 Training Loss: 0.21726543131754 Test Loss: 0.21375635590056546\n",
      "Epoch: 10104 Training Loss: 0.21726538534987846 Test Loss: 0.21375184486002707\n",
      "Epoch: 10105 Training Loss: 0.2172637692732214 Test Loss: 0.21378657209451657\n",
      "Epoch: 10106 Training Loss: 0.2172639971928476 Test Loss: 0.21374069688628278\n",
      "Epoch: 10107 Training Loss: 0.21726430706886035 Test Loss: 0.21376946125109508\n",
      "Epoch: 10108 Training Loss: 0.2172657920027468 Test Loss: 0.2136922161632553\n",
      "Epoch: 10109 Training Loss: 0.21726588808923566 Test Loss: 0.21374214871542158\n",
      "Epoch: 10110 Training Loss: 0.21726458142236968 Test Loss: 0.2137139287865363\n",
      "Epoch: 10111 Training Loss: 0.21726510172573094 Test Loss: 0.21371583431228097\n",
      "Epoch: 10112 Training Loss: 0.2172661539431657 Test Loss: 0.21369146432316555\n",
      "Epoch: 10113 Training Loss: 0.21726395638948742 Test Loss: 0.2138262251778698\n",
      "Epoch: 10114 Training Loss: 0.21726578691017184 Test Loss: 0.21419027133442178\n",
      "Epoch: 10115 Training Loss: 0.21726314613212316 Test Loss: 0.2139647970840634\n",
      "Epoch: 10116 Training Loss: 0.21726425271817493 Test Loss: 0.21383128065433524\n",
      "Epoch: 10117 Training Loss: 0.21726413704141095 Test Loss: 0.2138003515085749\n",
      "Epoch: 10118 Training Loss: 0.21726458416590477 Test Loss: 0.21371261954775936\n",
      "Epoch: 10119 Training Loss: 0.2172645413452394 Test Loss: 0.21383807314066317\n",
      "Epoch: 10120 Training Loss: 0.217265261182501 Test Loss: 0.21375586331567908\n",
      "Epoch: 10121 Training Loss: 0.2172641178187337 Test Loss: 0.21373290626742195\n",
      "Epoch: 10122 Training Loss: 0.21726351085194037 Test Loss: 0.21368699217090767\n",
      "Epoch: 10123 Training Loss: 0.217262776319731 Test Loss: 0.21368170336475922\n",
      "Epoch: 10124 Training Loss: 0.21726340146020287 Test Loss: 0.21376361504625943\n",
      "Epoch: 10125 Training Loss: 0.2172654464428462 Test Loss: 0.2138125105776123\n",
      "Epoch: 10126 Training Loss: 0.217264793750468 Test Loss: 0.21377805556108634\n",
      "Epoch: 10127 Training Loss: 0.21726486826523977 Test Loss: 0.213721784219198\n",
      "Epoch: 10128 Training Loss: 0.21726438276711785 Test Loss: 0.2137383117384119\n",
      "Epoch: 10129 Training Loss: 0.21726614328282834 Test Loss: 0.21532303509444356\n",
      "Epoch: 10130 Training Loss: 0.21726576144729712 Test Loss: 0.21383704908260992\n",
      "Epoch: 10131 Training Loss: 0.2172646209525861 Test Loss: 0.21374571347446772\n",
      "Epoch: 10132 Training Loss: 0.21726290165266263 Test Loss: 0.21366487770206144\n",
      "Epoch: 10133 Training Loss: 0.2172634362116474 Test Loss: 0.2137186472312374\n",
      "Epoch: 10134 Training Loss: 0.21726434338035422 Test Loss: 0.21373617288298424\n",
      "Epoch: 10135 Training Loss: 0.21726406611295956 Test Loss: 0.21372015091141686\n",
      "Epoch: 10136 Training Loss: 0.21726661737645145 Test Loss: 0.2137686964482452\n",
      "Epoch: 10137 Training Loss: 0.21726378294606788 Test Loss: 0.21376526131680074\n",
      "Epoch: 10138 Training Loss: 0.21726312426453462 Test Loss: 0.21372891373729028\n",
      "Epoch: 10139 Training Loss: 0.21726439878900417 Test Loss: 0.21366447585649623\n",
      "Epoch: 10140 Training Loss: 0.21726482709428177 Test Loss: 0.21373390439995488\n",
      "Epoch: 10141 Training Loss: 0.21726413398407282 Test Loss: 0.21376597426860997\n",
      "Epoch: 10142 Training Loss: 0.21726314980810157 Test Loss: 0.21411749839883987\n",
      "Epoch: 10143 Training Loss: 0.21726485965807085 Test Loss: 0.21381997712746895\n",
      "Epoch: 10144 Training Loss: 0.21726382681573198 Test Loss: 0.213844165637942\n",
      "Epoch: 10145 Training Loss: 0.2172627209200469 Test Loss: 0.21378317585135262\n",
      "Epoch: 10146 Training Loss: 0.21726421202240437 Test Loss: 0.21386182091729053\n",
      "Epoch: 10147 Training Loss: 0.21726721476776936 Test Loss: 0.21378554803646332\n",
      "Epoch: 10148 Training Loss: 0.21726690230064014 Test Loss: 0.21377302601014123\n",
      "Epoch: 10149 Training Loss: 0.21726421568941698 Test Loss: 0.2137136176802923\n",
      "Epoch: 10150 Training Loss: 0.2172651284617494 Test Loss: 0.21374680234632182\n",
      "Epoch: 10151 Training Loss: 0.21726422285309194 Test Loss: 0.2138678356380084\n",
      "Epoch: 10152 Training Loss: 0.21726362533625282 Test Loss: 0.21383625835423967\n",
      "Epoch: 10153 Training Loss: 0.21726263999472742 Test Loss: 0.21375841697943215\n",
      "Epoch: 10154 Training Loss: 0.21726632062637133 Test Loss: 0.21489153073397843\n",
      "Epoch: 10155 Training Loss: 0.2172652984443698 Test Loss: 0.2137794814647048\n",
      "Epoch: 10156 Training Loss: 0.21726528751505841 Test Loss: 0.21382829921949664\n",
      "Epoch: 10157 Training Loss: 0.21726410325827294 Test Loss: 0.21376208544055963\n",
      "Epoch: 10158 Training Loss: 0.21726402145430498 Test Loss: 0.2137738815523123\n",
      "Epoch: 10159 Training Loss: 0.2172633941441093 Test Loss: 0.21373701546239512\n",
      "Epoch: 10160 Training Loss: 0.2172658285204541 Test Loss: 0.21372827856204205\n",
      "Epoch: 10161 Training Loss: 0.21726434440245554 Test Loss: 0.21370615113043565\n",
      "Epoch: 10162 Training Loss: 0.21726310566049764 Test Loss: 0.21365401490904082\n",
      "Epoch: 10163 Training Loss: 0.21726539137489673 Test Loss: 0.2136706850186166\n",
      "Epoch: 10164 Training Loss: 0.21726464429056602 Test Loss: 0.21368555330452904\n",
      "Epoch: 10165 Training Loss: 0.21726634694996294 Test Loss: 0.21377572226425615\n",
      "Epoch: 10166 Training Loss: 0.2172657512083524 Test Loss: 0.2137448320067763\n",
      "Epoch: 10167 Training Loss: 0.21726485891390937 Test Loss: 0.21373761174936284\n",
      "Epoch: 10168 Training Loss: 0.21726464757204916 Test Loss: 0.21375575961359775\n",
      "Epoch: 10169 Training Loss: 0.21726358783230743 Test Loss: 0.21375391890165393\n",
      "Epoch: 10170 Training Loss: 0.21726418348425994 Test Loss: 0.21375254484907613\n",
      "Epoch: 10171 Training Loss: 0.21726568818773748 Test Loss: 0.21369871050609934\n",
      "Epoch: 10172 Training Loss: 0.21726492431046157 Test Loss: 0.2138299714155583\n",
      "Epoch: 10173 Training Loss: 0.21726577054758509 Test Loss: 0.21366285551147526\n",
      "Epoch: 10174 Training Loss: 0.217264436678479 Test Loss: 0.21367126834282416\n",
      "Epoch: 10175 Training Loss: 0.21726608138293851 Test Loss: 0.2137802851558352\n",
      "Epoch: 10176 Training Loss: 0.21726463146947064 Test Loss: 0.21379532195762982\n",
      "Epoch: 10177 Training Loss: 0.21726656403890154 Test Loss: 0.2142562128953953\n",
      "Epoch: 10178 Training Loss: 0.2172654154929013 Test Loss: 0.2137861961744717\n",
      "Epoch: 10179 Training Loss: 0.21726399757837703 Test Loss: 0.21373398217651587\n",
      "Epoch: 10180 Training Loss: 0.21726425693210139 Test Loss: 0.21377125011199824\n",
      "Epoch: 10181 Training Loss: 0.21726470271172507 Test Loss: 0.21377043345810767\n",
      "Epoch: 10182 Training Loss: 0.21726435551108292 Test Loss: 0.21372892670005045\n",
      "Epoch: 10183 Training Loss: 0.2172637596349854 Test Loss: 0.21365413157388233\n",
      "Epoch: 10184 Training Loss: 0.21726782303460382 Test Loss: 0.2137836943617593\n",
      "Epoch: 10185 Training Loss: 0.21726641168304586 Test Loss: 0.21367115167798265\n",
      "Epoch: 10186 Training Loss: 0.21726522173297677 Test Loss: 0.21368664217638314\n",
      "Epoch: 10187 Training Loss: 0.21726385276275995 Test Loss: 0.21367666085105394\n",
      "Epoch: 10188 Training Loss: 0.2172655612678591 Test Loss: 0.21388733162930074\n",
      "Epoch: 10189 Training Loss: 0.21726421892607112 Test Loss: 0.21439832363511468\n",
      "Epoch: 10190 Training Loss: 0.21726399863634155 Test Loss: 0.21392773655274372\n",
      "Epoch: 10191 Training Loss: 0.21726189591394005 Test Loss: 0.21386434865552326\n",
      "Epoch: 10192 Training Loss: 0.21726252802083926 Test Loss: 0.2137904998108474\n",
      "Epoch: 10193 Training Loss: 0.2172659672482923 Test Loss: 0.21374868194654614\n",
      "Epoch: 10194 Training Loss: 0.21726491289699695 Test Loss: 0.21373863580741612\n",
      "Epoch: 10195 Training Loss: 0.21726389402337595 Test Loss: 0.2137556559115164\n",
      "Epoch: 10196 Training Loss: 0.21726426065290877 Test Loss: 0.2137649113222762\n",
      "Epoch: 10197 Training Loss: 0.21726526710889543 Test Loss: 0.215715780802007\n",
      "Epoch: 10198 Training Loss: 0.2172666925367609 Test Loss: 0.21371757132214347\n",
      "Epoch: 10199 Training Loss: 0.2172653069618807 Test Loss: 0.21377214454244983\n",
      "Epoch: 10200 Training Loss: 0.21726567029199875 Test Loss: 0.2137267748818626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10201 Training Loss: 0.21726492977063436 Test Loss: 0.2136808996736288\n",
      "Epoch: 10202 Training Loss: 0.21726610954451933 Test Loss: 0.2137455190330652\n",
      "Epoch: 10203 Training Loss: 0.21726565605430684 Test Loss: 0.2137262822969762\n",
      "Epoch: 10204 Training Loss: 0.21726463743172827 Test Loss: 0.21365642598243204\n",
      "Epoch: 10205 Training Loss: 0.21726530833364824 Test Loss: 0.21377153529272194\n",
      "Epoch: 10206 Training Loss: 0.21726519929157698 Test Loss: 0.2137826962292264\n",
      "Epoch: 10207 Training Loss: 0.2172639634904018 Test Loss: 0.2139117275439365\n",
      "Epoch: 10208 Training Loss: 0.21726572703655303 Test Loss: 0.21380014410441223\n",
      "Epoch: 10209 Training Loss: 0.21726361805602246 Test Loss: 0.21376106138250636\n",
      "Epoch: 10210 Training Loss: 0.21726438070498363 Test Loss: 0.21371521209979294\n",
      "Epoch: 10211 Training Loss: 0.21726449570931253 Test Loss: 0.2145041775346448\n",
      "Epoch: 10212 Training Loss: 0.21726374901051126 Test Loss: 0.21372296383037326\n",
      "Epoch: 10213 Training Loss: 0.2172635315360432 Test Loss: 0.21367918858928667\n",
      "Epoch: 10214 Training Loss: 0.2172650976104283 Test Loss: 0.21378361658519832\n",
      "Epoch: 10215 Training Loss: 0.2172640747918549 Test Loss: 0.2136912828445232\n",
      "Epoch: 10216 Training Loss: 0.21726512347676408 Test Loss: 0.21372286012829192\n",
      "Epoch: 10217 Training Loss: 0.2172646329039988 Test Loss: 0.21383392505740947\n",
      "Epoch: 10218 Training Loss: 0.21726475624652258 Test Loss: 0.21372884892348942\n",
      "Epoch: 10219 Training Loss: 0.21726496974017503 Test Loss: 0.21385421177707206\n",
      "Epoch: 10220 Training Loss: 0.21726402143637338 Test Loss: 0.2152494325422109\n",
      "Epoch: 10221 Training Loss: 0.21725835469153526 Test Loss: 0.21498906254148084\n",
      "Epoch: 10222 Training Loss: 0.21726294593475357 Test Loss: 0.21432422849799565\n",
      "Epoch: 10223 Training Loss: 0.2172628835417447 Test Loss: 0.21402681092870607\n",
      "Epoch: 10224 Training Loss: 0.21726595154020903 Test Loss: 0.21386652639923145\n",
      "Epoch: 10225 Training Loss: 0.21726226255243866 Test Loss: 0.2144562542103045\n",
      "Epoch: 10226 Training Loss: 0.21726536939971858 Test Loss: 0.2137536077954099\n",
      "Epoch: 10227 Training Loss: 0.21726403972660735 Test Loss: 0.21382135118004672\n",
      "Epoch: 10228 Training Loss: 0.21726680881423355 Test Loss: 0.2137969811909313\n",
      "Epoch: 10229 Training Loss: 0.21726465644819212 Test Loss: 0.21375438556101997\n",
      "Epoch: 10230 Training Loss: 0.21726441623645282 Test Loss: 0.2137774203858381\n",
      "Epoch: 10231 Training Loss: 0.21726558355684028 Test Loss: 0.2137055678062281\n",
      "Epoch: 10232 Training Loss: 0.21726484481967026 Test Loss: 0.21375041895640862\n",
      "Epoch: 10233 Training Loss: 0.21726505018134123 Test Loss: 0.21371808983255017\n",
      "Epoch: 10234 Training Loss: 0.2172619282177209 Test Loss: 0.2147461404159366\n",
      "Epoch: 10235 Training Loss: 0.21726992341693127 Test Loss: 0.2138623653532176\n",
      "Epoch: 10236 Training Loss: 0.21726585538199375 Test Loss: 0.2138347157857797\n",
      "Epoch: 10237 Training Loss: 0.21726560171258721 Test Loss: 0.21401574073152277\n",
      "Epoch: 10238 Training Loss: 0.21726306084045865 Test Loss: 0.21410518377668047\n",
      "Epoch: 10239 Training Loss: 0.21726436294373191 Test Loss: 0.21392625879808458\n",
      "Epoch: 10240 Training Loss: 0.21726499482648612 Test Loss: 0.21379503677690612\n",
      "Epoch: 10241 Training Loss: 0.21726482037889686 Test Loss: 0.21375465777898348\n",
      "Epoch: 10242 Training Loss: 0.21726267092674073 Test Loss: 0.21376069842522166\n",
      "Epoch: 10243 Training Loss: 0.21726618505449502 Test Loss: 0.21670530606217478\n",
      "Epoch: 10244 Training Loss: 0.21726428736202982 Test Loss: 0.21384761373214664\n",
      "Epoch: 10245 Training Loss: 0.2172634874422341 Test Loss: 0.21377013531462383\n",
      "Epoch: 10246 Training Loss: 0.21726516072070126 Test Loss: 0.21384621375404853\n",
      "Epoch: 10247 Training Loss: 0.217264118751177 Test Loss: 0.21377592966841882\n",
      "Epoch: 10248 Training Loss: 0.21726448309443058 Test Loss: 0.2139843060381159\n",
      "Epoch: 10249 Training Loss: 0.21726555245447676 Test Loss: 0.21376397800354413\n",
      "Epoch: 10250 Training Loss: 0.2172636794986564 Test Loss: 0.21380834953159844\n",
      "Epoch: 10251 Training Loss: 0.21726403246430856 Test Loss: 0.2137568225599315\n",
      "Epoch: 10252 Training Loss: 0.2172654315596166 Test Loss: 0.2151828817315095\n",
      "Epoch: 10253 Training Loss: 0.21726424521379953 Test Loss: 0.21380719584594352\n",
      "Epoch: 10254 Training Loss: 0.21726322573946988 Test Loss: 0.21410033570437773\n",
      "Epoch: 10255 Training Loss: 0.21726557696697657 Test Loss: 0.2138040977462634\n",
      "Epoch: 10256 Training Loss: 0.2172650901239845 Test Loss: 0.21371878982159923\n",
      "Epoch: 10257 Training Loss: 0.21726528423357527 Test Loss: 0.21363855033616067\n",
      "Epoch: 10258 Training Loss: 0.21726458681081606 Test Loss: 0.2137402431896769\n",
      "Epoch: 10259 Training Loss: 0.2172644872097332 Test Loss: 0.21378479619637358\n",
      "Epoch: 10260 Training Loss: 0.21726383877611047 Test Loss: 0.21712697168767256\n",
      "Epoch: 10261 Training Loss: 0.21726677379381498 Test Loss: 0.2138687171056998\n",
      "Epoch: 10262 Training Loss: 0.21726579079236363 Test Loss: 0.2138075976915087\n",
      "Epoch: 10263 Training Loss: 0.21726624640747097 Test Loss: 0.21370199008442178\n",
      "Epoch: 10264 Training Loss: 0.21726527909617133 Test Loss: 0.2137019123078608\n",
      "Epoch: 10265 Training Loss: 0.2172658604925003 Test Loss: 0.21374217464094192\n",
      "Epoch: 10266 Training Loss: 0.2172655332945601 Test Loss: 0.21371298250504406\n",
      "Epoch: 10267 Training Loss: 0.21726471295963556 Test Loss: 0.21365581673270415\n",
      "Epoch: 10268 Training Loss: 0.217266844964343 Test Loss: 0.2137192046299246\n",
      "Epoch: 10269 Training Loss: 0.21726389584343356 Test Loss: 0.21426742568294044\n",
      "Epoch: 10270 Training Loss: 0.21726186605782286 Test Loss: 0.2144146956012066\n",
      "Epoch: 10271 Training Loss: 0.2172663802758451 Test Loss: 0.21391854595578474\n",
      "Epoch: 10272 Training Loss: 0.2172636502073847 Test Loss: 0.213843854531698\n",
      "Epoch: 10273 Training Loss: 0.2172655022101282 Test Loss: 0.21377515190280874\n",
      "Epoch: 10274 Training Loss: 0.21726459790151184 Test Loss: 0.2136917235783689\n",
      "Epoch: 10275 Training Loss: 0.2172638618182189 Test Loss: 0.21376918903313158\n",
      "Epoch: 10276 Training Loss: 0.2172641448864868 Test Loss: 0.21379992373748938\n",
      "Epoch: 10277 Training Loss: 0.21726466882996323 Test Loss: 0.21371315102092625\n",
      "Epoch: 10278 Training Loss: 0.2172650116373629 Test Loss: 0.21356785144220558\n",
      "Epoch: 10279 Training Loss: 0.21726512133393763 Test Loss: 0.21372030646453888\n",
      "Epoch: 10280 Training Loss: 0.21726428767583286 Test Loss: 0.21372527120168314\n",
      "Epoch: 10281 Training Loss: 0.21726557906497398 Test Loss: 0.21379306643736062\n",
      "Epoch: 10282 Training Loss: 0.21726506050097813 Test Loss: 0.2137061252049153\n",
      "Epoch: 10283 Training Loss: 0.21726382575776745 Test Loss: 0.2146138165601439\n",
      "Epoch: 10284 Training Loss: 0.21726502426121067 Test Loss: 0.21366779432309918\n",
      "Epoch: 10285 Training Loss: 0.21726691114091987 Test Loss: 0.21372971742842067\n",
      "Epoch: 10286 Training Loss: 0.21726515229284835 Test Loss: 0.21373463031452428\n",
      "Epoch: 10287 Training Loss: 0.21726717510306592 Test Loss: 0.21370022714903897\n",
      "Epoch: 10288 Training Loss: 0.2172647769844202 Test Loss: 0.2136914383976452\n",
      "Epoch: 10289 Training Loss: 0.21726492524290486 Test Loss: 0.2136412336275154\n",
      "Epoch: 10290 Training Loss: 0.21726427720377736 Test Loss: 0.21419986377694594\n",
      "Epoch: 10291 Training Loss: 0.2172615036691139 Test Loss: 0.2144187918334196\n",
      "Epoch: 10292 Training Loss: 0.21726574144459518 Test Loss: 0.2140347182124084\n",
      "Epoch: 10293 Training Loss: 0.2172652863853675 Test Loss: 0.2138017255611527\n",
      "Epoch: 10294 Training Loss: 0.21726558424720696 Test Loss: 0.21379821265314725\n",
      "Epoch: 10295 Training Loss: 0.21726505996303008 Test Loss: 0.213869274504387\n",
      "Epoch: 10296 Training Loss: 0.2172631913018284 Test Loss: 0.21373422846895906\n",
      "Epoch: 10297 Training Loss: 0.2172648254983692 Test Loss: 0.21372811004615988\n",
      "Epoch: 10298 Training Loss: 0.2172641974081488 Test Loss: 0.2136906347065148\n",
      "Epoch: 10299 Training Loss: 0.21726400621244338 Test Loss: 0.21372534897824413\n",
      "Epoch: 10300 Training Loss: 0.21726459045989704 Test Loss: 0.21373275071429992\n",
      "Epoch: 10301 Training Loss: 0.21726477567541327 Test Loss: 0.21371990461897367\n",
      "Epoch: 10302 Training Loss: 0.2172625405729606 Test Loss: 0.21376973346905861\n",
      "Epoch: 10303 Training Loss: 0.21726523613205312 Test Loss: 0.21377202787760832\n",
      "Epoch: 10304 Training Loss: 0.21726497487757898 Test Loss: 0.213762655802007\n",
      "Epoch: 10305 Training Loss: 0.2172650256688414 Test Loss: 0.2137102343998885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10306 Training Loss: 0.21726475610306978 Test Loss: 0.21378165920841297\n",
      "Epoch: 10307 Training Loss: 0.21726041137454885 Test Loss: 0.2148735254601054\n",
      "Epoch: 10308 Training Loss: 0.21726664292898418 Test Loss: 0.21392120332161915\n",
      "Epoch: 10309 Training Loss: 0.2172636794448616 Test Loss: 0.21394602700734044\n",
      "Epoch: 10310 Training Loss: 0.21726375907910572 Test Loss: 0.21403736261548265\n",
      "Epoch: 10311 Training Loss: 0.2172646412511595 Test Loss: 0.213884492784824\n",
      "Epoch: 10312 Training Loss: 0.21726213422492954 Test Loss: 0.2137384024777331\n",
      "Epoch: 10313 Training Loss: 0.21726613401219017 Test Loss: 0.21386025242331025\n",
      "Epoch: 10314 Training Loss: 0.21726501427330838 Test Loss: 0.21378920353483064\n",
      "Epoch: 10315 Training Loss: 0.21726387737388358 Test Loss: 0.2137333599640278\n",
      "Epoch: 10316 Training Loss: 0.21726476652133048 Test Loss: 0.21761355481609057\n",
      "Epoch: 10317 Training Loss: 0.21726552973513713 Test Loss: 0.21465034561829668\n",
      "Epoch: 10318 Training Loss: 0.21726672935033964 Test Loss: 0.21385860615276892\n",
      "Epoch: 10319 Training Loss: 0.2172636623650108 Test Loss: 0.2137678927571148\n",
      "Epoch: 10320 Training Loss: 0.21726808135726106 Test Loss: 0.2138128346466165\n",
      "Epoch: 10321 Training Loss: 0.21726513382329837 Test Loss: 0.21380932173861103\n",
      "Epoch: 10322 Training Loss: 0.21726342461886675 Test Loss: 0.2144702928795662\n",
      "Epoch: 10323 Training Loss: 0.21726286367352976 Test Loss: 0.21463176998297626\n",
      "Epoch: 10324 Training Loss: 0.21726512925073987 Test Loss: 0.21407412500331846\n",
      "Epoch: 10325 Training Loss: 0.21726327975842066 Test Loss: 0.21394382333811193\n",
      "Epoch: 10326 Training Loss: 0.21726468829471712 Test Loss: 0.21398666526046645\n",
      "Epoch: 10327 Training Loss: 0.21726407070344966 Test Loss: 0.2137917960868642\n",
      "Epoch: 10328 Training Loss: 0.21726378152050554 Test Loss: 0.21381297723697834\n",
      "Epoch: 10329 Training Loss: 0.2172635868640009 Test Loss: 0.21381317167838085\n",
      "Epoch: 10330 Training Loss: 0.21726363515380487 Test Loss: 0.21378855539682223\n",
      "Epoch: 10331 Training Loss: 0.21726254291303465 Test Loss: 0.21371959351272965\n",
      "Epoch: 10332 Training Loss: 0.21726512794173294 Test Loss: 0.21370523077446374\n",
      "Epoch: 10333 Training Loss: 0.2172641850622409 Test Loss: 0.21613927417668843\n",
      "Epoch: 10334 Training Loss: 0.21726544239030418 Test Loss: 0.21372120089499044\n",
      "Epoch: 10335 Training Loss: 0.21726529240141992 Test Loss: 0.213638654038242\n",
      "Epoch: 10336 Training Loss: 0.21726474834765194 Test Loss: 0.21374533755442285\n",
      "Epoch: 10337 Training Loss: 0.21726383440976538 Test Loss: 0.21369605314026494\n",
      "Epoch: 10338 Training Loss: 0.21726453729269737 Test Loss: 0.21369585869886243\n",
      "Epoch: 10339 Training Loss: 0.2172661709243927 Test Loss: 0.2137053733648256\n",
      "Epoch: 10340 Training Loss: 0.21726508463691432 Test Loss: 0.21370131602089307\n",
      "Epoch: 10341 Training Loss: 0.21726515539501548 Test Loss: 0.21372273050069024\n",
      "Epoch: 10342 Training Loss: 0.21726730477544517 Test Loss: 0.2139161478451537\n",
      "Epoch: 10343 Training Loss: 0.21726369532329512 Test Loss: 0.21406773436255574\n",
      "Epoch: 10344 Training Loss: 0.2172665702253042 Test Loss: 0.21379752562685833\n",
      "Epoch: 10345 Training Loss: 0.21726416013731423 Test Loss: 0.2138096069193347\n",
      "Epoch: 10346 Training Loss: 0.217266453974729 Test Loss: 0.21376730943290725\n",
      "Epoch: 10347 Training Loss: 0.21726409001578492 Test Loss: 0.21365212234605632\n",
      "Epoch: 10348 Training Loss: 0.21726568643044047 Test Loss: 0.21374135798705135\n",
      "Epoch: 10349 Training Loss: 0.21726565527428215 Test Loss: 0.21374799492025726\n",
      "Epoch: 10350 Training Loss: 0.2172651882367444 Test Loss: 0.21764912462999098\n",
      "Epoch: 10351 Training Loss: 0.2172661989694181 Test Loss: 0.2138058736444064\n",
      "Epoch: 10352 Training Loss: 0.21726515985101855 Test Loss: 0.21375423000789795\n",
      "Epoch: 10353 Training Loss: 0.2172658765054208 Test Loss: 0.2137124510318772\n",
      "Epoch: 10354 Training Loss: 0.21726399741699262 Test Loss: 0.21365367787727646\n",
      "Epoch: 10355 Training Loss: 0.217264217805346 Test Loss: 0.21424886301038015\n",
      "Epoch: 10356 Training Loss: 0.2172677828498839 Test Loss: 0.21385155441123765\n",
      "Epoch: 10357 Training Loss: 0.2172635464730676 Test Loss: 0.21456092849865935\n",
      "Epoch: 10358 Training Loss: 0.21726115104416216 Test Loss: 0.21471996860315784\n",
      "Epoch: 10359 Training Loss: 0.2172622824206536 Test Loss: 0.21423667801582244\n",
      "Epoch: 10360 Training Loss: 0.2172646142103038 Test Loss: 0.2139326883271278\n",
      "Epoch: 10361 Training Loss: 0.21726406851579422 Test Loss: 0.2137950108513858\n",
      "Epoch: 10362 Training Loss: 0.2172640419232286 Test Loss: 0.21377953331574545\n",
      "Epoch: 10363 Training Loss: 0.21726506131686601 Test Loss: 0.21376868348548503\n",
      "Epoch: 10364 Training Loss: 0.21726325788186632 Test Loss: 0.21375495592246735\n",
      "Epoch: 10365 Training Loss: 0.21726477250151974 Test Loss: 0.2139998483875571\n",
      "Epoch: 10366 Training Loss: 0.2172649669069819 Test Loss: 0.213855676568971\n",
      "Epoch: 10367 Training Loss: 0.21726335881885353 Test Loss: 0.21374502644817883\n",
      "Epoch: 10368 Training Loss: 0.21726385834845394 Test Loss: 0.2137186861195179\n",
      "Epoch: 10369 Training Loss: 0.21726508964879704 Test Loss: 0.2137218231074785\n",
      "Epoch: 10370 Training Loss: 0.2172643339124684 Test Loss: 0.2137349414207683\n",
      "Epoch: 10371 Training Loss: 0.2172630996444452 Test Loss: 0.2138590987376553\n",
      "Epoch: 10372 Training Loss: 0.2172645607382669 Test Loss: 0.21369125691900287\n",
      "Epoch: 10373 Training Loss: 0.2172645217728959 Test Loss: 0.21371096031445788\n",
      "Epoch: 10374 Training Loss: 0.21726505402766985 Test Loss: 0.21378359065967797\n",
      "Epoch: 10375 Training Loss: 0.21726288718185988 Test Loss: 0.21370921034183524\n",
      "Epoch: 10376 Training Loss: 0.21726068458043568 Test Loss: 0.21378762207809016\n",
      "Epoch: 10377 Training Loss: 0.21726641786944853 Test Loss: 0.2138148438744425\n",
      "Epoch: 10378 Training Loss: 0.21726432060721979 Test Loss: 0.21377125011199824\n",
      "Epoch: 10379 Training Loss: 0.21726358603018142 Test Loss: 0.21374694493668367\n",
      "Epoch: 10380 Training Loss: 0.21726537194600604 Test Loss: 0.21380694955350033\n",
      "Epoch: 10381 Training Loss: 0.21726411115714359 Test Loss: 0.21380833656883827\n",
      "Epoch: 10382 Training Loss: 0.21726418951824397 Test Loss: 0.2138585802272486\n",
      "Epoch: 10383 Training Loss: 0.21726436395686743 Test Loss: 0.21376312246137305\n",
      "Epoch: 10384 Training Loss: 0.217264105777663 Test Loss: 0.21376320023793405\n",
      "Epoch: 10385 Training Loss: 0.2172652995830265 Test Loss: 0.2137750482007274\n",
      "Epoch: 10386 Training Loss: 0.2172643946647357 Test Loss: 0.2137477745533344\n",
      "Epoch: 10387 Training Loss: 0.21726567098236543 Test Loss: 0.21366170182582034\n",
      "Epoch: 10388 Training Loss: 0.21726557368549343 Test Loss: 0.21371748058282228\n",
      "Epoch: 10389 Training Loss: 0.21726567378866113 Test Loss: 0.2137249600954391\n",
      "Epoch: 10390 Training Loss: 0.2172650304655449 Test Loss: 0.2136759997502854\n",
      "Epoch: 10391 Training Loss: 0.2172660790607961 Test Loss: 0.21375877993671685\n",
      "Epoch: 10392 Training Loss: 0.21726425176780004 Test Loss: 0.21371770094974515\n",
      "Epoch: 10393 Training Loss: 0.21726663998820148 Test Loss: 0.21642318454988319\n",
      "Epoch: 10394 Training Loss: 0.21726717287954728 Test Loss: 0.21378263141542556\n",
      "Epoch: 10395 Training Loss: 0.21726545912048878 Test Loss: 0.21373829877565176\n",
      "Epoch: 10396 Training Loss: 0.21726571617896806 Test Loss: 0.21374217464094192\n",
      "Epoch: 10397 Training Loss: 0.21726520000884106 Test Loss: 0.21372540082928482\n",
      "Epoch: 10398 Training Loss: 0.2172649620654494 Test Loss: 0.21371350101545078\n",
      "Epoch: 10399 Training Loss: 0.2172653164028691 Test Loss: 0.21380680696313847\n",
      "Epoch: 10400 Training Loss: 0.21725980802993913 Test Loss: 0.21558833094403737\n",
      "Epoch: 10401 Training Loss: 0.21726554252036928 Test Loss: 0.21405754563306387\n",
      "Epoch: 10402 Training Loss: 0.21726354808691176 Test Loss: 0.21396508226478708\n",
      "Epoch: 10403 Training Loss: 0.21726329721483512 Test Loss: 0.21382999734107863\n",
      "Epoch: 10404 Training Loss: 0.2172636578641787 Test Loss: 0.21376327801449507\n",
      "Epoch: 10405 Training Loss: 0.2172641886844245 Test Loss: 0.21377185936172613\n",
      "Epoch: 10406 Training Loss: 0.21726436132988775 Test Loss: 0.21595830108198605\n",
      "Epoch: 10407 Training Loss: 0.21726552459773318 Test Loss: 0.21378524989297945\n",
      "Epoch: 10408 Training Loss: 0.21726452685650505 Test Loss: 0.21370004567039663\n",
      "Epoch: 10409 Training Loss: 0.21726451781897768 Test Loss: 0.21370034381388048\n",
      "Epoch: 10410 Training Loss: 0.21726588149040615 Test Loss: 0.21376532613060156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10411 Training Loss: 0.21726334950338633 Test Loss: 0.21373942653578634\n",
      "Epoch: 10412 Training Loss: 0.21726569655282976 Test Loss: 0.2138069625162605\n",
      "Epoch: 10413 Training Loss: 0.21726290184991023 Test Loss: 0.2137571855172162\n",
      "Epoch: 10414 Training Loss: 0.2172637458097203 Test Loss: 0.21426975897977063\n",
      "Epoch: 10415 Training Loss: 0.2172635306842921 Test Loss: 0.21528917636488532\n",
      "Epoch: 10416 Training Loss: 0.21726430502465774 Test Loss: 0.21407285465282203\n",
      "Epoch: 10417 Training Loss: 0.21726493518597814 Test Loss: 0.21387626143211744\n",
      "Epoch: 10418 Training Loss: 0.2172629042796423 Test Loss: 0.21375842994219232\n",
      "Epoch: 10419 Training Loss: 0.21726550762547195 Test Loss: 0.21381485683720267\n",
      "Epoch: 10420 Training Loss: 0.21726506921573666 Test Loss: 0.21369079025963683\n",
      "Epoch: 10421 Training Loss: 0.21726407144761115 Test Loss: 0.21365514266917543\n",
      "Epoch: 10422 Training Loss: 0.2172655930695551 Test Loss: 0.2138098921000584\n",
      "Epoch: 10423 Training Loss: 0.2172642737878072 Test Loss: 0.21379205534206755\n",
      "Epoch: 10424 Training Loss: 0.2172631819415322 Test Loss: 0.2137614113770309\n",
      "Epoch: 10425 Training Loss: 0.2172628220094527 Test Loss: 0.21368088671086863\n",
      "Epoch: 10426 Training Loss: 0.21726447270306726 Test Loss: 0.21367343312377216\n",
      "Epoch: 10427 Training Loss: 0.21726496275581608 Test Loss: 0.21376575390168712\n",
      "Epoch: 10428 Training Loss: 0.21726429096628183 Test Loss: 0.21373508401113014\n",
      "Epoch: 10429 Training Loss: 0.21726475673964163 Test Loss: 0.21370676038016354\n",
      "Epoch: 10430 Training Loss: 0.21726521620107758 Test Loss: 0.21370084936152703\n",
      "Epoch: 10431 Training Loss: 0.21726443158590406 Test Loss: 0.2137834610320763\n",
      "Epoch: 10432 Training Loss: 0.2172607231154482 Test Loss: 0.2146024222939564\n",
      "Epoch: 10433 Training Loss: 0.2172656098535345 Test Loss: 0.21386608566538573\n",
      "Epoch: 10434 Training Loss: 0.2172651274037849 Test Loss: 0.21388225022731497\n",
      "Epoch: 10435 Training Loss: 0.21726422137373477 Test Loss: 0.21383903238491558\n",
      "Epoch: 10436 Training Loss: 0.2172637111568996 Test Loss: 0.21371217881391366\n",
      "Epoch: 10437 Training Loss: 0.21726587819995719 Test Loss: 0.2137802851558352\n",
      "Epoch: 10438 Training Loss: 0.2172644608413126 Test Loss: 0.21371295657952374\n",
      "Epoch: 10439 Training Loss: 0.21726490345600855 Test Loss: 0.21508899245961427\n",
      "Epoch: 10440 Training Loss: 0.2172652631729088 Test Loss: 0.21377502227520706\n",
      "Epoch: 10441 Training Loss: 0.21726372447111403 Test Loss: 0.21370442708333334\n",
      "Epoch: 10442 Training Loss: 0.21726575156698444 Test Loss: 0.21371291769124323\n",
      "Epoch: 10443 Training Loss: 0.21726547471201663 Test Loss: 0.21376257802544602\n",
      "Epoch: 10444 Training Loss: 0.2172633781760178 Test Loss: 0.21366157219821866\n",
      "Epoch: 10445 Training Loss: 0.21726454459085934 Test Loss: 0.2136877440109974\n",
      "Epoch: 10446 Training Loss: 0.21726507754496577 Test Loss: 0.21376698536390304\n",
      "Epoch: 10447 Training Loss: 0.21726381557261756 Test Loss: 0.21410583191468885\n",
      "Epoch: 10448 Training Loss: 0.21726605787460843 Test Loss: 0.2137268526584236\n",
      "Epoch: 10449 Training Loss: 0.2172646823952201 Test Loss: 0.21377104270783556\n",
      "Epoch: 10450 Training Loss: 0.21726513116045548 Test Loss: 0.21408338041407826\n",
      "Epoch: 10451 Training Loss: 0.21726189678362273 Test Loss: 0.21444823026176066\n",
      "Epoch: 10452 Training Loss: 0.217261573934096 Test Loss: 0.21402709610942974\n",
      "Epoch: 10453 Training Loss: 0.2172648947233184 Test Loss: 0.21380328109237284\n",
      "Epoch: 10454 Training Loss: 0.21726150129317665 Test Loss: 0.21456792838914995\n",
      "Epoch: 10455 Training Loss: 0.21726418178075774 Test Loss: 0.21379944411536317\n",
      "Epoch: 10456 Training Loss: 0.21726469977094234 Test Loss: 0.2137202027624575\n",
      "Epoch: 10457 Training Loss: 0.21726483795186674 Test Loss: 0.21381453276819848\n",
      "Epoch: 10458 Training Loss: 0.21726387997396587 Test Loss: 0.21377634447674418\n",
      "Epoch: 10459 Training Loss: 0.2172649789032236 Test Loss: 0.2136807441205068\n",
      "Epoch: 10460 Training Loss: 0.21726664530492146 Test Loss: 0.2137965015688051\n",
      "Epoch: 10461 Training Loss: 0.21726506978954793 Test Loss: 0.21439066264385553\n",
      "Epoch: 10462 Training Loss: 0.2172656530866267 Test Loss: 0.2137104418040512\n",
      "Epoch: 10463 Training Loss: 0.21726427255052666 Test Loss: 0.2137153417273946\n",
      "Epoch: 10464 Training Loss: 0.21726692493928756 Test Loss: 0.2137419413112589\n",
      "Epoch: 10465 Training Loss: 0.21726423861497002 Test Loss: 0.21371565283363864\n",
      "Epoch: 10466 Training Loss: 0.21726513330328193 Test Loss: 0.21378388880316185\n",
      "Epoch: 10467 Training Loss: 0.21726351919910109 Test Loss: 0.21379224978347006\n",
      "Epoch: 10468 Training Loss: 0.2172629397304193 Test Loss: 0.21634411171285972\n",
      "Epoch: 10469 Training Loss: 0.2172655099386486 Test Loss: 0.21380475884703196\n",
      "Epoch: 10470 Training Loss: 0.21726475687412866 Test Loss: 0.21367964228589253\n",
      "Epoch: 10471 Training Loss: 0.21726558852389402 Test Loss: 0.21366298513907694\n",
      "Epoch: 10472 Training Loss: 0.21726273117692319 Test Loss: 0.21369460131112616\n",
      "Epoch: 10473 Training Loss: 0.21726588251250745 Test Loss: 0.21372973039118084\n",
      "Epoch: 10474 Training Loss: 0.2172643000396724 Test Loss: 0.21374457275157296\n",
      "Epoch: 10475 Training Loss: 0.21726428946899307 Test Loss: 0.21374598569243125\n",
      "Epoch: 10476 Training Loss: 0.21726406699160808 Test Loss: 0.2137379228556069\n",
      "Epoch: 10477 Training Loss: 0.21726407134002154 Test Loss: 0.21499069584926198\n",
      "Epoch: 10478 Training Loss: 0.21726544762633193 Test Loss: 0.21368773104823724\n",
      "Epoch: 10479 Training Loss: 0.2172663813338096 Test Loss: 0.2137272156157083\n",
      "Epoch: 10480 Training Loss: 0.2172659108444385 Test Loss: 0.21369103655208002\n",
      "Epoch: 10481 Training Loss: 0.21726566644567014 Test Loss: 0.2136885865904083\n",
      "Epoch: 10482 Training Loss: 0.21726435213994177 Test Loss: 0.21365232975021903\n",
      "Epoch: 10483 Training Loss: 0.2172638206472609 Test Loss: 0.21367940895620952\n",
      "Epoch: 10484 Training Loss: 0.21726329273193465 Test Loss: 0.21367396459693905\n",
      "Epoch: 10485 Training Loss: 0.21726561234602715 Test Loss: 0.21383220101030714\n",
      "Epoch: 10486 Training Loss: 0.21726427238017643 Test Loss: 0.21378082959176223\n",
      "Epoch: 10487 Training Loss: 0.21726285708366605 Test Loss: 0.2136948605663295\n",
      "Epoch: 10488 Training Loss: 0.2172645693723332 Test Loss: 0.21373167480520602\n",
      "Epoch: 10489 Training Loss: 0.21726461067777822 Test Loss: 0.21370307895627588\n",
      "Epoch: 10490 Training Loss: 0.217264159877306 Test Loss: 0.2137350191973293\n",
      "Epoch: 10491 Training Loss: 0.2172657313401375 Test Loss: 0.21375130042410004\n",
      "Epoch: 10492 Training Loss: 0.21726459289859493 Test Loss: 0.21377265009009636\n",
      "Epoch: 10493 Training Loss: 0.21726413401097022 Test Loss: 0.21381271798177498\n",
      "Epoch: 10494 Training Loss: 0.21726347856609113 Test Loss: 0.21380133667834766\n",
      "Epoch: 10495 Training Loss: 0.2172658429105646 Test Loss: 0.2136848922037605\n",
      "Epoch: 10496 Training Loss: 0.2172657671585123 Test Loss: 0.21371565283363864\n",
      "Epoch: 10497 Training Loss: 0.21726411741527266 Test Loss: 0.21365851298681904\n",
      "Epoch: 10498 Training Loss: 0.21726382081761114 Test Loss: 0.21372756561023282\n",
      "Epoch: 10499 Training Loss: 0.21726430994688245 Test Loss: 0.21379978114712753\n",
      "Epoch: 10500 Training Loss: 0.21726529073378095 Test Loss: 0.2137629020944502\n",
      "Epoch: 10501 Training Loss: 0.2172636018906833 Test Loss: 0.21378493878673543\n",
      "Epoch: 10502 Training Loss: 0.2172647265069608 Test Loss: 0.2142644183225815\n",
      "Epoch: 10503 Training Loss: 0.21726618567313527 Test Loss: 0.2137440283156459\n",
      "Epoch: 10504 Training Loss: 0.21726454632125894 Test Loss: 0.21371096031445788\n",
      "Epoch: 10505 Training Loss: 0.21726323793295918 Test Loss: 0.21370293636591403\n",
      "Epoch: 10506 Training Loss: 0.21726452469574703 Test Loss: 0.21375914289400155\n",
      "Epoch: 10507 Training Loss: 0.21726543591699587 Test Loss: 0.21375868919739566\n",
      "Epoch: 10508 Training Loss: 0.2172645196928301 Test Loss: 0.2137628632061697\n",
      "Epoch: 10509 Training Loss: 0.21726429003383851 Test Loss: 0.2139058165253\n",
      "Epoch: 10510 Training Loss: 0.21726444516909252 Test Loss: 0.21383417134985266\n",
      "Epoch: 10511 Training Loss: 0.21726455195178193 Test Loss: 0.2136804200515026\n",
      "Epoch: 10512 Training Loss: 0.21726447441553523 Test Loss: 0.21373049519403073\n",
      "Epoch: 10513 Training Loss: 0.21726483797876414 Test Loss: 0.21369976048967293\n",
      "Epoch: 10514 Training Loss: 0.21726466154973287 Test Loss: 0.21368023857286025\n",
      "Epoch: 10515 Training Loss: 0.21726522439581966 Test Loss: 0.21381839567072847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10516 Training Loss: 0.21726332543917654 Test Loss: 0.2138193938032614\n",
      "Epoch: 10517 Training Loss: 0.21726505586565903 Test Loss: 0.21376832052820033\n",
      "Epoch: 10518 Training Loss: 0.21726271706475248 Test Loss: 0.21371728614141977\n",
      "Epoch: 10519 Training Loss: 0.217265536226377 Test Loss: 0.21371898426300176\n",
      "Epoch: 10520 Training Loss: 0.2172651144302709 Test Loss: 0.2136699591040472\n",
      "Epoch: 10521 Training Loss: 0.21726446320828405 Test Loss: 0.2138249418646132\n",
      "Epoch: 10522 Training Loss: 0.21726379560577883 Test Loss: 0.21402210544676517\n",
      "Epoch: 10523 Training Loss: 0.21726552137901062 Test Loss: 0.21400900009623552\n",
      "Epoch: 10524 Training Loss: 0.21726410894259074 Test Loss: 0.21380512180431666\n",
      "Epoch: 10525 Training Loss: 0.21726514748717904 Test Loss: 0.21378979982179835\n",
      "Epoch: 10526 Training Loss: 0.21726412855079744 Test Loss: 0.21373628954782575\n",
      "Epoch: 10527 Training Loss: 0.2172641952563566 Test Loss: 0.2136944198324838\n",
      "Epoch: 10528 Training Loss: 0.2172644833365072 Test Loss: 0.2137160287536835\n",
      "Epoch: 10529 Training Loss: 0.21726442760508843 Test Loss: 0.21363313190241054\n",
      "Epoch: 10530 Training Loss: 0.21726419869025834 Test Loss: 0.2137002141862788\n",
      "Epoch: 10531 Training Loss: 0.2172658240285878 Test Loss: 0.21372585452589068\n",
      "Epoch: 10532 Training Loss: 0.21726439056736468 Test Loss: 0.21380387737934056\n",
      "Epoch: 10533 Training Loss: 0.21726821054548706 Test Loss: 0.21369097173827917\n",
      "Epoch: 10534 Training Loss: 0.21726550409294637 Test Loss: 0.2137186083429569\n",
      "Epoch: 10535 Training Loss: 0.21726461854078566 Test Loss: 0.2138274436773256\n",
      "Epoch: 10536 Training Loss: 0.2172641206519268 Test Loss: 0.21704329707078954\n",
      "Epoch: 10537 Training Loss: 0.21726629455382213 Test Loss: 0.2139101590499562\n",
      "Epoch: 10538 Training Loss: 0.2172639765356422 Test Loss: 0.21377644817882552\n",
      "Epoch: 10539 Training Loss: 0.21726655477722917 Test Loss: 0.21381809752724462\n",
      "Epoch: 10540 Training Loss: 0.21726432551151292 Test Loss: 0.21374251167270628\n",
      "Epoch: 10541 Training Loss: 0.2172640204053063 Test Loss: 0.21379144609233966\n",
      "Epoch: 10542 Training Loss: 0.21726500746826546 Test Loss: 0.2138130679762995\n",
      "Epoch: 10543 Training Loss: 0.21726405556917763 Test Loss: 0.21376300579653154\n",
      "Epoch: 10544 Training Loss: 0.2172654975568775 Test Loss: 0.21375836512839147\n",
      "Epoch: 10545 Training Loss: 0.21726500747723126 Test Loss: 0.21368551441624853\n",
      "Epoch: 10546 Training Loss: 0.21726350605523687 Test Loss: 0.21371745465730196\n",
      "Epoch: 10547 Training Loss: 0.2172648115475829 Test Loss: 0.21388559461943826\n",
      "Epoch: 10548 Training Loss: 0.2172644880256211 Test Loss: 0.21400200020574492\n",
      "Epoch: 10549 Training Loss: 0.2172607026465246 Test Loss: 0.21480579503822875\n",
      "Epoch: 10550 Training Loss: 0.21726580027818107 Test Loss: 0.21389753332155279\n",
      "Epoch: 10551 Training Loss: 0.21726572028530491 Test Loss: 0.21377704446579324\n",
      "Epoch: 10552 Training Loss: 0.21726573244293101 Test Loss: 0.21398504491544548\n",
      "Epoch: 10553 Training Loss: 0.21726453446847008 Test Loss: 0.21378701282836227\n",
      "Epoch: 10554 Training Loss: 0.2172646319715555 Test Loss: 0.21380556253816235\n",
      "Epoch: 10555 Training Loss: 0.21726463509165422 Test Loss: 0.2137460375434719\n",
      "Epoch: 10556 Training Loss: 0.2172648890569322 Test Loss: 0.2137816073573723\n",
      "Epoch: 10557 Training Loss: 0.21726378282951248 Test Loss: 0.2137975385896185\n",
      "Epoch: 10558 Training Loss: 0.21726586462573455 Test Loss: 0.21378959241763565\n",
      "Epoch: 10559 Training Loss: 0.2172615543527867 Test Loss: 0.21375036710536796\n",
      "Epoch: 10560 Training Loss: 0.21726247113283217 Test Loss: 0.2138450989566741\n",
      "Epoch: 10561 Training Loss: 0.21726149002316483 Test Loss: 0.21385377104322634\n",
      "Epoch: 10562 Training Loss: 0.21726291801524936 Test Loss: 0.214107309669348\n",
      "Epoch: 10563 Training Loss: 0.21726459611731747 Test Loss: 0.21378417398388552\n",
      "Epoch: 10564 Training Loss: 0.21726530450525122 Test Loss: 0.2137631743124137\n",
      "Epoch: 10565 Training Loss: 0.21726431377527947 Test Loss: 0.21372905632765213\n",
      "Epoch: 10566 Training Loss: 0.21726436425273887 Test Loss: 0.21368984397814458\n",
      "Epoch: 10567 Training Loss: 0.21726628996333203 Test Loss: 0.21368835326072527\n",
      "Epoch: 10568 Training Loss: 0.21726470099925707 Test Loss: 0.21372276938897075\n",
      "Epoch: 10569 Training Loss: 0.21726422879741797 Test Loss: 0.213928916163919\n",
      "Epoch: 10570 Training Loss: 0.21726544964363714 Test Loss: 0.21431626936325263\n",
      "Epoch: 10571 Training Loss: 0.21726231311059027 Test Loss: 0.214066904745905\n",
      "Epoch: 10572 Training Loss: 0.2172656119336003 Test Loss: 0.21390405358991718\n",
      "Epoch: 10573 Training Loss: 0.2172628845907434 Test Loss: 0.2138324602655105\n",
      "Epoch: 10574 Training Loss: 0.21726378982283723 Test Loss: 0.21375080783921366\n",
      "Epoch: 10575 Training Loss: 0.21726572751174048 Test Loss: 0.21388387057233593\n",
      "Epoch: 10576 Training Loss: 0.2172646768633209 Test Loss: 0.21372232865512503\n",
      "Epoch: 10577 Training Loss: 0.21726311640152718 Test Loss: 0.21372647673837872\n",
      "Epoch: 10578 Training Loss: 0.21726413871801573 Test Loss: 0.2137401265248354\n",
      "Epoch: 10579 Training Loss: 0.21726591318451255 Test Loss: 0.21375327076364553\n",
      "Epoch: 10580 Training Loss: 0.21726622650339286 Test Loss: 0.21377141862788043\n",
      "Epoch: 10581 Training Loss: 0.21726538482089622 Test Loss: 0.213809088408928\n",
      "Epoch: 10582 Training Loss: 0.21726533918496935 Test Loss: 0.2141508386179914\n",
      "Epoch: 10583 Training Loss: 0.21726591009131122 Test Loss: 0.2137155491315573\n",
      "Epoch: 10584 Training Loss: 0.21726507756289737 Test Loss: 0.2136690387480753\n",
      "Epoch: 10585 Training Loss: 0.21726436971291163 Test Loss: 0.21370042159044148\n",
      "Epoch: 10586 Training Loss: 0.21726458086649003 Test Loss: 0.21376682981078104\n",
      "Epoch: 10587 Training Loss: 0.21726530465766986 Test Loss: 0.21371042884129102\n",
      "Epoch: 10588 Training Loss: 0.21726531685115916 Test Loss: 0.21691304725662366\n",
      "Epoch: 10589 Training Loss: 0.21726586545955404 Test Loss: 0.21386169128968885\n",
      "Epoch: 10590 Training Loss: 0.21726447528521795 Test Loss: 0.21377261120181587\n",
      "Epoch: 10591 Training Loss: 0.21726385168686382 Test Loss: 0.2137324007197754\n",
      "Epoch: 10592 Training Loss: 0.2172648077819465 Test Loss: 0.21370310488179622\n",
      "Epoch: 10593 Training Loss: 0.21726505049514427 Test Loss: 0.21377040753258733\n",
      "Epoch: 10594 Training Loss: 0.2172650125429088 Test Loss: 0.2137569003364925\n",
      "Epoch: 10595 Training Loss: 0.21726290859219255 Test Loss: 0.21375443741206063\n",
      "Epoch: 10596 Training Loss: 0.21726651995405824 Test Loss: 0.21367865711611977\n",
      "Epoch: 10597 Training Loss: 0.21726550359982733 Test Loss: 0.21374650420283794\n",
      "Epoch: 10598 Training Loss: 0.21726120878392036 Test Loss: 0.21464075317577255\n",
      "Epoch: 10599 Training Loss: 0.21726666216959306 Test Loss: 0.21374468941641447\n",
      "Epoch: 10600 Training Loss: 0.21726405676162916 Test Loss: 0.2137191268533636\n",
      "Epoch: 10601 Training Loss: 0.21726320320841208 Test Loss: 0.213764509476711\n",
      "Epoch: 10602 Training Loss: 0.2172647544981914 Test Loss: 0.21384097679894074\n",
      "Epoch: 10603 Training Loss: 0.2172637408516324 Test Loss: 0.21371182881938913\n",
      "Epoch: 10604 Training Loss: 0.21726567656805942 Test Loss: 0.21371539357843528\n",
      "Epoch: 10605 Training Loss: 0.2172652904199779 Test Loss: 0.2136815089233567\n",
      "Epoch: 10606 Training Loss: 0.2172658747122606 Test Loss: 0.21374338017763753\n",
      "Epoch: 10607 Training Loss: 0.21726550750891654 Test Loss: 0.21375044488192896\n",
      "Epoch: 10608 Training Loss: 0.21726554604392906 Test Loss: 0.21370201600994213\n",
      "Epoch: 10609 Training Loss: 0.21726246895414253 Test Loss: 0.21369754385768425\n",
      "Epoch: 10610 Training Loss: 0.21726501377122354 Test Loss: 0.21365918705034778\n",
      "Epoch: 10611 Training Loss: 0.21726549369261727 Test Loss: 0.21369480871528884\n",
      "Epoch: 10612 Training Loss: 0.2172653640919644 Test Loss: 0.21374251167270628\n",
      "Epoch: 10613 Training Loss: 0.21726612709059182 Test Loss: 0.21494303178012503\n",
      "Epoch: 10614 Training Loss: 0.21726633578754073 Test Loss: 0.2137234693780198\n",
      "Epoch: 10615 Training Loss: 0.21726520677802078 Test Loss: 0.21371382508445497\n",
      "Epoch: 10616 Training Loss: 0.21726552792404533 Test Loss: 0.2137362636223054\n",
      "Epoch: 10617 Training Loss: 0.21726540112072237 Test Loss: 0.21376343356761707\n",
      "Epoch: 10618 Training Loss: 0.2172653416953936 Test Loss: 0.21371700096069607\n",
      "Epoch: 10619 Training Loss: 0.21726466078763979 Test Loss: 0.21365413157388233\n",
      "Epoch: 10620 Training Loss: 0.2172632128645797 Test Loss: 0.21366323143152013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10621 Training Loss: 0.2172666430096764 Test Loss: 0.2137617743343156\n",
      "Epoch: 10622 Training Loss: 0.21726525948796463 Test Loss: 0.21378790725881386\n",
      "Epoch: 10623 Training Loss: 0.2172649310706755 Test Loss: 0.21368439961887412\n",
      "Epoch: 10624 Training Loss: 0.2172612234071417 Test Loss: 0.21470930025153978\n",
      "Epoch: 10625 Training Loss: 0.21726748035272536 Test Loss: 0.21377808148660668\n",
      "Epoch: 10626 Training Loss: 0.21726505021720444 Test Loss: 0.2137476578884929\n",
      "Epoch: 10627 Training Loss: 0.21726420808641775 Test Loss: 0.21376557242304475\n",
      "Epoch: 10628 Training Loss: 0.21726561330536787 Test Loss: 0.21374478015573564\n",
      "Epoch: 10629 Training Loss: 0.21726446777187675 Test Loss: 0.21384467118558856\n",
      "Epoch: 10630 Training Loss: 0.2172659913662969 Test Loss: 0.2137136954568533\n",
      "Epoch: 10631 Training Loss: 0.21726542923747416 Test Loss: 0.21373587473950037\n",
      "Epoch: 10632 Training Loss: 0.21726396895057457 Test Loss: 0.21372428603191038\n",
      "Epoch: 10633 Training Loss: 0.21726364350993138 Test Loss: 0.21372376752150365\n",
      "Epoch: 10634 Training Loss: 0.21726419595568905 Test Loss: 0.21461646096321813\n",
      "Epoch: 10635 Training Loss: 0.21726588834924387 Test Loss: 0.21382838995881784\n",
      "Epoch: 10636 Training Loss: 0.2172651688347511 Test Loss: 0.21383877312971222\n",
      "Epoch: 10637 Training Loss: 0.217264373209574 Test Loss: 0.21377253342525485\n",
      "Epoch: 10638 Training Loss: 0.21726438872937548 Test Loss: 0.21375338742848704\n",
      "Epoch: 10639 Training Loss: 0.21726369810269341 Test Loss: 0.21386337644851067\n",
      "Epoch: 10640 Training Loss: 0.21726391139013243 Test Loss: 0.21378606654687002\n",
      "Epoch: 10641 Training Loss: 0.2172651741604369 Test Loss: 0.21380304776268982\n",
      "Epoch: 10642 Training Loss: 0.21726329643481043 Test Loss: 0.21375648552816714\n",
      "Epoch: 10643 Training Loss: 0.21726479702298535 Test Loss: 0.21363033194621428\n",
      "Epoch: 10644 Training Loss: 0.21726596004875415 Test Loss: 0.21374755418641153\n",
      "Epoch: 10645 Training Loss: 0.21726488854588155 Test Loss: 0.21367970709969336\n",
      "Epoch: 10646 Training Loss: 0.21726571343543297 Test Loss: 0.2137861183979107\n",
      "Epoch: 10647 Training Loss: 0.2172638084986006 Test Loss: 0.2137392839454245\n",
      "Epoch: 10648 Training Loss: 0.21726439661031452 Test Loss: 0.2137803629323962\n",
      "Epoch: 10649 Training Loss: 0.21726381943687778 Test Loss: 0.21375898734087953\n",
      "Epoch: 10650 Training Loss: 0.21726503087797178 Test Loss: 0.21378407028180418\n",
      "Epoch: 10651 Training Loss: 0.21726529797814814 Test Loss: 0.2137083807251845\n",
      "Epoch: 10652 Training Loss: 0.21726391602545153 Test Loss: 0.21378215179329935\n",
      "Epoch: 10653 Training Loss: 0.21726740947806877 Test Loss: 0.2137175972476638\n",
      "Epoch: 10654 Training Loss: 0.21726543810465132 Test Loss: 0.21365444268012637\n",
      "Epoch: 10655 Training Loss: 0.21726485669935652 Test Loss: 0.213735304378053\n",
      "Epoch: 10656 Training Loss: 0.21726457740569086 Test Loss: 0.2137251934251221\n",
      "Epoch: 10657 Training Loss: 0.21726543009819108 Test Loss: 0.21368554034176887\n",
      "Epoch: 10658 Training Loss: 0.2172649909353285 Test Loss: 0.21375021155224594\n",
      "Epoch: 10659 Training Loss: 0.21726592671390618 Test Loss: 0.21374155242845386\n",
      "Epoch: 10660 Training Loss: 0.2172643385746849 Test Loss: 0.21369934568134757\n",
      "Epoch: 10661 Training Loss: 0.21726303478584105 Test Loss: 0.21380066261481895\n",
      "Epoch: 10662 Training Loss: 0.21726365549720725 Test Loss: 0.21371210103735266\n",
      "Epoch: 10663 Training Loss: 0.21726493653084827 Test Loss: 0.21377913147018027\n",
      "Epoch: 10664 Training Loss: 0.21726419919234322 Test Loss: 0.21374336721487736\n",
      "Epoch: 10665 Training Loss: 0.2172654387232916 Test Loss: 0.2137676464646716\n",
      "Epoch: 10666 Training Loss: 0.21726451076289233 Test Loss: 0.21373131184792132\n",
      "Epoch: 10667 Training Loss: 0.21726660137249676 Test Loss: 0.21379836820626924\n",
      "Epoch: 10668 Training Loss: 0.21726612626573813 Test Loss: 0.21376130767494955\n",
      "Epoch: 10669 Training Loss: 0.2172656312638672 Test Loss: 0.21374831898926144\n",
      "Epoch: 10670 Training Loss: 0.2172649088085917 Test Loss: 0.2136805756046246\n",
      "Epoch: 10671 Training Loss: 0.217265250055942 Test Loss: 0.21370928811839626\n",
      "Epoch: 10672 Training Loss: 0.21726610183393053 Test Loss: 0.21377904073085907\n",
      "Epoch: 10673 Training Loss: 0.21726405842030236 Test Loss: 0.21375736699585857\n",
      "Epoch: 10674 Training Loss: 0.21726275837916328 Test Loss: 0.21438307942915738\n",
      "Epoch: 10675 Training Loss: 0.2172652788092657 Test Loss: 0.213793481245686\n",
      "Epoch: 10676 Training Loss: 0.21726521187059572 Test Loss: 0.21384076939477806\n",
      "Epoch: 10677 Training Loss: 0.21726459384000402 Test Loss: 0.21372613970661436\n",
      "Epoch: 10678 Training Loss: 0.21726475304573165 Test Loss: 0.2137173120669401\n",
      "Epoch: 10679 Training Loss: 0.21726468008204344 Test Loss: 0.21370530855102474\n",
      "Epoch: 10680 Training Loss: 0.21726335529529375 Test Loss: 0.21386017464674922\n",
      "Epoch: 10681 Training Loss: 0.21726706515544875 Test Loss: 0.21378143884149012\n",
      "Epoch: 10682 Training Loss: 0.21726410534730456 Test Loss: 0.2137444301612111\n",
      "Epoch: 10683 Training Loss: 0.21726480528048803 Test Loss: 0.21372108423014893\n",
      "Epoch: 10684 Training Loss: 0.21726615452594275 Test Loss: 0.213725724898289\n",
      "Epoch: 10685 Training Loss: 0.21726502139215434 Test Loss: 0.2147510014509995\n",
      "Epoch: 10686 Training Loss: 0.2172653687541809 Test Loss: 0.21383824165654533\n",
      "Epoch: 10687 Training Loss: 0.21726495905294027 Test Loss: 0.21379459604306042\n",
      "Epoch: 10688 Training Loss: 0.21726572167500405 Test Loss: 0.21386630603230858\n",
      "Epoch: 10689 Training Loss: 0.21726380323567546 Test Loss: 0.21370720111400923\n",
      "Epoch: 10690 Training Loss: 0.2172652180569984 Test Loss: 0.21374767085125304\n",
      "Epoch: 10691 Training Loss: 0.21726580322792957 Test Loss: 0.21376717980530557\n",
      "Epoch: 10692 Training Loss: 0.21726341980423164 Test Loss: 0.21374448201225177\n",
      "Epoch: 10693 Training Loss: 0.21726551087109192 Test Loss: 0.21371578246124032\n",
      "Epoch: 10694 Training Loss: 0.21726404717718795 Test Loss: 0.21373972467927022\n",
      "Epoch: 10695 Training Loss: 0.21726519809015965 Test Loss: 0.21375586331567908\n",
      "Epoch: 10696 Training Loss: 0.21726286815643023 Test Loss: 0.21779114463038918\n",
      "Epoch: 10697 Training Loss: 0.21726582999084543 Test Loss: 0.21388269096116067\n",
      "Epoch: 10698 Training Loss: 0.21726290492517997 Test Loss: 0.21433540239726026\n",
      "Epoch: 10699 Training Loss: 0.21726448942428606 Test Loss: 0.21384474896214958\n",
      "Epoch: 10700 Training Loss: 0.21726287505113118 Test Loss: 0.21371893241196108\n",
      "Epoch: 10701 Training Loss: 0.21726571901216116 Test Loss: 0.21375535776803256\n",
      "Epoch: 10702 Training Loss: 0.2172630511215304 Test Loss: 0.2137688131130867\n",
      "Epoch: 10703 Training Loss: 0.21726544873809125 Test Loss: 0.21380261999160427\n",
      "Epoch: 10704 Training Loss: 0.2172638129456379 Test Loss: 0.2137544114865403\n",
      "Epoch: 10705 Training Loss: 0.21726246334155114 Test Loss: 0.21396399339293298\n",
      "Epoch: 10706 Training Loss: 0.21726520859807838 Test Loss: 0.21419417312523228\n",
      "Epoch: 10707 Training Loss: 0.217265295010468 Test Loss: 0.21400424276325394\n",
      "Epoch: 10708 Training Loss: 0.2172634148461437 Test Loss: 0.21385973391290353\n",
      "Epoch: 10709 Training Loss: 0.21726378780553202 Test Loss: 0.21627164988352182\n",
      "Epoch: 10710 Training Loss: 0.21726557144404318 Test Loss: 0.21377564448769512\n",
      "Epoch: 10711 Training Loss: 0.21726395318869648 Test Loss: 0.21374336721487736\n",
      "Epoch: 10712 Training Loss: 0.21726529680362822 Test Loss: 0.2137887368754646\n",
      "Epoch: 10713 Training Loss: 0.2172643242742324 Test Loss: 0.2137690594055299\n",
      "Epoch: 10714 Training Loss: 0.21726363220405637 Test Loss: 0.21377171677136428\n",
      "Epoch: 10715 Training Loss: 0.2172655500516421 Test Loss: 0.21375977806924976\n",
      "Epoch: 10716 Training Loss: 0.21726474677863677 Test Loss: 0.21367111278970213\n",
      "Epoch: 10717 Training Loss: 0.2172646336929893 Test Loss: 0.2137477745533344\n",
      "Epoch: 10718 Training Loss: 0.21726503444636056 Test Loss: 0.2137622928447223\n",
      "Epoch: 10719 Training Loss: 0.21726484105403385 Test Loss: 0.21370869183142854\n",
      "Epoch: 10720 Training Loss: 0.2172655633030959 Test Loss: 0.21366631656844007\n",
      "Epoch: 10721 Training Loss: 0.21726667339477584 Test Loss: 0.2136877828992779\n",
      "Epoch: 10722 Training Loss: 0.21726553646845365 Test Loss: 0.21375491703418684\n",
      "Epoch: 10723 Training Loss: 0.21726506354038466 Test Loss: 0.2138590598493748\n",
      "Epoch: 10724 Training Loss: 0.21726408228726451 Test Loss: 0.2137652742795609\n",
      "Epoch: 10725 Training Loss: 0.217265105867931 Test Loss: 0.21371269732432038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10726 Training Loss: 0.2172645533235495 Test Loss: 0.21370389561016645\n",
      "Epoch: 10727 Training Loss: 0.21726290397480508 Test Loss: 0.21385030998626156\n",
      "Epoch: 10728 Training Loss: 0.2172649264174248 Test Loss: 0.21405947708432888\n",
      "Epoch: 10729 Training Loss: 0.21726398523246912 Test Loss: 0.2137886850244239\n",
      "Epoch: 10730 Training Loss: 0.21726597948661064 Test Loss: 0.21374112465736833\n",
      "Epoch: 10731 Training Loss: 0.21726344933758 Test Loss: 0.21382026230819262\n",
      "Epoch: 10732 Training Loss: 0.21726483089578139 Test Loss: 0.21372667117978125\n",
      "Epoch: 10733 Training Loss: 0.21726495152166747 Test Loss: 0.2136921643122146\n",
      "Epoch: 10734 Training Loss: 0.21726513194944597 Test Loss: 0.21366277773491424\n",
      "Epoch: 10735 Training Loss: 0.2172667316186873 Test Loss: 0.2137359006650207\n",
      "Epoch: 10736 Training Loss: 0.21726382089830334 Test Loss: 0.21372856374276575\n",
      "Epoch: 10737 Training Loss: 0.2172652732235717 Test Loss: 0.21371283991468223\n",
      "Epoch: 10738 Training Loss: 0.2172636353331209 Test Loss: 0.2138007144658596\n",
      "Epoch: 10739 Training Loss: 0.2172647626839677 Test Loss: 0.2138088810047653\n",
      "Epoch: 10740 Training Loss: 0.21726485232404566 Test Loss: 0.21373338588954816\n",
      "Epoch: 10741 Training Loss: 0.21726371285143598 Test Loss: 0.21371867315675772\n",
      "Epoch: 10742 Training Loss: 0.2172662738428219 Test Loss: 0.2138141568481536\n",
      "Epoch: 10743 Training Loss: 0.21726286154863492 Test Loss: 0.2137272545039888\n",
      "Epoch: 10744 Training Loss: 0.21726529328006844 Test Loss: 0.21369841236261547\n",
      "Epoch: 10745 Training Loss: 0.21726492590637414 Test Loss: 0.2137538411250929\n",
      "Epoch: 10746 Training Loss: 0.21726377779073233 Test Loss: 0.21365070940519804\n",
      "Epoch: 10747 Training Loss: 0.2172663785813087 Test Loss: 0.2137324396080559\n",
      "Epoch: 10748 Training Loss: 0.21726574021628042 Test Loss: 0.21367150167250717\n",
      "Epoch: 10749 Training Loss: 0.21726419608121028 Test Loss: 0.2136821052103244\n",
      "Epoch: 10750 Training Loss: 0.2172654556776212 Test Loss: 0.21370345487632075\n",
      "Epoch: 10751 Training Loss: 0.21726358185211816 Test Loss: 0.213907981306248\n",
      "Epoch: 10752 Training Loss: 0.2172647163576741 Test Loss: 0.2138605505667941\n",
      "Epoch: 10753 Training Loss: 0.21726420389938872 Test Loss: 0.21382251782846182\n",
      "Epoch: 10754 Training Loss: 0.21726669559409903 Test Loss: 0.21376767239019193\n",
      "Epoch: 10755 Training Loss: 0.2172639449311938 Test Loss: 0.21381282168385632\n",
      "Epoch: 10756 Training Loss: 0.21726404633440266 Test Loss: 0.2137982256159074\n",
      "Epoch: 10757 Training Loss: 0.2172656511948427 Test Loss: 0.21375905215468036\n",
      "Epoch: 10758 Training Loss: 0.2172650901419161 Test Loss: 0.21372651562665923\n",
      "Epoch: 10759 Training Loss: 0.21726630722249887 Test Loss: 0.21377779630588298\n",
      "Epoch: 10760 Training Loss: 0.2172649404130401 Test Loss: 0.2138230881899092\n",
      "Epoch: 10761 Training Loss: 0.21726411725388825 Test Loss: 0.21370722703952957\n",
      "Epoch: 10762 Training Loss: 0.21726743701204354 Test Loss: 0.21380903655788733\n",
      "Epoch: 10763 Training Loss: 0.2172642656020309 Test Loss: 0.21368061449290512\n",
      "Epoch: 10764 Training Loss: 0.21726403237465056 Test Loss: 0.2137811277352461\n",
      "Epoch: 10765 Training Loss: 0.21726451339883782 Test Loss: 0.21374830602650127\n",
      "Epoch: 10766 Training Loss: 0.21726495047266878 Test Loss: 0.21382907698510673\n",
      "Epoch: 10767 Training Loss: 0.21726509167506808 Test Loss: 0.2137829036333891\n",
      "Epoch: 10768 Training Loss: 0.21726429291186064 Test Loss: 0.2137208120121854\n",
      "Epoch: 10769 Training Loss: 0.2172658567716929 Test Loss: 0.21382951771895242\n",
      "Epoch: 10770 Training Loss: 0.21726514832099852 Test Loss: 0.21372381937254434\n",
      "Epoch: 10771 Training Loss: 0.21726465206391546 Test Loss: 0.21374198019953938\n",
      "Epoch: 10772 Training Loss: 0.21726424791250562 Test Loss: 0.21371320287196693\n",
      "Epoch: 10773 Training Loss: 0.21726513751720836 Test Loss: 0.2137225101337674\n",
      "Epoch: 10774 Training Loss: 0.21726487173500475 Test Loss: 0.21376208544055963\n",
      "Epoch: 10775 Training Loss: 0.21726454756750527 Test Loss: 0.21373302293226346\n",
      "Epoch: 10776 Training Loss: 0.21726412193403635 Test Loss: 0.213731519252084\n",
      "Epoch: 10777 Training Loss: 0.21726638269661136 Test Loss: 0.21393227351880242\n",
      "Epoch: 10778 Training Loss: 0.21726494548768344 Test Loss: 0.21379349420844615\n",
      "Epoch: 10779 Training Loss: 0.21726364902389897 Test Loss: 0.2140877877525353\n",
      "Epoch: 10780 Training Loss: 0.2172646086425414 Test Loss: 0.213811253189876\n",
      "Epoch: 10781 Training Loss: 0.21726513082872084 Test Loss: 0.2137749315358859\n",
      "Epoch: 10782 Training Loss: 0.21726371342524725 Test Loss: 0.2136971549748792\n",
      "Epoch: 10783 Training Loss: 0.21726599341946531 Test Loss: 0.21377587781737814\n",
      "Epoch: 10784 Training Loss: 0.21726480695709283 Test Loss: 0.21374233019406394\n",
      "Epoch: 10785 Training Loss: 0.21726501854102964 Test Loss: 0.214072621323139\n",
      "Epoch: 10786 Training Loss: 0.21726268153328326 Test Loss: 0.2143692352012982\n",
      "Epoch: 10787 Training Loss: 0.21726234389915075 Test Loss: 0.21416425507476505\n",
      "Epoch: 10788 Training Loss: 0.21726639671015827 Test Loss: 0.2138689115471023\n",
      "Epoch: 10789 Training Loss: 0.21726397751291449 Test Loss: 0.21377061493675004\n",
      "Epoch: 10790 Training Loss: 0.21726332832616446 Test Loss: 0.21373526548977248\n",
      "Epoch: 10791 Training Loss: 0.2172644123273636 Test Loss: 0.21380479773531247\n",
      "Epoch: 10792 Training Loss: 0.2172641607380229 Test Loss: 0.2138303732611235\n",
      "Epoch: 10793 Training Loss: 0.21726425092501475 Test Loss: 0.2137707056760712\n",
      "Epoch: 10794 Training Loss: 0.21726399857358095 Test Loss: 0.21383988792708666\n",
      "Epoch: 10795 Training Loss: 0.2172648253907796 Test Loss: 0.2138571283981098\n",
      "Epoch: 10796 Training Loss: 0.2172628330732511 Test Loss: 0.21370121231881173\n",
      "Epoch: 10797 Training Loss: 0.2172630182887673 Test Loss: 0.21367099612486062\n",
      "Epoch: 10798 Training Loss: 0.21726561249844578 Test Loss: 0.21378897020514762\n",
      "Epoch: 10799 Training Loss: 0.21726373039750846 Test Loss: 0.21373930987094483\n",
      "Epoch: 10800 Training Loss: 0.21726481831676261 Test Loss: 0.213741098731848\n",
      "Epoch: 10801 Training Loss: 0.217263937355092 Test Loss: 0.21382250486570165\n",
      "Epoch: 10802 Training Loss: 0.21726250716638623 Test Loss: 0.21375613553364262\n",
      "Epoch: 10803 Training Loss: 0.2172638327331606 Test Loss: 0.21386844488773626\n",
      "Epoch: 10804 Training Loss: 0.2172653593221583 Test Loss: 0.21376103545698605\n",
      "Epoch: 10805 Training Loss: 0.21726622758825476 Test Loss: 0.21371932129476612\n",
      "Epoch: 10806 Training Loss: 0.2172653921818188 Test Loss: 0.21371998239553466\n",
      "Epoch: 10807 Training Loss: 0.2172655735330748 Test Loss: 0.21374895416450967\n",
      "Epoch: 10808 Training Loss: 0.217265640686924 Test Loss: 0.21378295548442974\n",
      "Epoch: 10809 Training Loss: 0.21726416853826971 Test Loss: 0.2137850295260566\n",
      "Epoch: 10810 Training Loss: 0.21726521005053812 Test Loss: 0.2137890479817086\n",
      "Epoch: 10811 Training Loss: 0.21726684094766416 Test Loss: 0.21392869579699614\n",
      "Epoch: 10812 Training Loss: 0.21726541840678662 Test Loss: 0.2138296084582736\n",
      "Epoch: 10813 Training Loss: 0.2172655359843004 Test Loss: 0.2138329917386774\n",
      "Epoch: 10814 Training Loss: 0.21726524980489959 Test Loss: 0.2137075381457736\n",
      "Epoch: 10815 Training Loss: 0.2172642775893068 Test Loss: 0.21379423308577572\n",
      "Epoch: 10816 Training Loss: 0.21726455675745124 Test Loss: 0.21371316398368642\n",
      "Epoch: 10817 Training Loss: 0.21726415190670895 Test Loss: 0.2137141750789795\n",
      "Epoch: 10818 Training Loss: 0.21726626467977334 Test Loss: 0.21377389451507248\n",
      "Epoch: 10819 Training Loss: 0.21726629941328623 Test Loss: 0.21379892560495647\n",
      "Epoch: 10820 Training Loss: 0.21726495368242552 Test Loss: 0.2137229119793326\n",
      "Epoch: 10821 Training Loss: 0.21726374879533203 Test Loss: 0.21373609510642322\n",
      "Epoch: 10822 Training Loss: 0.21726570130470427 Test Loss: 0.2137805184855182\n",
      "Epoch: 10823 Training Loss: 0.2172664008792557 Test Loss: 0.21380010521613174\n",
      "Epoch: 10824 Training Loss: 0.21726436514931896 Test Loss: 0.21381310686458002\n",
      "Epoch: 10825 Training Loss: 0.21726412738524334 Test Loss: 0.213749135643152\n",
      "Epoch: 10826 Training Loss: 0.21726305329125423 Test Loss: 0.21384756188110599\n",
      "Epoch: 10827 Training Loss: 0.21726349826395583 Test Loss: 0.21373374884683286\n",
      "Epoch: 10828 Training Loss: 0.2172665297357471 Test Loss: 0.21375625219848413\n",
      "Epoch: 10829 Training Loss: 0.21726565686122892 Test Loss: 0.21372879707244877\n",
      "Epoch: 10830 Training Loss: 0.21726534008154944 Test Loss: 0.21383736018885394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10831 Training Loss: 0.2172641570979077 Test Loss: 0.21376441873738983\n",
      "Epoch: 10832 Training Loss: 0.2172650010666836 Test Loss: 0.21372055275698204\n",
      "Epoch: 10833 Training Loss: 0.21726542624289666 Test Loss: 0.213877000309447\n",
      "Epoch: 10834 Training Loss: 0.2172646917375847 Test Loss: 0.21387912620211452\n",
      "Epoch: 10835 Training Loss: 0.2172647795307077 Test Loss: 0.2138336009884053\n",
      "Epoch: 10836 Training Loss: 0.2172639278603088 Test Loss: 0.21404587914891288\n",
      "Epoch: 10837 Training Loss: 0.21726412629141562 Test Loss: 0.21394804919792662\n",
      "Epoch: 10838 Training Loss: 0.21726324119651075 Test Loss: 0.21381055320082695\n",
      "Epoch: 10839 Training Loss: 0.21726531686012496 Test Loss: 0.21382080674411968\n",
      "Epoch: 10840 Training Loss: 0.2172649751734504 Test Loss: 0.21376631130037432\n",
      "Epoch: 10841 Training Loss: 0.21726266549346535 Test Loss: 0.21376295394549086\n",
      "Epoch: 10842 Training Loss: 0.21726463225846113 Test Loss: 0.21371903611404242\n",
      "Epoch: 10843 Training Loss: 0.21726406475015783 Test Loss: 0.2137572632937772\n",
      "Epoch: 10844 Training Loss: 0.2172644093327861 Test Loss: 0.21389308709481522\n",
      "Epoch: 10845 Training Loss: 0.21726393092661272 Test Loss: 0.21372868040760726\n",
      "Epoch: 10846 Training Loss: 0.21726449348579388 Test Loss: 0.21375202633866944\n",
      "Epoch: 10847 Training Loss: 0.21726636307943886 Test Loss: 0.213824540019048\n",
      "Epoch: 10848 Training Loss: 0.21726429201528052 Test Loss: 0.21664062188893757\n",
      "Epoch: 10849 Training Loss: 0.21726412882873727 Test Loss: 0.21374332832659684\n",
      "Epoch: 10850 Training Loss: 0.21726426764623352 Test Loss: 0.2137590003036397\n",
      "Epoch: 10851 Training Loss: 0.21726377600653793 Test Loss: 0.2137869998656021\n",
      "Epoch: 10852 Training Loss: 0.2172633568105141 Test Loss: 0.2136937328061949\n",
      "Epoch: 10853 Training Loss: 0.21726437913596847 Test Loss: 0.21372734524330997\n",
      "Epoch: 10854 Training Loss: 0.21726534675210535 Test Loss: 0.21374728196844803\n",
      "Epoch: 10855 Training Loss: 0.21726541024790774 Test Loss: 0.21370313080731657\n",
      "Epoch: 10856 Training Loss: 0.21726634699479194 Test Loss: 0.21364153177099926\n",
      "Epoch: 10857 Training Loss: 0.2172645140264439 Test Loss: 0.21376838534200115\n",
      "Epoch: 10858 Training Loss: 0.21726471985433649 Test Loss: 0.2137859498820285\n",
      "Epoch: 10859 Training Loss: 0.21726556216443918 Test Loss: 0.21371005292124615\n",
      "Epoch: 10860 Training Loss: 0.21726586473332415 Test Loss: 0.2149053101480368\n",
      "Epoch: 10861 Training Loss: 0.217263898075918 Test Loss: 0.21379173127306333\n",
      "Epoch: 10862 Training Loss: 0.21726499991009526 Test Loss: 0.21376887792688753\n",
      "Epoch: 10863 Training Loss: 0.21726588349874557 Test Loss: 0.21375814476146862\n",
      "Epoch: 10864 Training Loss: 0.21726441628128182 Test Loss: 0.21374187649745804\n",
      "Epoch: 10865 Training Loss: 0.21726375089332944 Test Loss: 0.21373545993117501\n",
      "Epoch: 10866 Training Loss: 0.21726555975263873 Test Loss: 0.2138503229490217\n",
      "Epoch: 10867 Training Loss: 0.21726318966108682 Test Loss: 0.21708783711472604\n",
      "Epoch: 10868 Training Loss: 0.2172647992375382 Test Loss: 0.21383361395116546\n",
      "Epoch: 10869 Training Loss: 0.21726084812561097 Test Loss: 0.21457363200362375\n",
      "Epoch: 10870 Training Loss: 0.21726588063865507 Test Loss: 0.21376196877571813\n",
      "Epoch: 10871 Training Loss: 0.21726647163735688 Test Loss: 0.21381594570905677\n",
      "Epoch: 10872 Training Loss: 0.21726520582764589 Test Loss: 0.2137476838140132\n",
      "Epoch: 10873 Training Loss: 0.2172633362429667 Test Loss: 0.21374917453143252\n",
      "Epoch: 10874 Training Loss: 0.21726564889959768 Test Loss: 0.21375019858948577\n",
      "Epoch: 10875 Training Loss: 0.21726462022635623 Test Loss: 0.21370852331554635\n",
      "Epoch: 10876 Training Loss: 0.21726403608649217 Test Loss: 0.21376771127847244\n",
      "Epoch: 10877 Training Loss: 0.21726621829071918 Test Loss: 0.2137840184307635\n",
      "Epoch: 10878 Training Loss: 0.21726604588733253 Test Loss: 0.21371544542947596\n",
      "Epoch: 10879 Training Loss: 0.2172648696549389 Test Loss: 0.21373615992022407\n",
      "Epoch: 10880 Training Loss: 0.21726411554142025 Test Loss: 0.21365172050049114\n",
      "Epoch: 10881 Training Loss: 0.21726432829987702 Test Loss: 0.21375297262016169\n",
      "Epoch: 10882 Training Loss: 0.21726359055791092 Test Loss: 0.2136865384743018\n",
      "Epoch: 10883 Training Loss: 0.2172640279993397 Test Loss: 0.2137091455280344\n",
      "Epoch: 10884 Training Loss: 0.21726576088245164 Test Loss: 0.21389628889657666\n",
      "Epoch: 10885 Training Loss: 0.21726570739248313 Test Loss: 0.213776331513984\n",
      "Epoch: 10886 Training Loss: 0.21726480262661094 Test Loss: 0.21368233854000743\n",
      "Epoch: 10887 Training Loss: 0.2172629621090585 Test Loss: 0.21364613355085882\n",
      "Epoch: 10888 Training Loss: 0.21726443067139237 Test Loss: 0.21373291923018212\n",
      "Epoch: 10889 Training Loss: 0.21726384521355555 Test Loss: 0.21374730789396834\n",
      "Epoch: 10890 Training Loss: 0.21726381065935865 Test Loss: 0.2136761941916879\n",
      "Epoch: 10891 Training Loss: 0.21726455244490098 Test Loss: 0.2138100217276601\n",
      "Epoch: 10892 Training Loss: 0.2172643223286536 Test Loss: 0.21386039501367207\n",
      "Epoch: 10893 Training Loss: 0.2172623921172283 Test Loss: 0.21371394174929648\n",
      "Epoch: 10894 Training Loss: 0.21726104990096157 Test Loss: 0.21380729954802485\n",
      "Epoch: 10895 Training Loss: 0.21726537032319607 Test Loss: 0.21375362075817006\n",
      "Epoch: 10896 Training Loss: 0.21726459492486594 Test Loss: 0.21365875927926223\n",
      "Epoch: 10897 Training Loss: 0.21726501378915514 Test Loss: 0.2138065088196546\n",
      "Epoch: 10898 Training Loss: 0.21726417600678194 Test Loss: 0.2136924494929383\n",
      "Epoch: 10899 Training Loss: 0.21726434536179623 Test Loss: 0.21377837963009053\n",
      "Epoch: 10900 Training Loss: 0.21726404585024942 Test Loss: 0.21377397229163347\n",
      "Epoch: 10901 Training Loss: 0.21726377588998252 Test Loss: 0.21377463339240205\n",
      "Epoch: 10902 Training Loss: 0.21726467721298715 Test Loss: 0.21375772995314324\n",
      "Epoch: 10903 Training Loss: 0.2172659081995272 Test Loss: 0.21379488122378412\n",
      "Epoch: 10904 Training Loss: 0.21726328336267264 Test Loss: 0.21371307324436525\n",
      "Epoch: 10905 Training Loss: 0.21726358511566973 Test Loss: 0.2137516374558644\n",
      "Epoch: 10906 Training Loss: 0.2172636943908518 Test Loss: 0.21372683969566342\n",
      "Epoch: 10907 Training Loss: 0.21726341510615194 Test Loss: 0.2137208120121854\n",
      "Epoch: 10908 Training Loss: 0.21726387422688745 Test Loss: 0.2137679705336758\n",
      "Epoch: 10909 Training Loss: 0.21726434374795206 Test Loss: 0.21371412322793884\n",
      "Epoch: 10910 Training Loss: 0.21726379297879916 Test Loss: 0.21372135644811247\n",
      "Epoch: 10911 Training Loss: 0.2172647385749289 Test Loss: 0.21372799338131837\n",
      "Epoch: 10912 Training Loss: 0.2172645215935799 Test Loss: 0.2136437224774676\n",
      "Epoch: 10913 Training Loss: 0.21726399495139737 Test Loss: 0.21369683090587502\n",
      "Epoch: 10914 Training Loss: 0.21726482910262118 Test Loss: 0.21378046663447753\n",
      "Epoch: 10915 Training Loss: 0.21726478538537572 Test Loss: 0.21369698645899704\n",
      "Epoch: 10916 Training Loss: 0.21726444446079424 Test Loss: 0.21377135381407958\n",
      "Epoch: 10917 Training Loss: 0.21726558812043298 Test Loss: 0.21380159593355103\n",
      "Epoch: 10918 Training Loss: 0.21726561967108654 Test Loss: 0.21378981278455852\n",
      "Epoch: 10919 Training Loss: 0.2172640290124752 Test Loss: 0.21375015970120526\n",
      "Epoch: 10920 Training Loss: 0.21726604846051742 Test Loss: 0.21376068546246152\n",
      "Epoch: 10921 Training Loss: 0.21726459529246378 Test Loss: 0.21371820649739168\n",
      "Epoch: 10922 Training Loss: 0.21726570742834633 Test Loss: 0.21372462306367473\n",
      "Epoch: 10923 Training Loss: 0.2172655536917573 Test Loss: 0.21375005599912392\n",
      "Epoch: 10924 Training Loss: 0.2172658914245136 Test Loss: 0.21384766558318732\n",
      "Epoch: 10925 Training Loss: 0.21726471001885284 Test Loss: 0.21377375192471063\n",
      "Epoch: 10926 Training Loss: 0.21726786242136742 Test Loss: 0.21380596438372756\n",
      "Epoch: 10927 Training Loss: 0.21726615556597567 Test Loss: 0.2137147454404269\n",
      "Epoch: 10928 Training Loss: 0.21726599638714544 Test Loss: 0.2137629798710112\n",
      "Epoch: 10929 Training Loss: 0.2172652443536926 Test Loss: 0.21372193977232\n",
      "Epoch: 10930 Training Loss: 0.21726481035513137 Test Loss: 0.2137571077406552\n",
      "Epoch: 10931 Training Loss: 0.21726619267542582 Test Loss: 0.2139131404847948\n",
      "Epoch: 10932 Training Loss: 0.21726491236801468 Test Loss: 0.21380613289960976\n",
      "Epoch: 10933 Training Loss: 0.2172632277029803 Test Loss: 0.21375424297065812\n",
      "Epoch: 10934 Training Loss: 0.21726578596876273 Test Loss: 0.21369066063203515\n",
      "Epoch: 10935 Training Loss: 0.21726525407262084 Test Loss: 0.21372088978874643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10936 Training Loss: 0.21726442580296246 Test Loss: 0.21374983563220107\n",
      "Epoch: 10937 Training Loss: 0.2172659939484476 Test Loss: 0.2136596148214333\n",
      "Epoch: 10938 Training Loss: 0.21726478228320859 Test Loss: 0.21381865492593183\n",
      "Epoch: 10939 Training Loss: 0.21726489012386252 Test Loss: 0.2137673612839479\n",
      "Epoch: 10940 Training Loss: 0.2172629548377939 Test Loss: 0.213971874751115\n",
      "Epoch: 10941 Training Loss: 0.21726637867993254 Test Loss: 0.21376755572535042\n",
      "Epoch: 10942 Training Loss: 0.2172636736888174 Test Loss: 0.21369928086754672\n",
      "Epoch: 10943 Training Loss: 0.2172657231902244 Test Loss: 0.21451097002097272\n",
      "Epoch: 10944 Training Loss: 0.21726483745874767 Test Loss: 0.213795257143829\n",
      "Epoch: 10945 Training Loss: 0.21726452312673186 Test Loss: 0.2137460375434719\n",
      "Epoch: 10946 Training Loss: 0.21726399119472675 Test Loss: 0.21369760867148507\n",
      "Epoch: 10947 Training Loss: 0.21726483840015678 Test Loss: 0.213712204739434\n",
      "Epoch: 10948 Training Loss: 0.21726471827635552 Test Loss: 0.21372086386322608\n",
      "Epoch: 10949 Training Loss: 0.21726474930699266 Test Loss: 0.2138260307364673\n",
      "Epoch: 10950 Training Loss: 0.21726487732966454 Test Loss: 0.21375262262563716\n",
      "Epoch: 10951 Training Loss: 0.21726392661406246 Test Loss: 0.21379533492039\n",
      "Epoch: 10952 Training Loss: 0.21726494566699944 Test Loss: 0.21383638798184135\n",
      "Epoch: 10953 Training Loss: 0.21726551573055602 Test Loss: 0.21375036710536796\n",
      "Epoch: 10954 Training Loss: 0.2172647406280973 Test Loss: 0.2137525189235558\n",
      "Epoch: 10955 Training Loss: 0.21726297682193785 Test Loss: 0.2137699408732213\n",
      "Epoch: 10956 Training Loss: 0.21726496566073558 Test Loss: 0.21385566360621083\n",
      "Epoch: 10957 Training Loss: 0.2172637745720098 Test Loss: 0.21368395888502842\n",
      "Epoch: 10958 Training Loss: 0.21726727913325444 Test Loss: 0.21375319298708453\n",
      "Epoch: 10959 Training Loss: 0.21726472037435296 Test Loss: 0.21376191692467744\n",
      "Epoch: 10960 Training Loss: 0.2172665897079897 Test Loss: 0.2136957938850616\n",
      "Epoch: 10961 Training Loss: 0.21726567930262872 Test Loss: 0.2137675297998301\n",
      "Epoch: 10962 Training Loss: 0.21726508673491174 Test Loss: 0.2137515856048237\n",
      "Epoch: 10963 Training Loss: 0.2172658771509585 Test Loss: 0.21373128592240098\n",
      "Epoch: 10964 Training Loss: 0.21726470271172507 Test Loss: 0.21369351243927207\n",
      "Epoch: 10965 Training Loss: 0.21726458595906498 Test Loss: 0.21381728087335405\n",
      "Epoch: 10966 Training Loss: 0.21726579103444027 Test Loss: 0.21375722440549672\n",
      "Epoch: 10967 Training Loss: 0.217263344652888 Test Loss: 0.2137749315358859\n",
      "Epoch: 10968 Training Loss: 0.21726477804238473 Test Loss: 0.21644621937470135\n",
      "Epoch: 10969 Training Loss: 0.21726355352915294 Test Loss: 0.2137392061688635\n",
      "Epoch: 10970 Training Loss: 0.21726432312660987 Test Loss: 0.21374470237917464\n",
      "Epoch: 10971 Training Loss: 0.21726560263606473 Test Loss: 0.21379980707264787\n",
      "Epoch: 10972 Training Loss: 0.21726396534632259 Test Loss: 0.2137437042466417\n",
      "Epoch: 10973 Training Loss: 0.21726393112386033 Test Loss: 0.21366004259251886\n",
      "Epoch: 10974 Training Loss: 0.2172648581876795 Test Loss: 0.2136749108784313\n",
      "Epoch: 10975 Training Loss: 0.21726458645218402 Test Loss: 0.21370923626735558\n",
      "Epoch: 10976 Training Loss: 0.21726377978114014 Test Loss: 0.21382491593909286\n",
      "Epoch: 10977 Training Loss: 0.21726271692129967 Test Loss: 0.21372791560475735\n",
      "Epoch: 10978 Training Loss: 0.21726369798613798 Test Loss: 0.21393634382549512\n",
      "Epoch: 10979 Training Loss: 0.2172636869761344 Test Loss: 0.21381020320630242\n",
      "Epoch: 10980 Training Loss: 0.21726489698270024 Test Loss: 0.2137724815742142\n",
      "Epoch: 10981 Training Loss: 0.2172635962601603 Test Loss: 0.21374577828826855\n",
      "Epoch: 10982 Training Loss: 0.21726309252559922 Test Loss: 0.2136732127568493\n",
      "Epoch: 10983 Training Loss: 0.2172651742411291 Test Loss: 0.213810864307071\n",
      "Epoch: 10984 Training Loss: 0.21726438349334773 Test Loss: 0.21372979520498167\n",
      "Epoch: 10985 Training Loss: 0.21726415107288946 Test Loss: 0.2160732289136336\n",
      "Epoch: 10986 Training Loss: 0.2172642975113165 Test Loss: 0.2137730778611819\n",
      "Epoch: 10987 Training Loss: 0.21726585380401278 Test Loss: 0.21387589847483274\n",
      "Epoch: 10988 Training Loss: 0.21726475754656374 Test Loss: 0.21371812872083068\n",
      "Epoch: 10989 Training Loss: 0.217263998932213 Test Loss: 0.21369167172732823\n",
      "Epoch: 10990 Training Loss: 0.21726438967078457 Test Loss: 0.2136877828992779\n",
      "Epoch: 10991 Training Loss: 0.21726528946063722 Test Loss: 0.21367749046770468\n",
      "Epoch: 10992 Training Loss: 0.2172659352045197 Test Loss: 0.21374510422473983\n",
      "Epoch: 10993 Training Loss: 0.21726511363231463 Test Loss: 0.21369499019393118\n",
      "Epoch: 10994 Training Loss: 0.21726422569525083 Test Loss: 0.21483226499449135\n",
      "Epoch: 10995 Training Loss: 0.21726501478435906 Test Loss: 0.21371709170001726\n",
      "Epoch: 10996 Training Loss: 0.2172643440796867 Test Loss: 0.21369075137135632\n",
      "Epoch: 10997 Training Loss: 0.21726797971197556 Test Loss: 0.2136954309277769\n",
      "Epoch: 10998 Training Loss: 0.21726596012944635 Test Loss: 0.21373543400565467\n",
      "Epoch: 10999 Training Loss: 0.21726580650941274 Test Loss: 0.21373214146457206\n",
      "Epoch: 11000 Training Loss: 0.21726552651641456 Test Loss: 0.21376307061033237\n",
      "Epoch: 11001 Training Loss: 0.21726484735699195 Test Loss: 0.21370594372627297\n",
      "Epoch: 11002 Training Loss: 0.21726498599517216 Test Loss: 0.2137407357745633\n",
      "Epoch: 11003 Training Loss: 0.21726386110992063 Test Loss: 0.21373898580194065\n",
      "Epoch: 11004 Training Loss: 0.217261960915997 Test Loss: 0.21371898426300176\n",
      "Epoch: 11005 Training Loss: 0.21726551433189106 Test Loss: 0.21378429064872703\n",
      "Epoch: 11006 Training Loss: 0.21726380150527586 Test Loss: 0.21371458988730488\n",
      "Epoch: 11007 Training Loss: 0.2172635184280422 Test Loss: 0.21371508247219126\n",
      "Epoch: 11008 Training Loss: 0.2172656856683474 Test Loss: 0.21375281706703966\n",
      "Epoch: 11009 Training Loss: 0.21726269004182838 Test Loss: 0.21384857297639906\n",
      "Epoch: 11010 Training Loss: 0.21726333652090654 Test Loss: 0.21378029811859536\n",
      "Epoch: 11011 Training Loss: 0.2172637176660711 Test Loss: 0.21599647641068015\n",
      "Epoch: 11012 Training Loss: 0.21726529328903424 Test Loss: 0.2137654298326829\n",
      "Epoch: 11013 Training Loss: 0.21726523327196262 Test Loss: 0.21370730481609057\n",
      "Epoch: 11014 Training Loss: 0.2172660894162962 Test Loss: 0.21371265843603987\n",
      "Epoch: 11015 Training Loss: 0.21726517737915943 Test Loss: 0.21370310488179622\n",
      "Epoch: 11016 Training Loss: 0.21726534142641957 Test Loss: 0.21380219222051874\n",
      "Epoch: 11017 Training Loss: 0.2172637549727689 Test Loss: 0.21375120968477887\n",
      "Epoch: 11018 Training Loss: 0.21726487325919092 Test Loss: 0.21378570358958532\n",
      "Epoch: 11019 Training Loss: 0.21726437176608007 Test Loss: 0.21367799601535123\n",
      "Epoch: 11020 Training Loss: 0.2172647967360797 Test Loss: 0.21673377228350324\n",
      "Epoch: 11021 Training Loss: 0.21726535606757255 Test Loss: 0.21380481069807264\n",
      "Epoch: 11022 Training Loss: 0.2172638328228186 Test Loss: 0.21380125890178667\n",
      "Epoch: 11023 Training Loss: 0.21726477723546264 Test Loss: 0.21365549266369996\n",
      "Epoch: 11024 Training Loss: 0.2172657072848935 Test Loss: 0.2137758907801383\n",
      "Epoch: 11025 Training Loss: 0.21726338328652434 Test Loss: 0.21373359329371083\n",
      "Epoch: 11026 Training Loss: 0.21726541378939912 Test Loss: 0.21378099810764442\n",
      "Epoch: 11027 Training Loss: 0.2172635794403177 Test Loss: 0.21382990660175746\n",
      "Epoch: 11028 Training Loss: 0.2172632314237877 Test Loss: 0.2137787944384159\n",
      "Epoch: 11029 Training Loss: 0.21726319082664095 Test Loss: 0.21403011643254885\n",
      "Epoch: 11030 Training Loss: 0.21726228471589862 Test Loss: 0.21438663122544335\n",
      "Epoch: 11031 Training Loss: 0.2172612550922823 Test Loss: 0.21389540742888527\n",
      "Epoch: 11032 Training Loss: 0.2172651762942975 Test Loss: 0.21494947427192843\n",
      "Epoch: 11033 Training Loss: 0.21726361058751026 Test Loss: 0.2137501078501646\n",
      "Epoch: 11034 Training Loss: 0.21726410159063397 Test Loss: 0.2136955475926184\n",
      "Epoch: 11035 Training Loss: 0.21726406416738075 Test Loss: 0.213720086097616\n",
      "Epoch: 11036 Training Loss: 0.2172650948130984 Test Loss: 0.21370919737907507\n",
      "Epoch: 11037 Training Loss: 0.21726565965855882 Test Loss: 0.21367813860571308\n",
      "Epoch: 11038 Training Loss: 0.2172634897554107 Test Loss: 0.21366481288826059\n",
      "Epoch: 11039 Training Loss: 0.21726547228228457 Test Loss: 0.21365940741727063\n",
      "Epoch: 11040 Training Loss: 0.21726367990211745 Test Loss: 0.21373652287750877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11041 Training Loss: 0.2172657631328677 Test Loss: 0.21380067557757912\n",
      "Epoch: 11042 Training Loss: 0.2172647309091691 Test Loss: 0.21381129207815652\n",
      "Epoch: 11043 Training Loss: 0.21726570051571378 Test Loss: 0.21374671160700065\n",
      "Epoch: 11044 Training Loss: 0.2172646460837262 Test Loss: 0.21369399206139827\n",
      "Epoch: 11045 Training Loss: 0.21726588815199627 Test Loss: 0.2138604727902331\n",
      "Epoch: 11046 Training Loss: 0.21726568822360068 Test Loss: 0.21374977081840024\n",
      "Epoch: 11047 Training Loss: 0.2172653368000663 Test Loss: 0.21390129252200143\n",
      "Epoch: 11048 Training Loss: 0.21726543184652225 Test Loss: 0.21377458154136136\n",
      "Epoch: 11049 Training Loss: 0.21726453341050556 Test Loss: 0.21378071292692072\n",
      "Epoch: 11050 Training Loss: 0.21726436487137912 Test Loss: 0.21370362339220292\n",
      "Epoch: 11051 Training Loss: 0.21726548209983662 Test Loss: 0.21374843565410295\n",
      "Epoch: 11052 Training Loss: 0.21726420068963195 Test Loss: 0.21377893702877773\n",
      "Epoch: 11053 Training Loss: 0.21726327861976394 Test Loss: 0.2137695779159366\n",
      "Epoch: 11054 Training Loss: 0.2172653059756426 Test Loss: 0.2137364710264681\n",
      "Epoch: 11055 Training Loss: 0.2172646684175364 Test Loss: 0.21369211246117395\n",
      "Epoch: 11056 Training Loss: 0.2172669665226724 Test Loss: 0.2137963719412034\n",
      "Epoch: 11057 Training Loss: 0.21726408206311948 Test Loss: 0.21377316860050308\n",
      "Epoch: 11058 Training Loss: 0.21726543711841323 Test Loss: 0.2137677760922733\n",
      "Epoch: 11059 Training Loss: 0.21726396191242084 Test Loss: 0.21374794306921657\n",
      "Epoch: 11060 Training Loss: 0.21726551573952183 Test Loss: 0.21372490824439844\n",
      "Epoch: 11061 Training Loss: 0.217264855551734 Test Loss: 0.21378894427962727\n",
      "Epoch: 11062 Training Loss: 0.21726446506420485 Test Loss: 0.21369303281714586\n",
      "Epoch: 11063 Training Loss: 0.21726240183615655 Test Loss: 0.21375844290495247\n",
      "Epoch: 11064 Training Loss: 0.21726190594667133 Test Loss: 0.21404752541945418\n",
      "Epoch: 11065 Training Loss: 0.2172666391005872 Test Loss: 0.21377082234091271\n",
      "Epoch: 11066 Training Loss: 0.21726134464270228 Test Loss: 0.21378605358410985\n",
      "Epoch: 11067 Training Loss: 0.21726334618603996 Test Loss: 0.21380147926870952\n",
      "Epoch: 11068 Training Loss: 0.21726390194017822 Test Loss: 0.21379140720405915\n",
      "Epoch: 11069 Training Loss: 0.21726470734704414 Test Loss: 0.21372365085666215\n",
      "Epoch: 11070 Training Loss: 0.21726612384497188 Test Loss: 0.21371662504065123\n",
      "Epoch: 11071 Training Loss: 0.21726503250974755 Test Loss: 0.2137679316453953\n",
      "Epoch: 11072 Training Loss: 0.21726458916882171 Test Loss: 0.21368105522675082\n",
      "Epoch: 11073 Training Loss: 0.21726438924939193 Test Loss: 0.21376745202326908\n",
      "Epoch: 11074 Training Loss: 0.21726470273862247 Test Loss: 0.21370770666165578\n",
      "Epoch: 11075 Training Loss: 0.2172652135740979 Test Loss: 0.21373688583479347\n",
      "Epoch: 11076 Training Loss: 0.21726587205838355 Test Loss: 0.21372003424657535\n",
      "Epoch: 11077 Training Loss: 0.2172642075215723 Test Loss: 0.21369592351266326\n",
      "Epoch: 11078 Training Loss: 0.2172657313311717 Test Loss: 0.21379961263124536\n",
      "Epoch: 11079 Training Loss: 0.2172647161604265 Test Loss: 0.21372674895634225\n",
      "Epoch: 11080 Training Loss: 0.21726365721864105 Test Loss: 0.21379152386890066\n",
      "Epoch: 11081 Training Loss: 0.2172624893154765 Test Loss: 0.21376475576915419\n",
      "Epoch: 11082 Training Loss: 0.21726491005483803 Test Loss: 0.21378342214379578\n",
      "Epoch: 11083 Training Loss: 0.21726394908235966 Test Loss: 0.21369086803619783\n",
      "Epoch: 11084 Training Loss: 0.21726606018778505 Test Loss: 0.21382573259298343\n",
      "Epoch: 11085 Training Loss: 0.21726390041599206 Test Loss: 0.21378934612519246\n",
      "Epoch: 11086 Training Loss: 0.2172639063603181 Test Loss: 0.21373997097171338\n",
      "Epoch: 11087 Training Loss: 0.21726569724319644 Test Loss: 0.21377101678231522\n",
      "Epoch: 11088 Training Loss: 0.21726379008284544 Test Loss: 0.21369290318954418\n",
      "Epoch: 11089 Training Loss: 0.21726485485240155 Test Loss: 0.2136782811960749\n",
      "Epoch: 11090 Training Loss: 0.21726566354075064 Test Loss: 0.21376745202326908\n",
      "Epoch: 11091 Training Loss: 0.2172640156713634 Test Loss: 0.2138096069193347\n",
      "Epoch: 11092 Training Loss: 0.21726413071155548 Test Loss: 0.2137719889893278\n",
      "Epoch: 11093 Training Loss: 0.21726288285137801 Test Loss: 0.21373969875374987\n",
      "Epoch: 11094 Training Loss: 0.21726240536868213 Test Loss: 0.2138384620234682\n",
      "Epoch: 11095 Training Loss: 0.21726380583575775 Test Loss: 0.2137671279542649\n",
      "Epoch: 11096 Training Loss: 0.21726571326508276 Test Loss: 0.21380622363893093\n",
      "Epoch: 11097 Training Loss: 0.2172634118963952 Test Loss: 0.2138542765908729\n",
      "Epoch: 11098 Training Loss: 0.2172642131251979 Test Loss: 0.2151630098201723\n",
      "Epoch: 11099 Training Loss: 0.21726471735287803 Test Loss: 0.21379427197405623\n",
      "Epoch: 11100 Training Loss: 0.2172649806784522 Test Loss: 0.21378127032560795\n",
      "Epoch: 11101 Training Loss: 0.217264511542917 Test Loss: 0.21376926680969258\n",
      "Epoch: 11102 Training Loss: 0.2172640524132157 Test Loss: 0.21373291923018212\n",
      "Epoch: 11103 Training Loss: 0.21726384414662522 Test Loss: 0.21383203249442498\n",
      "Epoch: 11104 Training Loss: 0.2172638056833391 Test Loss: 0.21374634864971595\n",
      "Epoch: 11105 Training Loss: 0.2172633389596044 Test Loss: 0.2136964290603098\n",
      "Epoch: 11106 Training Loss: 0.21726308196388572 Test Loss: 0.2140489253975523\n",
      "Epoch: 11107 Training Loss: 0.21726663026030743 Test Loss: 0.21376488539675587\n",
      "Epoch: 11108 Training Loss: 0.21726584575272354 Test Loss: 0.21373643213818758\n",
      "Epoch: 11109 Training Loss: 0.21726676296312744 Test Loss: 0.21541866137620128\n",
      "Epoch: 11110 Training Loss: 0.21726608284436408 Test Loss: 0.21378042774619704\n",
      "Epoch: 11111 Training Loss: 0.217266616811606 Test Loss: 0.213685086645163\n",
      "Epoch: 11112 Training Loss: 0.21726489822894657 Test Loss: 0.21376715387978523\n",
      "Epoch: 11113 Training Loss: 0.21726416709477578 Test Loss: 0.21373533030357333\n",
      "Epoch: 11114 Training Loss: 0.2172631248921407 Test Loss: 0.2137351358621708\n",
      "Epoch: 11115 Training Loss: 0.21726425210850048 Test Loss: 0.2137641465194263\n",
      "Epoch: 11116 Training Loss: 0.2172648581607821 Test Loss: 0.21370569743382978\n",
      "Epoch: 11117 Training Loss: 0.21726453898723375 Test Loss: 0.21370377894532494\n",
      "Epoch: 11118 Training Loss: 0.21726478292874624 Test Loss: 0.21403622189258786\n",
      "Epoch: 11119 Training Loss: 0.2172646140399536 Test Loss: 0.21368493109204098\n",
      "Epoch: 11120 Training Loss: 0.21726643532586298 Test Loss: 0.21464779195454364\n",
      "Epoch: 11121 Training Loss: 0.2172666412972084 Test Loss: 0.2138153753476094\n",
      "Epoch: 11122 Training Loss: 0.21726480788953612 Test Loss: 0.21372869337036743\n",
      "Epoch: 11123 Training Loss: 0.21726477808721373 Test Loss: 0.21372156385227514\n",
      "Epoch: 11124 Training Loss: 0.2172640196163158 Test Loss: 0.21372520638788228\n",
      "Epoch: 11125 Training Loss: 0.2172675919769473 Test Loss: 0.21375543554459356\n",
      "Epoch: 11126 Training Loss: 0.2172646795978902 Test Loss: 0.21378314992583228\n",
      "Epoch: 11127 Training Loss: 0.2172644093776151 Test Loss: 0.2137560966453621\n",
      "Epoch: 11128 Training Loss: 0.217263193211544 Test Loss: 0.21380656067069528\n",
      "Epoch: 11129 Training Loss: 0.2172634510679796 Test Loss: 0.21389352782866095\n",
      "Epoch: 11130 Training Loss: 0.2172640029130286 Test Loss: 0.21463061629732133\n",
      "Epoch: 11131 Training Loss: 0.21726686335320078 Test Loss: 0.21387891879795184\n",
      "Epoch: 11132 Training Loss: 0.21726453166217435 Test Loss: 0.21382531778465808\n",
      "Epoch: 11133 Training Loss: 0.21726188862474385 Test Loss: 0.21443739339426038\n",
      "Epoch: 11134 Training Loss: 0.21726669153259118 Test Loss: 0.21379521825554848\n",
      "Epoch: 11135 Training Loss: 0.2172642737698756 Test Loss: 0.21375964844164808\n",
      "Epoch: 11136 Training Loss: 0.21726530652255643 Test Loss: 0.21373910246678215\n",
      "Epoch: 11137 Training Loss: 0.2172655120097486 Test Loss: 0.21383133250537592\n",
      "Epoch: 11138 Training Loss: 0.21726401352853694 Test Loss: 0.213733217373666\n",
      "Epoch: 11139 Training Loss: 0.217267116045335 Test Loss: 0.21374522088958134\n",
      "Epoch: 11140 Training Loss: 0.2172639664222187 Test Loss: 0.21376073731350217\n",
      "Epoch: 11141 Training Loss: 0.21726356487985696 Test Loss: 0.21377414080751567\n",
      "Epoch: 11142 Training Loss: 0.21726496248684204 Test Loss: 0.21372799338131837\n",
      "Epoch: 11143 Training Loss: 0.2172655802394939 Test Loss: 0.21373713212723663\n",
      "Epoch: 11144 Training Loss: 0.2172647452096216 Test Loss: 0.21404790133949905\n",
      "Epoch: 11145 Training Loss: 0.21726449295681163 Test Loss: 0.21370821220930233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11146 Training Loss: 0.21726660040419024 Test Loss: 0.21368268853453196\n",
      "Epoch: 11147 Training Loss: 0.21726502116800933 Test Loss: 0.21373407291583704\n",
      "Epoch: 11148 Training Loss: 0.21726419918337742 Test Loss: 0.21375704292685435\n",
      "Epoch: 11149 Training Loss: 0.21726315186127 Test Loss: 0.21378189253809599\n",
      "Epoch: 11150 Training Loss: 0.217263606319789 Test Loss: 0.21403969591231284\n",
      "Epoch: 11151 Training Loss: 0.21726442800854948 Test Loss: 0.21378517211641845\n",
      "Epoch: 11152 Training Loss: 0.21726612706369441 Test Loss: 0.2139078646414065\n",
      "Epoch: 11153 Training Loss: 0.2172635455854533 Test Loss: 0.2138409119851399\n",
      "Epoch: 11154 Training Loss: 0.21726470454971425 Test Loss: 0.21382600481094696\n",
      "Epoch: 11155 Training Loss: 0.21726306001560497 Test Loss: 0.21373798766940771\n",
      "Epoch: 11156 Training Loss: 0.21726289825462405 Test Loss: 0.21398263384205427\n",
      "Epoch: 11157 Training Loss: 0.21726228229513236 Test Loss: 0.21375994658513195\n",
      "Epoch: 11158 Training Loss: 0.2172621227666359 Test Loss: 0.2137321285018119\n",
      "Epoch: 11159 Training Loss: 0.21726412106435364 Test Loss: 0.21380726065974434\n",
      "Epoch: 11160 Training Loss: 0.21726373292586434 Test Loss: 0.2137939997560927\n",
      "Epoch: 11161 Training Loss: 0.21726317800554557 Test Loss: 0.21377990923579032\n",
      "Epoch: 11162 Training Loss: 0.2172648358180061 Test Loss: 0.21379149794338032\n",
      "Epoch: 11163 Training Loss: 0.2172646769798763 Test Loss: 0.21365109828800308\n",
      "Epoch: 11164 Training Loss: 0.2172647751822942 Test Loss: 0.21377473709448339\n",
      "Epoch: 11165 Training Loss: 0.21726452768135873 Test Loss: 0.21407288057834237\n",
      "Epoch: 11166 Training Loss: 0.21726489069767377 Test Loss: 0.21388092802577785\n",
      "Epoch: 11167 Training Loss: 0.21726466112834023 Test Loss: 0.21381144763127855\n",
      "Epoch: 11168 Training Loss: 0.2172628041764746 Test Loss: 0.21371492691906924\n",
      "Epoch: 11169 Training Loss: 0.21726605432415122 Test Loss: 0.21371166030350697\n",
      "Epoch: 11170 Training Loss: 0.21726379382158445 Test Loss: 0.2136910754403605\n",
      "Epoch: 11171 Training Loss: 0.21726540849957657 Test Loss: 0.21369272171090184\n",
      "Epoch: 11172 Training Loss: 0.21726536640514105 Test Loss: 0.21371998239553466\n",
      "Epoch: 11173 Training Loss: 0.21726404623577886 Test Loss: 0.21368660328810263\n",
      "Epoch: 11174 Training Loss: 0.217265083569984 Test Loss: 0.21377408895647498\n",
      "Epoch: 11175 Training Loss: 0.2172642861785441 Test Loss: 0.21376839830476133\n",
      "Epoch: 11176 Training Loss: 0.2172643148870388 Test Loss: 0.21385015443313954\n",
      "Epoch: 11177 Training Loss: 0.21726587211217835 Test Loss: 0.21391229790538388\n",
      "Epoch: 11178 Training Loss: 0.2172627761493808 Test Loss: 0.2141879769258721\n",
      "Epoch: 11179 Training Loss: 0.21726450847661308 Test Loss: 0.21471256686710205\n",
      "Epoch: 11180 Training Loss: 0.21726670435368656 Test Loss: 0.21386148388552617\n",
      "Epoch: 11181 Training Loss: 0.21726492431046157 Test Loss: 0.2138047069959913\n",
      "Epoch: 11182 Training Loss: 0.21726522974840284 Test Loss: 0.21373969875374987\n",
      "Epoch: 11183 Training Loss: 0.21726345171351724 Test Loss: 0.22008656427537962\n",
      "Epoch: 11184 Training Loss: 0.21726702774116136 Test Loss: 0.21408160451593528\n",
      "Epoch: 11185 Training Loss: 0.21726548515717475 Test Loss: 0.21376296690825103\n",
      "Epoch: 11186 Training Loss: 0.21726486135260722 Test Loss: 0.2137229508676131\n",
      "Epoch: 11187 Training Loss: 0.21726479705884855 Test Loss: 0.21369762163424524\n",
      "Epoch: 11188 Training Loss: 0.21726422612560928 Test Loss: 0.21374689308564299\n",
      "Epoch: 11189 Training Loss: 0.21726458623700481 Test Loss: 0.21368207928480407\n",
      "Epoch: 11190 Training Loss: 0.21726587351980908 Test Loss: 0.21373535622909365\n",
      "Epoch: 11191 Training Loss: 0.21726487776002298 Test Loss: 0.21367414607558138\n",
      "Epoch: 11192 Training Loss: 0.21726504328664029 Test Loss: 0.21381785123480143\n",
      "Epoch: 11193 Training Loss: 0.21726396326625677 Test Loss: 0.21394489924720586\n",
      "Epoch: 11194 Training Loss: 0.21726225801574336 Test Loss: 0.2139112479218103\n",
      "Epoch: 11195 Training Loss: 0.2172647461599965 Test Loss: 0.21383265470691304\n",
      "Epoch: 11196 Training Loss: 0.21726557649178913 Test Loss: 0.21377074456435172\n",
      "Epoch: 11197 Training Loss: 0.2172643973634418 Test Loss: 0.21374579125102872\n",
      "Epoch: 11198 Training Loss: 0.21726555980643353 Test Loss: 0.21371948981064828\n",
      "Epoch: 11199 Training Loss: 0.2172658548619773 Test Loss: 0.21369439390696346\n",
      "Epoch: 11200 Training Loss: 0.2172648282867333 Test Loss: 0.21371400656309733\n",
      "Epoch: 11201 Training Loss: 0.21726661516189863 Test Loss: 0.21375310224776337\n",
      "Epoch: 11202 Training Loss: 0.217264935732892 Test Loss: 0.21365971852351465\n",
      "Epoch: 11203 Training Loss: 0.21726708592024377 Test Loss: 0.21374634864971595\n",
      "Epoch: 11204 Training Loss: 0.2172659193171204 Test Loss: 0.2136577222584488\n",
      "Epoch: 11205 Training Loss: 0.21726407632500686 Test Loss: 0.21405040315221144\n",
      "Epoch: 11206 Training Loss: 0.21726458775222518 Test Loss: 0.21433085246844139\n",
      "Epoch: 11207 Training Loss: 0.21726428760410646 Test Loss: 0.21481853743147367\n",
      "Epoch: 11208 Training Loss: 0.2172654754741097 Test Loss: 0.21412513346457868\n",
      "Epoch: 11209 Training Loss: 0.21726409173721872 Test Loss: 0.21383791758754114\n",
      "Epoch: 11210 Training Loss: 0.21726631658279508 Test Loss: 0.21377100381955505\n",
      "Epoch: 11211 Training Loss: 0.21726438097395764 Test Loss: 0.21366381475572768\n",
      "Epoch: 11212 Training Loss: 0.21726537765722126 Test Loss: 0.213757017001334\n",
      "Epoch: 11213 Training Loss: 0.21726402384817384 Test Loss: 0.21366522769658597\n",
      "Epoch: 11214 Training Loss: 0.21726641910672906 Test Loss: 0.21380089594450197\n",
      "Epoch: 11215 Training Loss: 0.21726405198285725 Test Loss: 0.21365586858374483\n",
      "Epoch: 11216 Training Loss: 0.21726659470194085 Test Loss: 0.2136816255881982\n",
      "Epoch: 11217 Training Loss: 0.21726501022076636 Test Loss: 0.21367746454218434\n",
      "Epoch: 11218 Training Loss: 0.21726428555990385 Test Loss: 0.21366123516645427\n",
      "Epoch: 11219 Training Loss: 0.21726611961311382 Test Loss: 0.2137171954020986\n",
      "Epoch: 11220 Training Loss: 0.21726501734857812 Test Loss: 0.21371586023780131\n",
      "Epoch: 11221 Training Loss: 0.21726547211193437 Test Loss: 0.21396598965799885\n",
      "Epoch: 11222 Training Loss: 0.21726024258438 Test Loss: 0.2149570963749071\n",
      "Epoch: 11223 Training Loss: 0.21726568802635304 Test Loss: 0.21397161549591165\n",
      "Epoch: 11224 Training Loss: 0.21726365857247698 Test Loss: 0.21375202633866944\n",
      "Epoch: 11225 Training Loss: 0.21726595704521082 Test Loss: 0.21380413663454392\n",
      "Epoch: 11226 Training Loss: 0.21726472439103178 Test Loss: 0.21387758363365456\n",
      "Epoch: 11227 Training Loss: 0.21726367705995855 Test Loss: 0.21381743642647605\n",
      "Epoch: 11228 Training Loss: 0.21726414988940373 Test Loss: 0.21368104226399065\n",
      "Epoch: 11229 Training Loss: 0.2172646498672942 Test Loss: 0.21375390593889376\n",
      "Epoch: 11230 Training Loss: 0.2172651761329131 Test Loss: 0.21371812872083068\n",
      "Epoch: 11231 Training Loss: 0.21726510240713182 Test Loss: 0.21372919891801395\n",
      "Epoch: 11232 Training Loss: 0.21726482937159522 Test Loss: 0.21374533755442285\n",
      "Epoch: 11233 Training Loss: 0.21726435732217472 Test Loss: 0.21370760295957444\n",
      "Epoch: 11234 Training Loss: 0.21726553022825618 Test Loss: 0.2137469319739235\n",
      "Epoch: 11235 Training Loss: 0.2172651572867995 Test Loss: 0.2137014974995354\n",
      "Epoch: 11236 Training Loss: 0.2172670019644836 Test Loss: 0.21433870790110307\n",
      "Epoch: 11237 Training Loss: 0.21726497794388291 Test Loss: 0.2137149009935489\n",
      "Epoch: 11238 Training Loss: 0.21726542372350657 Test Loss: 0.213723871223585\n",
      "Epoch: 11239 Training Loss: 0.2172655652397089 Test Loss: 0.21381191429064458\n",
      "Epoch: 11240 Training Loss: 0.2172647488497368 Test Loss: 0.21367019243373023\n",
      "Epoch: 11241 Training Loss: 0.2172652294077024 Test Loss: 0.2136614296078568\n",
      "Epoch: 11242 Training Loss: 0.2172645338318982 Test Loss: 0.2138014144549087\n",
      "Epoch: 11243 Training Loss: 0.21726377681346004 Test Loss: 0.21382280300918552\n",
      "Epoch: 11244 Training Loss: 0.21726630022020835 Test Loss: 0.2138133920453037\n",
      "Epoch: 11245 Training Loss: 0.21726502504123535 Test Loss: 0.21375804105938728\n",
      "Epoch: 11246 Training Loss: 0.21726498500893407 Test Loss: 0.21373877839777797\n",
      "Epoch: 11247 Training Loss: 0.2172658912541634 Test Loss: 0.21365807225297334\n",
      "Epoch: 11248 Training Loss: 0.21726587772476974 Test Loss: 0.21381229021068945\n",
      "Epoch: 11249 Training Loss: 0.2172649200517061 Test Loss: 0.21385174885264016\n",
      "Epoch: 11250 Training Loss: 0.21726525275464809 Test Loss: 0.21380120705074598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11251 Training Loss: 0.21726415473093624 Test Loss: 0.2138082847177976\n",
      "Epoch: 11252 Training Loss: 0.21726416618026406 Test Loss: 0.21437718137328104\n",
      "Epoch: 11253 Training Loss: 0.21726415254328083 Test Loss: 0.2140256313175308\n",
      "Epoch: 11254 Training Loss: 0.21726449086778 Test Loss: 0.21379585343079668\n",
      "Epoch: 11255 Training Loss: 0.21726528417081464 Test Loss: 0.21377972775714799\n",
      "Epoch: 11256 Training Loss: 0.2172657378851722 Test Loss: 0.21371917870440427\n",
      "Epoch: 11257 Training Loss: 0.21726521171817711 Test Loss: 0.21373704138791547\n",
      "Epoch: 11258 Training Loss: 0.21726447246099065 Test Loss: 0.2137863646903539\n",
      "Epoch: 11259 Training Loss: 0.21726484629902743 Test Loss: 0.2138431675054091\n",
      "Epoch: 11260 Training Loss: 0.2172651545522302 Test Loss: 0.21380483662359298\n",
      "Epoch: 11261 Training Loss: 0.21726453031730422 Test Loss: 0.21365700930663958\n",
      "Epoch: 11262 Training Loss: 0.2172636969819683 Test Loss: 0.21378142587872995\n",
      "Epoch: 11263 Training Loss: 0.21726408712879702 Test Loss: 0.21374557088410587\n",
      "Epoch: 11264 Training Loss: 0.21726442609883387 Test Loss: 0.2136855273790087\n",
      "Epoch: 11265 Training Loss: 0.21726504369010133 Test Loss: 0.2137098325543233\n",
      "Epoch: 11266 Training Loss: 0.2172655464563559 Test Loss: 0.21374401535288573\n",
      "Epoch: 11267 Training Loss: 0.21726525606302866 Test Loss: 0.21377892406601756\n",
      "Epoch: 11268 Training Loss: 0.2172652126506204 Test Loss: 0.2136380966395548\n",
      "Epoch: 11269 Training Loss: 0.21726457192758647 Test Loss: 0.21377294823358023\n",
      "Epoch: 11270 Training Loss: 0.21726472510829586 Test Loss: 0.21372481750507724\n",
      "Epoch: 11271 Training Loss: 0.21726525295189572 Test Loss: 0.21368775697375755\n",
      "Epoch: 11272 Training Loss: 0.21726378912350475 Test Loss: 0.21379429789957657\n",
      "Epoch: 11273 Training Loss: 0.2172641123495951 Test Loss: 0.2137543725982598\n",
      "Epoch: 11274 Training Loss: 0.21726434084303256 Test Loss: 0.21398888189245513\n",
      "Epoch: 11275 Training Loss: 0.21726326188061354 Test Loss: 0.21403801075349102\n",
      "Epoch: 11276 Training Loss: 0.2172654939884887 Test Loss: 0.2138097754352169\n",
      "Epoch: 11277 Training Loss: 0.21726421351072733 Test Loss: 0.21370278081279204\n",
      "Epoch: 11278 Training Loss: 0.21726612714438662 Test Loss: 0.21431807118691595\n",
      "Epoch: 11279 Training Loss: 0.2172645329532497 Test Loss: 0.21374290055551132\n",
      "Epoch: 11280 Training Loss: 0.21726418499051448 Test Loss: 0.2137482152871801\n",
      "Epoch: 11281 Training Loss: 0.21726586565680164 Test Loss: 0.2137591947450422\n",
      "Epoch: 11282 Training Loss: 0.2172654004662189 Test Loss: 0.21371035106473002\n",
      "Epoch: 11283 Training Loss: 0.2172632389281631 Test Loss: 0.21362618386296062\n",
      "Epoch: 11284 Training Loss: 0.21726756517816823 Test Loss: 0.2137554225818334\n",
      "Epoch: 11285 Training Loss: 0.2172654538665294 Test Loss: 0.21374715234084635\n",
      "Epoch: 11286 Training Loss: 0.2172654321334279 Test Loss: 0.2136920476473731\n",
      "Epoch: 11287 Training Loss: 0.21726493413697942 Test Loss: 0.21376163174395377\n",
      "Epoch: 11288 Training Loss: 0.21726464297259326 Test Loss: 0.21372916002973347\n",
      "Epoch: 11289 Training Loss: 0.21726522877113053 Test Loss: 0.2137137343451338\n",
      "Epoch: 11290 Training Loss: 0.21726490829754105 Test Loss: 0.2139170422756053\n",
      "Epoch: 11291 Training Loss: 0.21726518572632011 Test Loss: 0.21392036074220824\n",
      "Epoch: 11292 Training Loss: 0.21726428771169606 Test Loss: 0.21382091044620102\n",
      "Epoch: 11293 Training Loss: 0.21726365607998432 Test Loss: 0.21387168557777822\n",
      "Epoch: 11294 Training Loss: 0.2172647161604265 Test Loss: 0.21388523166215356\n",
      "Epoch: 11295 Training Loss: 0.2172655426369247 Test Loss: 0.2137700316125425\n",
      "Epoch: 11296 Training Loss: 0.21726545522036536 Test Loss: 0.21380895878132633\n",
      "Epoch: 11297 Training Loss: 0.21726679523104508 Test Loss: 0.2137910183212541\n",
      "Epoch: 11298 Training Loss: 0.21726513799239583 Test Loss: 0.2138058217933657\n",
      "Epoch: 11299 Training Loss: 0.21726453206563542 Test Loss: 0.2138002737320139\n",
      "Epoch: 11300 Training Loss: 0.21726496651248667 Test Loss: 0.2138564543345811\n",
      "Epoch: 11301 Training Loss: 0.2172612243754482 Test Loss: 0.21472914623735664\n",
      "Epoch: 11302 Training Loss: 0.21726289465037207 Test Loss: 0.21405911412704418\n",
      "Epoch: 11303 Training Loss: 0.2172640466751031 Test Loss: 0.21392024407736673\n",
      "Epoch: 11304 Training Loss: 0.21726370670089654 Test Loss: 0.21377812037488717\n",
      "Epoch: 11305 Training Loss: 0.21726333033450387 Test Loss: 0.21372223791580386\n",
      "Epoch: 11306 Training Loss: 0.21726445984610868 Test Loss: 0.21719729466158277\n",
      "Epoch: 11307 Training Loss: 0.2172649712643612 Test Loss: 0.21381514201792637\n",
      "Epoch: 11308 Training Loss: 0.21726372071444341 Test Loss: 0.21376642796521583\n",
      "Epoch: 11309 Training Loss: 0.2172641851250015 Test Loss: 0.2136939402103576\n",
      "Epoch: 11310 Training Loss: 0.21726511104119814 Test Loss: 0.21369808829361128\n",
      "Epoch: 11311 Training Loss: 0.21726553989338962 Test Loss: 0.2136612610919746\n",
      "Epoch: 11312 Training Loss: 0.21726572978905392 Test Loss: 0.21365135754320644\n",
      "Epoch: 11313 Training Loss: 0.2172645114711906 Test Loss: 0.21371134919726292\n",
      "Epoch: 11314 Training Loss: 0.2172659594390797 Test Loss: 0.21368591626181374\n",
      "Epoch: 11315 Training Loss: 0.21726432205071375 Test Loss: 0.21371619726956567\n",
      "Epoch: 11316 Training Loss: 0.217265371363229 Test Loss: 0.21389534261508442\n",
      "Epoch: 11317 Training Loss: 0.21726421448799965 Test Loss: 0.21373149332656366\n",
      "Epoch: 11318 Training Loss: 0.21726540733402244 Test Loss: 0.21372401381394684\n",
      "Epoch: 11319 Training Loss: 0.21726415794965878 Test Loss: 0.21365015200651083\n",
      "Epoch: 11320 Training Loss: 0.2172648957274881 Test Loss: 0.2136649684413826\n",
      "Epoch: 11321 Training Loss: 0.21726462472718833 Test Loss: 0.21364814277868482\n",
      "Epoch: 11322 Training Loss: 0.21726425251196152 Test Loss: 0.21368122374263301\n",
      "Epoch: 11323 Training Loss: 0.21726305081669317 Test Loss: 0.21368486627824015\n",
      "Epoch: 11324 Training Loss: 0.21726783181212295 Test Loss: 0.21384850816259823\n",
      "Epoch: 11325 Training Loss: 0.21726419988270987 Test Loss: 0.21376577982720743\n",
      "Epoch: 11326 Training Loss: 0.21726544983191898 Test Loss: 0.21377257231353536\n",
      "Epoch: 11327 Training Loss: 0.21726364871009593 Test Loss: 0.21382753441664676\n",
      "Epoch: 11328 Training Loss: 0.21726252916846178 Test Loss: 0.21374951156319688\n",
      "Epoch: 11329 Training Loss: 0.21726284401152826 Test Loss: 0.21371806390702983\n",
      "Epoch: 11330 Training Loss: 0.21726107016367172 Test Loss: 0.2138050310649955\n",
      "Epoch: 11331 Training Loss: 0.2172638084089426 Test Loss: 0.21400574644343343\n",
      "Epoch: 11332 Training Loss: 0.21726244849418475 Test Loss: 0.21396832295482904\n",
      "Epoch: 11333 Training Loss: 0.21726155280170314 Test Loss: 0.21392562362283635\n",
      "Epoch: 11334 Training Loss: 0.2172660688935778 Test Loss: 0.21373955616338802\n",
      "Epoch: 11335 Training Loss: 0.21726351780940192 Test Loss: 0.21372015091141686\n",
      "Epoch: 11336 Training Loss: 0.21726402409025047 Test Loss: 0.2137334118150685\n",
      "Epoch: 11337 Training Loss: 0.2172640429542957 Test Loss: 0.21370042159044148\n",
      "Epoch: 11338 Training Loss: 0.2172642223779045 Test Loss: 0.21370081047324652\n",
      "Epoch: 11339 Training Loss: 0.21726550689924207 Test Loss: 0.21373704138791547\n",
      "Epoch: 11340 Training Loss: 0.21726525339121996 Test Loss: 0.21374379498596288\n",
      "Epoch: 11341 Training Loss: 0.21726341232675364 Test Loss: 0.2172028038346541\n",
      "Epoch: 11342 Training Loss: 0.21726575409534032 Test Loss: 0.21387423924153126\n",
      "Epoch: 11343 Training Loss: 0.21726500011630867 Test Loss: 0.21374306907139348\n",
      "Epoch: 11344 Training Loss: 0.2172643348000827 Test Loss: 0.2137301451995062\n",
      "Epoch: 11345 Training Loss: 0.21726768202945212 Test Loss: 0.21410030977885738\n",
      "Epoch: 11346 Training Loss: 0.21726197924209414 Test Loss: 0.21429261232594643\n",
      "Epoch: 11347 Training Loss: 0.2172640373237727 Test Loss: 0.2140383866735359\n",
      "Epoch: 11348 Training Loss: 0.2172634137612818 Test Loss: 0.213917483009451\n",
      "Epoch: 11349 Training Loss: 0.2172639110315004 Test Loss: 0.2138312547288149\n",
      "Epoch: 11350 Training Loss: 0.21726554243967708 Test Loss: 0.21370712333744823\n",
      "Epoch: 11351 Training Loss: 0.21726460998741154 Test Loss: 0.21377714816787458\n",
      "Epoch: 11352 Training Loss: 0.2172643019762854 Test Loss: 0.2137724037976532\n",
      "Epoch: 11353 Training Loss: 0.21726391173083287 Test Loss: 0.21380089594450197\n",
      "Epoch: 11354 Training Loss: 0.2172637168053542 Test Loss: 0.2137423042685436\n",
      "Epoch: 11355 Training Loss: 0.21726605990087944 Test Loss: 0.21376847608132235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11356 Training Loss: 0.21726510994737042 Test Loss: 0.21382400854588113\n",
      "Epoch: 11357 Training Loss: 0.2172641030341279 Test Loss: 0.21375840401667198\n",
      "Epoch: 11358 Training Loss: 0.21726328705658265 Test Loss: 0.21376822978887916\n",
      "Epoch: 11359 Training Loss: 0.2172656323935581 Test Loss: 0.21387585958655225\n",
      "Epoch: 11360 Training Loss: 0.21726470825259006 Test Loss: 0.21377088715471354\n",
      "Epoch: 11361 Training Loss: 0.21726426309160665 Test Loss: 0.21383811202894368\n",
      "Epoch: 11362 Training Loss: 0.21726371232245373 Test Loss: 0.21379297569803946\n",
      "Epoch: 11363 Training Loss: 0.21726438568100315 Test Loss: 0.21379835524350907\n",
      "Epoch: 11364 Training Loss: 0.2172639112825428 Test Loss: 0.21372623044593556\n",
      "Epoch: 11365 Training Loss: 0.21726455938443093 Test Loss: 0.21369963086207125\n",
      "Epoch: 11366 Training Loss: 0.2172630930814789 Test Loss: 0.21374904490383084\n",
      "Epoch: 11367 Training Loss: 0.21726413341922737 Test Loss: 0.2137407746628438\n",
      "Epoch: 11368 Training Loss: 0.21726392546643994 Test Loss: 0.2137283044875624\n",
      "Epoch: 11369 Training Loss: 0.21726483243789912 Test Loss: 0.2138033329434135\n",
      "Epoch: 11370 Training Loss: 0.21726409676703304 Test Loss: 0.21374922638247318\n",
      "Epoch: 11371 Training Loss: 0.21726529453528057 Test Loss: 0.21379248311315308\n",
      "Epoch: 11372 Training Loss: 0.2172642663820556 Test Loss: 0.2136707239068971\n",
      "Epoch: 11373 Training Loss: 0.21726464454160843 Test Loss: 0.21370353265288175\n",
      "Epoch: 11374 Training Loss: 0.21726445380315884 Test Loss: 0.2139412178233182\n",
      "Epoch: 11375 Training Loss: 0.21726363654350403 Test Loss: 0.21371137512278326\n",
      "Epoch: 11376 Training Loss: 0.217265018209295 Test Loss: 0.21377171677136428\n",
      "Epoch: 11377 Training Loss: 0.21726493627980584 Test Loss: 0.21378050552275804\n",
      "Epoch: 11378 Training Loss: 0.21726380588955255 Test Loss: 0.21373400810203622\n",
      "Epoch: 11379 Training Loss: 0.21726513655786767 Test Loss: 0.21388249651975816\n",
      "Epoch: 11380 Training Loss: 0.21726372692774348 Test Loss: 0.21375174115794573\n",
      "Epoch: 11381 Training Loss: 0.21726557977327227 Test Loss: 0.21378768689189098\n",
      "Epoch: 11382 Training Loss: 0.21726422826843572 Test Loss: 0.2137767463223094\n",
      "Epoch: 11383 Training Loss: 0.2172655360291294 Test Loss: 0.2137390765412618\n",
      "Epoch: 11384 Training Loss: 0.2172660625547565 Test Loss: 0.2137125288084382\n",
      "Epoch: 11385 Training Loss: 0.21726250450354334 Test Loss: 0.21397941907753265\n",
      "Epoch: 11386 Training Loss: 0.21726371749572088 Test Loss: 0.21375697811305353\n",
      "Epoch: 11387 Training Loss: 0.21726361535731636 Test Loss: 0.21377244268593368\n",
      "Epoch: 11388 Training Loss: 0.21726400880355984 Test Loss: 0.21374562273514655\n",
      "Epoch: 11389 Training Loss: 0.21726365913732243 Test Loss: 0.21375954473956674\n",
      "Epoch: 11390 Training Loss: 0.21726524210327655 Test Loss: 0.21378589803098785\n",
      "Epoch: 11391 Training Loss: 0.21726434729840924 Test Loss: 0.21370389561016645\n",
      "Epoch: 11392 Training Loss: 0.21726614990855525 Test Loss: 0.21369108840312068\n",
      "Epoch: 11393 Training Loss: 0.21726427507888252 Test Loss: 0.21376859274616386\n",
      "Epoch: 11394 Training Loss: 0.2172638503868227 Test Loss: 0.21373350255438966\n",
      "Epoch: 11395 Training Loss: 0.2172639672829356 Test Loss: 0.213741176508409\n",
      "Epoch: 11396 Training Loss: 0.21726482107822934 Test Loss: 0.21367365349069503\n",
      "Epoch: 11397 Training Loss: 0.21726669388163103 Test Loss: 0.21374878564862748\n",
      "Epoch: 11398 Training Loss: 0.21726500003561647 Test Loss: 0.2137537244602514\n",
      "Epoch: 11399 Training Loss: 0.2172663735335628 Test Loss: 0.21383135843089626\n",
      "Epoch: 11400 Training Loss: 0.21726417256391437 Test Loss: 0.2137686964482452\n",
      "Epoch: 11401 Training Loss: 0.2172630268690388 Test Loss: 0.21378396657972285\n",
      "Epoch: 11402 Training Loss: 0.2172644508175471 Test Loss: 0.2139524435736235\n",
      "Epoch: 11403 Training Loss: 0.21726343878483226 Test Loss: 0.2138258233323046\n",
      "Epoch: 11404 Training Loss: 0.21726339619727772 Test Loss: 0.21375093746681534\n",
      "Epoch: 11405 Training Loss: 0.21726385534491063 Test Loss: 0.21372877114692843\n",
      "Epoch: 11406 Training Loss: 0.2172612635201352 Test Loss: 0.2137040382005283\n",
      "Epoch: 11407 Training Loss: 0.2172624555682017 Test Loss: 0.21378475730809307\n",
      "Epoch: 11408 Training Loss: 0.21726336278173755 Test Loss: 0.21388298910464454\n",
      "Epoch: 11409 Training Loss: 0.2172657943248892 Test Loss: 0.21373776730248487\n",
      "Epoch: 11410 Training Loss: 0.21726474158743803 Test Loss: 0.2136837514808657\n",
      "Epoch: 11411 Training Loss: 0.2172653707625203 Test Loss: 0.21363808367679463\n",
      "Epoch: 11412 Training Loss: 0.2172664392618496 Test Loss: 0.21370979366604279\n",
      "Epoch: 11413 Training Loss: 0.21726479906718796 Test Loss: 0.21366141664509664\n",
      "Epoch: 11414 Training Loss: 0.21726301348309798 Test Loss: 0.21374506533645932\n",
      "Epoch: 11415 Training Loss: 0.21726543308380278 Test Loss: 0.2141761808141194\n",
      "Epoch: 11416 Training Loss: 0.21726491386530344 Test Loss: 0.2138514118208758\n",
      "Epoch: 11417 Training Loss: 0.21726565216314922 Test Loss: 0.2138002348437334\n",
      "Epoch: 11418 Training Loss: 0.21726368980036173 Test Loss: 0.21372431195743072\n",
      "Epoch: 11419 Training Loss: 0.21726631335510674 Test Loss: 0.21378018145375385\n",
      "Epoch: 11420 Training Loss: 0.21726532609489993 Test Loss: 0.213754852220386\n",
      "Epoch: 11421 Training Loss: 0.21726480358595165 Test Loss: 0.21380278850748646\n",
      "Epoch: 11422 Training Loss: 0.21726364524929676 Test Loss: 0.2137835776969178\n",
      "Epoch: 11423 Training Loss: 0.21726445408109868 Test Loss: 0.21375564294875624\n",
      "Epoch: 11424 Training Loss: 0.21726345451981297 Test Loss: 0.21367581827164303\n",
      "Epoch: 11425 Training Loss: 0.21726482401004624 Test Loss: 0.2138086087868018\n",
      "Epoch: 11426 Training Loss: 0.21726384267623386 Test Loss: 0.21373426735723958\n",
      "Epoch: 11427 Training Loss: 0.21726551616988027 Test Loss: 0.21377157418100246\n",
      "Epoch: 11428 Training Loss: 0.2172634921941086 Test Loss: 0.21376828163991982\n",
      "Epoch: 11429 Training Loss: 0.2172646386062482 Test Loss: 0.21373378773511337\n",
      "Epoch: 11430 Training Loss: 0.21726463557580747 Test Loss: 0.21373564140981735\n",
      "Epoch: 11431 Training Loss: 0.2172653249293458 Test Loss: 0.21373455253796325\n",
      "Epoch: 11432 Training Loss: 0.21726461396822716 Test Loss: 0.21370834183690401\n",
      "Epoch: 11433 Training Loss: 0.21726451995283833 Test Loss: 0.2136787608182011\n",
      "Epoch: 11434 Training Loss: 0.21726588721955295 Test Loss: 0.21444045260565997\n",
      "Epoch: 11435 Training Loss: 0.21726269413023364 Test Loss: 0.21445769307668314\n",
      "Epoch: 11436 Training Loss: 0.21726497205335168 Test Loss: 0.21411415400671657\n",
      "Epoch: 11437 Training Loss: 0.2172624239099585 Test Loss: 0.2139305624344603\n",
      "Epoch: 11438 Training Loss: 0.21726517248383212 Test Loss: 0.2138026977681653\n",
      "Epoch: 11439 Training Loss: 0.21726526470606078 Test Loss: 0.2138032551668525\n",
      "Epoch: 11440 Training Loss: 0.21726347104378413 Test Loss: 0.2136866940274238\n",
      "Epoch: 11441 Training Loss: 0.21726505974785085 Test Loss: 0.21372216013924286\n",
      "Epoch: 11442 Training Loss: 0.2172657453626502 Test Loss: 0.21378058329931904\n",
      "Epoch: 11443 Training Loss: 0.21726518519733787 Test Loss: 0.21368529404932568\n",
      "Epoch: 11444 Training Loss: 0.21726678370999086 Test Loss: 0.21379152386890066\n",
      "Epoch: 11445 Training Loss: 0.21726319807997393 Test Loss: 0.21372394900014602\n",
      "Epoch: 11446 Training Loss: 0.21726547136777288 Test Loss: 0.2137635113441781\n",
      "Epoch: 11447 Training Loss: 0.21726459773116163 Test Loss: 0.2137269693232651\n",
      "Epoch: 11448 Training Loss: 0.21726439115910753 Test Loss: 0.21574691735193002\n",
      "Epoch: 11449 Training Loss: 0.2172644581695039 Test Loss: 0.21375889660155836\n",
      "Epoch: 11450 Training Loss: 0.2172651033126777 Test Loss: 0.21375365964645057\n",
      "Epoch: 11451 Training Loss: 0.2172653799435005 Test Loss: 0.21370217156306415\n",
      "Epoch: 11452 Training Loss: 0.21726511505787696 Test Loss: 0.21375373742301157\n",
      "Epoch: 11453 Training Loss: 0.21726498735797392 Test Loss: 0.21377932591158277\n",
      "Epoch: 11454 Training Loss: 0.2172648965613076 Test Loss: 0.21380630141549192\n",
      "Epoch: 11455 Training Loss: 0.21726451840175476 Test Loss: 0.2138175790168379\n",
      "Epoch: 11456 Training Loss: 0.2172648386960282 Test Loss: 0.21380014410441223\n",
      "Epoch: 11457 Training Loss: 0.2172651086652609 Test Loss: 0.2137792351722616\n",
      "Epoch: 11458 Training Loss: 0.21726537685926497 Test Loss: 0.21442818983454126\n",
      "Epoch: 11459 Training Loss: 0.2172615290512964 Test Loss: 0.21465996398634118\n",
      "Epoch: 11460 Training Loss: 0.21726472499174043 Test Loss: 0.21419657123586333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11461 Training Loss: 0.21726203443556485 Test Loss: 0.2139721988201192\n",
      "Epoch: 11462 Training Loss: 0.21726272608434824 Test Loss: 0.2138775577081342\n",
      "Epoch: 11463 Training Loss: 0.21726157433755705 Test Loss: 0.21376996679874163\n",
      "Epoch: 11464 Training Loss: 0.21726322600844392 Test Loss: 0.21364434468995566\n",
      "Epoch: 11465 Training Loss: 0.21726552924201806 Test Loss: 0.21382754737940693\n",
      "Epoch: 11466 Training Loss: 0.21726420892023723 Test Loss: 0.21367444421906526\n",
      "Epoch: 11467 Training Loss: 0.21726721350359143 Test Loss: 0.21368119781711267\n",
      "Epoch: 11468 Training Loss: 0.2172652425426008 Test Loss: 0.2136557908071838\n",
      "Epoch: 11469 Training Loss: 0.21726442113178016 Test Loss: 0.2137042844929715\n",
      "Epoch: 11470 Training Loss: 0.21726454095970996 Test Loss: 0.21378729800908597\n",
      "Epoch: 11471 Training Loss: 0.21726425663622995 Test Loss: 0.21381191429064458\n",
      "Epoch: 11472 Training Loss: 0.2172638571649682 Test Loss: 0.2138094902544932\n",
      "Epoch: 11473 Training Loss: 0.21726276194755206 Test Loss: 0.2139169904245646\n",
      "Epoch: 11474 Training Loss: 0.21726463830141096 Test Loss: 0.2137858721054675\n",
      "Epoch: 11475 Training Loss: 0.21726500208878488 Test Loss: 0.21371902315128224\n",
      "Epoch: 11476 Training Loss: 0.217264677670243 Test Loss: 0.2137794296136641\n",
      "Epoch: 11477 Training Loss: 0.21726514483330195 Test Loss: 0.21375154671654323\n",
      "Epoch: 11478 Training Loss: 0.21726313592904167 Test Loss: 0.2137454023682237\n",
      "Epoch: 11479 Training Loss: 0.21726479233387144 Test Loss: 0.21375612257088245\n",
      "Epoch: 11480 Training Loss: 0.21726345880546583 Test Loss: 0.21439873844344007\n",
      "Epoch: 11481 Training Loss: 0.21726510738315136 Test Loss: 0.2137844721273694\n",
      "Epoch: 11482 Training Loss: 0.2172640663460704 Test Loss: 0.2138315399095386\n",
      "Epoch: 11483 Training Loss: 0.21726411183854447 Test Loss: 0.21373027482710788\n",
      "Epoch: 11484 Training Loss: 0.21726548798140205 Test Loss: 0.21368754956959488\n",
      "Epoch: 11485 Training Loss: 0.2172646770695343 Test Loss: 0.21369222912601546\n",
      "Epoch: 11486 Training Loss: 0.21726549706375842 Test Loss: 0.21378597580754885\n",
      "Epoch: 11487 Training Loss: 0.21726519430659164 Test Loss: 0.2137384024777331\n",
      "Epoch: 11488 Training Loss: 0.21726491287009955 Test Loss: 0.21374148761465303\n",
      "Epoch: 11489 Training Loss: 0.2172642879537727 Test Loss: 0.21373593955330122\n",
      "Epoch: 11490 Training Loss: 0.2172654593625654 Test Loss: 0.21369597536370394\n",
      "Epoch: 11491 Training Loss: 0.21726452049078637 Test Loss: 0.2137163917109682\n",
      "Epoch: 11492 Training Loss: 0.2172663858436075 Test Loss: 0.2138283121822568\n",
      "Epoch: 11493 Training Loss: 0.21726612676782298 Test Loss: 0.2137388820998593\n",
      "Epoch: 11494 Training Loss: 0.21726426250882958 Test Loss: 0.21374630976143544\n",
      "Epoch: 11495 Training Loss: 0.217266437172818 Test Loss: 0.21379183497514467\n",
      "Epoch: 11496 Training Loss: 0.21726474563998005 Test Loss: 0.2137919905282667\n",
      "Epoch: 11497 Training Loss: 0.21726395127898088 Test Loss: 0.2137019771216616\n",
      "Epoch: 11498 Training Loss: 0.2172668889415967 Test Loss: 0.21370681223120422\n",
      "Epoch: 11499 Training Loss: 0.21726479345459657 Test Loss: 0.21372366381942232\n",
      "Epoch: 11500 Training Loss: 0.2172657238447279 Test Loss: 0.2138086087868018\n",
      "Epoch: 11501 Training Loss: 0.21726524316124107 Test Loss: 0.21372952298701817\n",
      "Epoch: 11502 Training Loss: 0.21726553490840428 Test Loss: 0.21378235919746202\n",
      "Epoch: 11503 Training Loss: 0.21726445778397446 Test Loss: 0.2136722146243164\n",
      "Epoch: 11504 Training Loss: 0.2172652827362865 Test Loss: 0.2137245582498739\n",
      "Epoch: 11505 Training Loss: 0.21726496162612516 Test Loss: 0.21374174686985636\n",
      "Epoch: 11506 Training Loss: 0.2172646245030433 Test Loss: 0.21387234667854677\n",
      "Epoch: 11507 Training Loss: 0.2172628211487358 Test Loss: 0.21377471116896304\n",
      "Epoch: 11508 Training Loss: 0.21726342996248413 Test Loss: 0.21370787517753798\n",
      "Epoch: 11509 Training Loss: 0.2172639393275682 Test Loss: 0.2137371191644765\n",
      "Epoch: 11510 Training Loss: 0.2172664082312125 Test Loss: 0.2138107476422295\n",
      "Epoch: 11511 Training Loss: 0.21726236366874188 Test Loss: 0.21373617288298424\n",
      "Epoch: 11512 Training Loss: 0.21726442982860708 Test Loss: 0.21378292955890943\n",
      "Epoch: 11513 Training Loss: 0.21726524933867794 Test Loss: 0.2138401860705705\n",
      "Epoch: 11514 Training Loss: 0.21726381634367645 Test Loss: 0.21371946388512797\n",
      "Epoch: 11515 Training Loss: 0.21726444700708172 Test Loss: 0.21369737534180205\n",
      "Epoch: 11516 Training Loss: 0.21726495530523549 Test Loss: 0.21371302139332457\n",
      "Epoch: 11517 Training Loss: 0.21726433436075845 Test Loss: 0.21371736391798077\n",
      "Epoch: 11518 Training Loss: 0.21726435513451928 Test Loss: 0.2136988530964612\n",
      "Epoch: 11519 Training Loss: 0.21726568113165212 Test Loss: 0.213700538255283\n",
      "Epoch: 11520 Training Loss: 0.21726495429209997 Test Loss: 0.2137385580308551\n",
      "Epoch: 11521 Training Loss: 0.21726538410363214 Test Loss: 0.2137128139891619\n",
      "Epoch: 11522 Training Loss: 0.21726033498592467 Test Loss: 0.2145527360342333\n",
      "Epoch: 11523 Training Loss: 0.21726655733248243 Test Loss: 0.21389341116381944\n",
      "Epoch: 11524 Training Loss: 0.21726539427085043 Test Loss: 0.21383611576387782\n",
      "Epoch: 11525 Training Loss: 0.21726467083830264 Test Loss: 0.21371936018304663\n",
      "Epoch: 11526 Training Loss: 0.21726491001897483 Test Loss: 0.21370232711618614\n",
      "Epoch: 11527 Training Loss: 0.21726520161371943 Test Loss: 0.21378487397293458\n",
      "Epoch: 11528 Training Loss: 0.2172634242333373 Test Loss: 0.21373999689723372\n",
      "Epoch: 11529 Training Loss: 0.21726495196995751 Test Loss: 0.21412711676688437\n",
      "Epoch: 11530 Training Loss: 0.21726237553049654 Test Loss: 0.21429206789001937\n",
      "Epoch: 11531 Training Loss: 0.217262160916119 Test Loss: 0.21391814411021956\n",
      "Epoch: 11532 Training Loss: 0.2172648284750151 Test Loss: 0.21390157770272514\n",
      "Epoch: 11533 Training Loss: 0.21726284950756425 Test Loss: 0.21369773829908675\n",
      "Epoch: 11534 Training Loss: 0.21726541235487096 Test Loss: 0.21373556363325635\n",
      "Epoch: 11535 Training Loss: 0.2172647609356365 Test Loss: 0.21375840401667198\n",
      "Epoch: 11536 Training Loss: 0.21726607855871122 Test Loss: 0.21371662504065123\n",
      "Epoch: 11537 Training Loss: 0.21726570498068268 Test Loss: 0.21368407554986993\n",
      "Epoch: 11538 Training Loss: 0.2172649883352462 Test Loss: 0.21374682827184216\n",
      "Epoch: 11539 Training Loss: 0.21726465405432327 Test Loss: 0.21376307061033237\n",
      "Epoch: 11540 Training Loss: 0.21726391919934507 Test Loss: 0.2137359006650207\n",
      "Epoch: 11541 Training Loss: 0.21726637131900994 Test Loss: 0.21610803392468408\n",
      "Epoch: 11542 Training Loss: 0.2172652910655156 Test Loss: 0.21381632162910164\n",
      "Epoch: 11543 Training Loss: 0.21726504640673902 Test Loss: 0.21378838688094007\n",
      "Epoch: 11544 Training Loss: 0.21726570981324939 Test Loss: 0.21372324901109696\n",
      "Epoch: 11545 Training Loss: 0.21726528235972287 Test Loss: 0.21368609774045608\n",
      "Epoch: 11546 Training Loss: 0.21726534195540184 Test Loss: 0.21373825988737125\n",
      "Epoch: 11547 Training Loss: 0.21726363793320316 Test Loss: 0.21376628537485398\n",
      "Epoch: 11548 Training Loss: 0.2172638448280261 Test Loss: 0.21378219068157986\n",
      "Epoch: 11549 Training Loss: 0.21726578977922814 Test Loss: 0.2137605299093395\n",
      "Epoch: 11550 Training Loss: 0.21726597709274179 Test Loss: 0.21367694603177764\n",
      "Epoch: 11551 Training Loss: 0.217266280280267 Test Loss: 0.2137994700408835\n",
      "Epoch: 11552 Training Loss: 0.21726446309172864 Test Loss: 0.21378724615804529\n",
      "Epoch: 11553 Training Loss: 0.21726448668075096 Test Loss: 0.21377556671113412\n",
      "Epoch: 11554 Training Loss: 0.21726396758777283 Test Loss: 0.2138121476203276\n",
      "Epoch: 11555 Training Loss: 0.21726411531727524 Test Loss: 0.21384386749445816\n",
      "Epoch: 11556 Training Loss: 0.21726262222450993 Test Loss: 0.21376444466291017\n",
      "Epoch: 11557 Training Loss: 0.21726479716643818 Test Loss: 0.2138929704299737\n",
      "Epoch: 11558 Training Loss: 0.21726455825474 Test Loss: 0.21368524219828502\n",
      "Epoch: 11559 Training Loss: 0.2172652246558279 Test Loss: 0.2136982438467333\n",
      "Epoch: 11560 Training Loss: 0.21726518286622962 Test Loss: 0.21380339775721435\n",
      "Epoch: 11561 Training Loss: 0.21726400696557063 Test Loss: 0.21367453495838643\n",
      "Epoch: 11562 Training Loss: 0.21726504278455544 Test Loss: 0.2138069884417808\n",
      "Epoch: 11563 Training Loss: 0.21726461591380597 Test Loss: 0.21386691528203647\n",
      "Epoch: 11564 Training Loss: 0.21726506798742193 Test Loss: 0.21376518354023974\n",
      "Epoch: 11565 Training Loss: 0.217263640237414 Test Loss: 0.21377108159611607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11566 Training Loss: 0.21726366329745408 Test Loss: 0.21370967700120128\n",
      "Epoch: 11567 Training Loss: 0.2172660969296374 Test Loss: 0.21381099393467268\n",
      "Epoch: 11568 Training Loss: 0.21726541028377094 Test Loss: 0.21375358186988958\n",
      "Epoch: 11569 Training Loss: 0.2172638653507445 Test Loss: 0.2137248823188781\n",
      "Epoch: 11570 Training Loss: 0.2172629835373228 Test Loss: 0.21566781858938622\n",
      "Epoch: 11571 Training Loss: 0.21725714474773006 Test Loss: 0.21555709069203302\n",
      "Epoch: 11572 Training Loss: 0.2172601612824969 Test Loss: 0.2147402293973001\n",
      "Epoch: 11573 Training Loss: 0.217257987586815 Test Loss: 0.2153211684569794\n",
      "Epoch: 11574 Training Loss: 0.21726727650627475 Test Loss: 0.21404122551801263\n",
      "Epoch: 11575 Training Loss: 0.2172639718196309 Test Loss: 0.21391783300397552\n",
      "Epoch: 11576 Training Loss: 0.21726763434035684 Test Loss: 0.2137037919080851\n",
      "Epoch: 11577 Training Loss: 0.21726656607413838 Test Loss: 0.21373789693008655\n",
      "Epoch: 11578 Training Loss: 0.21726434469832698 Test Loss: 0.2137154583922361\n",
      "Epoch: 11579 Training Loss: 0.2172648917197751 Test Loss: 0.21374334128935701\n",
      "Epoch: 11580 Training Loss: 0.21726670081219518 Test Loss: 0.2137817629104943\n",
      "Epoch: 11581 Training Loss: 0.21726348182964267 Test Loss: 0.2146995911441741\n",
      "Epoch: 11582 Training Loss: 0.21726479956030703 Test Loss: 0.21385277291069343\n",
      "Epoch: 11583 Training Loss: 0.21726404551851478 Test Loss: 0.21373289330466178\n",
      "Epoch: 11584 Training Loss: 0.2172666459594249 Test Loss: 0.2137636668973001\n",
      "Epoch: 11585 Training Loss: 0.21726549864173939 Test Loss: 0.21377176862240496\n",
      "Epoch: 11586 Training Loss: 0.21726567767981875 Test Loss: 0.21372785079095652\n",
      "Epoch: 11587 Training Loss: 0.21726735906337 Test Loss: 0.21387184113090024\n",
      "Epoch: 11588 Training Loss: 0.21726546213299788 Test Loss: 0.21373447476140225\n",
      "Epoch: 11589 Training Loss: 0.21726591998058967 Test Loss: 0.21384700448241875\n",
      "Epoch: 11590 Training Loss: 0.21726466158559607 Test Loss: 0.2137396469027092\n",
      "Epoch: 11591 Training Loss: 0.2172643709950212 Test Loss: 0.2137080566561803\n",
      "Epoch: 11592 Training Loss: 0.21726553265798823 Test Loss: 0.21371475840318704\n",
      "Epoch: 11593 Training Loss: 0.21726530637910363 Test Loss: 0.21373431920828023\n",
      "Epoch: 11594 Training Loss: 0.21726532912534066 Test Loss: 0.2136963123954683\n",
      "Epoch: 11595 Training Loss: 0.21726728985635238 Test Loss: 0.21373609510642322\n",
      "Epoch: 11596 Training Loss: 0.21726525877966635 Test Loss: 0.2138002737320139\n",
      "Epoch: 11597 Training Loss: 0.21726411284271416 Test Loss: 0.2137513133868602\n",
      "Epoch: 11598 Training Loss: 0.2172650423631628 Test Loss: 0.2137727926804582\n",
      "Epoch: 11599 Training Loss: 0.21726397877709244 Test Loss: 0.21376870941100537\n",
      "Epoch: 11600 Training Loss: 0.21726311304831764 Test Loss: 0.2137052048489434\n",
      "Epoch: 11601 Training Loss: 0.2172649742948019 Test Loss: 0.21363750035258708\n",
      "Epoch: 11602 Training Loss: 0.21726500986213432 Test Loss: 0.21383523429618642\n",
      "Epoch: 11603 Training Loss: 0.21726644091155697 Test Loss: 0.21377440006271903\n",
      "Epoch: 11604 Training Loss: 0.21726565646673368 Test Loss: 0.21375763921382207\n",
      "Epoch: 11605 Training Loss: 0.21726482461972071 Test Loss: 0.21382617332682913\n",
      "Epoch: 11606 Training Loss: 0.2172642205488811 Test Loss: 0.21433083950568121\n",
      "Epoch: 11607 Training Loss: 0.21726474477926316 Test Loss: 0.2137933905063648\n",
      "Epoch: 11608 Training Loss: 0.2172629182931892 Test Loss: 0.21377603337050016\n",
      "Epoch: 11609 Training Loss: 0.21726453163527695 Test Loss: 0.21407014543594696\n",
      "Epoch: 11610 Training Loss: 0.21726279429616194 Test Loss: 0.21447330023992514\n",
      "Epoch: 11611 Training Loss: 0.2172628839003767 Test Loss: 0.21400684827804767\n",
      "Epoch: 11612 Training Loss: 0.21726368062834733 Test Loss: 0.21386162647588802\n",
      "Epoch: 11613 Training Loss: 0.21726589175624825 Test Loss: 0.21381437721507646\n",
      "Epoch: 11614 Training Loss: 0.21726438821832483 Test Loss: 0.21375744477241956\n",
      "Epoch: 11615 Training Loss: 0.21726507527661812 Test Loss: 0.2137357451118987\n",
      "Epoch: 11616 Training Loss: 0.21726398109923487 Test Loss: 0.21369171061560874\n",
      "Epoch: 11617 Training Loss: 0.21726423020504873 Test Loss: 0.21374969304183922\n",
      "Epoch: 11618 Training Loss: 0.2172623041089261 Test Loss: 0.21370393449844696\n",
      "Epoch: 11619 Training Loss: 0.2172632774542098 Test Loss: 0.21368872918077014\n",
      "Epoch: 11620 Training Loss: 0.2172630396811684 Test Loss: 0.21382096229724168\n",
      "Epoch: 11621 Training Loss: 0.21726368085249237 Test Loss: 0.21379804413726505\n",
      "Epoch: 11622 Training Loss: 0.21726205687696465 Test Loss: 0.21375459296518265\n",
      "Epoch: 11623 Training Loss: 0.21726436892392115 Test Loss: 0.2137534781678082\n",
      "Epoch: 11624 Training Loss: 0.21726382440393152 Test Loss: 0.2138221678339373\n",
      "Epoch: 11625 Training Loss: 0.21726360783500936 Test Loss: 0.21373754693556202\n",
      "Epoch: 11626 Training Loss: 0.217264244066177 Test Loss: 0.21374017837587608\n",
      "Epoch: 11627 Training Loss: 0.21726482132030595 Test Loss: 0.21367895525960365\n",
      "Epoch: 11628 Training Loss: 0.21726447255961445 Test Loss: 0.2136843218423131\n",
      "Epoch: 11629 Training Loss: 0.21726361355519036 Test Loss: 0.2141320426157481\n",
      "Epoch: 11630 Training Loss: 0.21726496158129616 Test Loss: 0.21382899920854573\n",
      "Epoch: 11631 Training Loss: 0.21726324781327183 Test Loss: 0.2137521170779906\n",
      "Epoch: 11632 Training Loss: 0.21726391347019824 Test Loss: 0.21381608829941862\n",
      "Epoch: 11633 Training Loss: 0.21726340497479685 Test Loss: 0.2137734019301861\n",
      "Epoch: 11634 Training Loss: 0.2172641254038013 Test Loss: 0.2137153028391141\n",
      "Epoch: 11635 Training Loss: 0.21726499002081678 Test Loss: 0.21377907961913958\n",
      "Epoch: 11636 Training Loss: 0.21726411078954574 Test Loss: 0.21364711872063158\n",
      "Epoch: 11637 Training Loss: 0.21726326091230705 Test Loss: 0.21372337863869864\n",
      "Epoch: 11638 Training Loss: 0.21726422800842748 Test Loss: 0.21375464481622333\n",
      "Epoch: 11639 Training Loss: 0.21726453311463412 Test Loss: 0.2137986274614726\n",
      "Epoch: 11640 Training Loss: 0.21726388721833303 Test Loss: 0.21375894845259902\n",
      "Epoch: 11641 Training Loss: 0.21726449399684453 Test Loss: 0.2143701555572701\n",
      "Epoch: 11642 Training Loss: 0.2172620496774265 Test Loss: 0.21442583061219073\n",
      "Epoch: 11643 Training Loss: 0.21726493141137593 Test Loss: 0.21405413642713975\n",
      "Epoch: 11644 Training Loss: 0.21726445105065795 Test Loss: 0.21391994593388286\n",
      "Epoch: 11645 Training Loss: 0.21726291653589222 Test Loss: 0.2137812444000876\n",
      "Epoch: 11646 Training Loss: 0.21726344961551983 Test Loss: 0.21376974643181879\n",
      "Epoch: 11647 Training Loss: 0.21726585177774177 Test Loss: 0.21375940214920489\n",
      "Epoch: 11648 Training Loss: 0.21726519146443274 Test Loss: 0.21370761592233462\n",
      "Epoch: 11649 Training Loss: 0.21726431454633835 Test Loss: 0.21373275071429992\n",
      "Epoch: 11650 Training Loss: 0.21726500102185456 Test Loss: 0.21370608631663482\n",
      "Epoch: 11651 Training Loss: 0.2172645804002684 Test Loss: 0.2137083807251845\n",
      "Epoch: 11652 Training Loss: 0.2172658960777643 Test Loss: 0.21375923363332272\n",
      "Epoch: 11653 Training Loss: 0.21726467376115377 Test Loss: 0.21369858087849766\n",
      "Epoch: 11654 Training Loss: 0.21726542156274856 Test Loss: 0.21373503216008946\n",
      "Epoch: 11655 Training Loss: 0.21726493078376985 Test Loss: 0.21372174533091748\n",
      "Epoch: 11656 Training Loss: 0.21726392022144636 Test Loss: 0.21367673862761496\n",
      "Epoch: 11657 Training Loss: 0.2172649910967129 Test Loss: 0.21383828054482584\n",
      "Epoch: 11658 Training Loss: 0.2172633613023804 Test Loss: 0.2138192252873792\n",
      "Epoch: 11659 Training Loss: 0.21726725222688575 Test Loss: 0.2138074810266672\n",
      "Epoch: 11660 Training Loss: 0.21726398091095306 Test Loss: 0.21376380948766194\n",
      "Epoch: 11661 Training Loss: 0.21726397155065685 Test Loss: 0.21372506379752043\n",
      "Epoch: 11662 Training Loss: 0.21726403494783544 Test Loss: 0.2137623706212833\n",
      "Epoch: 11663 Training Loss: 0.2172657801051289 Test Loss: 0.21379934041328183\n",
      "Epoch: 11664 Training Loss: 0.21726294954797135 Test Loss: 0.21680683039980886\n",
      "Epoch: 11665 Training Loss: 0.21726565515772675 Test Loss: 0.2137901368535627\n",
      "Epoch: 11666 Training Loss: 0.2172656752052577 Test Loss: 0.21370405116328847\n",
      "Epoch: 11667 Training Loss: 0.21726353173329083 Test Loss: 0.21375327076364553\n",
      "Epoch: 11668 Training Loss: 0.21726450827936547 Test Loss: 0.21378034996963602\n",
      "Epoch: 11669 Training Loss: 0.21726380982553917 Test Loss: 0.21379762932893967\n",
      "Epoch: 11670 Training Loss: 0.21726443538740367 Test Loss: 0.2138045903311498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11671 Training Loss: 0.21726372445318243 Test Loss: 0.2137703427187865\n",
      "Epoch: 11672 Training Loss: 0.2172647041193558 Test Loss: 0.21374488385781698\n",
      "Epoch: 11673 Training Loss: 0.21726514218839066 Test Loss: 0.2141149836233673\n",
      "Epoch: 11674 Training Loss: 0.21726564210352053 Test Loss: 0.21376666129489885\n",
      "Epoch: 11675 Training Loss: 0.21726416887000435 Test Loss: 0.21375337446572687\n",
      "Epoch: 11676 Training Loss: 0.21726604341277148 Test Loss: 0.21386488012869012\n",
      "Epoch: 11677 Training Loss: 0.21726455793197116 Test Loss: 0.2137886850244239\n",
      "Epoch: 11678 Training Loss: 0.2172646945887094 Test Loss: 0.21380501810223532\n",
      "Epoch: 11679 Training Loss: 0.21726303269680944 Test Loss: 0.21375132634962038\n",
      "Epoch: 11680 Training Loss: 0.21726432782468957 Test Loss: 0.21378811466297654\n",
      "Epoch: 11681 Training Loss: 0.2172643430575854 Test Loss: 0.21378688320076059\n",
      "Epoch: 11682 Training Loss: 0.2172625658385877 Test Loss: 0.21408230450498433\n",
      "Epoch: 11683 Training Loss: 0.2172649554397225 Test Loss: 0.2138095809938144\n",
      "Epoch: 11684 Training Loss: 0.21726502333773315 Test Loss: 0.21380013114165206\n",
      "Epoch: 11685 Training Loss: 0.21726517200864465 Test Loss: 0.21376142433979106\n",
      "Epoch: 11686 Training Loss: 0.21726511602618348 Test Loss: 0.2162455817728244\n",
      "Epoch: 11687 Training Loss: 0.2172647309988271 Test Loss: 0.21375480036934533\n",
      "Epoch: 11688 Training Loss: 0.21726493426250063 Test Loss: 0.2137441968315281\n",
      "Epoch: 11689 Training Loss: 0.21726467994755644 Test Loss: 0.2137751648655689\n",
      "Epoch: 11690 Training Loss: 0.2172632930547035 Test Loss: 0.21370485485441887\n",
      "Epoch: 11691 Training Loss: 0.2172627912567554 Test Loss: 0.21371378619617448\n",
      "Epoch: 11692 Training Loss: 0.21726577902923278 Test Loss: 0.21383948608152145\n",
      "Epoch: 11693 Training Loss: 0.21726318324157334 Test Loss: 0.2137166380034114\n",
      "Epoch: 11694 Training Loss: 0.21726600607917626 Test Loss: 0.21373229701769406\n",
      "Epoch: 11695 Training Loss: 0.21726405181250705 Test Loss: 0.21386486716592998\n",
      "Epoch: 11696 Training Loss: 0.21726564850510244 Test Loss: 0.21367488495291095\n",
      "Epoch: 11697 Training Loss: 0.21726548256605827 Test Loss: 0.21372012498589651\n",
      "Epoch: 11698 Training Loss: 0.21726345174041467 Test Loss: 0.2136957938850616\n",
      "Epoch: 11699 Training Loss: 0.21726171621239143 Test Loss: 0.2137370284251553\n",
      "Epoch: 11700 Training Loss: 0.21726109362717283 Test Loss: 0.2138755484803082\n",
      "Epoch: 11701 Training Loss: 0.21726502626058428 Test Loss: 0.2140200314051383\n",
      "Epoch: 11702 Training Loss: 0.2172650404534472 Test Loss: 0.21382433261488532\n",
      "Epoch: 11703 Training Loss: 0.2172626862941236 Test Loss: 0.21374883749966817\n",
      "Epoch: 11704 Training Loss: 0.21726436152713538 Test Loss: 0.21376778905503346\n",
      "Epoch: 11705 Training Loss: 0.2172651359033642 Test Loss: 0.21372759153575316\n",
      "Epoch: 11706 Training Loss: 0.21726405562297246 Test Loss: 0.21366144257061698\n",
      "Epoch: 11707 Training Loss: 0.2172649068719787 Test Loss: 0.21420481555133003\n",
      "Epoch: 11708 Training Loss: 0.21726628803568482 Test Loss: 0.21376247432336465\n",
      "Epoch: 11709 Training Loss: 0.2172653713721948 Test Loss: 0.2137574706979399\n",
      "Epoch: 11710 Training Loss: 0.21726383051860776 Test Loss: 0.2137142658183007\n",
      "Epoch: 11711 Training Loss: 0.21726552700953364 Test Loss: 0.2137740500681945\n",
      "Epoch: 11712 Training Loss: 0.21726477659889076 Test Loss: 0.21372702117430578\n",
      "Epoch: 11713 Training Loss: 0.2172649498450627 Test Loss: 0.21369624758166747\n",
      "Epoch: 11714 Training Loss: 0.21726646675099534 Test Loss: 0.21368962361122174\n",
      "Epoch: 11715 Training Loss: 0.21726541989510958 Test Loss: 0.21398103942255361\n",
      "Epoch: 11716 Training Loss: 0.21726485677108295 Test Loss: 0.21409175435714664\n",
      "Epoch: 11717 Training Loss: 0.2172644461732622 Test Loss: 0.21390929054502494\n",
      "Epoch: 11718 Training Loss: 0.21726317874074125 Test Loss: 0.21423876502020947\n",
      "Epoch: 11719 Training Loss: 0.21726364121468633 Test Loss: 0.21373437105932092\n",
      "Epoch: 11720 Training Loss: 0.21726489138804045 Test Loss: 0.2137685797834037\n",
      "Epoch: 11721 Training Loss: 0.21726522357993178 Test Loss: 0.2137506263605713\n",
      "Epoch: 11722 Training Loss: 0.2172657747166825 Test Loss: 0.21376609093345147\n",
      "Epoch: 11723 Training Loss: 0.2172647600659538 Test Loss: 0.2137174416945418\n",
      "Epoch: 11724 Training Loss: 0.2172661185103203 Test Loss: 0.2137209805280676\n",
      "Epoch: 11725 Training Loss: 0.21726484169060573 Test Loss: 0.21374357461904003\n",
      "Epoch: 11726 Training Loss: 0.2172662878205056 Test Loss: 0.21377727779547626\n",
      "Epoch: 11727 Training Loss: 0.21726474556825365 Test Loss: 0.2137331266343448\n",
      "Epoch: 11728 Training Loss: 0.21726611933517398 Test Loss: 0.21369913827718487\n",
      "Epoch: 11729 Training Loss: 0.21726448122954398 Test Loss: 0.21377327230258442\n",
      "Epoch: 11730 Training Loss: 0.21726455042759577 Test Loss: 0.21445057652135102\n",
      "Epoch: 11731 Training Loss: 0.21726318876450673 Test Loss: 0.21443471010290566\n",
      "Epoch: 11732 Training Loss: 0.21726288043061176 Test Loss: 0.21409599317972153\n",
      "Epoch: 11733 Training Loss: 0.2172649498540285 Test Loss: 0.21408669888068121\n",
      "Epoch: 11734 Training Loss: 0.21726225658121523 Test Loss: 0.21387659846388182\n",
      "Epoch: 11735 Training Loss: 0.21726252126959114 Test Loss: 0.2137416561305352\n",
      "Epoch: 11736 Training Loss: 0.21726492144140525 Test Loss: 0.21371951573616862\n",
      "Epoch: 11737 Training Loss: 0.2172646161469168 Test Loss: 0.21373836358945258\n",
      "Epoch: 11738 Training Loss: 0.21726453074766267 Test Loss: 0.21371713058829775\n",
      "Epoch: 11739 Training Loss: 0.21726373656597953 Test Loss: 0.21370455671093502\n",
      "Epoch: 11740 Training Loss: 0.2172652220109166 Test Loss: 0.21421805052946136\n",
      "Epoch: 11741 Training Loss: 0.2172670282701436 Test Loss: 0.2137639520780238\n",
      "Epoch: 11742 Training Loss: 0.2172648985875786 Test Loss: 0.2139105608955214\n",
      "Epoch: 11743 Training Loss: 0.2172634090900995 Test Loss: 0.21383821573102502\n",
      "Epoch: 11744 Training Loss: 0.21726424763456578 Test Loss: 0.2138644523576046\n",
      "Epoch: 11745 Training Loss: 0.2172641710307624 Test Loss: 0.2137673612839479\n",
      "Epoch: 11746 Training Loss: 0.21726443409632834 Test Loss: 0.21376444466291017\n",
      "Epoch: 11747 Training Loss: 0.21726423557556349 Test Loss: 0.2143770387829192\n",
      "Epoch: 11748 Training Loss: 0.21726055537427805 Test Loss: 0.2147273573764535\n",
      "Epoch: 11749 Training Loss: 0.21726218595760108 Test Loss: 0.21405937338224754\n",
      "Epoch: 11750 Training Loss: 0.21726541777918054 Test Loss: 0.21395895087922773\n",
      "Epoch: 11751 Training Loss: 0.21726365165087863 Test Loss: 0.21455283973631464\n",
      "Epoch: 11752 Training Loss: 0.21726349016783758 Test Loss: 0.2138188623300945\n",
      "Epoch: 11753 Training Loss: 0.21726493064031704 Test Loss: 0.2137690594055299\n",
      "Epoch: 11754 Training Loss: 0.2172649675435538 Test Loss: 0.2137645483649915\n",
      "Epoch: 11755 Training Loss: 0.21726356501434396 Test Loss: 0.213700616031844\n",
      "Epoch: 11756 Training Loss: 0.21726516056828263 Test Loss: 0.2137881665140172\n",
      "Epoch: 11757 Training Loss: 0.21726417494881742 Test Loss: 0.21372537490376448\n",
      "Epoch: 11758 Training Loss: 0.2172655443762901 Test Loss: 0.21373805248320857\n",
      "Epoch: 11759 Training Loss: 0.21726510718590372 Test Loss: 0.21371758428490364\n",
      "Epoch: 11760 Training Loss: 0.21726525824171827 Test Loss: 0.2136917754294096\n",
      "Epoch: 11761 Training Loss: 0.2172644833275414 Test Loss: 0.2137107529102952\n",
      "Epoch: 11762 Training Loss: 0.21726396284486413 Test Loss: 0.2140099074894473\n",
      "Epoch: 11763 Training Loss: 0.21726559282747848 Test Loss: 0.21385401733566953\n",
      "Epoch: 11764 Training Loss: 0.2172623872487984 Test Loss: 0.21411117257187798\n",
      "Epoch: 11765 Training Loss: 0.21726568550696299 Test Loss: 0.21394159374336308\n",
      "Epoch: 11766 Training Loss: 0.21726427278363747 Test Loss: 0.21380319035305168\n",
      "Epoch: 11767 Training Loss: 0.2172631101254665 Test Loss: 0.2136404688246655\n",
      "Epoch: 11768 Training Loss: 0.21726576938203096 Test Loss: 0.2156280358784313\n",
      "Epoch: 11769 Training Loss: 0.21726331767479293 Test Loss: 0.21376369282282043\n",
      "Epoch: 11770 Training Loss: 0.21726484470311486 Test Loss: 0.2136372022091032\n",
      "Epoch: 11771 Training Loss: 0.2172636989544445 Test Loss: 0.21375424297065812\n",
      "Epoch: 11772 Training Loss: 0.21726353180501723 Test Loss: 0.2137232360483368\n",
      "Epoch: 11773 Training Loss: 0.2172654052449908 Test Loss: 0.2137496671163189\n",
      "Epoch: 11774 Training Loss: 0.21726404444261865 Test Loss: 0.21383288803659606\n",
      "Epoch: 11775 Training Loss: 0.21726405269115553 Test Loss: 0.21376536501888208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11776 Training Loss: 0.21726416786583466 Test Loss: 0.21374336721487736\n",
      "Epoch: 11777 Training Loss: 0.21726474295920556 Test Loss: 0.21380574401680472\n",
      "Epoch: 11778 Training Loss: 0.21726604803015895 Test Loss: 0.21370742148093208\n",
      "Epoch: 11779 Training Loss: 0.21726464997488382 Test Loss: 0.21366704248300944\n",
      "Epoch: 11780 Training Loss: 0.2172667113111481 Test Loss: 0.2138579709775207\n",
      "Epoch: 11781 Training Loss: 0.21726422507661058 Test Loss: 0.21377419265855632\n",
      "Epoch: 11782 Training Loss: 0.21726501975141277 Test Loss: 0.21378979982179835\n",
      "Epoch: 11783 Training Loss: 0.21726443439219975 Test Loss: 0.2136944198324838\n",
      "Epoch: 11784 Training Loss: 0.21726611525573455 Test Loss: 0.2136947827897685\n",
      "Epoch: 11785 Training Loss: 0.21726547949975436 Test Loss: 0.21377600744497982\n",
      "Epoch: 11786 Training Loss: 0.21726371540668926 Test Loss: 0.21371667689169188\n",
      "Epoch: 11787 Training Loss: 0.2172646676464775 Test Loss: 0.2137286674448471\n",
      "Epoch: 11788 Training Loss: 0.21726725966850055 Test Loss: 0.21378243697402305\n",
      "Epoch: 11789 Training Loss: 0.21726465919172722 Test Loss: 0.21382205116909578\n",
      "Epoch: 11790 Training Loss: 0.21726377911767086 Test Loss: 0.2137331266343448\n",
      "Epoch: 11791 Training Loss: 0.21726470631597705 Test Loss: 0.2136652147338258\n",
      "Epoch: 11792 Training Loss: 0.2172647524719204 Test Loss: 0.21368821067036345\n",
      "Epoch: 11793 Training Loss: 0.21726556360793314 Test Loss: 0.21370783628925746\n",
      "Epoch: 11794 Training Loss: 0.21726467868337848 Test Loss: 0.21371005292124615\n",
      "Epoch: 11795 Training Loss: 0.21726551215320145 Test Loss: 0.21384363416477514\n",
      "Epoch: 11796 Training Loss: 0.21726414271676298 Test Loss: 0.21379926263672083\n",
      "Epoch: 11797 Training Loss: 0.21726342578442087 Test Loss: 0.21376747794878942\n",
      "Epoch: 11798 Training Loss: 0.21726431119312878 Test Loss: 0.21381525868276788\n",
      "Epoch: 11799 Training Loss: 0.2172644852551886 Test Loss: 0.2137167546682529\n",
      "Epoch: 11800 Training Loss: 0.2172636599173471 Test Loss: 0.21373697657411464\n",
      "Epoch: 11801 Training Loss: 0.2172637643689283 Test Loss: 0.21368889769665234\n",
      "Epoch: 11802 Training Loss: 0.2172648675121125 Test Loss: 0.21372799338131837\n",
      "Epoch: 11803 Training Loss: 0.21726535051774176 Test Loss: 0.21376478169467453\n",
      "Epoch: 11804 Training Loss: 0.2172658156724613 Test Loss: 0.21370812146998114\n",
      "Epoch: 11805 Training Loss: 0.2172643432279356 Test Loss: 0.21370297525419454\n",
      "Epoch: 11806 Training Loss: 0.2172639503913666 Test Loss: 0.21384406193586067\n",
      "Epoch: 11807 Training Loss: 0.21725950060159005 Test Loss: 0.21467828036645828\n",
      "Epoch: 11808 Training Loss: 0.2172696159796164 Test Loss: 0.2138931778341364\n",
      "Epoch: 11809 Training Loss: 0.21726504140382208 Test Loss: 0.2138199512019486\n",
      "Epoch: 11810 Training Loss: 0.21726453602851944 Test Loss: 0.21394510665136854\n",
      "Epoch: 11811 Training Loss: 0.21726472415792095 Test Loss: 0.21366763876997716\n",
      "Epoch: 11812 Training Loss: 0.2172646187828623 Test Loss: 0.2137396469027092\n",
      "Epoch: 11813 Training Loss: 0.2172641588372731 Test Loss: 0.2137712630747584\n",
      "Epoch: 11814 Training Loss: 0.2172651741425053 Test Loss: 0.21374147465189286\n",
      "Epoch: 11815 Training Loss: 0.217264427219559 Test Loss: 0.21374997822256292\n",
      "Epoch: 11816 Training Loss: 0.2172636934404769 Test Loss: 0.2137236767821825\n",
      "Epoch: 11817 Training Loss: 0.2172640251751124 Test Loss: 0.21504792643540274\n",
      "Epoch: 11818 Training Loss: 0.2172653069349833 Test Loss: 0.2137437042466417\n",
      "Epoch: 11819 Training Loss: 0.21726426579927852 Test Loss: 0.21374927823351386\n",
      "Epoch: 11820 Training Loss: 0.2172644600881853 Test Loss: 0.213781970314657\n",
      "Epoch: 11821 Training Loss: 0.2172653397767122 Test Loss: 0.21373131184792132\n",
      "Epoch: 11822 Training Loss: 0.21726467801094343 Test Loss: 0.21369211246117395\n",
      "Epoch: 11823 Training Loss: 0.2172647302456998 Test Loss: 0.21367918858928667\n",
      "Epoch: 11824 Training Loss: 0.21726641379897488 Test Loss: 0.21373041741746973\n",
      "Epoch: 11825 Training Loss: 0.21726491681505197 Test Loss: 0.21367469051150845\n",
      "Epoch: 11826 Training Loss: 0.21726373598320245 Test Loss: 0.21372904336489196\n",
      "Epoch: 11827 Training Loss: 0.2172652331912704 Test Loss: 0.2138740188746084\n",
      "Epoch: 11828 Training Loss: 0.21726449761006233 Test Loss: 0.2137931960649623\n",
      "Epoch: 11829 Training Loss: 0.2172637888545307 Test Loss: 0.21373916728058298\n",
      "Epoch: 11830 Training Loss: 0.21726480016998148 Test Loss: 0.21387956693596022\n",
      "Epoch: 11831 Training Loss: 0.21726549621200733 Test Loss: 0.21369802347981046\n",
      "Epoch: 11832 Training Loss: 0.2172655557538915 Test Loss: 0.21370056418080333\n",
      "Epoch: 11833 Training Loss: 0.21726511242193147 Test Loss: 0.21374103391804714\n",
      "Epoch: 11834 Training Loss: 0.21726485590140024 Test Loss: 0.21380221814603909\n",
      "Epoch: 11835 Training Loss: 0.21726220795071083 Test Loss: 0.21664642920549274\n",
      "Epoch: 11836 Training Loss: 0.21726594030606045 Test Loss: 0.21376279839236886\n",
      "Epoch: 11837 Training Loss: 0.2172647646654097 Test Loss: 0.21369093284999868\n",
      "Epoch: 11838 Training Loss: 0.21726434750462267 Test Loss: 0.2137097158894818\n",
      "Epoch: 11839 Training Loss: 0.21726577318353057 Test Loss: 0.21367046465169376\n",
      "Epoch: 11840 Training Loss: 0.217266489828967 Test Loss: 0.21371352694097112\n",
      "Epoch: 11841 Training Loss: 0.21726489038387073 Test Loss: 0.2137552151776707\n",
      "Epoch: 11842 Training Loss: 0.21726502418948423 Test Loss: 0.21367807379191223\n",
      "Epoch: 11843 Training Loss: 0.21726438107258145 Test Loss: 0.21375330965192604\n",
      "Epoch: 11844 Training Loss: 0.21726518215793134 Test Loss: 0.2150377247431507\n",
      "Epoch: 11845 Training Loss: 0.21726610323259546 Test Loss: 0.2137970200792118\n",
      "Epoch: 11846 Training Loss: 0.2172645647818431 Test Loss: 0.213774011179914\n",
      "Epoch: 11847 Training Loss: 0.21726545080919127 Test Loss: 0.21375254484907613\n",
      "Epoch: 11848 Training Loss: 0.21726415114461586 Test Loss: 0.21377691483819156\n",
      "Epoch: 11849 Training Loss: 0.2172647087188117 Test Loss: 0.21373496734628863\n",
      "Epoch: 11850 Training Loss: 0.21726448675247736 Test Loss: 0.2141586421996124\n",
      "Epoch: 11851 Training Loss: 0.21725979907310397 Test Loss: 0.21535269388970743\n",
      "Epoch: 11852 Training Loss: 0.2172646292369862 Test Loss: 0.2141788381799538\n",
      "Epoch: 11853 Training Loss: 0.21726292241745765 Test Loss: 0.21398907633385766\n",
      "Epoch: 11854 Training Loss: 0.21726151188178758 Test Loss: 0.21382105303656287\n",
      "Epoch: 11855 Training Loss: 0.21726281253260107 Test Loss: 0.2138278325601306\n",
      "Epoch: 11856 Training Loss: 0.21726324771464803 Test Loss: 0.21375026340328662\n",
      "Epoch: 11857 Training Loss: 0.21726605567798718 Test Loss: 0.21377309082394208\n",
      "Epoch: 11858 Training Loss: 0.21726469362040288 Test Loss: 0.2142249856061511\n",
      "Epoch: 11859 Training Loss: 0.21726493125895732 Test Loss: 0.21378537952058113\n",
      "Epoch: 11860 Training Loss: 0.21726510284645606 Test Loss: 0.21388342983849024\n",
      "Epoch: 11861 Training Loss: 0.21726458367278573 Test Loss: 0.21380028669477408\n",
      "Epoch: 11862 Training Loss: 0.2172647918497182 Test Loss: 0.2138051866181175\n",
      "Epoch: 11863 Training Loss: 0.21726551777475864 Test Loss: 0.2137185564919162\n",
      "Epoch: 11864 Training Loss: 0.21726588978377204 Test Loss: 0.2136885865904083\n",
      "Epoch: 11865 Training Loss: 0.2172660250239137 Test Loss: 0.21371460285006505\n",
      "Epoch: 11866 Training Loss: 0.21726434236721873 Test Loss: 0.21369666238999283\n",
      "Epoch: 11867 Training Loss: 0.21726565357077995 Test Loss: 0.213735382154614\n",
      "Epoch: 11868 Training Loss: 0.21726461482894407 Test Loss: 0.21370727889057026\n",
      "Epoch: 11869 Training Loss: 0.21726406336045867 Test Loss: 0.21375575961359775\n",
      "Epoch: 11870 Training Loss: 0.21726553812712682 Test Loss: 0.21382730108696374\n",
      "Epoch: 11871 Training Loss: 0.21726498040051237 Test Loss: 0.2136913606210842\n",
      "Epoch: 11872 Training Loss: 0.21726552717091804 Test Loss: 0.21364921868777875\n",
      "Epoch: 11873 Training Loss: 0.21726413883457116 Test Loss: 0.21370071973392535\n",
      "Epoch: 11874 Training Loss: 0.21726577770229424 Test Loss: 0.21376561131132527\n",
      "Epoch: 11875 Training Loss: 0.21726410518592015 Test Loss: 0.21376132063770972\n",
      "Epoch: 11876 Training Loss: 0.2172649956961688 Test Loss: 0.21373056000783158\n",
      "Epoch: 11877 Training Loss: 0.21726427350986735 Test Loss: 0.21419955267070193\n",
      "Epoch: 11878 Training Loss: 0.21726386012368254 Test Loss: 0.21378054441103855\n",
      "Epoch: 11879 Training Loss: 0.21726641470452077 Test Loss: 0.21369377169447543\n",
      "Epoch: 11880 Training Loss: 0.21726589197142746 Test Loss: 0.21369224208877563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11881 Training Loss: 0.21726534102295852 Test Loss: 0.2137903442577254\n",
      "Epoch: 11882 Training Loss: 0.21726346583465375 Test Loss: 0.21369198283357227\n",
      "Epoch: 11883 Training Loss: 0.2172649507147454 Test Loss: 0.2137310785182383\n",
      "Epoch: 11884 Training Loss: 0.2172636355662317 Test Loss: 0.21368997360574626\n",
      "Epoch: 11885 Training Loss: 0.21726480566601747 Test Loss: 0.21363953550593343\n",
      "Epoch: 11886 Training Loss: 0.2172647377321436 Test Loss: 0.2136732127568493\n",
      "Epoch: 11887 Training Loss: 0.2172647939566814 Test Loss: 0.21367259054436127\n",
      "Epoch: 11888 Training Loss: 0.21726650795781655 Test Loss: 0.2138174105009557\n",
      "Epoch: 11889 Training Loss: 0.2172645778898441 Test Loss: 0.21377519079108925\n",
      "Epoch: 11890 Training Loss: 0.2172642028145268 Test Loss: 0.21376746498602925\n",
      "Epoch: 11891 Training Loss: 0.21726506631978296 Test Loss: 0.2138319806433843\n",
      "Epoch: 11892 Training Loss: 0.21726421883641311 Test Loss: 0.21370724000228974\n",
      "Epoch: 11893 Training Loss: 0.21726453993760866 Test Loss: 0.2137107529102952\n",
      "Epoch: 11894 Training Loss: 0.21726476217291701 Test Loss: 0.2136957938850616\n",
      "Epoch: 11895 Training Loss: 0.21726300631942302 Test Loss: 0.21380850508472046\n",
      "Epoch: 11896 Training Loss: 0.21726464743756216 Test Loss: 0.21378518507917862\n",
      "Epoch: 11897 Training Loss: 0.21726429852445203 Test Loss: 0.21373558955877667\n",
      "Epoch: 11898 Training Loss: 0.21726519639562328 Test Loss: 0.2136739386714187\n",
      "Epoch: 11899 Training Loss: 0.21726619245128082 Test Loss: 0.2137206175707829\n",
      "Epoch: 11900 Training Loss: 0.21726421286518968 Test Loss: 0.2137374302707205\n",
      "Epoch: 11901 Training Loss: 0.2172642953953875 Test Loss: 0.21369115321692153\n",
      "Epoch: 11902 Training Loss: 0.21726413901388716 Test Loss: 0.21374336721487736\n",
      "Epoch: 11903 Training Loss: 0.21726588971204563 Test Loss: 0.21381313279010036\n",
      "Epoch: 11904 Training Loss: 0.2172641383683495 Test Loss: 0.2137713927023601\n",
      "Epoch: 11905 Training Loss: 0.21726410070301966 Test Loss: 0.2137114010483036\n",
      "Epoch: 11906 Training Loss: 0.21726371789918192 Test Loss: 0.21387675401700382\n",
      "Epoch: 11907 Training Loss: 0.21726581761804012 Test Loss: 0.21386423199068175\n",
      "Epoch: 11908 Training Loss: 0.21726457861607398 Test Loss: 0.21398136349155783\n",
      "Epoch: 11909 Training Loss: 0.21726188860681225 Test Loss: 0.213789773896278\n",
      "Epoch: 11910 Training Loss: 0.21726582344581075 Test Loss: 0.21381477906064167\n",
      "Epoch: 11911 Training Loss: 0.21726325805221655 Test Loss: 0.21372358604286132\n",
      "Epoch: 11912 Training Loss: 0.21726551497742874 Test Loss: 0.21367440533078474\n",
      "Epoch: 11913 Training Loss: 0.21726517970130188 Test Loss: 0.21372077312390492\n",
      "Epoch: 11914 Training Loss: 0.217265552804143 Test Loss: 0.21373106555547813\n",
      "Epoch: 11915 Training Loss: 0.21726519186789378 Test Loss: 0.213686784766745\n",
      "Epoch: 11916 Training Loss: 0.21726461268611763 Test Loss: 0.2137866757965979\n",
      "Epoch: 11917 Training Loss: 0.21726509974428893 Test Loss: 0.21376545575820324\n",
      "Epoch: 11918 Training Loss: 0.2172643833319633 Test Loss: 0.21642808447322662\n",
      "Epoch: 11919 Training Loss: 0.21726499526581036 Test Loss: 0.21370781036373712\n",
      "Epoch: 11920 Training Loss: 0.2172647534940217 Test Loss: 0.2137675297998301\n",
      "Epoch: 11921 Training Loss: 0.21726530926609153 Test Loss: 0.21379433678785706\n",
      "Epoch: 11922 Training Loss: 0.21726476453988847 Test Loss: 0.2137748019082842\n",
      "Epoch: 11923 Training Loss: 0.2172651499976033 Test Loss: 0.21379451826649942\n",
      "Epoch: 11924 Training Loss: 0.21726456339214395 Test Loss: 0.2137342543944794\n",
      "Epoch: 11925 Training Loss: 0.21726388630382135 Test Loss: 0.21375797624558643\n",
      "Epoch: 11926 Training Loss: 0.2172645358940324 Test Loss: 0.21372838226412338\n",
      "Epoch: 11927 Training Loss: 0.2172624291818495 Test Loss: 0.216589743055279\n",
      "Epoch: 11928 Training Loss: 0.21726461259645963 Test Loss: 0.21385598767521505\n",
      "Epoch: 11929 Training Loss: 0.21726236706678043 Test Loss: 0.21446999473608236\n",
      "Epoch: 11930 Training Loss: 0.21726424713248094 Test Loss: 0.2140698861807436\n",
      "Epoch: 11931 Training Loss: 0.21726224755265366 Test Loss: 0.2144213325344125\n",
      "Epoch: 11932 Training Loss: 0.2172643627195869 Test Loss: 0.21400504645438437\n",
      "Epoch: 11933 Training Loss: 0.21726263639047544 Test Loss: 0.21395459539181136\n",
      "Epoch: 11934 Training Loss: 0.21726570053364538 Test Loss: 0.21386563196877986\n",
      "Epoch: 11935 Training Loss: 0.21726372661394044 Test Loss: 0.213779961086831\n",
      "Epoch: 11936 Training Loss: 0.21726423305617343 Test Loss: 0.2139782783546379\n",
      "Epoch: 11937 Training Loss: 0.21726502610816564 Test Loss: 0.2137523633704338\n",
      "Epoch: 11938 Training Loss: 0.21726482416246484 Test Loss: 0.21379812191382605\n",
      "Epoch: 11939 Training Loss: 0.2172646822607331 Test Loss: 0.2136889884359735\n",
      "Epoch: 11940 Training Loss: 0.21726518410351014 Test Loss: 0.2138431675054091\n",
      "Epoch: 11941 Training Loss: 0.21726577346147039 Test Loss: 0.2137969293398906\n",
      "Epoch: 11942 Training Loss: 0.21726447436174043 Test Loss: 0.21375522814043088\n",
      "Epoch: 11943 Training Loss: 0.2172631391477642 Test Loss: 0.21374964119079856\n",
      "Epoch: 11944 Training Loss: 0.21726358123347791 Test Loss: 0.2136934216999509\n",
      "Epoch: 11945 Training Loss: 0.21726452809378557 Test Loss: 0.2137455190330652\n",
      "Epoch: 11946 Training Loss: 0.2172652267089963 Test Loss: 0.21370029196283982\n",
      "Epoch: 11947 Training Loss: 0.2172649180702641 Test Loss: 0.21365432601528486\n",
      "Epoch: 11948 Training Loss: 0.21726446313655765 Test Loss: 0.2137020289727023\n",
      "Epoch: 11949 Training Loss: 0.21726512792380134 Test Loss: 0.21374577828826855\n",
      "Epoch: 11950 Training Loss: 0.21726489733236648 Test Loss: 0.21379621638808138\n",
      "Epoch: 11951 Training Loss: 0.21726288405279534 Test Loss: 0.21457622455565734\n",
      "Epoch: 11952 Training Loss: 0.21726593013884216 Test Loss: 0.21396329340388393\n",
      "Epoch: 11953 Training Loss: 0.21726267407373687 Test Loss: 0.21400066504144766\n",
      "Epoch: 11954 Training Loss: 0.2172646169000441 Test Loss: 0.2138299325272778\n",
      "Epoch: 11955 Training Loss: 0.21726295989450564 Test Loss: 0.21383480652510087\n",
      "Epoch: 11956 Training Loss: 0.21726423145129506 Test Loss: 0.21382669183723585\n",
      "Epoch: 11957 Training Loss: 0.2172633175492717 Test Loss: 0.21687406823679914\n",
      "Epoch: 11958 Training Loss: 0.21726403328916227 Test Loss: 0.21378681838695976\n",
      "Epoch: 11959 Training Loss: 0.2172648714301675 Test Loss: 0.2136899087919454\n",
      "Epoch: 11960 Training Loss: 0.21726593085610624 Test Loss: 0.21369951419722974\n",
      "Epoch: 11961 Training Loss: 0.2172643589539505 Test Loss: 0.21370836776242433\n",
      "Epoch: 11962 Training Loss: 0.2172637802742592 Test Loss: 0.2137083807251845\n",
      "Epoch: 11963 Training Loss: 0.21726321490878234 Test Loss: 0.21371411026517867\n",
      "Epoch: 11964 Training Loss: 0.21726559816213004 Test Loss: 0.21371990461897367\n",
      "Epoch: 11965 Training Loss: 0.21726685030796036 Test Loss: 0.21375210411523043\n",
      "Epoch: 11966 Training Loss: 0.21726573203050417 Test Loss: 0.21367793120155038\n",
      "Epoch: 11967 Training Loss: 0.21726681197019548 Test Loss: 0.2138047847725523\n",
      "Epoch: 11968 Training Loss: 0.21726574308533675 Test Loss: 0.2137477745533344\n",
      "Epoch: 11969 Training Loss: 0.2172643179174795 Test Loss: 0.21373066370991292\n",
      "Epoch: 11970 Training Loss: 0.21726566557598745 Test Loss: 0.2136558426582245\n",
      "Epoch: 11971 Training Loss: 0.21726675452630873 Test Loss: 0.21370326043491825\n",
      "Epoch: 11972 Training Loss: 0.21726600048451647 Test Loss: 0.2137188027843594\n",
      "Epoch: 11973 Training Loss: 0.21726569465207995 Test Loss: 0.21367299238992646\n",
      "Epoch: 11974 Training Loss: 0.21726536714930253 Test Loss: 0.2136535093613943\n",
      "Epoch: 11975 Training Loss: 0.21726580082509492 Test Loss: 0.21380509587879631\n",
      "Epoch: 11976 Training Loss: 0.21726436608176225 Test Loss: 0.21369646794859032\n",
      "Epoch: 11977 Training Loss: 0.21726512858727062 Test Loss: 0.21383839720966735\n",
      "Epoch: 11978 Training Loss: 0.21726345667160518 Test Loss: 0.21419397868382978\n",
      "Epoch: 11979 Training Loss: 0.21726211686713887 Test Loss: 0.21441766407328502\n",
      "Epoch: 11980 Training Loss: 0.21726407801057745 Test Loss: 0.21401851476219869\n",
      "Epoch: 11981 Training Loss: 0.2172620050725667 Test Loss: 0.21905898035135923\n",
      "Epoch: 11982 Training Loss: 0.21726458668529486 Test Loss: 0.21404810874366173\n",
      "Epoch: 11983 Training Loss: 0.21726369569089296 Test Loss: 0.21373588770226054\n",
      "Epoch: 11984 Training Loss: 0.21726559529307374 Test Loss: 0.2137494986004367\n",
      "Epoch: 11985 Training Loss: 0.21726348621391933 Test Loss: 0.21378089440556308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11986 Training Loss: 0.2172635240406336 Test Loss: 0.21369425131660164\n",
      "Epoch: 11987 Training Loss: 0.21726662225384719 Test Loss: 0.2136577741094895\n",
      "Epoch: 11988 Training Loss: 0.2172641833587387 Test Loss: 0.21374323758727568\n",
      "Epoch: 11989 Training Loss: 0.2172644972245329 Test Loss: 0.213712126962873\n",
      "Epoch: 11990 Training Loss: 0.21726541407630476 Test Loss: 0.21370957329911994\n",
      "Epoch: 11991 Training Loss: 0.2172650284303081 Test Loss: 0.21656093980218621\n",
      "Epoch: 11992 Training Loss: 0.21726639357212793 Test Loss: 0.2137993144877615\n",
      "Epoch: 11993 Training Loss: 0.2172661503837427 Test Loss: 0.21375286891808035\n",
      "Epoch: 11994 Training Loss: 0.21726611339981375 Test Loss: 0.21372349530354015\n",
      "Epoch: 11995 Training Loss: 0.21726497851769416 Test Loss: 0.21372949706149783\n",
      "Epoch: 11996 Training Loss: 0.21726453800099566 Test Loss: 0.2137619817384783\n",
      "Epoch: 11997 Training Loss: 0.21726455173660272 Test Loss: 0.21372718969018795\n",
      "Epoch: 11998 Training Loss: 0.21726482963160343 Test Loss: 0.21374297833207231\n",
      "Epoch: 11999 Training Loss: 0.21726501703477508 Test Loss: 0.21380589956992674\n",
      "Epoch: 12000 Training Loss: 0.21726599643197445 Test Loss: 0.21378981278455852\n",
      "Epoch: 12001 Training Loss: 0.21726545479897272 Test Loss: 0.21396099899533425\n",
      "Epoch: 12002 Training Loss: 0.21726308343427705 Test Loss: 0.21384839149775672\n",
      "Epoch: 12003 Training Loss: 0.2172655483033109 Test Loss: 0.21382036601027396\n",
      "Epoch: 12004 Training Loss: 0.21726412191610475 Test Loss: 0.21376226691920197\n",
      "Epoch: 12005 Training Loss: 0.2172633948703392 Test Loss: 0.21374575236274823\n",
      "Epoch: 12006 Training Loss: 0.2172639279320352 Test Loss: 0.2137812444000876\n",
      "Epoch: 12007 Training Loss: 0.21726427268501367 Test Loss: 0.21373242664529574\n",
      "Epoch: 12008 Training Loss: 0.21726357564778392 Test Loss: 0.21404498471846128\n",
      "Epoch: 12009 Training Loss: 0.21726568787393444 Test Loss: 0.21372801930683868\n",
      "Epoch: 12010 Training Loss: 0.21726496739113518 Test Loss: 0.2138622357256159\n",
      "Epoch: 12011 Training Loss: 0.21726464576992316 Test Loss: 0.2137877257801715\n",
      "Epoch: 12012 Training Loss: 0.2172642467648831 Test Loss: 0.2137630576475722\n",
      "Epoch: 12013 Training Loss: 0.21726543987987992 Test Loss: 0.21377853518321255\n",
      "Epoch: 12014 Training Loss: 0.21726242148919225 Test Loss: 0.22074906502203462\n",
      "Epoch: 12015 Training Loss: 0.21726574729029738 Test Loss: 0.21431634713981362\n",
      "Epoch: 12016 Training Loss: 0.21726494577458907 Test Loss: 0.21382008082955029\n",
      "Epoch: 12017 Training Loss: 0.2172641159269497 Test Loss: 0.21374028207795742\n",
      "Epoch: 12018 Training Loss: 0.21726384905988416 Test Loss: 0.21378443323908888\n",
      "Epoch: 12019 Training Loss: 0.21726664526009243 Test Loss: 0.21381687902778884\n",
      "Epoch: 12020 Training Loss: 0.21726556235272101 Test Loss: 0.21374393757632473\n",
      "Epoch: 12021 Training Loss: 0.21726521966187676 Test Loss: 0.21381778642100058\n",
      "Epoch: 12022 Training Loss: 0.21726361725806617 Test Loss: 0.21375910400572104\n",
      "Epoch: 12023 Training Loss: 0.2172642589404408 Test Loss: 0.2137780685238465\n",
      "Epoch: 12024 Training Loss: 0.21726329645274206 Test Loss: 0.2140871136890066\n",
      "Epoch: 12025 Training Loss: 0.2172668028519759 Test Loss: 0.21385777653611818\n",
      "Epoch: 12026 Training Loss: 0.21726403969970995 Test Loss: 0.2137128528774424\n",
      "Epoch: 12027 Training Loss: 0.21726477647336956 Test Loss: 0.21361145816741\n",
      "Epoch: 12028 Training Loss: 0.21726518404074954 Test Loss: 0.2136786441533596\n",
      "Epoch: 12029 Training Loss: 0.21726273750677866 Test Loss: 0.21367323868236965\n",
      "Epoch: 12030 Training Loss: 0.2172647480517805 Test Loss: 0.2136428021214957\n",
      "Epoch: 12031 Training Loss: 0.21726597500371014 Test Loss: 0.21366354253776415\n",
      "Epoch: 12032 Training Loss: 0.21726484693559928 Test Loss: 0.2137498226694409\n",
      "Epoch: 12033 Training Loss: 0.2172660624113037 Test Loss: 0.21368550145348839\n",
      "Epoch: 12034 Training Loss: 0.2172648703363398 Test Loss: 0.2136806533811856\n",
      "Epoch: 12035 Training Loss: 0.21726449497411685 Test Loss: 0.21374583013930923\n",
      "Epoch: 12036 Training Loss: 0.21726439103358633 Test Loss: 0.21368465887407748\n",
      "Epoch: 12037 Training Loss: 0.2172646973950051 Test Loss: 0.21367660900001328\n",
      "Epoch: 12038 Training Loss: 0.2172628808789018 Test Loss: 0.21371173808006796\n",
      "Epoch: 12039 Training Loss: 0.2172640056475979 Test Loss: 0.21374256352374696\n",
      "Epoch: 12040 Training Loss: 0.21726258334879697 Test Loss: 0.2137993922643225\n",
      "Epoch: 12041 Training Loss: 0.21726351606107075 Test Loss: 0.21371794724218832\n",
      "Epoch: 12042 Training Loss: 0.21726543760256647 Test Loss: 0.21378530174402013\n",
      "Epoch: 12043 Training Loss: 0.21726328642001078 Test Loss: 0.21431440272578847\n",
      "Epoch: 12044 Training Loss: 0.21726272924927598 Test Loss: 0.21385943576941965\n",
      "Epoch: 12045 Training Loss: 0.21726482019958082 Test Loss: 0.21383583058315414\n",
      "Epoch: 12046 Training Loss: 0.2172633652204354 Test Loss: 0.21374404127840607\n",
      "Epoch: 12047 Training Loss: 0.21726550488193686 Test Loss: 0.213754618890703\n",
      "Epoch: 12048 Training Loss: 0.2172645656604916 Test Loss: 0.21372778597715567\n",
      "Epoch: 12049 Training Loss: 0.21726506781707172 Test Loss: 0.2137815684690918\n",
      "Epoch: 12050 Training Loss: 0.21726286518875013 Test Loss: 0.21381728087335405\n",
      "Epoch: 12051 Training Loss: 0.2172654025821479 Test Loss: 0.2137206175707829\n",
      "Epoch: 12052 Training Loss: 0.21726254848079704 Test Loss: 0.21365566117958215\n",
      "Epoch: 12053 Training Loss: 0.21726604232790955 Test Loss: 0.2137535300188489\n",
      "Epoch: 12054 Training Loss: 0.2172647237454941 Test Loss: 0.21365947223107146\n",
      "Epoch: 12055 Training Loss: 0.2172661962796778 Test Loss: 0.2137231064207351\n",
      "Epoch: 12056 Training Loss: 0.21726544694493108 Test Loss: 0.21372021572521768\n",
      "Epoch: 12057 Training Loss: 0.21726552604122712 Test Loss: 0.21367544235159816\n",
      "Epoch: 12058 Training Loss: 0.21726703595383504 Test Loss: 0.21378121847456727\n",
      "Epoch: 12059 Training Loss: 0.2172650986863244 Test Loss: 0.21380217925775857\n",
      "Epoch: 12060 Training Loss: 0.2172638626520384 Test Loss: 0.21425603141675295\n",
      "Epoch: 12061 Training Loss: 0.21726239122961402 Test Loss: 0.21451895508123606\n",
      "Epoch: 12062 Training Loss: 0.21726188560326895 Test Loss: 0.21410028385333704\n",
      "Epoch: 12063 Training Loss: 0.21726482234240727 Test Loss: 0.2138277158952891\n",
      "Epoch: 12064 Training Loss: 0.21726560854452756 Test Loss: 0.21380884211648482\n",
      "Epoch: 12065 Training Loss: 0.21726152489116476 Test Loss: 0.21462155532796406\n",
      "Epoch: 12066 Training Loss: 0.21726662406493896 Test Loss: 0.21382487705081235\n",
      "Epoch: 12067 Training Loss: 0.21726366974386496 Test Loss: 0.21383147509573777\n",
      "Epoch: 12068 Training Loss: 0.21726322300490059 Test Loss: 0.21374844861686312\n",
      "Epoch: 12069 Training Loss: 0.21726584584238154 Test Loss: 0.21375435963549963\n",
      "Epoch: 12070 Training Loss: 0.21726565232453363 Test Loss: 0.21372470084023573\n",
      "Epoch: 12071 Training Loss: 0.21726616318690647 Test Loss: 0.21381148651955903\n",
      "Epoch: 12072 Training Loss: 0.21726404784065723 Test Loss: 0.21380223110879926\n",
      "Epoch: 12073 Training Loss: 0.21726535353025087 Test Loss: 0.21381165503544122\n",
      "Epoch: 12074 Training Loss: 0.21726413076535028 Test Loss: 0.21368638292117978\n",
      "Epoch: 12075 Training Loss: 0.21726419172383102 Test Loss: 0.21369854199021715\n",
      "Epoch: 12076 Training Loss: 0.21726584385197373 Test Loss: 0.21371131030898244\n",
      "Epoch: 12077 Training Loss: 0.21726415127013707 Test Loss: 0.21374196723677924\n",
      "Epoch: 12078 Training Loss: 0.2172640981567322 Test Loss: 0.21376929273521292\n",
      "Epoch: 12079 Training Loss: 0.21726494080753533 Test Loss: 0.21375354298160906\n",
      "Epoch: 12080 Training Loss: 0.21726453722097097 Test Loss: 0.2136769201062573\n",
      "Epoch: 12081 Training Loss: 0.21726616757118314 Test Loss: 0.21368920880289635\n",
      "Epoch: 12082 Training Loss: 0.21726504005895195 Test Loss: 0.21368835326072527\n",
      "Epoch: 12083 Training Loss: 0.2172651592772073 Test Loss: 0.21361714881912366\n",
      "Epoch: 12084 Training Loss: 0.2172659494153142 Test Loss: 0.21368218298688543\n",
      "Epoch: 12085 Training Loss: 0.21726350793805507 Test Loss: 0.21728116371986833\n",
      "Epoch: 12086 Training Loss: 0.21726513583163778 Test Loss: 0.21382474742321067\n",
      "Epoch: 12087 Training Loss: 0.21726515416670075 Test Loss: 0.21374734678224885\n",
      "Epoch: 12088 Training Loss: 0.21726434716392223 Test Loss: 0.21376946125109508\n",
      "Epoch: 12089 Training Loss: 0.21726352067845822 Test Loss: 0.21377502227520706\n",
      "Epoch: 12090 Training Loss: 0.21726594493241375 Test Loss: 0.2137388432115788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12091 Training Loss: 0.21726375955429317 Test Loss: 0.21378570358958532\n",
      "Epoch: 12092 Training Loss: 0.21726513549990314 Test Loss: 0.21372311938349528\n",
      "Epoch: 12093 Training Loss: 0.21726632958320646 Test Loss: 0.21379310532564114\n",
      "Epoch: 12094 Training Loss: 0.217263986138015 Test Loss: 0.21381844752176915\n",
      "Epoch: 12095 Training Loss: 0.21726456148242837 Test Loss: 0.21378777763121218\n",
      "Epoch: 12096 Training Loss: 0.21726587873790526 Test Loss: 0.2138086087868018\n",
      "Epoch: 12097 Training Loss: 0.21726403147807047 Test Loss: 0.21382623814062998\n",
      "Epoch: 12098 Training Loss: 0.21726491688677838 Test Loss: 0.21373198591145004\n",
      "Epoch: 12099 Training Loss: 0.21726423102093662 Test Loss: 0.21369825680949348\n",
      "Epoch: 12100 Training Loss: 0.2172679528056069 Test Loss: 0.2138293621658304\n",
      "Epoch: 12101 Training Loss: 0.2172645432190918 Test Loss: 0.21374321166175533\n",
      "Epoch: 12102 Training Loss: 0.21726469084997038 Test Loss: 0.21377699261475258\n",
      "Epoch: 12103 Training Loss: 0.21726342547958366 Test Loss: 0.21382198635529495\n",
      "Epoch: 12104 Training Loss: 0.21726631543517255 Test Loss: 0.2137849906377761\n",
      "Epoch: 12105 Training Loss: 0.21726616972297538 Test Loss: 0.21381042357322527\n",
      "Epoch: 12106 Training Loss: 0.21726581818288557 Test Loss: 0.21413947027732425\n",
      "Epoch: 12107 Training Loss: 0.21726700696740053 Test Loss: 0.21384935074200914\n",
      "Epoch: 12108 Training Loss: 0.21726420642774458 Test Loss: 0.21374410609220693\n",
      "Epoch: 12109 Training Loss: 0.21726509132540184 Test Loss: 0.21376369282282043\n",
      "Epoch: 12110 Training Loss: 0.21726347022789624 Test Loss: 0.21366566843043167\n",
      "Epoch: 12111 Training Loss: 0.21726514130974217 Test Loss: 0.21377077048987203\n",
      "Epoch: 12112 Training Loss: 0.2172642215799482 Test Loss: 0.21370558076898827\n",
      "Epoch: 12113 Training Loss: 0.2172631201133688 Test Loss: 0.21770043123473506\n",
      "Epoch: 12114 Training Loss: 0.21726551779269024 Test Loss: 0.21398876522761362\n",
      "Epoch: 12115 Training Loss: 0.2172642002951367 Test Loss: 0.21381445499163745\n",
      "Epoch: 12116 Training Loss: 0.21726419440460548 Test Loss: 0.21374816343613942\n",
      "Epoch: 12117 Training Loss: 0.21726325425968274 Test Loss: 0.21379320902772248\n",
      "Epoch: 12118 Training Loss: 0.2172645869990979 Test Loss: 0.2137227434634504\n",
      "Epoch: 12119 Training Loss: 0.21726308258252597 Test Loss: 0.21387922990419586\n",
      "Epoch: 12120 Training Loss: 0.21726459212753604 Test Loss: 0.21365682782799725\n",
      "Epoch: 12121 Training Loss: 0.21726432847022722 Test Loss: 0.21372833041308273\n",
      "Epoch: 12122 Training Loss: 0.21726547884525088 Test Loss: 0.21369959197379076\n",
      "Epoch: 12123 Training Loss: 0.21726454894823863 Test Loss: 0.21370893812387173\n",
      "Epoch: 12124 Training Loss: 0.2172642759037362 Test Loss: 0.21367969413693322\n",
      "Epoch: 12125 Training Loss: 0.21726565179555138 Test Loss: 0.2136952235236142\n",
      "Epoch: 12126 Training Loss: 0.21726389290265086 Test Loss: 0.2137117640055883\n",
      "Epoch: 12127 Training Loss: 0.2172646825207413 Test Loss: 0.2137205657197422\n",
      "Epoch: 12128 Training Loss: 0.21726504094656623 Test Loss: 0.2136935383647924\n",
      "Epoch: 12129 Training Loss: 0.21726546734212826 Test Loss: 0.21366988132748618\n",
      "Epoch: 12130 Training Loss: 0.2172628426935555 Test Loss: 0.21446749292336997\n",
      "Epoch: 12131 Training Loss: 0.21726592940364647 Test Loss: 0.21386266349670144\n",
      "Epoch: 12132 Training Loss: 0.21726473957013281 Test Loss: 0.21380567920300386\n",
      "Epoch: 12133 Training Loss: 0.21726430243354125 Test Loss: 0.2137157435729598\n",
      "Epoch: 12134 Training Loss: 0.2172642617736339 Test Loss: 0.2137911090605753\n",
      "Epoch: 12135 Training Loss: 0.21726343051836378 Test Loss: 0.21377587781737814\n",
      "Epoch: 12136 Training Loss: 0.21726472505450106 Test Loss: 0.2137268526584236\n",
      "Epoch: 12137 Training Loss: 0.21726551761337423 Test Loss: 0.21363130415322687\n",
      "Epoch: 12138 Training Loss: 0.2172659745195569 Test Loss: 0.21416144215580865\n",
      "Epoch: 12139 Training Loss: 0.21726061600998994 Test Loss: 0.2160584902553228\n",
      "Epoch: 12140 Training Loss: 0.2172650545118231 Test Loss: 0.21426778864022514\n",
      "Epoch: 12141 Training Loss: 0.21726332253425704 Test Loss: 0.21385924132801715\n",
      "Epoch: 12142 Training Loss: 0.21726479884304295 Test Loss: 0.21373175258176702\n",
      "Epoch: 12143 Training Loss: 0.21726552556603967 Test Loss: 0.21377704446579324\n",
      "Epoch: 12144 Training Loss: 0.21726365884145102 Test Loss: 0.21379475159618244\n",
      "Epoch: 12145 Training Loss: 0.2172629568102701 Test Loss: 0.21373649695198843\n",
      "Epoch: 12146 Training Loss: 0.21726457338901203 Test Loss: 0.21366724988717214\n",
      "Epoch: 12147 Training Loss: 0.21726528409908824 Test Loss: 0.21371121956966124\n",
      "Epoch: 12148 Training Loss: 0.21726455958167856 Test Loss: 0.2137464523517973\n",
      "Epoch: 12149 Training Loss: 0.2172632996176698 Test Loss: 0.2140702102497478\n",
      "Epoch: 12150 Training Loss: 0.2172657578251135 Test Loss: 0.21432663957138684\n",
      "Epoch: 12151 Training Loss: 0.21726592427520833 Test Loss: 0.21377753705067962\n",
      "Epoch: 12152 Training Loss: 0.21726401256023045 Test Loss: 0.2137456486606669\n",
      "Epoch: 12153 Training Loss: 0.21726530625358242 Test Loss: 0.2137798703475098\n",
      "Epoch: 12154 Training Loss: 0.21726567123340787 Test Loss: 0.21375865030911517\n",
      "Epoch: 12155 Training Loss: 0.21726454984481872 Test Loss: 0.21375368557197091\n",
      "Epoch: 12156 Training Loss: 0.21726430224525942 Test Loss: 0.21372261383584873\n",
      "Epoch: 12157 Training Loss: 0.21726465881516357 Test Loss: 0.21382455298180816\n",
      "Epoch: 12158 Training Loss: 0.2172641513328977 Test Loss: 0.21373040445470956\n",
      "Epoch: 12159 Training Loss: 0.2172656801992088 Test Loss: 0.21374233019406394\n",
      "Epoch: 12160 Training Loss: 0.2172662231232859 Test Loss: 0.21377586485461797\n",
      "Epoch: 12161 Training Loss: 0.21726508795426067 Test Loss: 0.21376476873191436\n",
      "Epoch: 12162 Training Loss: 0.21726257458024364 Test Loss: 0.2137116732662671\n",
      "Epoch: 12163 Training Loss: 0.2172655408616961 Test Loss: 0.21372707302534644\n",
      "Epoch: 12164 Training Loss: 0.21726387125024152 Test Loss: 0.21373157110312466\n",
      "Epoch: 12165 Training Loss: 0.2172654387860522 Test Loss: 0.21379125165093713\n",
      "Epoch: 12166 Training Loss: 0.21726516350906536 Test Loss: 0.2137816851339333\n",
      "Epoch: 12167 Training Loss: 0.2172651665933009 Test Loss: 0.2137408524394048\n",
      "Epoch: 12168 Training Loss: 0.2172665000499801 Test Loss: 0.21379414234645455\n",
      "Epoch: 12169 Training Loss: 0.21726629191787664 Test Loss: 0.21361656549491612\n",
      "Epoch: 12170 Training Loss: 0.21726601056207676 Test Loss: 0.21364619836465965\n",
      "Epoch: 12171 Training Loss: 0.21726551202768024 Test Loss: 0.2136971160865987\n",
      "Epoch: 12172 Training Loss: 0.21726569364791026 Test Loss: 0.21424572602241956\n",
      "Epoch: 12173 Training Loss: 0.21726519891501334 Test Loss: 0.21389298339273388\n",
      "Epoch: 12174 Training Loss: 0.2172629164462342 Test Loss: 0.2137107529102952\n",
      "Epoch: 12175 Training Loss: 0.21726594610693367 Test Loss: 0.2137162750461267\n",
      "Epoch: 12176 Training Loss: 0.2172652826376627 Test Loss: 0.21369978641519327\n",
      "Epoch: 12177 Training Loss: 0.21726266519759393 Test Loss: 0.21373665250511042\n",
      "Epoch: 12178 Training Loss: 0.2172645825430948 Test Loss: 0.21368966249950222\n",
      "Epoch: 12179 Training Loss: 0.217265674470062 Test Loss: 0.21373320441090582\n",
      "Epoch: 12180 Training Loss: 0.21726381663058209 Test Loss: 0.21375985584581078\n",
      "Epoch: 12181 Training Loss: 0.2172643514406093 Test Loss: 0.21375494295970718\n",
      "Epoch: 12182 Training Loss: 0.2172657633032179 Test Loss: 0.2136856440438502\n",
      "Epoch: 12183 Training Loss: 0.21726733055212294 Test Loss: 0.21368808104276177\n",
      "Epoch: 12184 Training Loss: 0.21726549161255143 Test Loss: 0.21366588879735451\n",
      "Epoch: 12185 Training Loss: 0.21726657431370947 Test Loss: 0.21378366843623897\n",
      "Epoch: 12186 Training Loss: 0.2172647701973089 Test Loss: 0.21379687748884996\n",
      "Epoch: 12187 Training Loss: 0.21726625358011176 Test Loss: 0.21366988132748618\n",
      "Epoch: 12188 Training Loss: 0.21726493716742015 Test Loss: 0.21372976927946136\n",
      "Epoch: 12189 Training Loss: 0.2172645412645472 Test Loss: 0.21367287572508495\n",
      "Epoch: 12190 Training Loss: 0.2172655483750373 Test Loss: 0.21368531997484602\n",
      "Epoch: 12191 Training Loss: 0.21726436185887 Test Loss: 0.21378375917556017\n",
      "Epoch: 12192 Training Loss: 0.2172646422553292 Test Loss: 0.2137277730143955\n",
      "Epoch: 12193 Training Loss: 0.21726364673761972 Test Loss: 0.213694588348366\n",
      "Epoch: 12194 Training Loss: 0.21726470264896447 Test Loss: 0.21388110950442019\n",
      "Epoch: 12195 Training Loss: 0.21726411113024618 Test Loss: 0.21393677159658064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12196 Training Loss: 0.21726602169760154 Test Loss: 0.21402196285640332\n",
      "Epoch: 12197 Training Loss: 0.21726321568880702 Test Loss: 0.2137095084853191\n",
      "Epoch: 12198 Training Loss: 0.21726314987086218 Test Loss: 0.21378117958628676\n",
      "Epoch: 12199 Training Loss: 0.21726460144300322 Test Loss: 0.21374270611410878\n",
      "Epoch: 12200 Training Loss: 0.21726486330715183 Test Loss: 0.21374343202867818\n",
      "Epoch: 12201 Training Loss: 0.21726470793878702 Test Loss: 0.21373330811298716\n",
      "Epoch: 12202 Training Loss: 0.21726426582617595 Test Loss: 0.21368152188611686\n",
      "Epoch: 12203 Training Loss: 0.21726552898200985 Test Loss: 0.2137323618314949\n",
      "Epoch: 12204 Training Loss: 0.2172640177872924 Test Loss: 0.2137765000298662\n",
      "Epoch: 12205 Training Loss: 0.21726487388679697 Test Loss: 0.21368354407670304\n",
      "Epoch: 12206 Training Loss: 0.21726329873005548 Test Loss: 0.21370380487084528\n",
      "Epoch: 12207 Training Loss: 0.21726461781455578 Test Loss: 0.21385461362263725\n",
      "Epoch: 12208 Training Loss: 0.21726437663450998 Test Loss: 0.2137248304678374\n",
      "Epoch: 12209 Training Loss: 0.21726506597011672 Test Loss: 0.21368563108109004\n",
      "Epoch: 12210 Training Loss: 0.21726694754207176 Test Loss: 0.21373302293226346\n",
      "Epoch: 12211 Training Loss: 0.217264808615766 Test Loss: 0.21389577038616997\n",
      "Epoch: 12212 Training Loss: 0.21726520629386753 Test Loss: 0.21375325780088536\n",
      "Epoch: 12213 Training Loss: 0.21726476440540146 Test Loss: 0.21362382464061005\n",
      "Epoch: 12214 Training Loss: 0.2172632559542191 Test Loss: 0.21383467689749922\n",
      "Epoch: 12215 Training Loss: 0.21726349829085323 Test Loss: 0.21396811555066636\n",
      "Epoch: 12216 Training Loss: 0.2172645141250677 Test Loss: 0.21381751420303707\n",
      "Epoch: 12217 Training Loss: 0.2172622600958092 Test Loss: 0.21381044949874561\n",
      "Epoch: 12218 Training Loss: 0.2172642701387262 Test Loss: 0.21378508137709726\n",
      "Epoch: 12219 Training Loss: 0.21726418095590405 Test Loss: 0.21378604062134968\n",
      "Epoch: 12220 Training Loss: 0.21726558321613984 Test Loss: 0.21374831898926144\n",
      "Epoch: 12221 Training Loss: 0.2172638916295071 Test Loss: 0.21366888319495328\n",
      "Epoch: 12222 Training Loss: 0.21726473931012458 Test Loss: 0.21368871621800997\n",
      "Epoch: 12223 Training Loss: 0.21726460986189033 Test Loss: 0.21376839830476133\n",
      "Epoch: 12224 Training Loss: 0.21726523201675046 Test Loss: 0.21356878476093766\n",
      "Epoch: 12225 Training Loss: 0.21726639207483917 Test Loss: 0.21375632997504512\n",
      "Epoch: 12226 Training Loss: 0.21726421909642132 Test Loss: 0.21364215398348732\n",
      "Epoch: 12227 Training Loss: 0.21726738053646327 Test Loss: 0.21372353419182064\n",
      "Epoch: 12228 Training Loss: 0.21726466927825328 Test Loss: 0.2137762148491425\n",
      "Epoch: 12229 Training Loss: 0.2172651050789405 Test Loss: 0.21376629833761415\n",
      "Epoch: 12230 Training Loss: 0.21726397828397337 Test Loss: 0.21375325780088536\n",
      "Epoch: 12231 Training Loss: 0.21726343745789373 Test Loss: 0.21377828889076936\n",
      "Epoch: 12232 Training Loss: 0.21726409308208885 Test Loss: 0.21391206457570086\n",
      "Epoch: 12233 Training Loss: 0.21726462987355807 Test Loss: 0.21386497086801132\n",
      "Epoch: 12234 Training Loss: 0.21726398105440586 Test Loss: 0.21388773347486592\n",
      "Epoch: 12235 Training Loss: 0.21726214728810153 Test Loss: 0.21412465384245247\n",
      "Epoch: 12236 Training Loss: 0.21726594693178736 Test Loss: 0.21382791033669163\n",
      "Epoch: 12237 Training Loss: 0.21726506050097813 Test Loss: 0.21379333865532416\n",
      "Epoch: 12238 Training Loss: 0.21726636032693797 Test Loss: 0.21386104315168047\n",
      "Epoch: 12239 Training Loss: 0.21726514735269203 Test Loss: 0.21372077312390492\n",
      "Epoch: 12240 Training Loss: 0.21726566083307874 Test Loss: 0.21392347180464852\n",
      "Epoch: 12241 Training Loss: 0.21726454531708922 Test Loss: 0.21370911960251407\n",
      "Epoch: 12242 Training Loss: 0.217264730326392 Test Loss: 0.21378803688641554\n",
      "Epoch: 12243 Training Loss: 0.2172645920558096 Test Loss: 0.21373166184244585\n",
      "Epoch: 12244 Training Loss: 0.21726286364663236 Test Loss: 0.2137362247340249\n",
      "Epoch: 12245 Training Loss: 0.21726587526814028 Test Loss: 0.21376539094440242\n",
      "Epoch: 12246 Training Loss: 0.21726600365841 Test Loss: 0.2138205086006358\n",
      "Epoch: 12247 Training Loss: 0.21726494564010204 Test Loss: 0.2137904220342864\n",
      "Epoch: 12248 Training Loss: 0.21726477331740762 Test Loss: 0.21376574093892695\n",
      "Epoch: 12249 Training Loss: 0.21726568140959193 Test Loss: 0.21367685529245647\n",
      "Epoch: 12250 Training Loss: 0.21726560584582147 Test Loss: 0.2137225101337674\n",
      "Epoch: 12251 Training Loss: 0.21726488670789235 Test Loss: 0.21370341598804024\n",
      "Epoch: 12252 Training Loss: 0.21726437908217364 Test Loss: 0.21367364052793486\n",
      "Epoch: 12253 Training Loss: 0.2172664654419884 Test Loss: 0.21379562010111366\n",
      "Epoch: 12254 Training Loss: 0.21726429677612083 Test Loss: 0.2136926957853815\n",
      "Epoch: 12255 Training Loss: 0.21726478475776964 Test Loss: 0.2137484097285826\n",
      "Epoch: 12256 Training Loss: 0.21726499710379954 Test Loss: 0.213756939224773\n",
      "Epoch: 12257 Training Loss: 0.21726335733949637 Test Loss: 0.21369217727497478\n",
      "Epoch: 12258 Training Loss: 0.21726423564728992 Test Loss: 0.2136844125816343\n",
      "Epoch: 12259 Training Loss: 0.2172648676197021 Test Loss: 0.21368169040199905\n",
      "Epoch: 12260 Training Loss: 0.21726563575573346 Test Loss: 0.21379457011754008\n",
      "Epoch: 12261 Training Loss: 0.21726544756357133 Test Loss: 0.2138451378449546\n",
      "Epoch: 12262 Training Loss: 0.21726406226663095 Test Loss: 0.21376384837594245\n",
      "Epoch: 12263 Training Loss: 0.21726584959008632 Test Loss: 0.2136846070230368\n",
      "Epoch: 12264 Training Loss: 0.2172642002054787 Test Loss: 0.21379459604306042\n",
      "Epoch: 12265 Training Loss: 0.2172637848378519 Test Loss: 0.2137894887155543\n",
      "Epoch: 12266 Training Loss: 0.21726507391381636 Test Loss: 0.21370223637686497\n",
      "Epoch: 12267 Training Loss: 0.21726618073297896 Test Loss: 0.21372879707244877\n",
      "Epoch: 12268 Training Loss: 0.21726558979703775 Test Loss: 0.21374685419736247\n",
      "Epoch: 12269 Training Loss: 0.21726457548700945 Test Loss: 0.21408109896828872\n",
      "Epoch: 12270 Training Loss: 0.21726127861854402 Test Loss: 0.21428279951649942\n",
      "Epoch: 12271 Training Loss: 0.21726580589973826 Test Loss: 0.2139906707533583\n",
      "Epoch: 12272 Training Loss: 0.21726408525494462 Test Loss: 0.21391502008501911\n",
      "Epoch: 12273 Training Loss: 0.21726563197216545 Test Loss: 0.2138407953202984\n",
      "Epoch: 12274 Training Loss: 0.21726374117440123 Test Loss: 0.21375447630034114\n",
      "Epoch: 12275 Training Loss: 0.21726400960151612 Test Loss: 0.213849571108932\n",
      "Epoch: 12276 Training Loss: 0.2172644021242821 Test Loss: 0.21382439742868614\n",
      "Epoch: 12277 Training Loss: 0.21726540555879384 Test Loss: 0.21382525297085722\n",
      "Epoch: 12278 Training Loss: 0.21726433526630437 Test Loss: 0.21373009334846554\n",
      "Epoch: 12279 Training Loss: 0.21726750096510178 Test Loss: 0.21374156539121403\n",
      "Epoch: 12280 Training Loss: 0.21726568608077423 Test Loss: 0.21376867052272486\n",
      "Epoch: 12281 Training Loss: 0.21726431278904135 Test Loss: 0.2137676853529521\n",
      "Epoch: 12282 Training Loss: 0.21726462959561824 Test Loss: 0.21385164515055882\n",
      "Epoch: 12283 Training Loss: 0.2172645740614471 Test Loss: 0.21377813333764734\n",
      "Epoch: 12284 Training Loss: 0.21726629566558145 Test Loss: 0.2138403545864527\n",
      "Epoch: 12285 Training Loss: 0.21726406516258467 Test Loss: 0.21381768271891924\n",
      "Epoch: 12286 Training Loss: 0.21726365543444665 Test Loss: 0.2137465301283583\n",
      "Epoch: 12287 Training Loss: 0.21726494295036175 Test Loss: 0.2137486560210258\n",
      "Epoch: 12288 Training Loss: 0.21726484678318067 Test Loss: 0.2135903288683365\n",
      "Epoch: 12289 Training Loss: 0.21726460608728812 Test Loss: 0.21379309236288097\n",
      "Epoch: 12290 Training Loss: 0.21726590680086225 Test Loss: 0.21377937776262346\n",
      "Epoch: 12291 Training Loss: 0.21726401425476682 Test Loss: 0.2138345213443772\n",
      "Epoch: 12292 Training Loss: 0.21726604351139528 Test Loss: 0.21381131800367686\n",
      "Epoch: 12293 Training Loss: 0.21726376655658372 Test Loss: 0.2137843813880482\n",
      "Epoch: 12294 Training Loss: 0.2172641056521418 Test Loss: 0.21386599492606456\n",
      "Epoch: 12295 Training Loss: 0.2172646647236264 Test Loss: 0.2137829036333891\n",
      "Epoch: 12296 Training Loss: 0.21726560667067515 Test Loss: 0.2137997552216072\n",
      "Epoch: 12297 Training Loss: 0.21726351601624172 Test Loss: 0.21373832470117207\n",
      "Epoch: 12298 Training Loss: 0.21726337029507875 Test Loss: 0.21375013377568494\n",
      "Epoch: 12299 Training Loss: 0.21726350570557063 Test Loss: 0.21378343510655595\n",
      "Epoch: 12300 Training Loss: 0.2172644338363201 Test Loss: 0.21373685990927313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12301 Training Loss: 0.21726472659661883 Test Loss: 0.21376007621273363\n",
      "Epoch: 12302 Training Loss: 0.21726531307655694 Test Loss: 0.21481979481920993\n",
      "Epoch: 12303 Training Loss: 0.21726023752766827 Test Loss: 0.21509306276630694\n",
      "Epoch: 12304 Training Loss: 0.2172618114202318 Test Loss: 0.21423011885917756\n",
      "Epoch: 12305 Training Loss: 0.21726514023384605 Test Loss: 0.21397007292745168\n",
      "Epoch: 12306 Training Loss: 0.21726279084432856 Test Loss: 0.21382367151411677\n",
      "Epoch: 12307 Training Loss: 0.21726204184131645 Test Loss: 0.2140153259231974\n",
      "Epoch: 12308 Training Loss: 0.21726326432827722 Test Loss: 0.21370227526514549\n",
      "Epoch: 12309 Training Loss: 0.2172652320795111 Test Loss: 0.2137616965577546\n",
      "Epoch: 12310 Training Loss: 0.21726455982375517 Test Loss: 0.21397282103260726\n",
      "Epoch: 12311 Training Loss: 0.2172651452277972 Test Loss: 0.21372217310200303\n",
      "Epoch: 12312 Training Loss: 0.2172652957635953 Test Loss: 0.21372991186982318\n",
      "Epoch: 12313 Training Loss: 0.2172632830130064 Test Loss: 0.21371241214359668\n",
      "Epoch: 12314 Training Loss: 0.21726447382379238 Test Loss: 0.21369797162876977\n",
      "Epoch: 12315 Training Loss: 0.21726588872580752 Test Loss: 0.21386065426887543\n",
      "Epoch: 12316 Training Loss: 0.21726581255236257 Test Loss: 0.2138801761856881\n",
      "Epoch: 12317 Training Loss: 0.21726463218673472 Test Loss: 0.2137406839235226\n",
      "Epoch: 12318 Training Loss: 0.21726412826389183 Test Loss: 0.213683155193898\n",
      "Epoch: 12319 Training Loss: 0.21726565550739296 Test Loss: 0.21377366118538946\n",
      "Epoch: 12320 Training Loss: 0.2172639879132436 Test Loss: 0.2139277754410242\n",
      "Epoch: 12321 Training Loss: 0.21726625615329662 Test Loss: 0.21382927142650923\n",
      "Epoch: 12322 Training Loss: 0.21726470788499222 Test Loss: 0.21374396350184507\n",
      "Epoch: 12323 Training Loss: 0.21726498735797392 Test Loss: 0.2137233527131783\n",
      "Epoch: 12324 Training Loss: 0.21726523823005053 Test Loss: 0.213779961086831\n",
      "Epoch: 12325 Training Loss: 0.21726550362672473 Test Loss: 0.21376320023793405\n",
      "Epoch: 12326 Training Loss: 0.2172650125608404 Test Loss: 0.2136993327185874\n",
      "Epoch: 12327 Training Loss: 0.217265823974793 Test Loss: 0.21368550145348839\n",
      "Epoch: 12328 Training Loss: 0.21726743458231146 Test Loss: 0.21371308620712542\n",
      "Epoch: 12329 Training Loss: 0.21726550296325545 Test Loss: 0.21364599096049697\n",
      "Epoch: 12330 Training Loss: 0.2172651807682322 Test Loss: 0.2137206564590634\n",
      "Epoch: 12331 Training Loss: 0.21726460972740333 Test Loss: 0.21374497459713815\n",
      "Epoch: 12332 Training Loss: 0.2172650301786393 Test Loss: 0.21376532613060156\n",
      "Epoch: 12333 Training Loss: 0.21726259893135905 Test Loss: 0.21369256615777982\n",
      "Epoch: 12334 Training Loss: 0.21726584051669576 Test Loss: 0.2138024125874416\n",
      "Epoch: 12335 Training Loss: 0.21726446169306368 Test Loss: 0.21371440840866252\n",
      "Epoch: 12336 Training Loss: 0.21726481022961017 Test Loss: 0.21374637457523626\n",
      "Epoch: 12337 Training Loss: 0.2172651714169018 Test Loss: 0.2137468412346023\n",
      "Epoch: 12338 Training Loss: 0.21726590520494968 Test Loss: 0.2136880680800016\n",
      "Epoch: 12339 Training Loss: 0.2172661110687055 Test Loss: 0.21375775587866358\n",
      "Epoch: 12340 Training Loss: 0.21726536659342288 Test Loss: 0.2137749704241664\n",
      "Epoch: 12341 Training Loss: 0.21726530927505733 Test Loss: 0.21373718397827732\n",
      "Epoch: 12342 Training Loss: 0.21726507751806834 Test Loss: 0.21403317564394844\n",
      "Epoch: 12343 Training Loss: 0.21726196969351613 Test Loss: 0.21431352125809705\n",
      "Epoch: 12344 Training Loss: 0.21726152224625347 Test Loss: 0.21396772666786132\n",
      "Epoch: 12345 Training Loss: 0.2172650078268975 Test Loss: 0.2138821205997133\n",
      "Epoch: 12346 Training Loss: 0.21726361536628216 Test Loss: 0.2137878942960537\n",
      "Epoch: 12347 Training Loss: 0.21726335837952926 Test Loss: 0.21767027985458479\n",
      "Epoch: 12348 Training Loss: 0.21726545817907966 Test Loss: 0.2138314232446971\n",
      "Epoch: 12349 Training Loss: 0.21726569142439162 Test Loss: 0.21372506379752043\n",
      "Epoch: 12350 Training Loss: 0.2172653997668864 Test Loss: 0.21369886605922137\n",
      "Epoch: 12351 Training Loss: 0.21726492959131832 Test Loss: 0.21373387847443454\n",
      "Epoch: 12352 Training Loss: 0.21726458782395158 Test Loss: 0.21367041280065308\n",
      "Epoch: 12353 Training Loss: 0.2172666270505507 Test Loss: 0.21369024582370977\n",
      "Epoch: 12354 Training Loss: 0.21726595738591126 Test Loss: 0.21374098206700648\n",
      "Epoch: 12355 Training Loss: 0.21726561570820252 Test Loss: 0.21368659032534246\n",
      "Epoch: 12356 Training Loss: 0.21726430260389146 Test Loss: 0.2137351358621708\n",
      "Epoch: 12357 Training Loss: 0.21726488119392476 Test Loss: 0.21385673951530476\n",
      "Epoch: 12358 Training Loss: 0.21726455297388325 Test Loss: 0.2137497448928799\n",
      "Epoch: 12359 Training Loss: 0.21726496521244554 Test Loss: 0.2137880887374562\n",
      "Epoch: 12360 Training Loss: 0.21726578779778613 Test Loss: 0.21368140522127535\n",
      "Epoch: 12361 Training Loss: 0.21726502965862282 Test Loss: 0.21370607335387465\n",
      "Epoch: 12362 Training Loss: 0.21726461048949638 Test Loss: 0.21366362031432515\n",
      "Epoch: 12363 Training Loss: 0.21726515399635055 Test Loss: 0.2137263600735372\n",
      "Epoch: 12364 Training Loss: 0.21726434988055993 Test Loss: 0.21370405116328847\n",
      "Epoch: 12365 Training Loss: 0.21726553121449427 Test Loss: 0.2137690594055299\n",
      "Epoch: 12366 Training Loss: 0.21726443535154047 Test Loss: 0.21383090473429037\n",
      "Epoch: 12367 Training Loss: 0.21726448755043365 Test Loss: 0.21378452397841005\n",
      "Epoch: 12368 Training Loss: 0.21726388542517283 Test Loss: 0.21391889595030927\n",
      "Epoch: 12369 Training Loss: 0.21726506607770632 Test Loss: 0.21375655034196797\n",
      "Epoch: 12370 Training Loss: 0.21726419991857307 Test Loss: 0.21382959549551345\n",
      "Epoch: 12371 Training Loss: 0.21726348462697256 Test Loss: 0.21376283728064935\n",
      "Epoch: 12372 Training Loss: 0.21726356647576953 Test Loss: 0.21377647410434586\n",
      "Epoch: 12373 Training Loss: 0.2172646263499983 Test Loss: 0.21372034535281936\n",
      "Epoch: 12374 Training Loss: 0.2172658724528788 Test Loss: 0.2137678927571148\n",
      "Epoch: 12375 Training Loss: 0.21726485030674045 Test Loss: 0.21371259362223904\n",
      "Epoch: 12376 Training Loss: 0.2172646403456136 Test Loss: 0.2136990475378637\n",
      "Epoch: 12377 Training Loss: 0.21726467273905245 Test Loss: 0.2137221860647632\n",
      "Epoch: 12378 Training Loss: 0.21726534076295032 Test Loss: 0.2137327636770601\n",
      "Epoch: 12379 Training Loss: 0.2172641298418728 Test Loss: 0.21368061449290512\n",
      "Epoch: 12380 Training Loss: 0.21726644891801725 Test Loss: 0.2137559022039596\n",
      "Epoch: 12381 Training Loss: 0.21726350809943948 Test Loss: 0.2137588836387982\n",
      "Epoch: 12382 Training Loss: 0.21726388488722478 Test Loss: 0.2137454412565042\n",
      "Epoch: 12383 Training Loss: 0.21726559099845508 Test Loss: 0.21365783892329032\n",
      "Epoch: 12384 Training Loss: 0.2172635773154229 Test Loss: 0.21371461581282522\n",
      "Epoch: 12385 Training Loss: 0.2172635065214585 Test Loss: 0.21371154363866546\n",
      "Epoch: 12386 Training Loss: 0.217266016587095 Test Loss: 0.21386031723711107\n",
      "Epoch: 12387 Training Loss: 0.2172648714032701 Test Loss: 0.21370988440536398\n",
      "Epoch: 12388 Training Loss: 0.2172648069301954 Test Loss: 0.2137331266343448\n",
      "Epoch: 12389 Training Loss: 0.2172661455780734 Test Loss: 0.21377635743950435\n",
      "Epoch: 12390 Training Loss: 0.2172668898650742 Test Loss: 0.21371013069780714\n",
      "Epoch: 12391 Training Loss: 0.2172654586632329 Test Loss: 0.21370152342505574\n",
      "Epoch: 12392 Training Loss: 0.21726477147941842 Test Loss: 0.2137831369630721\n",
      "Epoch: 12393 Training Loss: 0.21726225685018924 Test Loss: 0.21760315868243602\n",
      "Epoch: 12394 Training Loss: 0.2172676264683836 Test Loss: 0.2140089352824347\n",
      "Epoch: 12395 Training Loss: 0.21726412118090907 Test Loss: 0.21375722440549672\n",
      "Epoch: 12396 Training Loss: 0.21726762232618355 Test Loss: 0.21374090429044548\n",
      "Epoch: 12397 Training Loss: 0.2172653273501121 Test Loss: 0.2136673017382128\n",
      "Epoch: 12398 Training Loss: 0.21726390008425742 Test Loss: 0.21369995493107544\n",
      "Epoch: 12399 Training Loss: 0.21726551354290058 Test Loss: 0.21370559373174844\n",
      "Epoch: 12400 Training Loss: 0.21726474744210605 Test Loss: 0.2137455579213457\n",
      "Epoch: 12401 Training Loss: 0.21726411923533026 Test Loss: 0.21367198129463338\n",
      "Epoch: 12402 Training Loss: 0.2172650771146073 Test Loss: 0.2136639573460895\n",
      "Epoch: 12403 Training Loss: 0.21726643024225384 Test Loss: 0.21368145707231603\n",
      "Epoch: 12404 Training Loss: 0.2172635436488403 Test Loss: 0.21372578971208983\n",
      "Epoch: 12405 Training Loss: 0.21726490768786658 Test Loss: 0.21369456242284565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12406 Training Loss: 0.21726406520741368 Test Loss: 0.21368276631109298\n",
      "Epoch: 12407 Training Loss: 0.21726200996789405 Test Loss: 0.21366135183129578\n",
      "Epoch: 12408 Training Loss: 0.2172636626160532 Test Loss: 0.21363880959136403\n",
      "Epoch: 12409 Training Loss: 0.21726583912699662 Test Loss: 0.21368139225851518\n",
      "Epoch: 12410 Training Loss: 0.21726659631578502 Test Loss: 0.21368032931218142\n",
      "Epoch: 12411 Training Loss: 0.21726564887270028 Test Loss: 0.2137364191754274\n",
      "Epoch: 12412 Training Loss: 0.21726515814751637 Test Loss: 0.21365542784989913\n",
      "Epoch: 12413 Training Loss: 0.21726683813240266 Test Loss: 0.21369925494202638\n",
      "Epoch: 12414 Training Loss: 0.2172646209525861 Test Loss: 0.21369999381935595\n",
      "Epoch: 12415 Training Loss: 0.21726510832456045 Test Loss: 0.213713902861016\n",
      "Epoch: 12416 Training Loss: 0.21726404707856414 Test Loss: 0.21413656661904668\n",
      "Epoch: 12417 Training Loss: 0.21726591885986454 Test Loss: 0.21377048530914836\n",
      "Epoch: 12418 Training Loss: 0.217264292535297 Test Loss: 0.2137394913495872\n",
      "Epoch: 12419 Training Loss: 0.2172651424035699 Test Loss: 0.21370425856745115\n",
      "Epoch: 12420 Training Loss: 0.2172652499124892 Test Loss: 0.2137047641150977\n",
      "Epoch: 12421 Training Loss: 0.21726558621071737 Test Loss: 0.21375363372093023\n",
      "Epoch: 12422 Training Loss: 0.21726543993367473 Test Loss: 0.21395966383103696\n",
      "Epoch: 12423 Training Loss: 0.2172656772046313 Test Loss: 0.21377825000248885\n",
      "Epoch: 12424 Training Loss: 0.21726528056656266 Test Loss: 0.21376047805829881\n",
      "Epoch: 12425 Training Loss: 0.21726597061943348 Test Loss: 0.21370167897817777\n",
      "Epoch: 12426 Training Loss: 0.21726454095074416 Test Loss: 0.21369416057728044\n",
      "Epoch: 12427 Training Loss: 0.2172650966600534 Test Loss: 0.21373921913162366\n",
      "Epoch: 12428 Training Loss: 0.21726493919369116 Test Loss: 0.21385339512318147\n",
      "Epoch: 12429 Training Loss: 0.21726525828654727 Test Loss: 0.21374296536931214\n",
      "Epoch: 12430 Training Loss: 0.2172654191599139 Test Loss: 0.21376441873738983\n",
      "Epoch: 12431 Training Loss: 0.21726519504178732 Test Loss: 0.21373680805823245\n",
      "Epoch: 12432 Training Loss: 0.21726419912958259 Test Loss: 0.2137273711688303\n",
      "Epoch: 12433 Training Loss: 0.21726429145940088 Test Loss: 0.21380001447681055\n",
      "Epoch: 12434 Training Loss: 0.21726506987024014 Test Loss: 0.21375486518314618\n",
      "Epoch: 12435 Training Loss: 0.21726445496871297 Test Loss: 0.21370803073065997\n",
      "Epoch: 12436 Training Loss: 0.21726572981595133 Test Loss: 0.21382988067623712\n",
      "Epoch: 12437 Training Loss: 0.21726524388747095 Test Loss: 0.213723871223585\n",
      "Epoch: 12438 Training Loss: 0.21726413250471568 Test Loss: 0.213808932855806\n",
      "Epoch: 12439 Training Loss: 0.21726422220755426 Test Loss: 0.21380390330486088\n",
      "Epoch: 12440 Training Loss: 0.2172655669432111 Test Loss: 0.21368723846335086\n",
      "Epoch: 12441 Training Loss: 0.21726532806737614 Test Loss: 0.21368704402194832\n",
      "Epoch: 12442 Training Loss: 0.2172645778181177 Test Loss: 0.21366275180939392\n",
      "Epoch: 12443 Training Loss: 0.2172638980041916 Test Loss: 0.2137775500134398\n",
      "Epoch: 12444 Training Loss: 0.2172642955657377 Test Loss: 0.21375242818423462\n",
      "Epoch: 12445 Training Loss: 0.2172643225886618 Test Loss: 0.21366566843043167\n",
      "Epoch: 12446 Training Loss: 0.21726625535534033 Test Loss: 0.2138180586389641\n",
      "Epoch: 12447 Training Loss: 0.21726568514833094 Test Loss: 0.2137110380910189\n",
      "Epoch: 12448 Training Loss: 0.21726462289816492 Test Loss: 0.21372772116335484\n",
      "Epoch: 12449 Training Loss: 0.21726393501501795 Test Loss: 0.21375669293232982\n",
      "Epoch: 12450 Training Loss: 0.21726495791428355 Test Loss: 0.2137636668973001\n",
      "Epoch: 12451 Training Loss: 0.21726388052984552 Test Loss: 0.21377048530914836\n",
      "Epoch: 12452 Training Loss: 0.2172646967494674 Test Loss: 0.21375592812947994\n",
      "Epoch: 12453 Training Loss: 0.2172634979591186 Test Loss: 0.21369115321692153\n",
      "Epoch: 12454 Training Loss: 0.217267461901107 Test Loss: 0.2139244828999416\n",
      "Epoch: 12455 Training Loss: 0.21726423070713358 Test Loss: 0.21374392461356456\n",
      "Epoch: 12456 Training Loss: 0.21726460896531025 Test Loss: 0.2136803422749416\n",
      "Epoch: 12457 Training Loss: 0.2172670263873254 Test Loss: 0.21378067403864023\n",
      "Epoch: 12458 Training Loss: 0.21726401370785298 Test Loss: 0.21374532459166268\n",
      "Epoch: 12459 Training Loss: 0.21726445836675154 Test Loss: 0.21367863119059946\n",
      "Epoch: 12460 Training Loss: 0.21726686892096317 Test Loss: 0.21375480036934533\n",
      "Epoch: 12461 Training Loss: 0.21726446726979187 Test Loss: 0.213739245057144\n",
      "Epoch: 12462 Training Loss: 0.21726423992397698 Test Loss: 0.2137420190878199\n",
      "Epoch: 12463 Training Loss: 0.2172649227055832 Test Loss: 0.21365135754320644\n",
      "Epoch: 12464 Training Loss: 0.21726575148629224 Test Loss: 0.21374995229704258\n",
      "Epoch: 12465 Training Loss: 0.2172662054516922 Test Loss: 0.21372481750507724\n",
      "Epoch: 12466 Training Loss: 0.2172665587132158 Test Loss: 0.21370092713808803\n",
      "Epoch: 12467 Training Loss: 0.2172646843587305 Test Loss: 0.21372735820607014\n",
      "Epoch: 12468 Training Loss: 0.21726522894148076 Test Loss: 0.213692812450223\n",
      "Epoch: 12469 Training Loss: 0.21726420711811126 Test Loss: 0.21374527274062202\n",
      "Epoch: 12470 Training Loss: 0.21726347314178154 Test Loss: 0.2137400487482744\n",
      "Epoch: 12471 Training Loss: 0.21726438854109367 Test Loss: 0.21379988484920887\n",
      "Epoch: 12472 Training Loss: 0.21726465122113015 Test Loss: 0.21370349376460127\n",
      "Epoch: 12473 Training Loss: 0.21726314950326434 Test Loss: 0.21371802501874934\n",
      "Epoch: 12474 Training Loss: 0.21726587563573813 Test Loss: 0.21371601579092334\n",
      "Epoch: 12475 Training Loss: 0.21726507009438517 Test Loss: 0.21366157219821866\n",
      "Epoch: 12476 Training Loss: 0.2172666102127765 Test Loss: 0.21372428603191038\n",
      "Epoch: 12477 Training Loss: 0.2172649359301396 Test Loss: 0.21686887016997186\n",
      "Epoch: 12478 Training Loss: 0.21726469683015964 Test Loss: 0.21384518969599528\n",
      "Epoch: 12479 Training Loss: 0.21726591788259225 Test Loss: 0.21371541950395562\n",
      "Epoch: 12480 Training Loss: 0.2172647965746953 Test Loss: 0.21396932108736194\n",
      "Epoch: 12481 Training Loss: 0.2172616186555112 Test Loss: 0.21427172931931612\n",
      "Epoch: 12482 Training Loss: 0.21726547457752962 Test Loss: 0.21401961659681296\n",
      "Epoch: 12483 Training Loss: 0.2172633520945028 Test Loss: 0.21387007819551743\n",
      "Epoch: 12484 Training Loss: 0.21726423434724876 Test Loss: 0.2138261085130283\n",
      "Epoch: 12485 Training Loss: 0.21726410789359205 Test Loss: 0.21369045322787247\n",
      "Epoch: 12486 Training Loss: 0.21726276355243046 Test Loss: 0.21372192680955984\n",
      "Epoch: 12487 Training Loss: 0.21726360022304433 Test Loss: 0.21381542719865004\n",
      "Epoch: 12488 Training Loss: 0.21726448070056173 Test Loss: 0.213754929996947\n",
      "Epoch: 12489 Training Loss: 0.21726450619929966 Test Loss: 0.21380037743409525\n",
      "Epoch: 12490 Training Loss: 0.2172650182720556 Test Loss: 0.2137948423355036\n",
      "Epoch: 12491 Training Loss: 0.21726427947212498 Test Loss: 0.2137980182117447\n",
      "Epoch: 12492 Training Loss: 0.21726364197677941 Test Loss: 0.2137408524394048\n",
      "Epoch: 12493 Training Loss: 0.21726447946328117 Test Loss: 0.21376514465195923\n",
      "Epoch: 12494 Training Loss: 0.2172651696596048 Test Loss: 0.21367704973385898\n",
      "Epoch: 12495 Training Loss: 0.2172648815704884 Test Loss: 0.21374611532003293\n",
      "Epoch: 12496 Training Loss: 0.21726553442425103 Test Loss: 0.2139458455286981\n",
      "Epoch: 12497 Training Loss: 0.21726394695746482 Test Loss: 0.2141608977198816\n",
      "Epoch: 12498 Training Loss: 0.21726195199502504 Test Loss: 0.21402388134490816\n",
      "Epoch: 12499 Training Loss: 0.21726589361216905 Test Loss: 0.21381489572548318\n",
      "Epoch: 12500 Training Loss: 0.21726325607077454 Test Loss: 0.21381919936185886\n",
      "Epoch: 12501 Training Loss: 0.21726333078279392 Test Loss: 0.2137712630747584\n",
      "Epoch: 12502 Training Loss: 0.21726306481230848 Test Loss: 0.21376832052820033\n",
      "Epoch: 12503 Training Loss: 0.21726546276956976 Test Loss: 0.21371871204503823\n",
      "Epoch: 12504 Training Loss: 0.21726579389453077 Test Loss: 0.21372087682598626\n",
      "Epoch: 12505 Training Loss: 0.21726543845431756 Test Loss: 0.21365420935044335\n",
      "Epoch: 12506 Training Loss: 0.2172655005783524 Test Loss: 0.21370893812387173\n",
      "Epoch: 12507 Training Loss: 0.21726574200944063 Test Loss: 0.21376379652490177\n",
      "Epoch: 12508 Training Loss: 0.21726438619205382 Test Loss: 0.21374144872637252\n",
      "Epoch: 12509 Training Loss: 0.21726503364840427 Test Loss: 0.2136714498214665\n",
      "Epoch: 12510 Training Loss: 0.21726483604215113 Test Loss: 0.21374703567600484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12511 Training Loss: 0.21726265201786651 Test Loss: 0.21375820957526945\n",
      "Epoch: 12512 Training Loss: 0.21726578354799647 Test Loss: 0.21555338334262503\n",
      "Epoch: 12513 Training Loss: 0.21726657252951506 Test Loss: 0.2137557336880774\n",
      "Epoch: 12514 Training Loss: 0.2172669794961864 Test Loss: 0.21375518925215037\n",
      "Epoch: 12515 Training Loss: 0.21726475430990957 Test Loss: 0.21373120814583996\n",
      "Epoch: 12516 Training Loss: 0.2172639691657538 Test Loss: 0.21381897899493602\n",
      "Epoch: 12517 Training Loss: 0.2172662354691938 Test Loss: 0.21374474126745513\n",
      "Epoch: 12518 Training Loss: 0.21726562561541257 Test Loss: 0.21372760449851333\n",
      "Epoch: 12519 Training Loss: 0.21726674230592202 Test Loss: 0.21367505346879315\n",
      "Epoch: 12520 Training Loss: 0.21726503361254107 Test Loss: 0.21384591561056468\n",
      "Epoch: 12521 Training Loss: 0.217265532353151 Test Loss: 0.21375026340328662\n",
      "Epoch: 12522 Training Loss: 0.21726429709888967 Test Loss: 0.21370578817315095\n",
      "Epoch: 12523 Training Loss: 0.2172656370109456 Test Loss: 0.21370253452034885\n",
      "Epoch: 12524 Training Loss: 0.2172654314340954 Test Loss: 0.21393286980577014\n",
      "Epoch: 12525 Training Loss: 0.21726507474763584 Test Loss: 0.2137348117931666\n",
      "Epoch: 12526 Training Loss: 0.21726595828249137 Test Loss: 0.21372374159598334\n",
      "Epoch: 12527 Training Loss: 0.21726588868994431 Test Loss: 0.21368207928480407\n",
      "Epoch: 12528 Training Loss: 0.21726557828494933 Test Loss: 0.21370393449844696\n",
      "Epoch: 12529 Training Loss: 0.21726605636835386 Test Loss: 0.2137754111580121\n",
      "Epoch: 12530 Training Loss: 0.2172658716638883 Test Loss: 0.21378855539682223\n",
      "Epoch: 12531 Training Loss: 0.21726476637787767 Test Loss: 0.2137657668644473\n",
      "Epoch: 12532 Training Loss: 0.2172658567627271 Test Loss: 0.21384604523816636\n",
      "Epoch: 12533 Training Loss: 0.21726609565649366 Test Loss: 0.2136934994765119\n",
      "Epoch: 12534 Training Loss: 0.21726469036581714 Test Loss: 0.21364678168886722\n",
      "Epoch: 12535 Training Loss: 0.21726598072389117 Test Loss: 0.2136912050679622\n",
      "Epoch: 12536 Training Loss: 0.21726707742066448 Test Loss: 0.2137469578994438\n",
      "Epoch: 12537 Training Loss: 0.217265635190888 Test Loss: 0.21373905061574147\n",
      "Epoch: 12538 Training Loss: 0.2172659865606276 Test Loss: 0.21380094779554262\n",
      "Epoch: 12539 Training Loss: 0.21726408009960907 Test Loss: 0.21472845921106776\n",
      "Epoch: 12540 Training Loss: 0.217265455641758 Test Loss: 0.2137526355883973\n",
      "Epoch: 12541 Training Loss: 0.21726730427336033 Test Loss: 0.21381672347466682\n",
      "Epoch: 12542 Training Loss: 0.2172639372743998 Test Loss: 0.21375844290495247\n",
      "Epoch: 12543 Training Loss: 0.21726406513568727 Test Loss: 0.2137245582498739\n",
      "Epoch: 12544 Training Loss: 0.21726658414919311 Test Loss: 0.2137771870561551\n",
      "Epoch: 12545 Training Loss: 0.21726325802531915 Test Loss: 0.2136879514151601\n",
      "Epoch: 12546 Training Loss: 0.21726626830195692 Test Loss: 0.21377664262022805\n",
      "Epoch: 12547 Training Loss: 0.2172639233505109 Test Loss: 0.21373724879207814\n",
      "Epoch: 12548 Training Loss: 0.21726356477226735 Test Loss: 0.2136712942683445\n",
      "Epoch: 12549 Training Loss: 0.21726465088939553 Test Loss: 0.21377408895647498\n",
      "Epoch: 12550 Training Loss: 0.21726455776162096 Test Loss: 0.21367877378096128\n",
      "Epoch: 12551 Training Loss: 0.21726388760386248 Test Loss: 0.21371693614689524\n",
      "Epoch: 12552 Training Loss: 0.21726564111728244 Test Loss: 0.21371108994205956\n",
      "Epoch: 12553 Training Loss: 0.2172657266868868 Test Loss: 0.2137129306540034\n",
      "Epoch: 12554 Training Loss: 0.21726588978377204 Test Loss: 0.21371910092784327\n",
      "Epoch: 12555 Training Loss: 0.21726504122450607 Test Loss: 0.21371674170549274\n",
      "Epoch: 12556 Training Loss: 0.21726401901560713 Test Loss: 0.2137320377624907\n",
      "Epoch: 12557 Training Loss: 0.21726442057590048 Test Loss: 0.21371496580734975\n",
      "Epoch: 12558 Training Loss: 0.2172645262826938 Test Loss: 0.21366958318400234\n",
      "Epoch: 12559 Training Loss: 0.2172663904520292 Test Loss: 0.21480622280931427\n",
      "Epoch: 12560 Training Loss: 0.21726487076669823 Test Loss: 0.21373114333203913\n",
      "Epoch: 12561 Training Loss: 0.21726615242794534 Test Loss: 0.213757017001334\n",
      "Epoch: 12562 Training Loss: 0.21726493478251707 Test Loss: 0.21378322770239327\n",
      "Epoch: 12563 Training Loss: 0.21726477048421453 Test Loss: 0.21376122989838856\n",
      "Epoch: 12564 Training Loss: 0.217264064337731 Test Loss: 0.2137377932280052\n",
      "Epoch: 12565 Training Loss: 0.21726670686411084 Test Loss: 0.21375421704513778\n",
      "Epoch: 12566 Training Loss: 0.21726521109057104 Test Loss: 0.21380984024901772\n",
      "Epoch: 12567 Training Loss: 0.21726294893829687 Test Loss: 0.21392409401713655\n",
      "Epoch: 12568 Training Loss: 0.2172627555818334 Test Loss: 0.2139821801454484\n",
      "Epoch: 12569 Training Loss: 0.21726559442339105 Test Loss: 0.21422660595117207\n",
      "Epoch: 12570 Training Loss: 0.21726441276668784 Test Loss: 0.2139956095649822\n",
      "Epoch: 12571 Training Loss: 0.21726403956522294 Test Loss: 0.21376965569249762\n",
      "Epoch: 12572 Training Loss: 0.21726376117710317 Test Loss: 0.2138132364921817\n",
      "Epoch: 12573 Training Loss: 0.21726303870389607 Test Loss: 0.21385379696874668\n",
      "Epoch: 12574 Training Loss: 0.21726538773478155 Test Loss: 0.21379267755455558\n",
      "Epoch: 12575 Training Loss: 0.21726494494973536 Test Loss: 0.21373671731891128\n",
      "Epoch: 12576 Training Loss: 0.21726373780326005 Test Loss: 0.21366583694631386\n",
      "Epoch: 12577 Training Loss: 0.21726647671200022 Test Loss: 0.21379337754360464\n",
      "Epoch: 12578 Training Loss: 0.21726436135678515 Test Loss: 0.21375975214372941\n",
      "Epoch: 12579 Training Loss: 0.2172643270088017 Test Loss: 0.2137320377624907\n",
      "Epoch: 12580 Training Loss: 0.21726285448358376 Test Loss: 0.21917466002309652\n",
      "Epoch: 12581 Training Loss: 0.217265738163112 Test Loss: 0.21394911214426038\n",
      "Epoch: 12582 Training Loss: 0.21726535013221232 Test Loss: 0.21375866327187534\n",
      "Epoch: 12583 Training Loss: 0.21726497334442701 Test Loss: 0.2142986140839041\n",
      "Epoch: 12584 Training Loss: 0.21726523736036785 Test Loss: 0.21426133318566157\n",
      "Epoch: 12585 Training Loss: 0.21726489159425386 Test Loss: 0.21397104513446427\n",
      "Epoch: 12586 Training Loss: 0.2172624648119425 Test Loss: 0.21390125363372092\n",
      "Epoch: 12587 Training Loss: 0.21726471177614984 Test Loss: 0.21381354759842572\n",
      "Epoch: 12588 Training Loss: 0.2172643685114943 Test Loss: 0.21372400085118667\n",
      "Epoch: 12589 Training Loss: 0.21726386553902632 Test Loss: 0.21366976466264467\n",
      "Epoch: 12590 Training Loss: 0.21726583039430647 Test Loss: 0.21368739401647288\n",
      "Epoch: 12591 Training Loss: 0.21726378773380559 Test Loss: 0.21364824648076616\n",
      "Epoch: 12592 Training Loss: 0.2172637980175793 Test Loss: 0.21363323560449188\n",
      "Epoch: 12593 Training Loss: 0.21726554659980873 Test Loss: 0.21432910249581874\n",
      "Epoch: 12594 Training Loss: 0.21726687193347227 Test Loss: 0.21372253605928773\n",
      "Epoch: 12595 Training Loss: 0.21726394264491455 Test Loss: 0.21367336830997133\n",
      "Epoch: 12596 Training Loss: 0.21726475106428964 Test Loss: 0.21375538369355288\n",
      "Epoch: 12597 Training Loss: 0.21726542616220446 Test Loss: 0.21370077158496603\n",
      "Epoch: 12598 Training Loss: 0.21726511131913798 Test Loss: 0.21371497877010992\n",
      "Epoch: 12599 Training Loss: 0.21726529595187713 Test Loss: 0.21391494230845812\n",
      "Epoch: 12600 Training Loss: 0.21726324745463982 Test Loss: 0.21616071458200595\n",
      "Epoch: 12601 Training Loss: 0.2172655171561184 Test Loss: 0.21395165284525328\n",
      "Epoch: 12602 Training Loss: 0.2172633810988689 Test Loss: 0.2138103328339041\n",
      "Epoch: 12603 Training Loss: 0.2172649928540099 Test Loss: 0.21390161659100562\n",
      "Epoch: 12604 Training Loss: 0.21726286753778998 Test Loss: 0.21379433678785706\n",
      "Epoch: 12605 Training Loss: 0.21726450711381134 Test Loss: 0.21370165305265743\n",
      "Epoch: 12606 Training Loss: 0.21726282365916008 Test Loss: 0.21417448269253744\n",
      "Epoch: 12607 Training Loss: 0.21726631449376346 Test Loss: 0.21388626868296698\n",
      "Epoch: 12608 Training Loss: 0.2172642298374509 Test Loss: 0.21373076741199426\n",
      "Epoch: 12609 Training Loss: 0.2172665616360669 Test Loss: 0.21379332569256398\n",
      "Epoch: 12610 Training Loss: 0.21726550239841 Test Loss: 0.21380009225337157\n",
      "Epoch: 12611 Training Loss: 0.21726482498731853 Test Loss: 0.21369721978868006\n",
      "Epoch: 12612 Training Loss: 0.21726530854882747 Test Loss: 0.21383746389093528\n",
      "Epoch: 12613 Training Loss: 0.21726520426759652 Test Loss: 0.21382059933995698\n",
      "Epoch: 12614 Training Loss: 0.21726482229757826 Test Loss: 0.2137361858457444\n",
      "Epoch: 12615 Training Loss: 0.2172640290304068 Test Loss: 0.21388880938395985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12616 Training Loss: 0.2172657061462368 Test Loss: 0.21475997168103564\n",
      "Epoch: 12617 Training Loss: 0.21726401368992138 Test Loss: 0.2138702467113996\n",
      "Epoch: 12618 Training Loss: 0.2172631444824158 Test Loss: 0.21383949904428162\n",
      "Epoch: 12619 Training Loss: 0.21726358931166456 Test Loss: 0.2137902146301237\n",
      "Epoch: 12620 Training Loss: 0.21726399684318137 Test Loss: 0.2138220122808153\n",
      "Epoch: 12621 Training Loss: 0.21726168264443263 Test Loss: 0.21479065453435275\n",
      "Epoch: 12622 Training Loss: 0.21726958979051178 Test Loss: 0.2137865591317564\n",
      "Epoch: 12623 Training Loss: 0.21726418437187422 Test Loss: 0.2138255511143411\n",
      "Epoch: 12624 Training Loss: 0.21726545451206708 Test Loss: 0.2138354028120686\n",
      "Epoch: 12625 Training Loss: 0.21726463937730708 Test Loss: 0.21383992681536715\n",
      "Epoch: 12626 Training Loss: 0.2172633699454125 Test Loss: 0.2137927164428361\n",
      "Epoch: 12627 Training Loss: 0.21726383913474248 Test Loss: 0.21369676609207416\n",
      "Epoch: 12628 Training Loss: 0.21726352275852406 Test Loss: 0.21372580267485\n",
      "Epoch: 12629 Training Loss: 0.21726534269059752 Test Loss: 0.21375850771875332\n",
      "Epoch: 12630 Training Loss: 0.21726609530682742 Test Loss: 0.21375021155224594\n",
      "Epoch: 12631 Training Loss: 0.21726715292167434 Test Loss: 0.21378203512845784\n",
      "Epoch: 12632 Training Loss: 0.21726535276815778 Test Loss: 0.2153830915623009\n",
      "Epoch: 12633 Training Loss: 0.21726478005072414 Test Loss: 0.2138217271000916\n",
      "Epoch: 12634 Training Loss: 0.21726535278608938 Test Loss: 0.21375559109771558\n",
      "Epoch: 12635 Training Loss: 0.21726457192758647 Test Loss: 0.21366679619056625\n",
      "Epoch: 12636 Training Loss: 0.21726692277852952 Test Loss: 0.21371667689169188\n",
      "Epoch: 12637 Training Loss: 0.21726462115879955 Test Loss: 0.21369791977772912\n",
      "Epoch: 12638 Training Loss: 0.21726430516811054 Test Loss: 0.2137344099476014\n",
      "Epoch: 12639 Training Loss: 0.21726485475377771 Test Loss: 0.2136936939179144\n",
      "Epoch: 12640 Training Loss: 0.2172647048097225 Test Loss: 0.21374067096076246\n",
      "Epoch: 12641 Training Loss: 0.21726597789069807 Test Loss: 0.21371837501327387\n",
      "Epoch: 12642 Training Loss: 0.21725972786671274 Test Loss: 0.21507623710360915\n",
      "Epoch: 12643 Training Loss: 0.21726681346748422 Test Loss: 0.2139687377631544\n",
      "Epoch: 12644 Training Loss: 0.217264315057389 Test Loss: 0.21383304358971805\n",
      "Epoch: 12645 Training Loss: 0.2172630033427771 Test Loss: 0.21376751683706993\n",
      "Epoch: 12646 Training Loss: 0.21726601689193223 Test Loss: 0.21381194021616493\n",
      "Epoch: 12647 Training Loss: 0.21726296113178617 Test Loss: 0.21381979564882658\n",
      "Epoch: 12648 Training Loss: 0.21726371456390398 Test Loss: 0.21411883356313716\n",
      "Epoch: 12649 Training Loss: 0.21726504689089227 Test Loss: 0.2137706667877907\n",
      "Epoch: 12650 Training Loss: 0.21726564232766557 Test Loss: 0.21369060878099447\n",
      "Epoch: 12651 Training Loss: 0.217264932083811 Test Loss: 0.21374996525980275\n",
      "Epoch: 12652 Training Loss: 0.21726529597877453 Test Loss: 0.21365999074147818\n",
      "Epoch: 12653 Training Loss: 0.21726356878894618 Test Loss: 0.2137602447286158\n",
      "Epoch: 12654 Training Loss: 0.21726484914118632 Test Loss: 0.21517382076215222\n",
      "Epoch: 12655 Training Loss: 0.21726444108068727 Test Loss: 0.21362989121236858\n",
      "Epoch: 12656 Training Loss: 0.21726620027842505 Test Loss: 0.2137525578118363\n",
      "Epoch: 12657 Training Loss: 0.21726432210450855 Test Loss: 0.21373801359492806\n",
      "Epoch: 12658 Training Loss: 0.21726647430916554 Test Loss: 0.21376174840879528\n",
      "Epoch: 12659 Training Loss: 0.2172643729406 Test Loss: 0.21369219023773495\n",
      "Epoch: 12660 Training Loss: 0.21726577477944314 Test Loss: 0.2137518059717466\n",
      "Epoch: 12661 Training Loss: 0.21726436299752672 Test Loss: 0.2137820092029375\n",
      "Epoch: 12662 Training Loss: 0.21726547462235862 Test Loss: 0.2155261615462727\n",
      "Epoch: 12663 Training Loss: 0.21726499916593378 Test Loss: 0.21376386133870262\n",
      "Epoch: 12664 Training Loss: 0.21726391714617663 Test Loss: 0.21381288649765717\n",
      "Epoch: 12665 Training Loss: 0.21726445649289913 Test Loss: 0.21386904117470398\n",
      "Epoch: 12666 Training Loss: 0.2172647337782254 Test Loss: 0.21382324374303122\n",
      "Epoch: 12667 Training Loss: 0.21726365484270377 Test Loss: 0.2137803629323962\n",
      "Epoch: 12668 Training Loss: 0.2172656536963012 Test Loss: 0.2138570506215488\n",
      "Epoch: 12669 Training Loss: 0.21726296074625673 Test Loss: 0.21383211027098598\n",
      "Epoch: 12670 Training Loss: 0.21726409288484125 Test Loss: 0.2136673017382128\n",
      "Epoch: 12671 Training Loss: 0.21726564317045086 Test Loss: 0.21383973237396464\n",
      "Epoch: 12672 Training Loss: 0.21726229438103206 Test Loss: 0.21452228651059918\n",
      "Epoch: 12673 Training Loss: 0.21726709880409975 Test Loss: 0.21381917343633855\n",
      "Epoch: 12674 Training Loss: 0.21726372198758717 Test Loss: 0.21379550343627216\n",
      "Epoch: 12675 Training Loss: 0.21726351908254565 Test Loss: 0.2137868443124801\n",
      "Epoch: 12676 Training Loss: 0.21726520407931468 Test Loss: 0.2137694223628146\n",
      "Epoch: 12677 Training Loss: 0.21726381071315345 Test Loss: 0.21376900755448922\n",
      "Epoch: 12678 Training Loss: 0.2172644701747114 Test Loss: 0.2152340975969324\n",
      "Epoch: 12679 Training Loss: 0.2172647740167401 Test Loss: 0.21373298404398294\n",
      "Epoch: 12680 Training Loss: 0.2172640887426412 Test Loss: 0.2137096640384411\n",
      "Epoch: 12681 Training Loss: 0.2172636774365222 Test Loss: 0.21369001249402675\n",
      "Epoch: 12682 Training Loss: 0.21726300825603603 Test Loss: 0.21366942763088032\n",
      "Epoch: 12683 Training Loss: 0.21726336589287049 Test Loss: 0.21370608631663482\n",
      "Epoch: 12684 Training Loss: 0.2172639618855234 Test Loss: 0.21377572226425615\n",
      "Epoch: 12685 Training Loss: 0.21726384131343213 Test Loss: 0.21368301260353617\n",
      "Epoch: 12686 Training Loss: 0.21726604048095455 Test Loss: 0.21373234886873474\n",
      "Epoch: 12687 Training Loss: 0.21726464094632225 Test Loss: 0.21373557659601652\n",
      "Epoch: 12688 Training Loss: 0.21726558510792385 Test Loss: 0.21373124703412047\n",
      "Epoch: 12689 Training Loss: 0.2172649958754848 Test Loss: 0.21375809291042794\n",
      "Epoch: 12690 Training Loss: 0.21726446035715935 Test Loss: 0.2156508114480461\n",
      "Epoch: 12691 Training Loss: 0.21726412566380954 Test Loss: 0.2137891776093103\n",
      "Epoch: 12692 Training Loss: 0.21726495954605934 Test Loss: 0.2137601669520548\n",
      "Epoch: 12693 Training Loss: 0.2172647945932533 Test Loss: 0.21392808654726825\n",
      "Epoch: 12694 Training Loss: 0.21726513354535854 Test Loss: 0.2137672835073869\n",
      "Epoch: 12695 Training Loss: 0.21726345551501686 Test Loss: 0.21387364295456354\n",
      "Epoch: 12696 Training Loss: 0.21726239140893006 Test Loss: 0.2141277130538521\n",
      "Epoch: 12697 Training Loss: 0.21726415262397303 Test Loss: 0.21379301458631994\n",
      "Epoch: 12698 Training Loss: 0.21726579437868404 Test Loss: 0.21373056000783158\n",
      "Epoch: 12699 Training Loss: 0.2172649649255399 Test Loss: 0.21373604325538256\n",
      "Epoch: 12700 Training Loss: 0.21726365804349473 Test Loss: 0.2136799015410959\n",
      "Epoch: 12701 Training Loss: 0.21726222075387458 Test Loss: 0.21449786467044307\n",
      "Epoch: 12702 Training Loss: 0.21726561524198088 Test Loss: 0.21384756188110599\n",
      "Epoch: 12703 Training Loss: 0.21726310942613405 Test Loss: 0.21374851343066395\n",
      "Epoch: 12704 Training Loss: 0.2172652743084336 Test Loss: 0.2138899630696148\n",
      "Epoch: 12705 Training Loss: 0.2172658914783084 Test Loss: 0.2137947256706621\n",
      "Epoch: 12706 Training Loss: 0.21726536998249563 Test Loss: 0.21381048838702613\n",
      "Epoch: 12707 Training Loss: 0.21726558349407968 Test Loss: 0.21374268018858847\n",
      "Epoch: 12708 Training Loss: 0.21726546813111874 Test Loss: 0.21370298821695471\n",
      "Epoch: 12709 Training Loss: 0.2172643778897221 Test Loss: 0.21371786946562732\n",
      "Epoch: 12710 Training Loss: 0.21726614145380496 Test Loss: 0.21379127757645747\n",
      "Epoch: 12711 Training Loss: 0.21726495811153118 Test Loss: 0.21370196415890144\n",
      "Epoch: 12712 Training Loss: 0.21726547396785514 Test Loss: 0.21372135644811247\n",
      "Epoch: 12713 Training Loss: 0.21726368598093052 Test Loss: 0.21371573061019963\n",
      "Epoch: 12714 Training Loss: 0.2172652295242578 Test Loss: 0.21368193669444224\n",
      "Epoch: 12715 Training Loss: 0.2172644485402337 Test Loss: 0.21366767765825767\n",
      "Epoch: 12716 Training Loss: 0.21726867044624726 Test Loss: 0.21370796591685914\n",
      "Epoch: 12717 Training Loss: 0.21726572631032315 Test Loss: 0.21368119781711267\n",
      "Epoch: 12718 Training Loss: 0.21726495899914547 Test Loss: 0.21371706577449692\n",
      "Epoch: 12719 Training Loss: 0.21726433900504336 Test Loss: 0.21603448322349209\n",
      "Epoch: 12720 Training Loss: 0.21726575266081216 Test Loss: 0.2137460375434719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12721 Training Loss: 0.21726508201890044 Test Loss: 0.21368301260353617\n",
      "Epoch: 12722 Training Loss: 0.2172646984081406 Test Loss: 0.21374063207248195\n",
      "Epoch: 12723 Training Loss: 0.2172658697093437 Test Loss: 0.21375329668916587\n",
      "Epoch: 12724 Training Loss: 0.21726501779686816 Test Loss: 0.21378171105945365\n",
      "Epoch: 12725 Training Loss: 0.2172643477646309 Test Loss: 0.2137877257801715\n",
      "Epoch: 12726 Training Loss: 0.217265575146919 Test Loss: 0.2137505874722908\n",
      "Epoch: 12727 Training Loss: 0.2172645337153428 Test Loss: 0.21368717364955\n",
      "Epoch: 12728 Training Loss: 0.21726492897267807 Test Loss: 0.2137318822093687\n",
      "Epoch: 12729 Training Loss: 0.21726517989854952 Test Loss: 0.21375780772970426\n",
      "Epoch: 12730 Training Loss: 0.21726448816907393 Test Loss: 0.21375222078007194\n",
      "Epoch: 12731 Training Loss: 0.21726316468236534 Test Loss: 0.2137232360483368\n",
      "Epoch: 12732 Training Loss: 0.21726479956927283 Test Loss: 0.21374886342518848\n",
      "Epoch: 12733 Training Loss: 0.21726450879041612 Test Loss: 0.21374671160700065\n",
      "Epoch: 12734 Training Loss: 0.21726332022108039 Test Loss: 0.21379457011754008\n",
      "Epoch: 12735 Training Loss: 0.2172639571515805 Test Loss: 0.2139748561859536\n",
      "Epoch: 12736 Training Loss: 0.21726172776930885 Test Loss: 0.21413696846461186\n",
      "Epoch: 12737 Training Loss: 0.21726406061692358 Test Loss: 0.21376029657965648\n",
      "Epoch: 12738 Training Loss: 0.21726334391769234 Test Loss: 0.21364555022665127\n",
      "Epoch: 12739 Training Loss: 0.21726583666140134 Test Loss: 0.21366386660676834\n",
      "Epoch: 12740 Training Loss: 0.21726506268863358 Test Loss: 0.21364477246104122\n",
      "Epoch: 12741 Training Loss: 0.21726487195914976 Test Loss: 0.21407062505807317\n",
      "Epoch: 12742 Training Loss: 0.21726629163993683 Test Loss: 0.21908487994617448\n",
      "Epoch: 12743 Training Loss: 0.21726784677604474 Test Loss: 0.21415986069906817\n",
      "Epoch: 12744 Training Loss: 0.21726631549793318 Test Loss: 0.21383216212202666\n",
      "Epoch: 12745 Training Loss: 0.217263152040586 Test Loss: 0.21376729647014708\n",
      "Epoch: 12746 Training Loss: 0.21726443443702878 Test Loss: 0.2137550985128292\n",
      "Epoch: 12747 Training Loss: 0.2172646197691004 Test Loss: 0.21372485639335775\n",
      "Epoch: 12748 Training Loss: 0.2172660745689298 Test Loss: 0.21380255517780344\n",
      "Epoch: 12749 Training Loss: 0.21726412555621993 Test Loss: 0.21373849321705427\n",
      "Epoch: 12750 Training Loss: 0.21726474634827833 Test Loss: 0.21374360054456037\n",
      "Epoch: 12751 Training Loss: 0.21726449968116235 Test Loss: 0.21379135535301846\n",
      "Epoch: 12752 Training Loss: 0.21726453913068658 Test Loss: 0.21377574818977646\n",
      "Epoch: 12753 Training Loss: 0.21726440526231244 Test Loss: 0.2137987441263141\n",
      "Epoch: 12754 Training Loss: 0.2172654584480537 Test Loss: 0.21374912268039184\n",
      "Epoch: 12755 Training Loss: 0.21726382386598347 Test Loss: 0.21389294450445337\n",
      "Epoch: 12756 Training Loss: 0.217262005646378 Test Loss: 0.21436010941814007\n",
      "Epoch: 12757 Training Loss: 0.21726572937662708 Test Loss: 0.21389385189766513\n",
      "Epoch: 12758 Training Loss: 0.21726198766994706 Test Loss: 0.21374898009003\n",
      "Epoch: 12759 Training Loss: 0.21726479964099923 Test Loss: 0.2137929108842386\n",
      "Epoch: 12760 Training Loss: 0.21726486904526446 Test Loss: 0.21378749245048848\n",
      "Epoch: 12761 Training Loss: 0.21726596202123036 Test Loss: 0.21386884673330148\n",
      "Epoch: 12762 Training Loss: 0.21726287739120523 Test Loss: 0.21380385145382022\n",
      "Epoch: 12763 Training Loss: 0.21726236265560636 Test Loss: 0.21378054441103855\n",
      "Epoch: 12764 Training Loss: 0.2172648917377067 Test Loss: 0.2137901368535627\n",
      "Epoch: 12765 Training Loss: 0.217264210372697 Test Loss: 0.21410403009102555\n",
      "Epoch: 12766 Training Loss: 0.21726521572589014 Test Loss: 0.21524509001755468\n",
      "Epoch: 12767 Training Loss: 0.21726683777377062 Test Loss: 0.21379231459727088\n",
      "Epoch: 12768 Training Loss: 0.21726397322726163 Test Loss: 0.21382469557217001\n",
      "Epoch: 12769 Training Loss: 0.21726393702335736 Test Loss: 0.21383304358971805\n",
      "Epoch: 12770 Training Loss: 0.21726425530929142 Test Loss: 0.2137432764755562\n",
      "Epoch: 12771 Training Loss: 0.2172667777029042 Test Loss: 0.21379718859509397\n",
      "Epoch: 12772 Training Loss: 0.2172645189397028 Test Loss: 0.21395476390769352\n",
      "Epoch: 12773 Training Loss: 0.2172650330476956 Test Loss: 0.21388725385273974\n",
      "Epoch: 12774 Training Loss: 0.21726462028015103 Test Loss: 0.21381041061046513\n",
      "Epoch: 12775 Training Loss: 0.2172636820539097 Test Loss: 0.21372203051164118\n",
      "Epoch: 12776 Training Loss: 0.2172655650962561 Test Loss: 0.21378063515035972\n",
      "Epoch: 12777 Training Loss: 0.21726493064031704 Test Loss: 0.213727734126115\n",
      "Epoch: 12778 Training Loss: 0.21726478338600208 Test Loss: 0.2136846459113173\n",
      "Epoch: 12779 Training Loss: 0.21726464256913222 Test Loss: 0.2137140195258575\n",
      "Epoch: 12780 Training Loss: 0.2172640290035094 Test Loss: 0.2139237569853722\n",
      "Epoch: 12781 Training Loss: 0.21726418863062968 Test Loss: 0.2137099492191648\n",
      "Epoch: 12782 Training Loss: 0.2172661716864858 Test Loss: 0.2137233138248978\n",
      "Epoch: 12783 Training Loss: 0.21726416053180947 Test Loss: 0.21374468941641447\n",
      "Epoch: 12784 Training Loss: 0.21726219132811583 Test Loss: 0.2138409897617009\n",
      "Epoch: 12785 Training Loss: 0.21726288306655722 Test Loss: 0.2191342680624137\n",
      "Epoch: 12786 Training Loss: 0.21726517925301184 Test Loss: 0.21411003184898322\n",
      "Epoch: 12787 Training Loss: 0.21726375741146675 Test Loss: 0.21372585452589068\n",
      "Epoch: 12788 Training Loss: 0.21726574303154195 Test Loss: 0.21379944411536317\n",
      "Epoch: 12789 Training Loss: 0.2172651416594084 Test Loss: 0.21367658307449294\n",
      "Epoch: 12790 Training Loss: 0.2172659351955539 Test Loss: 0.21373552474497584\n",
      "Epoch: 12791 Training Loss: 0.217266614874993 Test Loss: 0.21371575653571998\n",
      "Epoch: 12792 Training Loss: 0.217265867172022 Test Loss: 0.21377752408791945\n",
      "Epoch: 12793 Training Loss: 0.21726607117089122 Test Loss: 0.21372703413706595\n",
      "Epoch: 12794 Training Loss: 0.2172677070709342 Test Loss: 0.21376810016127748\n",
      "Epoch: 12795 Training Loss: 0.21726627913264449 Test Loss: 0.21378005182615217\n",
      "Epoch: 12796 Training Loss: 0.21726456353559678 Test Loss: 0.21371328064852793\n",
      "Epoch: 12797 Training Loss: 0.21726466419464413 Test Loss: 0.21378344806931612\n",
      "Epoch: 12798 Training Loss: 0.21726488689617415 Test Loss: 0.21374629679867527\n",
      "Epoch: 12799 Training Loss: 0.21726484123334988 Test Loss: 0.21372337863869864\n",
      "Epoch: 12800 Training Loss: 0.21726390586719904 Test Loss: 0.21552364677080016\n",
      "Epoch: 12801 Training Loss: 0.2172653145021193 Test Loss: 0.21398538194720984\n",
      "Epoch: 12802 Training Loss: 0.2172647480876437 Test Loss: 0.21434310227679992\n",
      "Epoch: 12803 Training Loss: 0.2172617694423517 Test Loss: 0.21395683794932038\n",
      "Epoch: 12804 Training Loss: 0.21726644026601932 Test Loss: 0.213882483556998\n",
      "Epoch: 12805 Training Loss: 0.2172639352481288 Test Loss: 0.21375117079649836\n",
      "Epoch: 12806 Training Loss: 0.21726436997291987 Test Loss: 0.2137700316125425\n",
      "Epoch: 12807 Training Loss: 0.2172647383776813 Test Loss: 0.21370051232976267\n",
      "Epoch: 12808 Training Loss: 0.21726509213232392 Test Loss: 0.21365332788275193\n",
      "Epoch: 12809 Training Loss: 0.21726529198899308 Test Loss: 0.21374186353469787\n",
      "Epoch: 12810 Training Loss: 0.21726442557881742 Test Loss: 0.21370967700120128\n",
      "Epoch: 12811 Training Loss: 0.21726492525187066 Test Loss: 0.21368474961339864\n",
      "Epoch: 12812 Training Loss: 0.21726489892827905 Test Loss: 0.21490350832437347\n",
      "Epoch: 12813 Training Loss: 0.21726595435547055 Test Loss: 0.2137205397942219\n",
      "Epoch: 12814 Training Loss: 0.21726447749977076 Test Loss: 0.21373495438352846\n",
      "Epoch: 12815 Training Loss: 0.21726300302000826 Test Loss: 0.21375279114151932\n",
      "Epoch: 12816 Training Loss: 0.21726442280838493 Test Loss: 0.21372564712172798\n",
      "Epoch: 12817 Training Loss: 0.21726659229014036 Test Loss: 0.2138274696028459\n",
      "Epoch: 12818 Training Loss: 0.21726575650714078 Test Loss: 0.2139460010818201\n",
      "Epoch: 12819 Training Loss: 0.2172637279139816 Test Loss: 0.21384809335427285\n",
      "Epoch: 12820 Training Loss: 0.2172645974083928 Test Loss: 0.21387241149234762\n",
      "Epoch: 12821 Training Loss: 0.21726472058953217 Test Loss: 0.21380287924680763\n",
      "Epoch: 12822 Training Loss: 0.21726288754049192 Test Loss: 0.21362496536350484\n",
      "Epoch: 12823 Training Loss: 0.21726386482176224 Test Loss: 0.2136545982332484\n",
      "Epoch: 12824 Training Loss: 0.21726442412635766 Test Loss: 0.21366253144247108\n",
      "Epoch: 12825 Training Loss: 0.21726584382507633 Test Loss: 0.21378935908795263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12826 Training Loss: 0.21726453627956185 Test Loss: 0.21377434821167834\n",
      "Epoch: 12827 Training Loss: 0.21726423725216829 Test Loss: 0.2137665575928175\n",
      "Epoch: 12828 Training Loss: 0.2172645731827986 Test Loss: 0.21370909367699373\n",
      "Epoch: 12829 Training Loss: 0.21726277562039853 Test Loss: 0.21373475994212593\n",
      "Epoch: 12830 Training Loss: 0.2172650836506762 Test Loss: 0.2139244051233806\n",
      "Epoch: 12831 Training Loss: 0.2172644524941519 Test Loss: 0.21418537141107838\n",
      "Epoch: 12832 Training Loss: 0.2172635211805431 Test Loss: 0.21393139205111103\n",
      "Epoch: 12833 Training Loss: 0.21726316210918048 Test Loss: 0.21520478879619306\n",
      "Epoch: 12834 Training Loss: 0.21726624866685282 Test Loss: 0.21374452090053228\n",
      "Epoch: 12835 Training Loss: 0.21726157803146703 Test Loss: 0.2136721238849952\n",
      "Epoch: 12836 Training Loss: 0.21726465913793241 Test Loss: 0.2136847236878783\n",
      "Epoch: 12837 Training Loss: 0.21726508955913904 Test Loss: 0.21370454374817485\n",
      "Epoch: 12838 Training Loss: 0.21726459776702484 Test Loss: 0.2136475983427578\n",
      "Epoch: 12839 Training Loss: 0.21726481379799895 Test Loss: 0.21364908906017707\n",
      "Epoch: 12840 Training Loss: 0.21726633209363075 Test Loss: 0.21363615222552962\n",
      "Epoch: 12841 Training Loss: 0.21726567168169791 Test Loss: 0.21815858703010513\n",
      "Epoch: 12842 Training Loss: 0.2172673955272825 Test Loss: 0.21396689705121058\n",
      "Epoch: 12843 Training Loss: 0.21726546649037715 Test Loss: 0.21370536040206542\n",
      "Epoch: 12844 Training Loss: 0.21726607415650295 Test Loss: 0.21371400656309733\n",
      "Epoch: 12845 Training Loss: 0.21726399283546832 Test Loss: 0.21382990660175746\n",
      "Epoch: 12846 Training Loss: 0.21726496349997756 Test Loss: 0.2140105426646955\n",
      "Epoch: 12847 Training Loss: 0.21726233424298314 Test Loss: 0.21388044840365164\n",
      "Epoch: 12848 Training Loss: 0.21726431633053273 Test Loss: 0.21382789737393146\n",
      "Epoch: 12849 Training Loss: 0.21726572059910793 Test Loss: 0.21380626252721144\n",
      "Epoch: 12850 Training Loss: 0.21726381694438512 Test Loss: 0.2137390376529813\n",
      "Epoch: 12851 Training Loss: 0.21726485292475434 Test Loss: 0.2151147235385473\n",
      "Epoch: 12852 Training Loss: 0.21726525757824902 Test Loss: 0.2136799793176569\n",
      "Epoch: 12853 Training Loss: 0.21726556010230497 Test Loss: 0.21375398371545476\n",
      "Epoch: 12854 Training Loss: 0.21726565793712505 Test Loss: 0.21370590483799246\n",
      "Epoch: 12855 Training Loss: 0.21726371806056635 Test Loss: 0.21372317123453594\n",
      "Epoch: 12856 Training Loss: 0.21726468118483697 Test Loss: 0.21373880432329828\n",
      "Epoch: 12857 Training Loss: 0.21726546747661526 Test Loss: 0.2137776666782813\n",
      "Epoch: 12858 Training Loss: 0.2172657726993773 Test Loss: 0.21367207203395455\n",
      "Epoch: 12859 Training Loss: 0.21726624313495363 Test Loss: 0.21540813561494504\n",
      "Epoch: 12860 Training Loss: 0.21726576081072524 Test Loss: 0.21383378246704762\n",
      "Epoch: 12861 Training Loss: 0.21726382501360597 Test Loss: 0.21378572951510566\n",
      "Epoch: 12862 Training Loss: 0.21726365247573232 Test Loss: 0.2137928331076776\n",
      "Epoch: 12863 Training Loss: 0.21726169778767046 Test Loss: 0.21460953884928852\n",
      "Epoch: 12864 Training Loss: 0.21726743035045343 Test Loss: 0.21390914795466312\n",
      "Epoch: 12865 Training Loss: 0.21726340060845178 Test Loss: 0.21375990769685144\n",
      "Epoch: 12866 Training Loss: 0.21726476603717723 Test Loss: 0.2137767463223094\n",
      "Epoch: 12867 Training Loss: 0.21726578595979693 Test Loss: 0.21379524418106882\n",
      "Epoch: 12868 Training Loss: 0.21726485222542186 Test Loss: 0.2138408730968594\n",
      "Epoch: 12869 Training Loss: 0.21726530834261404 Test Loss: 0.21376421133322715\n",
      "Epoch: 12870 Training Loss: 0.2172645898591884 Test Loss: 0.21372619155765504\n",
      "Epoch: 12871 Training Loss: 0.21726450744554598 Test Loss: 0.2137300674229452\n",
      "Epoch: 12872 Training Loss: 0.2172650788181095 Test Loss: 0.21372716376466763\n",
      "Epoch: 12873 Training Loss: 0.21726445859089655 Test Loss: 0.2140509346253783\n",
      "Epoch: 12874 Training Loss: 0.21726594816010208 Test Loss: 0.2147051003172454\n",
      "Epoch: 12875 Training Loss: 0.21726505152621137 Test Loss: 0.21375822253802962\n",
      "Epoch: 12876 Training Loss: 0.21726509442756897 Test Loss: 0.21372558230792715\n",
      "Epoch: 12877 Training Loss: 0.21726349578939477 Test Loss: 0.21369483464080918\n",
      "Epoch: 12878 Training Loss: 0.21726596177915372 Test Loss: 0.21371259362223904\n",
      "Epoch: 12879 Training Loss: 0.21726602335627473 Test Loss: 0.2136601462946002\n",
      "Epoch: 12880 Training Loss: 0.2172666262436286 Test Loss: 0.213745039410939\n",
      "Epoch: 12881 Training Loss: 0.2172652760567648 Test Loss: 0.21369429020488212\n",
      "Epoch: 12882 Training Loss: 0.21726583946769706 Test Loss: 0.21375679663441116\n",
      "Epoch: 12883 Training Loss: 0.2172663539612193 Test Loss: 0.2137265674776999\n",
      "Epoch: 12884 Training Loss: 0.2172653470659084 Test Loss: 0.21379262570351493\n",
      "Epoch: 12885 Training Loss: 0.21726462322989956 Test Loss: 0.21376167063223425\n",
      "Epoch: 12886 Training Loss: 0.21726421458662346 Test Loss: 0.21375092450405517\n",
      "Epoch: 12887 Training Loss: 0.21726309762713997 Test Loss: 0.2137363025105859\n",
      "Epoch: 12888 Training Loss: 0.21726376964978505 Test Loss: 0.2137508596902543\n",
      "Epoch: 12889 Training Loss: 0.21726431630363532 Test Loss: 0.21374199316229955\n",
      "Epoch: 12890 Training Loss: 0.21726397494869543 Test Loss: 0.2137543725982598\n",
      "Epoch: 12891 Training Loss: 0.21726542324831913 Test Loss: 0.21378723319528511\n",
      "Epoch: 12892 Training Loss: 0.21726362632249094 Test Loss: 0.2137310785182383\n",
      "Epoch: 12893 Training Loss: 0.21726215216549724 Test Loss: 0.21513266399861952\n",
      "Epoch: 12894 Training Loss: 0.21726367070320568 Test Loss: 0.21377888517773708\n",
      "Epoch: 12895 Training Loss: 0.21726247271081314 Test Loss: 0.21367872192992063\n",
      "Epoch: 12896 Training Loss: 0.21726452950141634 Test Loss: 0.21372800634407854\n",
      "Epoch: 12897 Training Loss: 0.21726430075693648 Test Loss: 0.21368249409312945\n",
      "Epoch: 12898 Training Loss: 0.21726538693682526 Test Loss: 0.21369235875361714\n",
      "Epoch: 12899 Training Loss: 0.21726467843233607 Test Loss: 0.21369021989818945\n",
      "Epoch: 12900 Training Loss: 0.21726484150232392 Test Loss: 0.2137746852434427\n",
      "Epoch: 12901 Training Loss: 0.21726458599492818 Test Loss: 0.2139248458572263\n",
      "Epoch: 12902 Training Loss: 0.2172653518715777 Test Loss: 0.21398997076430923\n",
      "Epoch: 12903 Training Loss: 0.21726259813340276 Test Loss: 0.2137880887374562\n",
      "Epoch: 12904 Training Loss: 0.21726252996641807 Test Loss: 0.21372376752150365\n",
      "Epoch: 12905 Training Loss: 0.21726223600470201 Test Loss: 0.21372674895634225\n",
      "Epoch: 12906 Training Loss: 0.21726491216180127 Test Loss: 0.21371552320603696\n",
      "Epoch: 12907 Training Loss: 0.21726378338539212 Test Loss: 0.2136877051227169\n",
      "Epoch: 12908 Training Loss: 0.2172644516513666 Test Loss: 0.21366658878640357\n",
      "Epoch: 12909 Training Loss: 0.21726497123746377 Test Loss: 0.21373536919185382\n",
      "Epoch: 12910 Training Loss: 0.21726303292992027 Test Loss: 0.21372179718195816\n",
      "Epoch: 12911 Training Loss: 0.2172656951272674 Test Loss: 0.2138582561582444\n",
      "Epoch: 12912 Training Loss: 0.21726499981147143 Test Loss: 0.213708264060343\n",
      "Epoch: 12913 Training Loss: 0.21726433897814595 Test Loss: 0.2137198916562135\n",
      "Epoch: 12914 Training Loss: 0.21726312704393294 Test Loss: 0.2137077714754566\n",
      "Epoch: 12915 Training Loss: 0.21726567405763517 Test Loss: 0.21372871929588774\n",
      "Epoch: 12916 Training Loss: 0.21726497122849797 Test Loss: 0.21373967282822953\n",
      "Epoch: 12917 Training Loss: 0.21726406967238254 Test Loss: 0.21380239962468142\n",
      "Epoch: 12918 Training Loss: 0.21726514182079282 Test Loss: 0.21370689000776522\n",
      "Epoch: 12919 Training Loss: 0.21726635631922495 Test Loss: 0.2143860479012358\n",
      "Epoch: 12920 Training Loss: 0.21726427620857344 Test Loss: 0.21368723846335086\n",
      "Epoch: 12921 Training Loss: 0.21726273346320243 Test Loss: 0.21375658923024848\n",
      "Epoch: 12922 Training Loss: 0.2172647601107828 Test Loss: 0.21380919211100935\n",
      "Epoch: 12923 Training Loss: 0.2172650751421311 Test Loss: 0.21378063515035972\n",
      "Epoch: 12924 Training Loss: 0.2172642300795275 Test Loss: 0.21379345532016567\n",
      "Epoch: 12925 Training Loss: 0.21726384045271524 Test Loss: 0.21371947684788814\n",
      "Epoch: 12926 Training Loss: 0.2172636476341998 Test Loss: 0.2137455579213457\n",
      "Epoch: 12927 Training Loss: 0.2172633240494774 Test Loss: 0.21362732458585537\n",
      "Epoch: 12928 Training Loss: 0.21726654088023767 Test Loss: 0.2137634594931374\n",
      "Epoch: 12929 Training Loss: 0.2172636765489079 Test Loss: 0.21370514003514257\n",
      "Epoch: 12930 Training Loss: 0.21726424076676226 Test Loss: 0.21363055231313716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12931 Training Loss: 0.21726587198665712 Test Loss: 0.2137783148162897\n",
      "Epoch: 12932 Training Loss: 0.21726530004028236 Test Loss: 0.21370421967917064\n",
      "Epoch: 12933 Training Loss: 0.217265307939153 Test Loss: 0.21371596393988265\n",
      "Epoch: 12934 Training Loss: 0.21726468070964952 Test Loss: 0.21368837918624561\n",
      "Epoch: 12935 Training Loss: 0.21726349208651896 Test Loss: 0.2136934476254712\n",
      "Epoch: 12936 Training Loss: 0.2172648038549257 Test Loss: 0.21370481596613836\n",
      "Epoch: 12937 Training Loss: 0.2172649218448663 Test Loss: 0.21377250749973453\n",
      "Epoch: 12938 Training Loss: 0.21726611519297395 Test Loss: 0.21369480871528884\n",
      "Epoch: 12939 Training Loss: 0.2172656201821372 Test Loss: 0.21510574034575103\n",
      "Epoch: 12940 Training Loss: 0.21726522529239975 Test Loss: 0.21373028778986805\n",
      "Epoch: 12941 Training Loss: 0.21726685680816607 Test Loss: 0.21370357154116226\n",
      "Epoch: 12942 Training Loss: 0.21726498809316958 Test Loss: 0.21379117387437613\n",
      "Epoch: 12943 Training Loss: 0.2172651714258676 Test Loss: 0.21369882717094085\n",
      "Epoch: 12944 Training Loss: 0.21726504403976757 Test Loss: 0.21381191429064458\n",
      "Epoch: 12945 Training Loss: 0.21726353728312162 Test Loss: 0.2137494597121562\n",
      "Epoch: 12946 Training Loss: 0.21726448001916085 Test Loss: 0.21388787606522777\n",
      "Epoch: 12947 Training Loss: 0.2172648190788557 Test Loss: 0.2138160494111381\n",
      "Epoch: 12948 Training Loss: 0.21726413154537497 Test Loss: 0.21375246707251513\n",
      "Epoch: 12949 Training Loss: 0.21726467435289662 Test Loss: 0.2138913760104731\n",
      "Epoch: 12950 Training Loss: 0.21726499828728527 Test Loss: 0.21370682519396436\n",
      "Epoch: 12951 Training Loss: 0.21726287077444412 Test Loss: 0.213688871771132\n",
      "Epoch: 12952 Training Loss: 0.21726615284037218 Test Loss: 0.2137881665140172\n",
      "Epoch: 12953 Training Loss: 0.2172649161695143 Test Loss: 0.2137903831460059\n",
      "Epoch: 12954 Training Loss: 0.21726398357379595 Test Loss: 0.21374590791587023\n",
      "Epoch: 12955 Training Loss: 0.21726396421663166 Test Loss: 0.21379226274623023\n",
      "Epoch: 12956 Training Loss: 0.21726451744241404 Test Loss: 0.2137922757089904\n",
      "Epoch: 12957 Training Loss: 0.21726410391277642 Test Loss: 0.21376911125657055\n",
      "Epoch: 12958 Training Loss: 0.21726365532685704 Test Loss: 0.21373780619076538\n",
      "Epoch: 12959 Training Loss: 0.21726527591331196 Test Loss: 0.21380652178241477\n",
      "Epoch: 12960 Training Loss: 0.2172642459310636 Test Loss: 0.21366494251586227\n",
      "Epoch: 12961 Training Loss: 0.21726588807130406 Test Loss: 0.21373733953139934\n",
      "Epoch: 12962 Training Loss: 0.21726685169765952 Test Loss: 0.21372578971208983\n",
      "Epoch: 12963 Training Loss: 0.21726568998089765 Test Loss: 0.213704323381252\n",
      "Epoch: 12964 Training Loss: 0.21726461168194794 Test Loss: 0.21361893768002682\n",
      "Epoch: 12965 Training Loss: 0.2172651254313087 Test Loss: 0.21364522615764708\n",
      "Epoch: 12966 Training Loss: 0.21726250166138444 Test Loss: 0.21377067975055086\n",
      "Epoch: 12967 Training Loss: 0.21726272865753313 Test Loss: 0.21372013794865669\n",
      "Epoch: 12968 Training Loss: 0.2172646030209842 Test Loss: 0.21372988594430287\n",
      "Epoch: 12969 Training Loss: 0.21726400983462696 Test Loss: 0.21386604677710525\n",
      "Epoch: 12970 Training Loss: 0.21726369476741544 Test Loss: 0.21405136239646383\n",
      "Epoch: 12971 Training Loss: 0.21726215462212672 Test Loss: 0.21420453037060636\n",
      "Epoch: 12972 Training Loss: 0.21726434650941875 Test Loss: 0.21386568381982055\n",
      "Epoch: 12973 Training Loss: 0.21726414311125822 Test Loss: 0.2141079059563157\n",
      "Epoch: 12974 Training Loss: 0.21726357409670036 Test Loss: 0.21377512597728843\n",
      "Epoch: 12975 Training Loss: 0.21726479351735717 Test Loss: 0.21378578136614634\n",
      "Epoch: 12976 Training Loss: 0.21726420440147357 Test Loss: 0.21377828889076936\n",
      "Epoch: 12977 Training Loss: 0.21726438205881957 Test Loss: 0.21375556517219524\n",
      "Epoch: 12978 Training Loss: 0.21726612960998187 Test Loss: 0.2137270600625863\n",
      "Epoch: 12979 Training Loss: 0.21726532779840213 Test Loss: 0.2143949014664304\n",
      "Epoch: 12980 Training Loss: 0.21726545164301078 Test Loss: 0.2137854702599023\n",
      "Epoch: 12981 Training Loss: 0.21726364877285653 Test Loss: 0.21374362647008072\n",
      "Epoch: 12982 Training Loss: 0.21726528319354235 Test Loss: 0.2137267748818626\n",
      "Epoch: 12983 Training Loss: 0.21726497440239154 Test Loss: 0.21378219068157986\n",
      "Epoch: 12984 Training Loss: 0.21726487265848224 Test Loss: 0.2137340858785972\n",
      "Epoch: 12985 Training Loss: 0.21726562690648793 Test Loss: 0.21376962976697728\n",
      "Epoch: 12986 Training Loss: 0.2172651237726355 Test Loss: 0.21378694801456144\n",
      "Epoch: 12987 Training Loss: 0.21726480898336384 Test Loss: 0.21379582750527637\n",
      "Epoch: 12988 Training Loss: 0.2172653442506469 Test Loss: 0.2137659613058498\n",
      "Epoch: 12989 Training Loss: 0.21726433324003333 Test Loss: 0.2137812444000876\n",
      "Epoch: 12990 Training Loss: 0.21726426975319676 Test Loss: 0.21377618892362218\n",
      "Epoch: 12991 Training Loss: 0.217265234966499 Test Loss: 0.21415624408898137\n",
      "Epoch: 12992 Training Loss: 0.2172650358808887 Test Loss: 0.21374270611410878\n",
      "Epoch: 12993 Training Loss: 0.21726425859077456 Test Loss: 0.213673575714134\n",
      "Epoch: 12994 Training Loss: 0.21726530855779327 Test Loss: 0.21371101216549857\n",
      "Epoch: 12995 Training Loss: 0.21726440105735179 Test Loss: 0.21369854199021715\n",
      "Epoch: 12996 Training Loss: 0.21726449805835238 Test Loss: 0.21362720792101386\n",
      "Epoch: 12997 Training Loss: 0.2172649769755764 Test Loss: 0.21978883559984602\n",
      "Epoch: 12998 Training Loss: 0.21726781464261413 Test Loss: 0.21414228319628067\n",
      "Epoch: 12999 Training Loss: 0.21726385321105 Test Loss: 0.21382975104863544\n"
     ]
    }
   ],
   "source": [
    "training_data_size=int(x_variable.size()[0])\n",
    "print(\"Training data size: \",training_data_size)\n",
    "batch_size_div=batch_size_factor(training_data_size,2,2000)\n",
    "batch_size=int(training_data_size/batch_size_div)\n",
    "nb_of_epochs=13000 \n",
    "#my_net=my_net.cpu()\n",
    "my_net=my_net.cuda()\n",
    "\n",
    "# Train the network #\n",
    "\n",
    "my_net.train()\n",
    "\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "    my_net.eval()\n",
    "    test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "    my_net.train()\n",
    "    print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial set edges: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (3, 4)}\n",
      "Edge: (5, 0) targeting: 4\n",
      "(5, 0, 4)\n",
      "Vertex locked: 5\n",
      "edges inserted: (0, 4)\n",
      "set of interior edges updated: {(0, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (0, 4), (3, 4)}\n",
      "element inserted: (5, 0, 4)\n",
      "Edge: (4, 5) targeting: 1\n",
      "found (4, 0) (5, 0) Canceling creation\n",
      "Edge: (4, 5) targeting: 0\n",
      "(4, 5, 0)\n",
      "Element (5, 0, 4) already in set\n",
      "Edge: (3, 4) targeting: 1\n",
      "(3, 4, 1)\n",
      "edges inserted: (3, 1)\n",
      "set of interior edges updated: {(3, 1), (0, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (3, 1), (2, 3), (5, 0), (0, 4), (3, 4)}\n",
      "edges inserted: (4, 1)\n",
      "set of interior edges updated: {(3, 1), (4, 1), (0, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (3, 1), (2, 3), (5, 0), (0, 4), (3, 4)}\n",
      "element inserted: (3, 4, 1)\n",
      "Edge: (2, 3) targeting: 1\n",
      "(2, 3, 1)\n",
      "Vertex locked: 2\n",
      "Vertex locked: 3\n",
      "element inserted: (2, 3, 1)\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "Element (2, 3, 1) already in set\n",
      "Edge: (0, 1) targeting: 5\n",
      "Element inserted: (0, 1, 4)\n",
      "Edge: (0, 1) targeting: 4\n",
      "Element inserted: (0, 1, 4)\n",
      "Edge: (0, 1) targeting: 3\n",
      "Element inserted: (0, 1, 4)\n",
      "Edge: (0, 1) targeting: 2\n",
      "Element inserted: (0, 1, 4)\n",
      "Final edges: {(0, 1), (1, 2), (4, 1), (4, 5), (3, 1), (2, 3), (5, 0), (0, 4), (3, 4)}\n",
      "Elements created: {(5, 0, 4), (0, 1, 4), (3, 4, 1), (2, 3, 1)}\n",
      "Set of locked vertices: {2, 3, 5}\n",
      "Vertex locked: 0\n",
      "Vertex locked: 4\n",
      "Vertex locked: 1\n",
      "Set of open vertices: set()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Result using connection table \n",
    "\n",
    "random_contour=apply_procrustes(generate_contour(6))\n",
    "original_quality_matrix,_=quality_matrix(random_contour)\n",
    "ordered_matrix=order_quality_matrix(original_quality_matrix,random_contour,check_for_equal=False)\n",
    "triangulate_old_vers(random_contour,ordered_matrix,recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating result using NN\n",
    "nb_of_grid_points=20\n",
    "\n",
    "directory='../polygon_datasets/validation_sets/grid_patch_regression/'\n",
    "polygon_data=str(nb_of_edges)+'_'+str(nb_of_points)+'_valdation_polygon_data.pkl'\n",
    "point_coordinates=str(nb_of_edges)+'_'+str(nb_of_points)+'_valdation_point_coordinates.pkl'\n",
    "estimated_points=str(nb_of_edges)+'_'+str(nb_of_points)+'_'+str(nb_of_grid_points)+'_valdation_estimated_points.pkl'\n",
    "interpolatd_estimated_points=str(nb_of_edges)+'_'+str(nb_of_points)+'_'+str(nb_of_grid_points)+'_valdation_estimated_interpolated_points.pkl'\n",
    "polygons=[]\n",
    "\n",
    "if nb_of_points==1:\n",
    "    \n",
    "    with open(os.path.join(directory,polygon_data),'rb') as f:\n",
    "        polygons=pickle.load(f)\n",
    "else:\n",
    "    with open(os.path.join(directory,polygon_data),'rb') as f:\n",
    "        try:\n",
    "            while True:\n",
    "                polygons.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "point_coordinates_list=[]\n",
    "with open(os.path.join(directory,point_coordinates),'rb') as f:\n",
    "    try:\n",
    "        while True:\n",
    "            point_coordinates_list.append(pickle.load(f))\n",
    "    except EOFError:\n",
    "        pass\n",
    "    \n",
    "with open(os.path.join(directory,estimated_points),'rb') as f:\n",
    "    estimated_points=pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(directory,interpolatd_estimated_points),'rb') as f:\n",
    "       interpolatd_estimated_points=pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]\n",
      "   [ 0.10896638  0.01804129]]]\n",
      "\n",
      "\n",
      " [[[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[-0.00480932  0.45412145]\n",
      "   [-0.13440754  0.86278798]\n",
      "   [ 0.10896638  0.01804129]]]\n",
      "\n",
      "\n",
      " [[[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[-0.13440754  0.86278798]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.10896638  0.01804129]]]\n",
      "\n",
      "\n",
      " [[[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.10896638  0.01804129]]]\n",
      "\n",
      "\n",
      " [[[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.10896638  0.01804129]]]\n",
      "\n",
      "\n",
      " [[[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.00480932  0.45412145]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.13440754  0.86278798]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.38091777  0.3462756 ]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.32090279 -0.3235997 ]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.10896638  0.01804129]]]]\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "[0, 1, 2]\n",
      "[0, 1, 3]\n",
      "[0, 1, 4]\n",
      "[0, 1, 5]\n",
      "[0, 1, 6]\n",
      "[0, 1, 7]\n",
      "[0, 1, 8]\n",
      "[0, 1, 9]\n",
      "[1, 2, 0]\n",
      "[1, 2, 1]\n",
      "[1, 2, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 4]\n",
      "[1, 2, 5]\n",
      "[1, 2, 6]\n",
      "[1, 2, 7]\n",
      "[1, 2, 8]\n",
      "[1, 2, 9]\n",
      "[2, 3, 0]\n",
      "[2, 3, 1]\n",
      "[2, 3, 2]\n",
      "[2, 3, 3]\n",
      "[2, 3, 4]\n",
      "[2, 3, 5]\n",
      "[2, 3, 6]\n",
      "[2, 3, 7]\n",
      "[2, 3, 8]\n",
      "[2, 3, 9]\n",
      "[3, 4, 0]\n",
      "[3, 4, 1]\n",
      "[3, 4, 2]\n",
      "[3, 4, 3]\n",
      "[3, 4, 4]\n",
      "[3, 4, 5]\n",
      "[3, 4, 6]\n",
      "[3, 4, 7]\n",
      "[3, 4, 8]\n",
      "[3, 4, 9]\n",
      "[4, 5, 0]\n",
      "[4, 5, 1]\n",
      "[4, 5, 2]\n",
      "[4, 5, 3]\n",
      "[4, 5, 4]\n",
      "[4, 5, 5]\n",
      "[4, 5, 6]\n",
      "[4, 5, 7]\n",
      "[4, 5, 8]\n",
      "[4, 5, 9]\n",
      "[5, 0, 0]\n",
      "[5, 0, 1]\n",
      "[5, 0, 2]\n",
      "[5, 0, 3]\n",
      "[5, 0, 4]\n",
      "[5, 0, 5]\n",
      "[5, 0, 6]\n",
      "[5, 0, 7]\n",
      "[5, 0, 8]\n",
      "[5, 0, 9]\n",
      "[[0.35677198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.35677198 0.35677198 0.09868233 0.0359273  0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.35677198 0.35677198 0.13809839 0.1261251  0.0359273  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.35677198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.35677198 0.270732   0.05763165 0.02992599 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.35677198 0.20314547 0.1896699  0.17245336 0.01278575 0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[(0.35677198, 1)]\n",
      "checked (5, 0)\n",
      "[(0.35677198, 2), (0.09868233, 1), (0.0359273, 1)]\n",
      "0\n",
      "triangle [4 5 8]\n",
      "[4, 5, 8]\n",
      "0.7452051219027406\n",
      "1\n",
      "triangle [4 5 7]\n",
      "[4, 5, 7]\n",
      "0.6625858490308267\n",
      "[(0.7452051 , 8) (0.66258585, 7)]\n",
      "[(0.7452051, 8), (0.66258585, 7)]\n",
      "checked (4, 5)\n",
      "[(0.35677198, 2), (0.13809839, 1), (0.1261251, 1), (0.0359273, 1)]\n",
      "0\n",
      "triangle [3 4 8]\n",
      "[3, 4, 8]\n",
      "0.6450062205358374\n",
      "1\n",
      "triangle [3 4 7]\n",
      "[3, 4, 7]\n",
      "0.7452051219027404\n",
      "[(0.64500624, 8) (0.7452051 , 7)]\n",
      "[(0.7452051, 7), (0.64500624, 8)]\n",
      "checked (3, 4)\n",
      "[(0.35677198, 1)]\n",
      "checked (2, 3)\n",
      "[(0.35677198, 1), (0.270732, 1), (0.05763165, 1), (0.029925993, 1)]\n",
      "checked (1, 2)\n",
      "[(0.35677198, 1), (0.20314547, 1), (0.1896699, 1), (0.17245336, 1), (0.012785754, 1)]\n",
      "checked (0, 1)\n",
      "initial set edges: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (3, 4)}\n",
      "meshing polygon:  [[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [-0.13440754  0.86278798]\n",
      " [-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [ 0.61250609 -1.12049258]]  with inner points : [[-0.38091777  0.3462756 ]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.32090279 -0.3235997 ]\n",
      " [ 0.10896638  0.01804129]]\n",
      "Edge: (5, 0) targeting: 8\n",
      "(5, 0, 8)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (5, 8)\n",
      "set of interior edges updated: {(5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (3, 4), (5, 8)}\n",
      "edges inserted: (0, 8)\n",
      "set of interior edges updated: {(0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (5, 0, 8)\n",
      "Spotted edges linked with point:  (5, 8)   (0, 8)\n",
      "Edge: (4, 5) targeting: 8\n",
      "(4, 5, 8)\n",
      "[[-0.43294286 -0.45065737]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Vertex locked: 5\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (4, 8)\n",
      "set of interior edges updated: {(0, 8), (5, 8), (4, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 8), (4, 5), (2, 3), (5, 0), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (4, 5, 8)\n",
      "Spotted edges linked with point:  (4, 8)   (5, 8)\n",
      "Edge: (3, 4) targeting: 7\n",
      "(3, 4, 7)\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [-0.3490263   0.07266232]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (3, 7)\n",
      "set of interior edges updated: {(3, 7), (0, 8), (5, 8), (4, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 8), (4, 5), (2, 3), (5, 0), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "edges inserted: (4, 7)\n",
      "set of interior edges updated: {(4, 7), (4, 8), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (4, 8), (4, 5), (2, 3), (5, 0), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (3, 4, 7)\n",
      "Spotted edges linked with point:  (3, 7)   (4, 7)\n",
      "Edge: (2, 3) targeting: 6\n",
      "(2, 3, 6)\n",
      "[[-0.13440754  0.86278798]\n",
      " [-1.03542791 -0.00446935]\n",
      " [-0.38091777  0.3462756 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (2, 6)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (4, 8), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (5, 0), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "edges inserted: (3, 6)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (4, 8), (3, 6), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (5, 0), (3, 6), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (2, 3, 6)\n",
      "Spotted edges linked with point:  (2, 6)   (3, 6)\n",
      "Edge: (1, 2) targeting: 6\n",
      "(1, 2, 6)\n",
      "[[-0.00480932  0.45412145]\n",
      " [-0.13440754  0.86278798]\n",
      " [-0.38091777  0.3462756 ]]\n",
      "Vertex locked: 2\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 6)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (4, 8), (1, 6), (3, 6), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (1, 2, 6)\n",
      "Spotted edges linked with point:  (1, 6)   (2, 6)\n",
      "Edge: (0, 1) targeting: 9\n",
      "(0, 1, 9)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [ 0.10896638  0.01804129]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (0, 9)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (4, 8), (1, 6), (3, 6), (0, 9), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (0, 9), (3, 7), (0, 8), (3, 4), (5, 8)}\n",
      "edges inserted: (1, 9)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (4, 8), (1, 6), (1, 9), (3, 6), (0, 9), (3, 7), (0, 8), (5, 8)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (0, 9), (3, 7), (1, 9), (0, 8), (3, 4), (5, 8)}\n",
      "element inserted: (0, 1, 9)\n",
      "Spotted edges linked with point:  (0, 9)   (1, 9)\n",
      "Final edges: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 8), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (0, 9), (3, 7), (1, 9), (0, 8), (3, 4), (5, 8)}\n",
      "Elements created: {(1, 2, 6), (3, 4, 7), (2, 3, 6), (0, 1, 9), (4, 5, 8), (5, 0, 8)}\n",
      "Set of locked vertices: {2, 5}\n",
      "set of orphan vertex: set()\n",
      "Set of open vertices: {0, 1, 3, 4, 6, 7, 8, 9}\n",
      "Examining if vtx 0 is locked\n",
      "Examining if vtx 1 is locked\n",
      "Examining if vtx 3 is locked\n",
      "Examining if vtx 4 is locked\n",
      "Checking if interior point 6 is closed\n",
      "found 6 in (2, 6)\n",
      "found index [1] in element (1, 2, 6) \n",
      "Interior vertex 6 is open\n",
      "Interior vertex 6 is open\n",
      "Interior vertex 6 is open\n",
      "Interior vertex 6 is open\n",
      "Interior vertex 6 is open\n",
      "set of open vertices {0, 1, 3, 4, 6, 7, 8, 9}\n",
      "[]\n",
      "Starting with vertex 0\n",
      " Initial edges to visit {(4, 7), (4, 8), (1, 6), (1, 9), (3, 6), (0, 9), (3, 7), (0, 8)}\n",
      "Candidate edge (0, 5) found in element (5, 0, 8)\n",
      "Candidate edge (1, 2) found in element (1, 2, 6)\n",
      "Candidate edge (3, 2) found in element (2, 3, 6)\n",
      "Candidate edge (4, 5) found in element (4, 5, 8)\n",
      "Edges to visit: {(4, 7), (4, 8), (1, 6), (1, 9), (3, 6), (0, 9), (3, 7), (0, 8)}\n",
      "Visiting vertex 0\n",
      "0  in  (0, 9)\n",
      "Found vertex: 9\n",
      "Removing edge (0, 9)\n",
      "{(4, 7), (4, 8), (1, 6), (1, 9), (3, 6), (3, 7), (0, 8)}\n",
      "Visiting vertex 9\n",
      "9  in  (1, 9)\n",
      "Found vertex: 1\n",
      "Removing edge (1, 9)\n",
      "{(4, 7), (4, 8), (1, 6), (3, 6), (3, 7), (0, 8)}\n",
      "Visiting vertex 1\n",
      "1  in  (1, 6)\n",
      "Found vertex: 6\n",
      "Removing edge (1, 6)\n",
      "{(4, 7), (4, 8), (3, 6), (3, 7), (0, 8)}\n",
      "Visiting vertex 6\n",
      "6  in  (3, 6)\n",
      "Found vertex: 3\n",
      "Removing edge (3, 6)\n",
      "{(4, 7), (4, 8), (3, 7), (0, 8)}\n",
      "Visiting vertex 3\n",
      "3  in  (3, 7)\n",
      "Found vertex: 7\n",
      "Removing edge (3, 7)\n",
      "{(4, 7), (4, 8), (0, 8)}\n",
      "Visiting vertex 7\n",
      "7  in  (4, 7)\n",
      "Found vertex: 4\n",
      "Removing edge (4, 7)\n",
      "{(4, 8), (0, 8)}\n",
      "Visiting vertex 4\n",
      "4  in  (4, 8)\n",
      "Found vertex: 8\n",
      "Removing edge (4, 8)\n",
      "{(0, 8)}\n",
      "Visiting vertex 8\n",
      "8  in  (0, 8)\n",
      "Found vertex: 0\n",
      "Removing edge (0, 8)\n",
      "set()\n",
      "Back to starting vertex\n",
      "Starting with vertex 1\n",
      "Starting with vertex 3\n",
      "Starting with vertex 4\n",
      "Starting with vertex 6\n",
      "Starting with vertex 7\n",
      "Starting with vertex 8\n",
      "Starting with vertex 9\n",
      "found polygon [0, 9, 1, 6, 3, 7, 4, 8]\n",
      "remeshing subpolygon [0, 9, 1, 6, 3, 7, 4, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked (3, 4)\n",
      "checked (7, 0)\n",
      "checked (6, 7)\n",
      "checked (5, 6)\n",
      "checked (4, 5)\n",
      "0\n",
      "triangle [2 3 5]\n",
      "0.772374612634636\n",
      "1\n",
      "triangle [2 3 1]\n",
      "0.7401905176187213\n",
      "[(0.77237463, 5) (0.7401905 , 1)]\n",
      "[(0.77237463, 5), (0.7401905, 1)]\n",
      "checked (2, 3)\n",
      "0\n",
      "triangle [1 2 5]\n",
      "0.7723746126346361\n",
      "1\n",
      "triangle [1 2 3]\n",
      "0.7401905176187213\n",
      "[(0.77237463, 5) (0.7401905 , 3)]\n",
      "[(0.77237463, 5), (0.7401905, 3)]\n",
      "checked (1, 2)\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.2531275  0.59681199]\n",
      " [0.         0.         0.         0.59681199 0.         0.59681199\n",
      "  0.33172422 0.        ]\n",
      " [0.         0.59681199 0.         0.         0.1074673  0.59681199\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.1074673  0.         0.         0.59681199\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.1074673  0.59681199 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.59681199 0.33172422 0.         0.         0.\n",
      "  0.         0.51205993]\n",
      " [0.2531275  0.59681199 0.         0.         0.         0.51205993\n",
      "  0.         0.        ]\n",
      " [0.         0.59681199 0.         0.         0.         0.\n",
      "  0.2531275  0.        ]] {(3, 4): [((0.5968119899550415, 5), (0.1074672958015445, 2), (0.0, 7), (0.0, 6), (0.0, 4), (0.0, 3), (0.0, 1), (0.0, 0))], (7, 0): [((0.5968119899550413, 1), (0.2531274961828347, 6), (0.0, 7), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 2), (0.0, 0))], (6, 7): [((0.5968119899550413, 1), (0.5120599307737846, 5), (0.2531274961828347, 0), (0.0, 7), (0.0, 6), (0.0, 4), (0.0, 3), (0.0, 2))], (5, 6): [((0.5968119899550413, 1), (0.5120599307737846, 7), (0.3317242220656009, 2), (0.0, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 0))], (4, 5): [((0.5968119899550413, 3), (0.10746729580154447, 2), (0.0, 7), (0.0, 6), (0.0, 5), (0.0, 4), (0.0, 1), (0.0, 0))], (2, 3): [((0.77237463, 5), (0.7401905, 1), (0.10746729580154447, 4), (0.0, 7), (0.0, 6), (0.0, 3), (0.0, 2), (0.0, 0))], (1, 2): [((0.77237463, 5), (0.7401905, 3), (0.3317242220656009, 6), (0.0, 7), (0.0, 4), (0.0, 2), (0.0, 1), (0.0, 0))], (0, 1): [((0.5968119899550413, 7), (0.25312749618283487, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 2), (0.0, 1), (0.0, 0))]}\n",
      "initial set edges: {(0, 1), (1, 2), (6, 7), (7, 0), (4, 5), (5, 6), (2, 3), (3, 4)}\n",
      "Edge: (3, 4) targeting: 5\n",
      "(3, 4, 5)\n",
      "Vertex locked: 4\n",
      "edges inserted: (3, 5)\n",
      "set of interior edges updated: {(3, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (6, 7), (7, 0), (4, 5), (5, 6), (2, 3), (3, 4), (3, 5)}\n",
      "element inserted: (3, 4, 5)\n",
      "Edge: (7, 0) targeting: 1\n",
      "(7, 0, 1)\n",
      "Vertex locked: 0\n",
      "edges inserted: (7, 1)\n",
      "set of interior edges updated: {(7, 1), (3, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (6, 7), (7, 0), (7, 1), (4, 5), (5, 6), (2, 3), (3, 4), (3, 5)}\n",
      "element inserted: (7, 0, 1)\n",
      "Edge: (6, 7) targeting: 1\n",
      "(6, 7, 1)\n",
      "Vertex locked: 7\n",
      "edges inserted: (6, 1)\n",
      "set of interior edges updated: {(7, 1), (6, 1), (3, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (6, 7), (7, 0), (7, 1), (4, 5), (5, 6), (6, 1), (2, 3), (3, 4), (3, 5)}\n",
      "element inserted: (6, 7, 1)\n",
      "Edge: (5, 6) targeting: 1\n",
      "(5, 6, 1)\n",
      "Vertex locked: 6\n",
      "edges inserted: (5, 1)\n",
      "set of interior edges updated: {(5, 1), (7, 1), (6, 1), (3, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (6, 7), (7, 0), (7, 1), (4, 5), (5, 6), (6, 1), (2, 3), (5, 1), (3, 4), (3, 5)}\n",
      "element inserted: (5, 6, 1)\n",
      "Edge: (4, 5) targeting: 3\n",
      "(4, 5, 3)\n",
      "Element (3, 4, 5) already in set\n",
      "Edge: (2, 3) targeting: 5\n",
      "(2, 3, 5)\n",
      "Vertex locked: 3\n",
      "edges inserted: (2, 5)\n",
      "set of interior edges updated: {(7, 1), (6, 1), (5, 1), (2, 5), (3, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (6, 7), (7, 0), (7, 1), (4, 5), (5, 6), (6, 1), (2, 3), (5, 1), (2, 5), (3, 4), (3, 5)}\n",
      "element inserted: (2, 3, 5)\n",
      "Edge: (1, 2) targeting: 5\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (1, 2) targeting: 3\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (1, 2) targeting: 6\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (1, 2) targeting: 7\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (1, 2) targeting: 4\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (1, 2) targeting: 0\n",
      "Element inserted: (1, 2, 5)\n",
      "Edge: (0, 1) targeting: 7\n",
      "(0, 1, 7)\n",
      "Element (7, 0, 1) already in set\n",
      "Final edges: {(0, 1), (1, 2), (6, 7), (7, 0), (7, 1), (4, 5), (5, 6), (6, 1), (2, 3), (5, 1), (2, 5), (3, 4), (3, 5)}\n",
      "Elements created: {(2, 3, 5), (5, 6, 1), (6, 7, 1), (7, 0, 1), (3, 4, 5), (1, 2, 5)}\n",
      "Set of locked vertices: {0, 3, 4, 6, 7}\n",
      "Vertex locked: 2\n",
      "Vertex locked: 5\n",
      "Vertex locked: 1\n",
      "Set of open vertices: set()\n",
      "{(2, 3, 5), (5, 6, 1), (6, 7, 1), (7, 0, 1), (3, 4, 5), (1, 2, 5)}\n",
      "(2, 3, 5)\n",
      "(5, 6, 1)\n",
      "(6, 7, 1)\n",
      "(7, 0, 1)\n",
      "(3, 4, 5)\n",
      "(1, 2, 5)\n",
      "Minimum quality: 0.35677197097809094\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADnCAYAAAAZ4WrqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzsnXmcTfUbx993FjOGGWOfEWM3DLLLvjP2rNkykjbaJCptWkVR0k9UkhTZEmUXkiUVsoss2QkxMcZs9/P74xxjMDNmuXfuHc779bqvMvec7/c5557P+W7P93lskrCwsMheeLjaAAsLi/RjCdfCIhtiCdfCIhtiCdfCIhtiCdfCIhvildqXBQoUUIkSJbLIFAsLi6Rs3rz5rKSCyX2XqnBLlCjBpk2bnGOVhYVFqthstsMpfWd1lS0ssiGWcC0ssiGWcC0ssiGWcC0ssiGWcC0ssiGWcC0ssiGWcC0ssiGWcLMJgwYNwsvLC5vNhpeXF4MGDXK1SRYuJFUHDAv3YNCgQUycODHx3wkJCYn//vjjj11lloULsaW2kb5mzZqyPKdcj5enJwl2+01/9/T0JD4+3gUWWWQFNptts6SayX1ndZXdnQ0bkhUtGC2vxZ2JJVx3ZvVqNjdvnuLXnp6eWWiMhTthCdddWbqUDa1b0+zKlRQPeeSRR7LQIAt3whKuOzJ/Pj+1b0+r2FgKASU8PcmVMyeeHtd+rkeCg62JqTsYS7juxqxZLOvalTYJCRQHRhYsyN8JCbwzejTxx46xwDys66lTcPasKy21cCGWcN2JqVNZ0KsXHe12ygM/lSzJF5UqUbBgQQYMGADBwbSsXZtcwHcS/PCDqy22cBGWcN2FiROZ1b8/3SSqAqtCQzk6cSJLVq9myJAh+Pn5AZCzWzfaAPMB+7x5rrTYwoVYwnUHPviALwcNojdQF1hRuTJ5167lncmTyZMnDwMHDrx2bOfOdAZOARuXLYNLl1xjs4VLsYTrat5+m0lDhvAA0AxYUqMGAWvW8Oe5c3z77bc88cQT5MmT59rxZcrQLiwMb+C7uDhYssQ1dlu4FEu4rkKCl15i3MsvMxBoB/xQrx65Vq2CvHkZPXo0vr6+PP300zedmqdbN5oB3wGyust3JJZwXYEEzz7LyJEjeQboCsxr2hTf5cshIIDDhw/z9ddf88gjj1CwYDJB/jp3pgtwANj5ww8QG5u19lu4HEu4WY3djgYO5OUPPuAloA8ws00bcixeDLlyAfDee+9hs9kYOnRo8mVUqcK9xYphA76LioJVq7LKegs3wRJuVpKQgPr3Z+gnn/A28BDwZZcueM2fD76+AJw6dYrJkyfTr18/ihYtmnw5NhuFu3enHjAP4LvvssZ+C7fBEm5WEReHvXdvHp82jfeBJ4FPevXCc9YsyJEj8bBx48YRFxfHc889l3p55uzyNuDQvHlgbTi4s5CU4qdGjRqycABXrii+Y0c9AAL0HMj+4INSfPx1h/3777/y9/dXz549b11mfLwO5M8vQGNBWrvWScZbuApgk1LQptXiOpvLl4nr0IE+33/PVOA1YNTjj2P77DO4YXfPhAkTuHjxIsOHD791uZ6elOrShSoYs8tWd/kOIyVFy2pxM8/Fi7rSqJE6mS3taJCGDZPs9mQOvaj8+fOrffv2aS9/8WK9BrKBToWEJFuuRfYFq8V1AZGRRDdvTueff2Y+8BHw3IgRMHo02Gw3Hf7ZZ59x7tw5XnzxxbTX0awZnXPlQsCCI0dg+3ZHWW/h5ljCdQbnznGpSRPa/fYbS4HPgCdGjYLXXktWtDExMYwZM4YmTZpQt27dtNfj40PlDh0ohdVdvtOwhOto/vmHyEaNaL11Kz8DXwEPjR8Pzz+f4inTpk3jxIkTvPTSS+muztalC52BlUDknDkZtdoiu5FSH1rWGDf9HDumc2XLqibICzQXpE8/TfWUuLg4lS5dWrVq1ZI9I2PUixe13ttbgGaAtH9/Bo23cDewxrhZwOHD/FO/Pk3/+osdwHybja5ffQUPP5zqaXPmzOHAgQO8+OKL2JLpRt+S3Lmp06oVQVjd5TsJS7iOYP9+TtSrR+PDh/kLWOjhQbvZs+H++1M9zW63M3LkSMLCwujYsWOGq/fo2pV7gcVA9Ny5GS7HIvtgCTez7N7N4Xr1aHTiBMeApV5etFiwALp1u+WpCxcuZOfOnQwfPhwPj0z8FB060NlmIwr48ddf4eTJjJdlkS2whJsZtm5lf4MGNDpzhnPAjz4+NFq8GNq3v+Wpknj77bcpWbIkPXv2zJwdBQrQtGFD8mB2lxcsuMUJFtkdS7gZ5bff2NOoEY3OnycKWJUzJ/csXw4tW6bp9NWrV/Pbb7/x/PPP4+WV+UwwObp2pT3wPRD/7beZLs/CvbGEmxHWrWNb06Y0vngRO7DG359qq1dDo0ZpLmLkyJEEBwfTr18/x9jUqROdgXPAulWr4MIFx5Rr4ZZYwk0vK1fye4sWNL18GR/g58BAKq5ZA/fck+Yifv31V1auXMmzzz6Lr7mdL9OEhNC6WjV8gXl2Oyxa5JhyLdwSS7jpYfFi1rVpQ/OYGAKBnwsUoNy6dVCtWrqKGTlyJHnz5uXRRx91qHm5unWjFUYESCukze2NJdy0Mm8eqzp2JDwujmDg56AgSq5fDxUrpquYHTt28P333/P000+TO3dux9po7tE9CmxetAiiox1bvoXbYAk3LcyYweLu3WmbkEAp4OeQEIpu2ADlyqW7qFGjRpE7d26efPJJx9tZoQIdypTBE/guJgaWL3d8HRZugSXcWzFlCt/16UMnu52KwOrSpSm8fj2ULJnuog4cOMDMmTMZOHAg+fLlc7ytQP5u3WiM5UV1u2MJNzUmTOCbAQPoDtQAVlaoQIH16yGlWFC3YPTo0Xh7e/PMM8841MzrMDcd7AH+nD8frMTXtyWWcFNizBimPPEEfYAGwPIqVQj8+WcoXDhDxR0/fpypU6fy4IMPEhwc7FBTr6NmTToFBQHwXWQk/Pyz8+qycBmWcG9EgjfeYMKwYQwAWgKLa9XC/6efoECBDBc7duxY7HY7w4YNc5SlyWOzUbRbN2phdpet2eXbk5S2DelO3NZnt0svvKAxZqiZjqArDRtK//2XqWLPnDkjPz8/9e3b10GG3oKVK/WOeQ1Hg4KkhISsqdfCoWBt60sDEnr6ad4cNYqhQHdgbosW+CxdCv7+mSp6/PjxXL58mRdeeMEhpt6SRo3obOYbmn/qFGzalDX1WmQdKSlad1KLm5Ag+0MPabjZSkWA4tq3l65cyXTRkZGRCgwMVOfOnR1gaDp44AF5m9dz9RMWFpa1NlhkCqwWNxXi41G/fjwzeTLvAI8CX3Trhte8eeDjk+niJ06cyIULF9IXBC6zSFRcupS4G/68e/duKoaFZZ0dFs4jJUXrTmhxY2KU0LWrHjFbpKdB9r59pbg4hxR/+fJlFSpUSK1atXJIealy8aK0YIH06KP6vXDh61raGz+RlSpJ/ftLH30krV8vXbrkfPss0g2ptLh3rnCjoxXXrp36mg/zcJD9kUccOpHzv//9T4B++uknh5WZiN0u7d4tjR2rqMaNNd7TU3VAvqkINumnLKiHGet5Behs2bJSnz7S2LHS6tXS+fOOt9kiXVjCvZGoKMU2b67u5kP8JkhPP+3QgOKxsbEKCQlRvXr1MhYELjkuXZJ++EEaOFCbg4P1AOiuGwQZCGpyC9G+DeoKKnnD34uDOpv3YxHoZPHiUvfu0jvvSMuWSf/845jrsEgTlnCT8t9/im7YUB3Mh3UMSC+84PAsAFOnThWghQsXZrwQu13680/pgw8U3bSpJnh6qi4oZxKxeYBCQcNAR8uX1+UhQ1S/cuUURRtWvry0Zo30wQdS3746FxqqlTab3gP1MsuyJTk+GNQO9AroO9Dh4GDZO3aUXn/deIkcP25lUHASlnCvcv68omrVUivzoZwA0htvOPzBi4+PV2hoqKpUqZL+1jYqSlq4UHr8cW0rUkQDQMVuEFMeUDhojq+vEjp1kj77TDp6VHFxcerYsaNsNpvmzJmjsLCw60VbqlTydV66JG3YIP3vf9KDD+q/ypW11tNTH4L6gSqZL4ir5eQHtQQ9D5oF+it/fiW0bi299JL07bfSoUOWmB2AJVxJOnNG/919txqbIpgC0nvvOaWqOXPmCNCsWbPSdsK+fdK4cYpp2VKTPD1VP5lWtSxoCOjvcuWk554zxqExMYlF2O12PfTQQwL0v//971rZvXrJy2w59e67ab+IK1ekTZuMuNCPPaaoGjW00dtbH4MeAlWH65abAkCNQc+AvgLtCghQfLNmRq6kb76R9u61HEHSiSXckyd1vkIF1QF5YgYOT/pwOxC73a5q1aqpbNmyir8hjWYily9LixdLTz6pnUWL6mFQyA2tagCoFWimj48SOnaUPvlEOnIkxXpfeeUVAXrppZeu/2Ly5MTub0zLlpm7uNhYaft2aepU6amnFFOvnrb4+moyaBCozg0vHD9QXdDjoM9Bf/j5KaZ+fWM+4csvpR07HDaDfztyZwv36FGdLV06sYWYB9LnnzutuiVLlgjQ5zfWsX+/NH68YsLD9ZmXlxqaD3bSVrUMaDDoYJky0tCh0sqV17WqKfHxxx8L0IABA27umh86pCfMOr709k5TeekiIcEYh8+YIQ0dqrgmTbTT31/TzGtpBPJPcp05QDVAD4MmgX7z8VF07drSoEHS5MnSli2OtzGbcucK9+BBnQoJUSWMZZLFHh7GA+YEbhxPVihfXlq6VHr6ae0OCdFjGLO2SVtVf1AL0Nc5chieWhMnSn//na56586dK5vNpg4dOiguhdZrx113CVBrkH7+2RGXmzp2u3TwoDR3rvTii0oID9e+vHk1EyOpdwtQ3iT3wRN0N+gB0HjQOk9PXaxSRXroIenjj6WNG41eyh3GnSncvXt1NChI5cyWbaWnpzFx4gRuFO3VT05QriT/toFKgZ4E7S9VShoyRFqxIsOulT/99JNy5MihunXrKioqKuUDH35YPqB8II0YkbGLzCx2u3TsmPT999Jrr8neoYP+DgrSPNDLoLagwjfcq/Kg3hgz/6s8PHS+QgUpIkIaN854AWVy84e7c+cJd8cOHSpQQCXNVm2dt7e0aJHTqkttzTQ3qJnZTY1r29ZoQQ4dynSd27ZtU0BAgCpUqKBz586lfvA336iqaU9knTqZrtuhnD5t9ExGjpS6ddPxkBD9AHoddC/GjHrS+1kK1A00ErQU9E+pUlLPnsbE248/Sre6F9mIO0u4W7ZoX2Cgiprdsd98fIwf1BkcOaKEfv1SFa4GD5aWL5eiox1W7aFDhxQcHKy77rpLR1KZsErk9Gm9aNozzsPDcI90Z/79V1q1ShozRurdW/+UKaNloHdA3UGlb7jHRTG2YL4G+h50rGhR2Tt3lt56y5gEPHXK1VeUIe4c4f7yi3bmzq0gUEHQVj8/ae1ax9dz7pw0dKgmeXoqIBXRAg6v+syZMypXrpwCAwO1c+fONJ93ODRUgBqC8TBnNy5eNH7LDz+U+vXT+bAwrfbw0FhQH1CFG+YPCplj+hcx0p0eLFRI9nbtpFdflebPN2bo3Xyt+c4Q7po12pIzpwpgrFnuDgiQfvvNsXVERUkjR2p+zpwK4tpssH8KonX0NrpLly6pdu3a8vX11dr0vpAGD1Yu01YNHepQu1zG5cvSr78ak3oPP6yLVatqvZeXPgL1B1XByFOc1B20GWgoxpLgn4GBSmjZ0vCcmz3bmPl3IzHf/sJdvlwbfXwUiLEe+lfevNLWrY4rPzZWmjRJG/LnV5kkD0JL0OkqVaRVq272UnKwaGNjY9WmTRt5eHho/vz56S/ghx9Uz7TteMWKDrXNrYiJkf74w1jye/xxRd9zj3738dEnoEdBNTGWpJLOQTQAPQWaCtqRO7fiGjUyJg6//trYyJHSeryTub2F+/33WuPlpdzmxMXfBQtKu3Y5pmy7XZozR/tKlFCNJD92DdC+EiWM5Y4kb+gnnnhCgYGBjqn7OjPs6meOpT+9RYb7FImM1BibTYBGgHT2rGONdGfi441n4quvpGeeUWzDhtqaK5emgJ4A1eP6NXVf0D2ggaDPQJt9fXWlTh3pySelL76Qtm0zXuZO5vYV7uzZWuHhoZwYSwfHihSR/vrLMWWvXKnTVaqoZZIftCxoQ/78hhdTMmum7777rjFzGxnpGBtMnn/+eQF6/fXXM1XOuZo1E188mjPHQdZlUxISjGdl1izp+ecV36KFdufJo68xXEubwHXzF96gaqABGD7uv3h7K6p6denRR43n4fffHToBKd2uwv3qK/1gs8kHY/H+dPHi6XZeSJYtWxTVrJl6cc2xPgg0P2dOY3tbKuulM2fOFJCuSaNbMW7cOAF67LHHMr898KWXlNdsUfTYY44x8HbCbpcOH5a++0565RUltG2r/QULajboBQwX1PxJxOwBqgjqC/oAtMbTM9kgBRkdRt1+wv30U80xJx5qgs6VLWss7meG/fuV0KOHnuGa83wA6GNPT2MyJw3rgxs2bBCgxQ6atf3mm28EqEuXLin7PaeHVasSexC7Q0IyX96dwokThh/Am2/K3qmTjtx1l+aDXgW1BxW5YVIyaZCC4pmYuLy9hPvhh/rKfNvVA12oWNFYxM8op05Jjz+ud222xHGOD8b+04QHHkjVsf9Gjh07JkCTJk3KuD0mK1askLe3txo1aqRoR3XBoqM12ctLYPgRp+faLG7g7FnD6230aKlHD50sWVKLQW+BuoBKpCDY9CwV3j7CHTVKn2Gs1zUFXaxWLeOeMpGR0quv6uscOZSPaz6zD4FiOnbM0ARXfHy8vLy89OKLL2bMJpPNmzcrd+7cqly5ss47OIRMdOPGwpwT0NSpDi37jiUqyvD+euIJHQ4J0QtmF9rjjheu3S69+qrGmxfcGnT5nnukCxfSX9aVK9K4cfoxICDRnc6G4V53vm5dY0N5JihevLjuv//+DJ+/f/9+FSpUSCEhITp+/HimbEmWkSNV2BwOKKsCtN9uJCQYu5hGj1Z048aa5Ompxty8nn+r+F+3InsL126Xhg3TaPNiO4GuNG6cfre9+Hhp2jRtCw5WpSQ3rwHocGioEXXCAYvvDRs2VOPGjTN07qlTp1S6dGnlz59fe/bsybQtyfLrr+psXvuG/PndyuHArTl2TPriCyX07Kkf8+RRDwxHn6RCzGU+T+NtNn12ixb39h7jJiTIPmiQRpgX2xMU26pV+rZ42e3SokU6Wr68GiS5cRVB24KDpWnTHLrA3qdPH5UsWTLd5/3333+qXr26cubMqV9++cVh9txEXJzm5cwpMLyL5KwXRHbn0iVjQmrwYB0sU0ZDzOFFUk8sL/NvQ0D7S5c2AgQ89pheTjL0WuDlJZ8MiFbKrsKNj5f9wQf1nHmxD4Di7703fVvgfvlFkfXqqRPX/FiLgX709ze2hjkgU8GNDB8+XN7e3umaBY6JiVGLFi3k6emZueByaSShfXt5mBMomjDB6fVlCxISjLXYkSMV1aCBPvLwUAOu35aJ2cr2AP0YEKCEHj2kKVOko0eNl//gweqapPXdFhysM0WLyoax20nLl6fLpOwn3Lg4JfTqlRi5YSAYNymt3iq7dyumY0c9bL71wNiL+lWOHNIrrxgTU05i4sSJhlthGsenCQkJ6tWrlwB98cUXTrPrOj78UMXNrlxCp05ZU6c7cviwNHmyErp102J/f3Xl+j3BV10iG4MmeXoqunFjadQoY3ybNH5WVJRiOnZM3DoZDDpTrZr022/6xvzbRh+fdDtoZC/hxsQovnNnPWRe8BCQvV+/tHVnjx5VQv/+GpFkYsAPNNpmkx5/PEu2dy1cuFBAmrq7drtdzzzzjAC98847TrctkZ07FWHenx9y5XKZL26W899/xkb+J57QnyVK6ClQuSQv96vd34oYkTr+Dg01fJaXLk3Z8ebkSZ2uUiVx00l1UEynTsbxkybpAbPRiA8PT7e52Ue40dGKa9NGfcyb8DLI/thjt44OeO6cNGyYPvHySnRT88ZYq4y77z7HuUGmge3btwvSFuHxqovkU0895big6WnBbteavHkFxqZ0bdqUdXVnJfHxRtibN95QZN26GmuzqS7X+yXbMILK9watDgyU7r/fmPc4ceLW5e/apW3BwYnldQfp2WcTn1d7p04qAroPjDjW6SR7CPfSJcU0a5Y4RngbjLddag90VJQ0apQW+Pldt82uFyiqWTNp8+ass9/kwoULAjRmzJhUj5s2bZoA9ejRQwmuCFvau/e1sK2jR2d9/c7i4EHpk0+U0LmzFvj5qRPG3uyk3d8AjO19k728FNOihbFhf9u29M2w//ij5ufMmdhavwJGdJOrxMZqe65cAiPCpXbvTveluL9wIyMVXb++2pk34QOQXn455RsZFyd9+qk2Fiigskl+kBag03ffbURHdCEBAQF66qmnUvx+yZIl8vLyUvPmzXXFCRNkacKRYVtdyYULhm/xwIHaWayYBmFEy0y6FJMDVNnswR0PCzNiPa9YkfFNAV98odHmTisPjGB/N4VGWrtW75r1Hw0OztCym3sL999/dalGDbUwL3ISSG+/nfyxdrs0d672lyypmkl+mBqgfcWLG5uh3WBdslKlSuqUwqTPr7/+Kj8/P1WrVs3hu4jShbPDtjqLuDjDeX/ECJ2vXVujbTbV5vp4zjaM1YN+mGvV/fpJ06dnfo7DbpdeflkPcM3BYmOBAsb+3xt5+WU1N8fLeuihDFXnvsL95x9FVq6shuaba2pqY4HVq3WmWrXE9CFgxB5aly+fNGlSluyPTCtt27ZV9erVb/r73r17VaBAAZUsWVInT550gWXXc13Y1jVrXG1O8tjtxhzFhAlK6NhRc3LmVHuu36UDRnSLluZLKCY83HiOdu503Iv8yhUl9Oyp+mZ9+TFyNeno0WQPv1S9unJgTK5mdAulewr3xAn9Gxqq2uZM3iwwBHgjf/yhqBYt1DtJ96cwaF7OnEbL7Ia5XR999FEVKFDgur+dOHFCJUqUUMGCBfVXFk6WpUrSsK2vvupqa67x77/Gw/7II/ojOFiPYmQWTNr99QFVxQgQd7JyZWn4cCPAnDOGHmfP6nzduok7fcqDolq0SHlZ8cwZLTKPXWazGdeTAdxPuIcP65+SJVXVHH/Mt9ludng/cEAJPXtqCNe22fmDJnh6GjN3bhzB4e233xaQGOv4woULqlKlinLnzq1N7jSDO3PmtbCt99zjOjtiYowW/+WXdaZaNb1lDn+S+vraMLbIDQD9VqiQNGCANHOmdOaMc23bv1/7SpRIXK1oDUp4+OHUU6d8842eMu2/XLt2hqt2L+EeOKATd92lMPPClnp4GFEIrnL6tPTEExrj4ZHoteJjTiwk9OtnLJq7OV999ZUA7d27V9HR0WrSpIm8vb21PJ2eM07n9OlE97wPbLasC9t6NX3o+PGKa9dOM3x81Ibrsxtg/rsNaIaPjxGTevx447ysmsdYv14/BgQkNhyDwYjffKv6H3hAoRgZFfXaaxmu3n2Eu2ePDhcqpDIYLmGrvbyMUJmSsTg+YoRm+Pgkjl88zTdsTPv2xnglm1C8ePGbnMpnOCn1SWbJsrCtZ84YLeSAAfq9UCEN4OaULL5mS/sWpufRyy8bLbErJs5mz9bHnp6ymTZO8vJK21jVbtffhQoJ0PsgZcLv3D2Eu327DuTPr+IY62gbcuSQliwxxiQffqiVefIkjiFsGAGuz9epI61b5zgbsoCU0pE4Ouqjw0gatvXZZx1X7pUrxphz+HCdrlxZr5lj0qQO9x7m2PVR0B/BwdIjjxjicGU2ArtdGjVKT3HNkefHPHnSLsDt2/WJee6ugIBMeaW5XribNunPPHl0lzkRssnX18gu8PXX2lGkiCon+THrgw6XK2dkO3eDpZ3rsNuNlmPLFmnBAiNV5wsvSH36KLpBA20MDk5WtFc/bknSsK2ZebnY7Uav6IMPFBMeri+9vdUSY7Y36T3IjxHuZU7OnEb60AkTjFljd/itY2OVMGBA4spFAGhfyZLSgQNpL+O999QFI7uCvVu3TJmTmnC9cDYbNrCjVStaREUB8JOfH5XfeINjTz5Jnz17+Nk8LAyYHhRE1VGj4P77wdPT6aZdhwQXLsDRo9c+x47B0aPEHj7MzgMH+OPUKf6Mj2c/cBQ4DVwAooGENFSRL18+QkJCKF68eLL/LVy4MB4eHs68yptp1IguNhsbJD7dvZvXzp6FAgXSdu4//8CPP8Ly5fyycCGfnDvHKuAYxpMPkBOoDXS12XikVi0C27SBVq2gdm3wcv7jl2b++4/LXbpQbeVK9gHFge316xPw/feQL1+ai4lfsoSVQDfA1rq1k4zFycL96Se2tGlDyytXyAmszJWL4NKl6TJ0KPMxftxiwOf+/rR8/XUYOBB8fZ1jS2RkohCTCjP+8GH+PHCALSdPsis2lgPAEQxRngcuk7IovYHcQGEgGAgBZqRiQq9evThy5AiHDh1izZo1REZGXvd9jhw5KFas2HWCTvr/RYsWxdfR9ycggAE1ajB00yZ+AF5bvRq6d0/+2CtXYN06WL6cE4sWMXH3bhYAe4FY8xAPoDTQChhUtCgVO3QwhNq0KeTJ41jbHcXRoxxp2ZJqe/fyL9AAWNO7Nx5TpoCPT9rLuXyZX9euJRIIBwgPd4q54EzhLlvGLx070iY2lrzAUm9vPoiK4vPt24kH8gEfeHsT8fzzMHRo5n7US5duFuXRo9iPHOGvgwfZcvw4O69c4QBwGDiFIcooID6FIr2AXBhiDDL/Wxao5OtL9aJFKV2yJB7Fi0OxYtc+RYuytVMndu/de1N5YWFhTJgw4bq/RUZGcuTIEQ4fPnzTf1esWMGJEyeM8UwSgoKCUm218+bNi81mS9ftCwwPJ++mTewGWLXqmnAl2LEDVqwgdulSvvrpJ2bEx7MJ+C/J+QWB+kB/Pz/ah4fjER4OLVtCqVLpssMlbNnCL61a0fTcOWKA/sCUESNgxAhI531kzRqWxcXhAbQIDYWiRZ1gsIHtxgcjKTVr1tSmTZvSX+qCBfzUrRvt4+MJAloDnwNAJOXAAAAgAElEQVRXMLpOr9psvPDYY/DKKxAcnHpZ0dHXdVuTivLwoUNsPnqUnZcv8xeGKE8C/2KIMi6FIj0xRJkXo7UshiHKijlyUK1IESqULo1HSMhNoqRYMQgISN3eTZvwr1WLS0n+FBYWxq5du1I/LxliY2M5duxYiuI+cuQIV65cue6c3LlzJ9taX/1vcHAwXjd2UVevJlezZlxOanNAABM9Pfnk/HnWACe41v31A6oA3Ww2HqpTh4Cr3d+aNbN+iJMZFi7k665d6Rcbix1418ODYVOmQL9+GStv8GDu+fBDPIBfnnkG3n8/U+bZbLbNkmom953jW9xZs1jWuzed7HauYPzgE8yKngLGdu+O18iRUKYMxMTAgQPJdmGPHTjA5iNH2HHpEvu4JspzwCWudc1uxBPj5RAEFMJoKUsDYV5eVAsOplLp0ngVL35NiEmFGRiY/rfsjezZw13APsDeuTPMm5fhonLkyEGpUqUolULLJYkzZ86kKOzff/+ds2fPXneOp6cnRYsWvU7QX0yZcp1oAXb/9x+Nr56D8WJrDQwqUYLQ9u0NoTZpAv7+Gb4+lzJhAq888QRvYVzf/Jw5uXfRIqNLn0HOLV7M78Cr4NRuMjhIuBUrVmT37t3JfhcD9AQ+L1UKv5o1ISaGf7p0YdORI2yPjGQvhihPAGcxRBmTQj0eGKIsiCHKYkApoIKnJ9UKF6Zy6dL4piTK/PkzL8q0sGcPJwB/gAoVnFqVzWajUKFCFCpUiJo1k30xExUVxZEjR5IV99q1azl27BgJCSlPrS3OnZvw1q3xaN3a6P6GhDjrcrKGhAQYNoxuH3zAtxi9h1+Cg7l75crM/V5HjvDjX38hIDxHDmjUyEEGJ0+mhZuaaL2BGsA2oMTBg1w8eJAYrnW5kuIB+GKMfQsCRTFF6eFBlYIFqVayJH4lS94symLFjFnQrJ6NTQH7rl1cAkLB6cJNC7ly5aJChQpUSMGWhISEm7vOSWhz4UL26v6mxuXLxPbuTZ0FC/gDo1e2rWpVCi1dCoULZ67sZctYBgQCtRo1gpw5M29vKmRauCmJFowx5kbAhiHKPBiivAtDlKFAlQIFqF6yJAElS14/lrz6KVQoWz04e7ZtQ0B5cAvhpoYkxo0bl/pB2ejep8rp0/wTHk6Vbds4BVQDNnbqRI7p08HPL9PFa+lSlgEtAK+2bTNd3q1w6nLQ8sBAapQqRb6URBkU5F5reZklNpYfjxwBoCZA+fIuNSc14uPjeeqpp5g4cSL+/v5cvHjxpmPCwsJcYJkT2L2b7S1aUPfkSS5jrLHOefZZePddx/TU4uPZtXw5J3D+MlAiKXlmKI2eU6TiKXS8SBFj0/Odwq5d6mVe+4ZChVxtTYpcvHhRbdu2FaDnn39eCQkJybpq7s5AuBW3Y+XK60LMvAyOD0m7fr3GmOUfCQpymBcYznR5TMk3F1AB0BIPD2nkyFsHfLsdmDtXVcxrT3DTcDDHjh1T1apV5enpqU8++STZY06fPq28efOqUaNGWRvEztFMnXpziBlnxK1+9VW1BFUAY7uhg3CqcKWbxRtWrJj2BATobvPfz4FiW7TIkvCoLuXNN5UPI4qgBg92tTU3sW3bNhUtWlS5c+fWkiVLUj32s88+E6ApU6ZkkXUOxG6XXnlF/ZPsOtqQP7/hY+4EomrWlA/mtr/Zsx1WrtOFmyyHD+vyPffoMfPm1QEdKlDA5YHcnEqvXvI0d7wohdbMVSxdulT+/v666667tHXr1lsen5CQoPr16yt//vw64+zN6o7kyhUl9OqV5hAzmebcOS0xW/UlmYh2kRyuEa5kxIEaPlyzzZ0WgaB5YGQTSC2CQDbluNnzaAnSzz+72pxEPvnkE3l6eqpq1ao6lo4E4Dt37pSXl5f69+/vROscyLlziqxXL3F7aCi3CDHjCGbN0mBzu2JUrVoOLdp1wr3K0qU6kC9fYmTGJ0DRDRpkPou8O5GQoKne3gL0Ajg/pEqaTErQ888/L0Bt2rTRf//9l+4yXnjhBQFa467B5K5yQ4iZcNIQYsYRPPigKlx9WY8Y4dCiXS9cSTpxQjFNmmiIeWOrgfYFBjpnssAVHDyoh81rWxwQ4GprFB0drfvuu0+AHnvsMcVl8AGOiopSiRIlVKFCBcW4awjXDRuuCzHzFKQtxExmsdt1pHBhARoDmc6tfCPuIVzJiAbwxhv63mZTPoyEStMxIy+460ORVhYtUh3zwYlu0MClppw5c0b16tUToPfeey/TM8OLFi0SoLfeestBFjqQ2bM1KUmImY/TGmLGEezcqc/M33yHv7/DW3f3Ee5V1qzRkcKFE/PVDgBF1ahhpI/IrowZoyBzrKNHH3WZGfv27VOZMmXk6+urOQ58gLt16yZfX1/t37/fYWVmCrtdGj36+hAzAQEOb/VSZexYdcPIPWTv2tXhxbufcCXpzBnFtW6tl8w3ZRhoZ+7c0ty5zqvTmQwYIG/zR9S4cS4xYe3atcqXL58KFizo8OTYx48fl7+/v1q1auX6td24OCU89JDCuSHETBa/VOJatFAgZoLwzz5zePnuKVzJcMoYM0YrPDxUGCONxGeYGfoymtfFRVysXVuAGoC0bFmW1z9jxgzlyJFD5cqVc1qrOH78eAGaOXOmU8pPE5GRimreXKGmaIuDzterl/UB5i5f1oYcOYz7AdKRIw6vwn2Fe5WNG3WyaNHE/EG9QJGVKhkxdLMDdrvmm5nZBjnpR0y5antiAPaGDRvqnBMf4Pj4eNWoUUNBQUE6f/680+pJkSNHdDg0NDF8bwNQQu/ezslecCuWLtUIs7d4tmxZp1Th/sKVpPPnldCli9423dPKgDb7+hq5St2dU6f0jPkwzfDxybKIhbGxsRowYIAA9e7dO0sy/23atEkeHh4aNGiQ0+u6ji1btCF//sTwrg+AkTbFVd32Z55RHVBtJ3rJZQ/hSsaPMGGC1np7qyhGepIPMTPSu2GOoERWr1ZT84E6U6VKllR54cIFtWzZUoBeeeWVLB13PvXUU7LZbPr111+zpsKFC/V1jhyJuYNGJ5eyJov5NzRUHph5cW/hPppRso9wr/LHHzpburTamz9UJ9C5smWl7dtdY8+t+PhjhWAkL1Pfvk6v7vDhw6pUqZK8vLxc4kscGRmpIkWKqGrVqhleH04zEyboFa5ltpifM6fr3WaPHNFs06Z13t5GgnUnkP2EK0kXL8rep48+MKf6QzCzH3zyiXsEz07Kk0/KFyPzuUaOdGpVmzZtUlBQkAICAvTjjz86ta7UmDt3rgC9//77zqkgPl4aMkTdTIH4gbYFB2cos7vDmTxZA0B5QHHNmzutmuwpXMkQ6Bdf6HcfH5Uy37ijQAnduxuZyN2EmGbNBEbeG333ndPq+f777+Xn56eQkBDtdHEuJbvdrrZt2ypXrlw64ujJuKgoxdx7r6qZog0Cna5a1W12l9m7dVNRUFeQxoxxWj3ZV7hX2b1bFypU0H1c80M9Xby49PvvrrZMkrQ6f34BigCnzYR/9NFH8vDwUM2aNd0iKbYkHTp0SDlz5lSnTp0cV+ipUzpdtaqCuOYaG3PvvU7rjqab+HjtCggQoE9B2rHDaVVlf+FK0uXLsj/yiCZh7K8MBq3y9DQyj7uy6xwZqRHmQ/axh4fD3d7i4+M1ePBgAerYsaMuudkk3ahRowRowYIFmS9s1y5tCw6Wn3k/u4I0ZEimEmc5nF9+0fumfYcLF3bqs3d7CPcqM2dqm5+fyptraK+C4jt0cF2i619/VTvzh9xfpoxDi46KilKnTp0E6Omnn1a8Oz3AJrGxsapUqZKKFSumi5nJr5sVIWYcwYgRCsfISi8nb3e8vYQrSfv362LVqupn/siNQceCg6W1a7PelqlTVdZ8iciB/qqnTp1SrVq1ZLPZ9OGHHzqsXGewbt06ARo6dGjGCpg6Ve8mCTHzlbNCzDiAy7Vryxf0NBj5fp3I7SdcyfCWGTxY0zCSZBcALXZFfKvnn1duc4ZRL7/skCJ37dqlEiVKyM/PzzFd0CzgoYcekqenZ5qiayRihpgZYL6AfXBuiJlM8++/Wma+YBaD03t5t6dwr7Jggf5MEt9qGCi2efMsm4FMaN8+cZOEpk/PdHkrV65Unjx5VLhwYf3uJpNvaeHcuXMqWLCg6tSpo4S0vDjNEDNXd4jlAx0ODc1Sd9F0M3u2hpgvmKgs0MbtLVzJiG9Vp44Gmg/BPZjxrVascHrV24oVE6AukOmWYurUqfLy8lLFihX1999/O8jCrGPatGkCNHHixNQPTC7ETPPmzg0x4wgGDFBFUAswwi85mdtfuJIxm/vii5rDtfhW317tvjrLu+fKFY01H753IMNLFna7Xa+++qoANW/e3DUO/A7AbrerWbNmypMnT8pLVvv3a1/JkokhZlqBEh56yIhP5s7Y7ToaFGQEJwBp3TqnV3lnCPcqy5frQP78qmU+GI+DouvXd06Uvx071MOs57fg4AwVceXKFfXt21eA+vfv777hYdLIn3/+qRw5cqh37943f+mqEDOOYNcufW7avT137iwJdnhnCVeSTp5UTNOmeta80VVBewMDpR9+cGw9s2ersllHQuvW6T7933//VePGjRPDwrh8g7qDGDFihAAtX7782h9vDDHj6enQGMRO5/33dR+oCMjeuXOWVHnnCVcyFu3feksLbTblx4hv9fXVBX1HtWqvv6685qy2nn02XaceOHBAoaGhypEjh6Y7YFLLnYiOjlbZsmXlbUa9TPpxSYgZBxDfqpXyYm4nzKKY2XemcK/y8886GhSkhuaD0x90qXp16cCBzJfds6c8QKVAmjw5zadt3LhRBQsWVN68ed0/7GkGKV68+E2iBVTG0zPLQ8xkmsuXtdGMdvENSFk0cXhnC1cy4lu1aaNXzG5aBdCOXLky3VU7XKFCou90WpObzZ07V76+vipVqpT+zC4RPm7F6dPSqlXS+PHSo4/qYp06yYr26ifbsWyZXjefnbMO9o5LDUu4kjEBMnasfvT0VGEMf+dPyUR8q/h4TfbyEqDhcMvUE3a7XWPGjJHNZlOdOnX0zz//ZPBCXMiZM9JPPxmuiIMGKap+ff3g768hoCaQGPwgNdFmS+EOGaJ6oFogPfVUllVrCTcpv/6qUyEhamk+RD1BkRUrSnv2pK+c/fsTPX6WBwamemhcXJwGDhwoQN26ddPly5czcQFZwL//Gu6jkyZJTz6pmCZNtDQwUM+BmoOKmU4INwrSC2MLXt3bTLjny5eXJ6b/9KJFWVavJdwbuXBBCV276h2MPb6lQZt8faUvv0x7GT/8oNrmgxjdsGGKhyXNRfvcc8+lzasoq7hwwZgk+uwzafBgxTVvrpX58ulFjJQaxc2eSXICLYQRb+lhjPSVZ6pWlR54QHrvPWnJEoWVLZusaMPCwlx91enj6FHNNW1f6+WVpSGULOEmh90uTZyodd7eKmbOdn4IskdESGnZ5fLuuypstjwaODDZQ5Lmop00aZKDLyAdXLwo/fqrNGWK9OyzimvZUmsLFNAroNbm5FrOZETmieEDXgMjaP1Ub2+drFRJioiQRo82NgIcOpSib/hN6Vezm2gl6fPP9TCGU09s06ZZWrUl3NTYulXnypRRR/PhuhczvtW2bamf17+/vM1xncaPv+nrpLloFy9e7CTjbyAqStq0yeg5DBumhNattaFwYb0OaocROTNXMgL1wPAVrgbqB/rM01NHw8KkPn2MTRsLFhgzwRncVjh06FB5eXllr3SdJvbu3RUC6ozpLJKFWMK9FRcvyt63r8aZLW8x0Hpvb2nixBS9es7XrClAjeAmn+j05qJNN9HR0h9/SF99Jb3wghLatdPvRYpopPniKYexbp2cQPOCqoD6YGz8PxgaKvXsKb35pjRvnrR3r8M3rm/btk2APvroI4eW63Ti47XHjHYxCW79MncwlnDTytSp18W3egeU0K3bzfGt7HbNzZlTgJ6E69KFXs1FW6VKFR3NrJvllSvGwzJjhvTSS0ro2FHbihXTuxibGsqD/DGWKZIK1Ibhq10ZY/JtvIeH9pUuLXXvLr32mpEUa/fuLPUPrlKlimo5OH+s09m4UePMe3qoYMEsd820hJse9uzRhYoVE32QW4FOhYRIv/127ZgTJzTY/H6Wr69kt1+Xi7Z169bpy0UbGyvt3CnNmmUE+e7aVbtLltQHNpu6gyqaY6zkBBqAsaWwG2iszaadxYtLXboYu1dmzjTKdQP/57FjxwrQnvTO3ruS119XG7MHo379srx6S7jp5fJl2R99VJ+as6pBoJWentL77xtv3ZUr1dgUz7nq1dOeizYuzlh2mjtXev116b77tL9MGX3k4aFeZgsZmIxAMVvW8uZYazToj6JFldChg/Tii0aLvG2ba1JxpJETJ07Iw8NDL774oqtNSTPRdeoo59Ve1YwZWV6/JdyMMnu2tufKlRjf6hVQXNu20ujRKoaxLHKmR4/EXLTvvvuusVEgPl7at88I1frWW1KvXjocGqpJnp7qY44x85pjzhsFmgtUFtQR9Bbo9+BgJbRtK73wgjGm3bJFcvd14BRo3bq1QkJC3GtJLCXOn9cKDw8BWgiG80kWYwk3Mxw4oEvVqqm/KaxGprBuFNyc3r2lPn10NCxMn3l5qR+oOih/CgL1w1g/bgt6HbShUCFjh9GwYUZ6jd9/d++0KxlgxowZArR69WpXm3Jr5s7VUAxPsEvVqrnEhNSE64VF6pQqRa6NG5kyfDjN3n+fvikcdt+MGXgACTf8PSdQHAgFagEtChSgXrVqeFWuDBUrGp+wMPD3d+JFuAf33nsv/v7+TJs2jSZNmrjanNRZtoylQAMgV9u2rrbmJmyGsJOnZs2a2rRpUxaa4+b88AO2jh1T/Lo4UA6oCTTPl4+GVauS40aBBgZmlbVuyYMPPsicOXM4ffo0fn5+rjYneSSOFy1K0RMnGA089/PP0LBhlpths9k2S6qZ3HdWi5seOnRI9eu/x42D6tUNkebLl0VGZS8iIiL44osvWLBgAb169XK1Ocmzdy/LT5wAINzPD+rUcbFBN+PhagNuJ06+9x5cumSJNhUaNWpESEgI06ZNc7UpKbNsGcuAIODuli3B29vVFt2EJdx0EhYWluJ3FY4fZ3LbtqhPHzhzJgutyj54eHhw//33s3z5ck6ePOlqc5IlYelSVgCtAFvr1q42J1ks4aaTXbt23STesCJF2BsYSFXgYaDZjBn8Va4cfP01pDKHcKfSt29f7HY733zzjatNuZkrV9i8ejX/AuEA4eEuNih5LOFmgF27dl03Nb/r+HHK7dvHqt69+RT4A7j7wgVG9e1LXHg4/P23iy12L8qXL0+tWrXcs7u8bh3LYmKwAS1LloSSJV1tUbJYwnUUBQviMX06Dy9ezO4iRWgLDAdqr1jB5goVYNw4SLhxsejOJSIigm3btrF9+3ZXm3I95vi2OlCwXTtXW5MilnAdTZs2FNm7l2+ffppvgdNA7StXGPbMM1yuXRvc7UF1ET179sTLy4uvvvrK1aZcR+TixWzEvbvJgOU55VQ2btT5ChX0iOktVRK03MNDeumljMW5us3o2LGjgoKCUvbtzmqOH9e35m+1xtMzbQEVnAipeE5ZLa4zueceArdu5ZM33+QnLy+8gFZ2Ow+8/TbnKlWCn392tYUuJSIiglOnTrFy5UpXm2KwfDnLAH+gboMGkDu3qy1KEUu4ziZHDnj5ZRrv2MH2evV4EZgOhB04wMzGjdGjj0JkpKutdAnt27cnMDDQbbrLWrqUZUAzwLtNG1ebkyqWcLOK8uXxXbuWtydOZJOfHyFAL6Djp59ytFw5mD/f1RZmOT4+PvTo0YN58+Zx8eJF1xqTkMC+Zcs4TDYY32IJN2vx8IDHHqPK3r1s7NCB94FVQNg//zChc2fsXbuCmzolOIuIiAiio6P59ttvXWvIli0su3ABgPD8+eHuu11rzy2whOsKihbFc8ECnpkzh50FClAPeAJoMG8eu8uVg8mT7xjHjbp161K6dGnXd5fNZaAyQKm2bY2XrBvj3tbdzths0K0bJfftY+mDDzIN2AtUvXSJ1x9+mJgmTeCvv1xspPOx2WxERESwevVqjhw54jI7YpYs4SeyRzcZLOG6nrx5sX3+OX1XrmRP8eJ0B14Dqv/8MxsqVoRRoyAuzsVGOpf7778fSUyfPt01BkRGsm7jRi5jCrdlS9fYkQ4s4boLzZpRaPdupj/3HIs8PLgINIiL44nhw7lYvTrcxvuiS5UqRYMGDfjqq6+MsCxZzapVLLPb8QaaVqkChQplvQ3pxBKuO+HnB6NH03bTJnZVqcKTwMdA2M6dLKxdG4YOhagoV1vpFCIiItizZw+bN2/O+srN8W19ILcbRrtIDku47ki1avhv2sSH773HBh8f8gAdJHqNHcs/YWGwYoWrLXQ43bt3x8fHJ+s3HkicXLSI7WSf8S1YwnVfvLxg6FDq7NrFlqZNeQOYB1Q4coQvW7VC/frBuXOuttJhBAYG0rFjR7755hvisnJM/9dfLD92DIDwnDmhbt2sqzsTWMJ1d0qXJsfKlbwyZQpbAwKoADwAtJo2jYOhoTBz5m2zdBQREcHZs2dZunRp1lVqdpMLAVVatDA83bIBlnCzAzYb9O9Phb17+bl7dz4GfgUqnTvH2F69iG/XDly4lOIowsPDKViwYJZ2l+1Jol14uGm0i+SwhJudCArCY/ZsBi5YwO6gIFoAQ4E6S5awtXx5+N//wG53tZUZxtvbm169evH9999z/vx551cYE8OWlSs5S/Ya34Il3OxJx44U3buXBQMHMhs4CtSMjmb4k08SXbcu7NrlagszTEREBLGxscyZM8f5la1fz7KYGABaFS8OpUs7v04HYQk3uxIQgO3jj+m+bh17ypalHzAKuPu331hdpQq89hqYD2V2onr16lSoUCFrusvm+LYaUMiNo10khyXc7E79+uTbsYPPX32VHz09sQPNEhJ4+PXXOV+lCmzY4GoL08VVF8j169dz4MABp9b13+LF/EL26yaDJdzbAx8feP11mm/dyo6aNRkGTAHC9u7l2/r10eOPg6u3zaWDPn36YLPZ+Prrr51XyalTrNq5k3gg3MMDmjZ1Xl1OwBLu7USlSvht3Mi748fzu68vwUA3oMvHH3M8NBQWLnS1hWmiWLFiNG3alGnTpjnPBdKMdpEbqFe/frbL3WQJ93bD0xOefJLqe/fyW+vWvAssBcJOnuSTDh2w9+wJ//zjaitvSUREBAcPHmSDk7r6V6NdNAVyuHm0i+SwhHu7EhKC1+LFDJsxgx1581ITeAxoMmsWe8uVgy+/dGvHjS5duuDn5+ecfbp2O/uXLuUQ2XN8C5Zwb29sNujVizL79vFj3758DuwAqkRG8vYDDxDbogUcPOhqK5PF39+fzp07M2vWLK5cueLYwv/4g2XmOnF4vnxQtapjy88CLOHeCRQogG3aNB5ctow9RYvSEXgZqLlqFb+FhcHYsRAf72orbyIiIoILFy6w0NFjc3MZqDRQpk0bt492kRzZz2KLjNOqFUF//snsIUOYb7PxL1AnJoZnhg7lUq1asHWrqy28jubNmxMcHOzw7nLskiWsJvt2k8ES7p1Hrlwwdiz3/voruypW5DFgHFBp61aW1qgBw4dDdLSrrQTA09OTPn36sHjxYs44Kvvhf/+xfsMGojCF26qVY8rNYizh3qnUqkWeP/7g45EjWevtTU6gjd1O31GjOFuxIqxe7WoLAaO7HB8fz8yZMx1T4OrVLLPb8QKaVq4MhQs7ptwsxhLunYy3NwwfToOdO/mjQQNeAWYCFQ4dYnqzZuihhyArnP1ToXLlylStWtVx3eUk0S78s0m0i+SwhGsB5crhu2YNb3z6KVty56Y0cD/Q9vPPORwaCt9+69Klo759+/L777+zZ8+eTJd1etEitpK9x7dgCdfiKh4e8PDDVN67l/WdOvEhsBaoeOYMH3brRkLnznD8uEtM6927Nx4eHplvdffvZ7m5bznc1xfq13eAda7BEq7F9RQpgud33/HUt9+yq2BBGgGDgXoLFrAjNBQ++STL9/wGBQURHh7O119/jT0zdZvd5IJA1ebNs020i+SwhGuRPF26UHzfPhY9/DDTgYNA9agoXnnsMa40agR792apOX379uXo0aOsWbMmw2XYly5lOdkv2kVyWMK1SJnAQGyffkrvn35iT6lS9ALeAqquX8/aypXh7bchNjZLTLn33nvx9/fP+D7d2Fi2rlzJGbL/+BYs4VqkhcaNKbBzJ9NefJGlHh5cARrFxTHw5ZeJrFYNfvvN6Sb4+fnRvXt35s6dy+XLl9NfwIYNLDPXp1uFhECZMg62MGuxhGuRNnLmhLffJnzLFnZWq8YzwKdAxd27WVCnDjzzDFy65FQT+vbty6VLl5ifkZSk5vi2KlC4bVvDjzsbYwnXIn1UqULu337j/bFj+cXHh3xAJ4nu48ZxqkIFWLbMaVU3atSIkJCQDHWXLy5ezHpuj24yWMK1yAheXjBkCLX37GFz8+a8DfwAVDh2jCmtW6O+feHsWYdX6+HhQd++fVmxYgUn05NH+PRpVm/ffi3aRbNmDrctq7GEa5FxSpbEe8UKXvzyS7blycPdwACg+ddfs79cOZg+3eGOG3379sVutzNjxoy0n7RiBcuAXED9unUhIMChNrkCS7gWmcNmg4gIQvftY3XPnnwCbAYqnz/P6PvvJ751azh82GHVhYaGUrt27fR1l83xbXaNdpEclnAtHEOhQnh88w2PLFzIniJFaA28ANRevpwt5cvDhx9CQoJDqoqIiGD79u1s27bt1gfb7RxYvJgD3D7jW7CEa+Fo2rWjyJ9/8t2TT/ItcBKodeUKwwYP5nKdOrBzZ6ar6NGjB15eXmlzgdy2jWX//gtAeN68UL16put3ByzhWjgef5UCnXsAAASiSURBVH8YP54uGzawJzSUAcAYoPKmTaysUgVefRUyEY6mQIECtGvXjunTpxN/q8gdZje5JFAmPDxbRrtIjtvjKizck7p1Cdy+nU9ff53Vnp54Ai3sdh58803+vftuWLcuw0VHRERw6tQpVq5cmepxsUuWsAqjm2zL5m6OSbGEa+FccuSAV1+lyfbtbLvnHoYD04AKf/3F7IYN0cCBEBmZ7mLbtWtH3rx5U5+kunSJX9av5xLZO9pFcljCtcgawsLIuWEDIydMYJOfH8WAHsC9kyZxrHx5+P77dBXn4+NDjx49+O6777iYUpaG1atZlpCAF9CsYkUIDs7sVbgNlnAtsg4PDxg0iKp//snGtm0ZC/wIhJ06xcf33ou9e3c4dSrNxUVERBAdHc23336b/AHm+LYuEJCNo10khyVci6ynWDG8Fi5kyKxZ7MyfnzrA40DDuXPZXa4cTJmSJseNOnXqUKZMmRS7y/8sWsQWbq9loKtYwrVwDTYb3HcfpfbtY9kDD/Al8CdQ7eJF3hgwgNhmzWD//lsUYaNv376sXr2aI2Zki0QOHmTF338DEO7jAw0aOOUyXIUlXAvXki8fti++IGLFCvYUL05XYARQ/aef+KViRXj33VSDtd9///0ATJ8+/fovzG5yAaB6s2ZGRsPbCEu4Fu5BixYU2r2bGcOGsdBm4z+gfmwsTz3/PBdr1IAtW5I9rVSpUjRo0OCmzH52M6lXS7J/tIvksIRr4T74+cG779Lu99/ZVbkyjwP/Aypu386iWrXguecgmU30ERER/Pnnn2zatMn4Q1wc2378kX+4Pce3YAnXwh2pUQP/zZv5aPRo1ufIgT/Q3m6n93vv8U9YGNzgdNG9e3d8fHyuuUD+8gvLTIG3uusuKFcuiy/A+VjCtXBPvL3hueeou2sXWxo35jVgLlDh8GGmtWiB+vcH0wc5MDCQjh078s033xAbG5s4vr0bCG7XLttHu0gOS7gW7k2ZMvisXs2Izz9nq78/oUA/IHzqVA6VKwezZ4NEREQEZ8+eZenSpVy6zaJdJIclXAv3x2aDBx8kbN8+1nXrxv+AX4BK587xfo8exHfoQHjFihQsWJCvJk9m9datxGFGu2je3MXGOwdLuBbZh6AgPObM4fH589lduDDNgGeBuosWsbtSJXpVrMj3S5YwE/ADGtxzD+TJ41qbnYQlXIv/t3fHqA3DYBiGP6EMIUumjqWLb1DfI2NvIIK3XsYGH6B06tAThKyhc8kaunVPQ4lQt9DBuBhcot+8zygtHvxiY9mWPauVbvd7va7XepZ0kHR/POpls9H3+awnSV+SHjPcrHsshAublku5ptHDdqv3olAh6ePXdJLU7HaqqupKB/i/XOp5J7Qsy3RZGwNydTpptlgodpzL3vu/P7bPlHPuLaVUds1xxYV983lntJIUR/rPVW4IF5PgvR80bh3hYhJCCIPGrZtd+wCAMdR1LUlq21YxRnnvFUK4jE8ND6eATPFwCpgYwgUMIlzAIMIFDCJcwCDCBQzqXQ5yzn1KGm9zUwBD3KWUbromesMFkCdulQGDCBcwiHABgwgXMIhwAYN+AIG0V/T9oGDIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "contour=polygons[5]\n",
    "#point=point_coordinates_list[94]\n",
    "point=point_coordinates_list[5]\n",
    "point=sort_points(np.array(point).reshape(1,2*nb_of_points),nb_of_points)\n",
    "point=np.array(point).reshape(nb_of_points,2)\n",
    "\n",
    "contour_with_points=np.vstack([contour,point])\n",
    "quality,_=Triangulation_with_points.quality_matrix(contour,point)\n",
    "ordered_matrix=Triangulation_with_points.order_quality_matrix(quality,contour,contour_with_points,check_for_equal=True)\n",
    "\n",
    "initial_elements,sub_elements=Triangulation_with_points.triangulate(contour,point,ordered_matrix,recursive=True,plot_mesh=True)\n",
    "\n",
    "total_elements=concat_element_list(initial_elements,sub_elements)\n",
    "\n",
    "mesh_minimum_quality=compute_minimum_quality_triangulated_contour(contour_with_points,total_elements)\n",
    "\n",
    "\n",
    "print(\"Minimum quality:\",mesh_minimum_quality )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked (1, 2)\n",
      "checked (2, 3)\n",
      "checked (4, 5)\n",
      "checked (3, 4)\n",
      "checked (5, 0)\n",
      "checked (0, 1)\n",
      "initial set edges: {(0, 1), (1, 2), (4, 5), (2, 3), (5, 0), (3, 4)}\n",
      "meshing polygon:  [[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [-0.13440754  0.86278798]\n",
      " [-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [ 0.61250609 -1.12049258]]  with inner points : [[-0.38091777  0.3462756 ]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.32090279 -0.3235997 ]\n",
      " [ 0.10896638  0.01804129]]\n",
      "Edge: (1, 2) targeting: 6\n",
      "(1, 2, 6)\n",
      "[[-0.00480932  0.45412145]\n",
      " [-0.13440754  0.86278798]\n",
      " [-0.38091777  0.3462756 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 6)\n",
      "set of interior edges updated: {(1, 6)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (2, 3), (1, 6), (5, 0), (3, 4)}\n",
      "edges inserted: (2, 6)\n",
      "set of interior edges updated: {(2, 6), (1, 6)}\n",
      "set of edges updated: {(0, 1), (1, 2), (2, 6), (4, 5), (2, 3), (1, 6), (5, 0), (3, 4)}\n",
      "element inserted: (1, 2, 6)\n",
      "Spotted edges linked with point:  (1, 6)   (2, 6)\n",
      "Edge: (2, 3) targeting: 6\n",
      "(2, 3, 6)\n",
      "[[-0.13440754  0.86278798]\n",
      " [-1.03542791 -0.00446935]\n",
      " [-0.38091777  0.3462756 ]]\n",
      "Vertex locked: 2\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (3, 6)\n",
      "set of interior edges updated: {(2, 6), (1, 6), (3, 6)}\n",
      "set of edges updated: {(0, 1), (1, 2), (2, 6), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (3, 4)}\n",
      "element inserted: (2, 3, 6)\n",
      "Spotted edges linked with point:  (2, 6)   (3, 6)\n",
      "Edge: (4, 5) targeting: 7\n",
      "(4, 5, 7)\n",
      "[[-0.43294286 -0.45065737]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [-0.3490263   0.07266232]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (4, 7)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (1, 6), (3, 6)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 5), (2, 3), (1, 6), (5, 0), (3, 6), (3, 4)}\n",
      "edges inserted: (5, 7)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (5, 7), (1, 6), (3, 6)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 5), (5, 7), (2, 3), (1, 6), (5, 0), (3, 6), (3, 4)}\n",
      "element inserted: (4, 5, 7)\n",
      "Spotted edges linked with point:  (4, 7)   (5, 7)\n",
      "Edge: (3, 4) targeting: 8\n",
      "(3, 4, 8)\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "found line intersection (3, 8) with  [4 7]\n",
      "Edge: (3, 4) targeting: 9\n",
      "(3, 4, 9)\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [ 0.10896638  0.01804129]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "found line intersection (3, 9) with  [4 7]\n",
      "Edge: (3, 4) targeting: 0\n",
      "(3, 4, 0)\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [ 0.99508155  0.25870986]]\n",
      "set of forbidden inter section edges updated: {(1, 4), (1, 5), (2, 5), (3, 4), (2, 4), (3, 5)}\n",
      "found line intersection (4, 0) with  [5 7]\n",
      "Edge: (3, 4) targeting: 2\n",
      "(3, 4, 2)\n",
      " Target vertex 2 is locked\n",
      "Edge: (3, 4) targeting: 1\n",
      "(3, 4, 1)\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [-0.00480932  0.45412145]]\n",
      "edges : (3, 1) and (4, 1) intersecting\n",
      "Abandoning creation of element (3, 4, 1)\n",
      "Edge: (5, 0) targeting: 3\n",
      "(5, 0, 3)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-1.03542791 -0.00446935]]\n",
      "edges : (5, 3) and (0, 3) intersecting\n",
      "Abandoning creation of element (5, 0, 3)\n",
      "Edge: (5, 0) targeting: 4\n",
      "(5, 0, 4)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.43294286 -0.45065737]]\n",
      "Vertex locked: 5\n",
      "set of forbidden inter section edges updated: {(1, 4), (1, 5), (0, 5), (2, 5), (3, 4), (2, 4), (3, 5)}\n",
      "found line intersection (0, 4) with  [5 7]\n",
      "Edge: (5, 0) targeting: 2\n",
      "(5, 0, 2)\n",
      " Target vertex 2 is locked\n",
      "Edge: (5, 0) targeting: 1\n",
      "(5, 0, 1)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]]\n",
      "Vertex locked: 0\n",
      "edges : (5, 1) and (0, 1) intersecting\n",
      "Abandoning creation of element (5, 0, 1)\n",
      "Unlocking vertex 0\n",
      "Edge: (0, 1) targeting: 5\n",
      "(0, 1, 5)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [ 0.61250609 -1.12049258]]\n",
      "Vertex locked: 0\n",
      "edges : (0, 5) and (1, 5) intersecting\n",
      "Abandoning creation of element (0, 1, 5)\n",
      "Unlocking vertex 0\n",
      "Edge: (0, 1) targeting: 2\n",
      "(0, 1, 2)\n",
      " Target vertex 2 is locked\n",
      "Edge: (0, 1) targeting: 4\n",
      "(0, 1, 4)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [-0.43294286 -0.45065737]]\n",
      "edges : (0, 4) and (1, 4) intersecting\n",
      "Abandoning creation of element (0, 1, 4)\n",
      "Edge: (0, 1) targeting: 3\n",
      "(0, 1, 3)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.00480932  0.45412145]\n",
      " [-1.03542791 -0.00446935]]\n",
      "set of forbidden inter section edges updated: {(1, 0), (1, 4), (1, 5), (2, 0), (0, 5), (2, 5), (3, 4), (2, 4), (3, 5)}\n",
      "edges inserted: (0, 3)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (5, 7), (1, 6), (3, 6), (0, 3)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (4, 5), (5, 7), (2, 3), (1, 6), (5, 0), (3, 6), (0, 3), (3, 4)}\n",
      "edges inserted: (1, 3)\n",
      "set of interior edges updated: {(4, 7), (2, 6), (1, 3), (5, 7), (1, 6), (3, 6), (0, 3)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 7), (2, 6), (1, 3), (4, 5), (5, 7), (2, 3), (1, 6), (5, 0), (3, 6), (0, 3), (3, 4)}\n",
      "element inserted: (0, 1, 3)\n",
      "Spotted edges linked with point:  (0, 3)   (1, 3)\n",
      "Final edges: {(0, 1), (1, 2), (4, 7), (2, 6), (1, 3), (4, 5), (5, 7), (2, 3), (1, 6), (5, 0), (3, 6), (0, 3), (3, 4)}\n",
      "Elements created: {(4, 5, 7), (1, 2, 6), (2, 3, 6), (0, 1, 3)}\n",
      "Set of locked vertices: {2}\n",
      "set of orphan vertex: {8, 9}\n",
      "Set of open vertices: {0, 1, 3, 4, 5, 6, 7, 8, 9}\n",
      "Examining if vtx 0 is locked\n",
      "Examining if vtx 1 is locked\n",
      "1 locked after all\n",
      "Removed: 1 from set of open vertices\n",
      "Added new element: (1, 6, 3)\n",
      "Removed: (1, 6) from set of edges\n",
      "Removed: (1, 3) from set of edges\n",
      "New set of elements {(4, 5, 7), (1, 2, 6), (2, 3, 6), (1, 6, 3), (0, 1, 3)}\n",
      "Re-evaluting set of open vertices\n",
      "Examining if vtx 0 is locked\n",
      "Examining if vtx 3 is locked\n",
      "Examining if vtx 4 is locked\n",
      "Examining if vtx 5 is locked\n",
      "Checking if interior point 6 is closed\n",
      "found 6 in (2, 6)\n",
      "Interior vertex 6 is open\n",
      "found index [1] in element (1, 2, 6) \n",
      "Interior vertex 6 is open\n",
      "Interior vertex 6 is open\n",
      "found index [3] in element (1, 6, 3) \n",
      "Interior vertex 6 is open\n",
      "set of open vertices {0, 3, 4, 5, 6, 7, 8, 9}\n",
      "Edge (3, 6) is common for two elements\n",
      "[]\n",
      "Starting with vertex 0\n",
      " Initial edges to visit {(4, 7), (0, 3), (5, 7)}\n",
      "Candidate edge (0, 1) found in element (0, 1, 3)\n",
      "Added edge (0, 5)\n",
      "Added edge (3, 4)\n",
      "Candidate edge (3, 2) found in element (2, 3, 6)\n",
      "Edges to visit: {(4, 7), (0, 5), (0, 3), (5, 7), (3, 4)}\n",
      "Visiting vertex 0\n",
      "0  in  (0, 5)\n",
      "Found vertex: 5\n",
      "Removing edge (0, 5)\n",
      "{(4, 7), (0, 3), (5, 7), (3, 4)}\n",
      "Visiting vertex 5\n",
      "5  in  (5, 7)\n",
      "Found vertex: 7\n",
      "Removing edge (5, 7)\n",
      "{(4, 7), (0, 3), (3, 4)}\n",
      "Visiting vertex 7\n",
      "7  in  (4, 7)\n",
      "Found vertex: 4\n",
      "Removing edge (4, 7)\n",
      "{(0, 3), (3, 4)}\n",
      "Visiting vertex 4\n",
      "4  in  (3, 4)\n",
      "Found vertex: 3\n",
      "Removing edge (3, 4)\n",
      "{(0, 3)}\n",
      "Visiting vertex 3\n",
      "3  in  (0, 3)\n",
      "Found vertex: 0\n",
      "Removing edge (0, 3)\n",
      "set()\n",
      "Back to starting vertex\n",
      "Starting with vertex 3\n",
      "Starting with vertex 4\n",
      "Starting with vertex 5\n",
      "Starting with vertex 6\n",
      "Starting with vertex 7\n",
      "Starting with vertex 8\n",
      "Starting with vertex 9\n",
      "found polygon [0, 5, 7, 4, 3]\n",
      "remeshing subpolygon [0, 5, 7, 4, 3]\n",
      "Point  [0.10896638 0.01804129]  is inside  [0, 5, 7, 4, 3]\n",
      "Point  [ 0.32090279 -0.3235997 ]  is inside  [0, 5, 7, 4, 3]\n",
      "[[[[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[-0.43294286 -0.45065737]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-1.03542791 -0.00446935]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.43294286 -0.45065737]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-1.03542791 -0.00446935]\n",
      "   [ 0.32090279 -0.3235997 ]]]]\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "[0, 1, 2]\n",
      "[0, 1, 3]\n",
      "Invalid triangulation 0 1 3\n",
      "[0, 1, 4]\n",
      "[0, 1, 5]\n",
      "Invalid triangulation 0 1 5\n",
      "[0, 1, 6]\n",
      "Invalid triangulation 0 1 6\n",
      "[1, 2, 0]\n",
      "[1, 2, 1]\n",
      "[1, 2, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 4]\n",
      "[1, 2, 5]\n",
      "[1, 2, 6]\n",
      "[2, 3, 0]\n",
      "[2, 3, 1]\n",
      "[2, 3, 2]\n",
      "[2, 3, 3]\n",
      "[2, 3, 4]\n",
      "[2, 3, 5]\n",
      "[2, 3, 6]\n",
      "[3, 4, 0]\n",
      "[3, 4, 1]\n",
      "[3, 4, 2]\n",
      "[3, 4, 3]\n",
      "[3, 4, 4]\n",
      "[3, 4, 5]\n",
      "[3, 4, 6]\n",
      "[4, 0, 0]\n",
      "[4, 0, 1]\n",
      "[4, 0, 2]\n",
      "[4, 0, 3]\n",
      "[4, 0, 4]\n",
      "[4, 0, 5]\n",
      "[4, 0, 6]\n",
      "[[0.01278575 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.01278575 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.01278575 0.01278575 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.01278575 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.01278575 0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "[(0.012785754, 1)]\n",
      "checked (4, 0)\n",
      "[(0.012785754, 1)]\n",
      "checked (3, 4)\n",
      "[(0.012785754, 2)]\n",
      "0\n",
      "triangle [2 3 6]\n",
      "[2, 3, 6]\n",
      "0.48621789030802587\n",
      "1\n",
      "triangle [2 3 5]\n",
      "[2, 3, 5]\n",
      "0.43028323513730043\n",
      "[(0.4862179 , 6) (0.43028325, 5)]\n",
      "[(0.4862179, 6), (0.43028325, 5)]\n",
      "checked (2, 3)\n",
      "[(0.012785754, 1)]\n",
      "checked (1, 2)\n",
      "[(0.012785754, 1)]\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.01278575 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.01278575 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.01278575\n",
      "  0.01278575]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.01278575]\n",
      " [0.         0.         0.01278575 0.         0.         0.\n",
      "  0.        ]] {(4, 0): [((0.012785754059788103, 2), (0.0, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 1), (0.0, 0))], (3, 4): [((0.012785754059788075, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (2, 3): [((0.4862179, 6), (0.43028325, 5), (0.0, 4), (0.0, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (1, 2): [((0.012785754059788075, 0), (0.0, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 2), (0.0, 1))], (0, 1): [((0.012785754059788075, 2), (0.0, 6), (0.0, 5), (0.0, 4), (0.0, 3), (0.0, 1), (0.0, 0))]}\n",
      "[[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]]\n",
      "initial set edges: {(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)}\n",
      "meshing polygon:  [[-1.03542791 -0.00446935]\n",
      " [-0.43294286 -0.45065737]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]]  with inner points : [[ 0.10896638  0.01804129]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Edge: (4, 0) targeting: 2\n",
      "(4, 0, 2)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-1.03542791 -0.00446935]\n",
      " [-0.3490263   0.07266232]]\n",
      "set of forbidden inter section edges updated: {(0, 3), (1, 3), (1, 4), (0, 4)}\n",
      "edges inserted: (4, 2)\n",
      "set of interior edges updated: {(4, 2)}\n",
      "set of edges updated: {(0, 1), (1, 2), (2, 3), (4, 2), (3, 4), (4, 0)}\n",
      "edges inserted: (0, 2)\n",
      "set of interior edges updated: {(4, 2), (0, 2)}\n",
      "set of edges updated: {(0, 1), (1, 2), (2, 3), (4, 2), (3, 4), (0, 2), (4, 0)}\n",
      "element inserted: (4, 0, 2)\n",
      "Spotted edges linked with point:  (4, 2)   (0, 2)\n",
      "Edge: (3, 4) targeting: 6\n",
      "found (3, 2) (4, 2) Canceling creation\n",
      "Edge: (3, 4) targeting: 5\n",
      "found (3, 2) (4, 2) Canceling creation\n",
      "Edge: (3, 4) targeting: 2\n",
      "(3, 4, 2)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]]\n",
      "Vertex locked: 3\n",
      "Vertex locked: 4\n",
      "set of forbidden inter section edges updated: {(1, 3), (1, 4), (4, 3), (0, 4), (0, 3)}\n",
      "element inserted: (3, 4, 2)\n",
      "Spotted edges linked with point:  (3, 2)   (4, 2)\n",
      "Edge: (2, 3) targeting: 6\n",
      "found (2, 4) (3, 4) Canceling creation\n",
      "Edge: (2, 3) targeting: 5\n",
      "found (2, 4) (3, 4) Canceling creation\n",
      "Edge: (2, 3) targeting: 4\n",
      "(2, 3, 4)\n",
      "Element (3, 4, 2) already in set\n",
      "Edge: (1, 2) targeting: 0\n",
      "(1, 2, 0)\n",
      "[[-0.43294286 -0.45065737]\n",
      " [-0.3490263   0.07266232]\n",
      " [-1.03542791 -0.00446935]]\n",
      "Vertex locked: 1\n",
      "Vertex locked: 2\n",
      "Vertex locked: 0\n",
      "set of forbidden inter section edges updated: {(1, 2), (1, 3), (1, 4), (4, 3), (0, 4), (0, 3)}\n",
      "element inserted: (1, 2, 0)\n",
      "Spotted edges linked with point:  (1, 0)   (2, 0)\n",
      "Edge: (0, 1) targeting: 2\n",
      "(0, 1, 2)\n",
      "Element (1, 2, 0) already in set\n",
      "Final edges: {(0, 1), (1, 2), (2, 3), (4, 2), (3, 4), (0, 2), (4, 0)}\n",
      "Elements created: {(4, 0, 2), (1, 2, 0), (3, 4, 2)}\n",
      "Set of locked vertices: {0, 1, 2, 3, 4}\n",
      "set of orphan vertex: {5, 6}\n",
      "Set of open vertices: {5, 6}\n",
      "remeshing subpolygon [3, 4, 2]\n",
      "Point  [0.10896638 0.01804129]  is inside  [3, 4, 2]\n",
      "Point  [ 0.32090279 -0.3235997 ]  is inside  [3, 4, 2]\n",
      "[[[[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.32090279 -0.3235997 ]]]]\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "[0, 1, 2]\n",
      "[0, 1, 3]\n",
      "[0, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0]\n",
      "[1, 2, 1]\n",
      "[1, 2, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 4]\n",
      "[2, 0, 0]\n",
      "[2, 0, 1]\n",
      "[2, 0, 2]\n",
      "[2, 0, 3]\n",
      "[2, 0, 4]\n",
      "[[0.1896699  0.         0.         0.         0.        ]\n",
      " [0.1896699  0.09868233 0.         0.         0.        ]\n",
      " [0.1896699  0.         0.         0.         0.        ]]\n",
      "[(0.1896699, 1)]\n",
      "checked (1, 2)\n",
      "[(0.1896699, 1), (0.09868233, 1)]\n",
      "checked (2, 0)\n",
      "[(0.1896699, 1)]\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.         0.         0.18966991]\n",
      " [0.         0.         0.         0.18966991 0.        ]\n",
      " [0.         0.         0.         0.09868233 0.18966991]] {(1, 2): [((0.1896699113014106, 3), (0.0, 4), (0.0, 2), (0.0, 1), (0.0, 0))], (2, 0): [((0.18966991130141056, 4), (0.09868232639419078, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (0, 1): [((0.18966991130141056, 4), (0.0, 3), (0.0, 2), (0.0, 1), (0.0, 0))]}\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]]\n",
      "initial set edges: {(0, 1), (2, 0), (1, 2)}\n",
      "meshing polygon:  [[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]]  with inner points : [[ 0.10896638  0.01804129]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.10896638  0.01804129]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 3)\n",
      "set of interior edges updated: {(1, 3)}\n",
      "set of edges updated: {(0, 1), (2, 0), (1, 3), (1, 2)}\n",
      "edges inserted: (2, 3)\n",
      "set of interior edges updated: {(1, 3), (2, 3)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3)}\n",
      "element inserted: (1, 2, 3)\n",
      "Spotted edges linked with point:  (1, 3)   (2, 3)\n",
      "Edge: (2, 0) targeting: 4\n",
      "(2, 0, 4)\n",
      "[[-0.3490263   0.07266232]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (2, 4)\n",
      "set of interior edges updated: {(1, 3), (2, 3), (2, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3), (2, 4)}\n",
      "edges inserted: (0, 4)\n",
      "set of interior edges updated: {(1, 3), (2, 3), (2, 4), (0, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "element inserted: (2, 0, 4)\n",
      "Spotted edges linked with point:  (2, 4)   (0, 4)\n",
      "Edge: (0, 1) targeting: 4\n",
      "(0, 1, 4)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Vertex locked: 0\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 4)\n",
      "set of interior edges updated: {(1, 3), (1, 4), (2, 3), (0, 4), (2, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "element inserted: (0, 1, 4)\n",
      "Spotted edges linked with point:  (0, 4)   (1, 4)\n",
      "Final edges: {(0, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "Elements created: {(2, 0, 4), (0, 1, 4), (1, 2, 3)}\n",
      "Set of locked vertices: {0}\n",
      "set of orphan vertex: set()\n",
      "Set of open vertices: {1, 2, 3, 4}\n",
      "Examining if vtx 1 is locked\n",
      "Examining if vtx 2 is locked\n",
      "Checking if interior point 3 is closed\n",
      "found 3 in (1, 3)\n",
      "Interior vertex 3 is open\n",
      "Interior vertex 3 is open\n",
      "found index [2] in element (1, 2, 3) \n",
      "Interior vertex 3 is open\n",
      "set of open vertices {1, 2, 3, 4}\n",
      "[]\n",
      "Starting with vertex 1\n",
      " Initial edges to visit {(1, 3), (2, 3), (2, 4), (1, 4)}\n",
      "Candidate edge (1, 0) found in element (0, 1, 4)\n",
      "Candidate edge (2, 0) found in element (2, 0, 4)\n",
      "Edges to visit: {(1, 3), (2, 3), (2, 4), (1, 4)}\n",
      "Visiting vertex 1\n",
      "1  in  (1, 3)\n",
      "Found vertex: 3\n",
      "Removing edge (1, 3)\n",
      "{(2, 3), (2, 4), (1, 4)}\n",
      "Visiting vertex 3\n",
      "3  in  (2, 3)\n",
      "Found vertex: 2\n",
      "Removing edge (2, 3)\n",
      "{(2, 4), (1, 4)}\n",
      "Visiting vertex 2\n",
      "2  in  (2, 4)\n",
      "Found vertex: 4\n",
      "Removing edge (2, 4)\n",
      "{(1, 4)}\n",
      "Visiting vertex 4\n",
      "4  in  (1, 4)\n",
      "Found vertex: 1\n",
      "Removing edge (1, 4)\n",
      "set()\n",
      "Back to starting vertex\n",
      "Starting with vertex 2\n",
      "Starting with vertex 3\n",
      "Starting with vertex 4\n",
      "found polygon [1, 3, 2, 4]\n",
      "remeshing subpolygon [1, 3, 2, 4]\n",
      "checked (2, 3)\n",
      "checked (3, 0)\n",
      "checked (1, 2)\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.         0.51205993]\n",
      " [0.         0.         0.         0.51205993]\n",
      " [0.         0.51205993 0.         0.        ]\n",
      " [0.         0.51205993 0.         0.        ]] {(2, 3): [((0.5120599307737848, 1), (0.0, 3), (0.0, 2), (0.0, 0))], (3, 0): [((0.5120599307737846, 1), (0.0, 3), (0.0, 2), (0.0, 0))], (1, 2): [((0.5120599307737846, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (0, 1): [((0.5120599307737846, 3), (0.0, 2), (0.0, 1), (0.0, 0))]}\n",
      "initial set edges: {(0, 1), (3, 0), (2, 3), (1, 2)}\n",
      "Edge: (2, 3) targeting: 1\n",
      "(2, 3, 1)\n",
      "Vertex locked: 2\n",
      "edges inserted: (3, 1)\n",
      "set of interior edges updated: {(3, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "element inserted: (2, 3, 1)\n",
      "Edge: (3, 0) targeting: 1\n",
      "(3, 0, 1)\n",
      "Vertex locked: 3\n",
      "Vertex locked: 0\n",
      "Vertex locked: 1\n",
      "element inserted: (3, 0, 1)\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "Element (2, 3, 1) already in set\n",
      "Edge: (0, 1) targeting: 3\n",
      "(0, 1, 3)\n",
      "Element (3, 0, 1) already in set\n",
      "Final edges: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "Elements created: {(3, 0, 1), (2, 3, 1)}\n",
      "Set of locked vertices: {0, 1, 2, 3}\n",
      "Set of open vertices: set()\n",
      "{(3, 0, 1), (2, 3, 1)}\n",
      "(3, 0, 1)\n",
      "(2, 3, 1)\n",
      "(2, 0, 4)\n",
      "(0, 1, 4)\n",
      "(1, 2, 3)\n",
      "remeshing subpolygon [3, 4, 2]\n",
      "Point  [0.10896638 0.01804129]  is inside  [3, 4, 2]\n",
      "Point  [ 0.32090279 -0.3235997 ]  is inside  [3, 4, 2]\n",
      "[[[[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[ 0.99508155  0.25870986]\n",
      "   [-0.3490263   0.07266232]\n",
      "   [ 0.32090279 -0.3235997 ]]]\n",
      "\n",
      "\n",
      " [[[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.61250609 -1.12049258]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.99508155  0.25870986]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [-0.3490263   0.07266232]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.10896638  0.01804129]]\n",
      "\n",
      "  [[-0.3490263   0.07266232]\n",
      "   [ 0.61250609 -1.12049258]\n",
      "   [ 0.32090279 -0.3235997 ]]]]\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "[0, 1, 2]\n",
      "[0, 1, 3]\n",
      "[0, 1, 4]\n",
      "[1, 2, 0]\n",
      "[1, 2, 1]\n",
      "[1, 2, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 4]\n",
      "[2, 0, 0]\n",
      "[2, 0, 1]\n",
      "[2, 0, 2]\n",
      "[2, 0, 3]\n",
      "[2, 0, 4]\n",
      "[[0.1896699  0.         0.         0.         0.        ]\n",
      " [0.1896699  0.09868233 0.         0.         0.        ]\n",
      " [0.1896699  0.         0.         0.         0.        ]]\n",
      "[(0.1896699, 1)]\n",
      "checked (1, 2)\n",
      "[(0.1896699, 1), (0.09868233, 1)]\n",
      "checked (2, 0)\n",
      "[(0.1896699, 1)]\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.         0.         0.18966991]\n",
      " [0.         0.         0.         0.18966991 0.        ]\n",
      " [0.         0.         0.         0.09868233 0.18966991]] {(1, 2): [((0.1896699113014106, 3), (0.0, 4), (0.0, 2), (0.0, 1), (0.0, 0))], (2, 0): [((0.18966991130141056, 4), (0.09868232639419078, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (0, 1): [((0.18966991130141056, 4), (0.0, 3), (0.0, 2), (0.0, 1), (0.0, 0))]}\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]]\n",
      "initial set edges: {(0, 1), (2, 0), (1, 2)}\n",
      "meshing polygon:  [[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]]  with inner points : [[ 0.10896638  0.01804129]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "[[ 0.99508155  0.25870986]\n",
      " [-0.3490263   0.07266232]\n",
      " [ 0.10896638  0.01804129]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 3)\n",
      "set of interior edges updated: {(1, 3)}\n",
      "set of edges updated: {(0, 1), (2, 0), (1, 3), (1, 2)}\n",
      "edges inserted: (2, 3)\n",
      "set of interior edges updated: {(1, 3), (2, 3)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3)}\n",
      "element inserted: (1, 2, 3)\n",
      "Spotted edges linked with point:  (1, 3)   (2, 3)\n",
      "Edge: (2, 0) targeting: 4\n",
      "(2, 0, 4)\n",
      "[[-0.3490263   0.07266232]\n",
      " [ 0.61250609 -1.12049258]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (2, 4)\n",
      "set of interior edges updated: {(1, 3), (2, 3), (2, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3), (2, 4)}\n",
      "edges inserted: (0, 4)\n",
      "set of interior edges updated: {(1, 3), (2, 3), (2, 4), (0, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "element inserted: (2, 0, 4)\n",
      "Spotted edges linked with point:  (2, 4)   (0, 4)\n",
      "Edge: (0, 1) targeting: 4\n",
      "(0, 1, 4)\n",
      "[[ 0.61250609 -1.12049258]\n",
      " [ 0.99508155  0.25870986]\n",
      " [ 0.32090279 -0.3235997 ]]\n",
      "Vertex locked: 0\n",
      "set of forbidden inter section edges updated: set()\n",
      "edges inserted: (1, 4)\n",
      "set of interior edges updated: {(1, 3), (1, 4), (2, 3), (0, 4), (2, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "element inserted: (0, 1, 4)\n",
      "Spotted edges linked with point:  (0, 4)   (1, 4)\n",
      "Final edges: {(0, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 3), (0, 4), (2, 4)}\n",
      "Elements created: {(2, 0, 4), (0, 1, 4), (1, 2, 3)}\n",
      "Set of locked vertices: {0}\n",
      "set of orphan vertex: set()\n",
      "Set of open vertices: {1, 2, 3, 4}\n",
      "Examining if vtx 1 is locked\n",
      "Examining if vtx 2 is locked\n",
      "Checking if interior point 3 is closed\n",
      "found 3 in (1, 3)\n",
      "Interior vertex 3 is open\n",
      "Interior vertex 3 is open\n",
      "found index [2] in element (1, 2, 3) \n",
      "Interior vertex 3 is open\n",
      "set of open vertices {1, 2, 3, 4}\n",
      "[]\n",
      "Starting with vertex 1\n",
      " Initial edges to visit {(1, 3), (2, 3), (2, 4), (1, 4)}\n",
      "Candidate edge (1, 0) found in element (0, 1, 4)\n",
      "Candidate edge (2, 0) found in element (2, 0, 4)\n",
      "Edges to visit: {(1, 3), (2, 3), (2, 4), (1, 4)}\n",
      "Visiting vertex 1\n",
      "1  in  (1, 3)\n",
      "Found vertex: 3\n",
      "Removing edge (1, 3)\n",
      "{(2, 3), (2, 4), (1, 4)}\n",
      "Visiting vertex 3\n",
      "3  in  (2, 3)\n",
      "Found vertex: 2\n",
      "Removing edge (2, 3)\n",
      "{(2, 4), (1, 4)}\n",
      "Visiting vertex 2\n",
      "2  in  (2, 4)\n",
      "Found vertex: 4\n",
      "Removing edge (2, 4)\n",
      "{(1, 4)}\n",
      "Visiting vertex 4\n",
      "4  in  (1, 4)\n",
      "Found vertex: 1\n",
      "Removing edge (1, 4)\n",
      "set()\n",
      "Back to starting vertex\n",
      "Starting with vertex 2\n",
      "Starting with vertex 3\n",
      "Starting with vertex 4\n",
      "found polygon [1, 3, 2, 4]\n",
      "remeshing subpolygon [1, 3, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked (2, 3)\n",
      "checked (3, 0)\n",
      "checked (1, 2)\n",
      "checked (0, 1)\n",
      "[[0.         0.         0.         0.51205993]\n",
      " [0.         0.         0.         0.51205993]\n",
      " [0.         0.51205993 0.         0.        ]\n",
      " [0.         0.51205993 0.         0.        ]] {(2, 3): [((0.5120599307737848, 1), (0.0, 3), (0.0, 2), (0.0, 0))], (3, 0): [((0.5120599307737846, 1), (0.0, 3), (0.0, 2), (0.0, 0))], (1, 2): [((0.5120599307737846, 3), (0.0, 2), (0.0, 1), (0.0, 0))], (0, 1): [((0.5120599307737846, 3), (0.0, 2), (0.0, 1), (0.0, 0))]}\n",
      "initial set edges: {(0, 1), (3, 0), (2, 3), (1, 2)}\n",
      "Edge: (2, 3) targeting: 1\n",
      "(2, 3, 1)\n",
      "Vertex locked: 2\n",
      "edges inserted: (3, 1)\n",
      "set of interior edges updated: {(3, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "element inserted: (2, 3, 1)\n",
      "Edge: (3, 0) targeting: 1\n",
      "(3, 0, 1)\n",
      "Vertex locked: 3\n",
      "Vertex locked: 0\n",
      "Vertex locked: 1\n",
      "element inserted: (3, 0, 1)\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "Element (2, 3, 1) already in set\n",
      "Edge: (0, 1) targeting: 3\n",
      "(0, 1, 3)\n",
      "Element (3, 0, 1) already in set\n",
      "Final edges: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "Elements created: {(3, 0, 1), (2, 3, 1)}\n",
      "Set of locked vertices: {0, 1, 2, 3}\n",
      "Set of open vertices: set()\n",
      "{(3, 0, 1), (2, 3, 1)}\n",
      "(3, 0, 1)\n",
      "(2, 3, 1)\n",
      "(2, 0, 4)\n",
      "(0, 1, 4)\n",
      "(1, 2, 3)\n",
      "(4, 0, 2)\n",
      "(1, 2, 0)\n",
      "(3, 4, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_net.eval()\n",
    "my_net=my_net.cpu()\n",
    "random_contour_reshaped=contour_with_points.reshape(2*(nb_of_edges+nb_of_points))\n",
    "\n",
    "predicted_quality_matrix=[]\n",
    "for i in range(int(nb_of_edges)):\n",
    "    random_contour_with_edge_label=np.hstack([random_contour_reshaped,np.array([i])])\n",
    "    random_contour_with_edge_label=random_contour_with_edge_label.reshape(1,len(random_contour_with_edge_label))\n",
    "    random_variable=Variable(torch.from_numpy(random_contour_with_edge_label).type(torch.FloatTensor))\n",
    "    edge_quality=my_net(random_variable).data.numpy()\n",
    "    predicted_quality_matrix.append(edge_quality)\n",
    "    \n",
    "predicted_quality_matrix=np.array(predicted_quality_matrix).reshape(nb_of_edges,nb_of_edges+nb_of_points)\n",
    "\n",
    "predicted_ordered_matrix=order_quality_matrix(predicted_quality_matrix,contour)\n",
    "initial_elements,sub_elements=Triangulation_with_points.triangulate(contour,point,predicted_ordered_matrix,recursive=True,plot_mesh=True)\n",
    "\n",
    "total_elements=concat_element_list(initial_elements,sub_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03696144, 0.0220573 , 0.0557451 , 0.03919806, 0.02444022,\n",
       "        0.0580776 , 0.30621246, 0.2165569 , 0.11561495, 0.1461322 ],\n",
       "       [0.03845082, 0.02555588, 0.05018008, 0.04015041, 0.02658573,\n",
       "        0.05162774, 0.26370913, 0.21751833, 0.13909605, 0.14460115],\n",
       "       [0.04254132, 0.02985019, 0.04396331, 0.04320887, 0.02885462,\n",
       "        0.04424058, 0.21048313, 0.24234888, 0.18221878, 0.13296922],\n",
       "       [0.04622987, 0.03520481, 0.03728956, 0.046074  , 0.03208628,\n",
       "        0.03622207, 0.15574867, 0.25884166, 0.22619171, 0.13083151],\n",
       "       [0.04397148, 0.0446196 , 0.03159457, 0.04522923, 0.04033174,\n",
       "        0.02903466, 0.1351051 , 0.2001392 , 0.23657507, 0.19857702],\n",
       "       [0.04046234, 0.05217236, 0.02822042, 0.04341208, 0.04744304,\n",
       "        0.02463004, 0.13321878, 0.13595076, 0.23202547, 0.26696324]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_quality_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92793718,  0.30245348,  0.14672391,  0.99925893, -0.04473495,\n",
       "         0.62180314, -0.65217113,  0.11886655, -0.51649281, -1.06434669,\n",
       "         0.1387378 , -0.97803541,  0.21685002,  0.39102825, -0.10502397,\n",
       "         0.13173418, -0.20441173, -0.37291855,  0.15934794, -0.10010848,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_contour_with_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  0.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  1.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  2.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  3.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  4.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  5.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons_reshaped_with_edges_label[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1765, 4, 6)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.30624237,\n",
       "         0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.2693817 , 0.30624237,\n",
       "         0.10932361]],\n",
       "\n",
       "       [[0.        , 0.2693817 , 0.        , 0.        , 0.2693817 ,\n",
       "         0.30624237]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.53670315]],\n",
       "\n",
       "       [[0.18435143, 0.        , 0.        , 0.        , 0.15697959,\n",
       "         0.53670315]],\n",
       "\n",
       "       [[0.        , 0.        , 0.18435143, 0.        , 0.53670315,\n",
       "         0.18435143]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_matrices_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7060"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_matrices_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  0.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  1.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  2.        ],\n",
       "       [ 1.03153405, -0.38372593, -0.50582285,  0.97770921, -0.55412024,\n",
       "         0.15050598,  0.02840905, -0.74448926,  0.13299216, -0.21108984,\n",
       "         0.02614578,  0.13334986,  3.        ],\n",
       "       [ 0.92125729, -0.26643743, -0.32783887,  0.97283146, -0.79636824,\n",
       "         0.26435126,  0.20294982, -0.97074528,  0.03293677,  0.14602019,\n",
       "         0.27105076, -0.339819  ,  0.        ],\n",
       "       [ 0.92125729, -0.26643743, -0.32783887,  0.97283146, -0.79636824,\n",
       "         0.26435126,  0.20294982, -0.97074528,  0.03293677,  0.14602019,\n",
       "         0.27105076, -0.339819  ,  1.        ],\n",
       "       [ 0.92125729, -0.26643743, -0.32783887,  0.97283146, -0.79636824,\n",
       "         0.26435126,  0.20294982, -0.97074528,  0.03293677,  0.14602019,\n",
       "         0.27105076, -0.339819  ,  2.        ]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons_reshaped_with_edges_label[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 6)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons_reshaped_with_edges_label.shape[1],quality_matrices_reshaped.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 6)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons_reshaped_with_edges_label.shape[1],quality_matrices_reshaped.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07884784, -0.26079613],\n",
       "       [-0.08445999,  0.84091032],\n",
       "       [-1.16314955, -0.00757443],\n",
       "       [ 0.1687617 , -0.57253976],\n",
       "       [ 0.19944424,  0.00707185],\n",
       "       [-0.2198509 ,  0.066967  ]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour_with_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22158, 21)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons_reshaped_with_edges_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
