{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.spatial\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from itertools import permutations\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import triplot\n",
    "from numpy import pi,sin,cos,tan,sqrt\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "import triangle as tri\n",
    "import triangle\n",
    "import triangle.plot as plot\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "import cTimer as ctimer\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  ======================================\n",
    "  Main idea of finding the connectivity:\n",
    "  ======================================\n",
    "  \n",
    "1) Build set of elements and edges\n",
    "2) Sort edges according to maximum quality\n",
    "3) for each pair of the edge put into set of edges  of the maximum quality\n",
    "4) Put also to the set of elements the element that is formed\n",
    "5) Proceed to the next edge and check if the edge formed from each pair already exist\n",
    "6) If a pair of edges already exists proceed to next edge\n",
    "7) If yes proceed to the next element\n",
    "\n",
    "(*) Add function computing the quality of the mesh given that every point of a contour is connected \n",
    "    to a point of the mesh -> normalize the qualities for each point -> Add as parameter to neural network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                             Issues:\n",
    "\n",
    "(1) Avoiding the consideration of triangles that are invalid ( by setting quality to 0):\n",
    "     \n",
    "     1st Approach (Failed):\n",
    "     See if formed triangle contains points of the polygon\n",
    "     \n",
    "     2nd Approach :\n",
    "     Calculate the angles of the polygon. Once that is done, any edge departing from a point in the polygon\n",
    "     can have a greater angle from the departed edge greater than the edge has with the pre-existing edges\n",
    "     \n",
    "     \n",
    "(2) Avoiding interseactions when creating elements:\n",
    "\n",
    "    Here the main idea is whenever a new element is created to check if it includes a forabideen vertex. If it does \n",
    "    the new element can't be formed and we porceed to check the connection with the second smallest quality.\n",
    "    \n",
    "   ==========================================================================\n",
    "      How to check if the vertex is locked ( So no new connections with it.)\n",
    "   ==========================================================================\n",
    "    \n",
    "    \n",
    "    So the idea is that given  a vertex we check the edges and the elements that include those edges.\n",
    "    If start from an edge of the contour and end up to an edge of the contour again then the vertex\n",
    "    \n",
    "    \n",
    "    + For every vertex of the element that is to be created  and\n",
    "        \n",
    "        for vtx in element:\n",
    "        \n",
    "            if closed_ring(vtx,adj_vertices,elements):\n",
    "            \n",
    "                  don't created element\n",
    "                  proceed to connection with second most great quality\n",
    "                  if doesn't exist:\n",
    "                  proceeed  to next edge\n",
    "                  \n",
    "    + for the vtx and the adjacent v1 look for the element \n",
    "        (vtx,v1,v2) ->  (vtx, v2) -> (vtx,v3,v2) -> (vtx,v3)-> ... -> (vtx,vn) \n",
    "        Check if end is edge of contour if yes then vtx is forbidden -> insert to forbidden vertices\n",
    "                \n",
    "               \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE_accuracy(model,variable,labels):\n",
    "    net.eval()\n",
    "    predictions=model(variable).data.numpy()\n",
    "    predictions[np.where(predictions>0.5)]=1\n",
    "    predictions[np.where(predictions<=0.5)]=0\n",
    "    diff=labels-predictions\n",
    "    correct_prediction=0\n",
    "    for i in diff:\n",
    "        if (not i.any()):\n",
    "            correct_prediction+=1\n",
    "    net.train()\n",
    "    return  100*correct_prediction/variable.size()[0],diff\n",
    "\n",
    "def plot_contour(contour):    \n",
    "    plot_coords=np.vstack([contour,contour[0]])\n",
    "    (s,t)=zip(*plot_coords)\n",
    "    plt.plot(s,t)\n",
    "    indices=[i for i in range(contour.shape[0])]\n",
    "    for index,i in enumerate(indices):\n",
    "        plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def connectivity_information(triangulation,print_info=False):\n",
    "    \n",
    "    segments= tuple(triangulation['segments'].tolist())\n",
    "    triangles=tuple(triangulation['triangles'].tolist())\n",
    "    vertices=triangulation['vertices']   \n",
    "    \n",
    "    connect_info={str(r):[0 for i in range(len(vertices))] for r in tuple(triangulation['segments'].tolist())}\n",
    "    for segment in segments:\n",
    "        for triangle in triangles:\n",
    "            if set(segment).issubset(set(triangle)):\n",
    "                connection=set(triangle)-set(segment)\n",
    "                if print_info: print(\"segment:\",segment,\"is connected to:\",connection,\"to form triangle:\",triangle)\n",
    "                connect_info[str(segment)][tuple(connection)[0]]=1    \n",
    "    return connect_info\n",
    "\n",
    "\n",
    "\n",
    "def get_labels(triangulation,connect_info):\n",
    "    indices=[]\n",
    "    vertices=list(range(triangulation['vertices'].shape[0]))\n",
    "    for i in triangulation['segments']:\n",
    "           indices.append(set(vertices)-set(i)) \n",
    "    labels=[]\n",
    "    list_values=list(connect_info.values())\n",
    "    for i in range(len(list_values)):\n",
    "        for j in indices[i]:\n",
    "            labels.append(list_values[i][j])\n",
    "    return  labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rot(theta):\n",
    "    return np.array([[cos(theta),-sin(theta)],     \n",
    "                     [sin(theta),cos(theta)]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_reference_polygon(nb_of_points,plot=False):\n",
    "    angles=np.empty(nb_of_points)\n",
    "    points=np.empty([nb_of_points,2])\n",
    "    plot_coords=np.empty([nb_of_points,2])\n",
    "    indices=[]\n",
    "    angle_division=2*pi/nb_of_points\n",
    "   \n",
    "    for i in range(nb_of_points):\n",
    "        angle=i*angle_division\n",
    "        angles[i]=angle\n",
    "        point=np.array([1,0]) #pick edge length of 1\n",
    "        points[i]=np.dot(rot(angle),point.T)  #rotate it according to the  chosen angle\n",
    "        indices.append(i)\n",
    "   \n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([points,points[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def generate_contour(nb_of_points,plot=False):\n",
    "    \n",
    "    angles=np.empty(nb_of_points)\n",
    "    points=np.empty([nb_of_points,2])\n",
    "    plot_coords=np.empty([nb_of_points,2])\n",
    "    indices=[]\n",
    "    angle_division=2*pi/nb_of_points\n",
    "   \n",
    "    for i in range(nb_of_points):\n",
    "        angle=((i+1)*angle_division-i*angle_division)*np.random.random_sample()+i*angle_division\n",
    "        angles[i]=angle\n",
    "        point=np.array([np.random.uniform(0.3,1),0]) #pick random point at (1,0)\n",
    "       #point=np.array([1,0]) #pick edge length of 1\n",
    "\n",
    "        points[i]=np.dot(rot(angle),point.T)  #rotate it according to the  chosen angle\n",
    "        indices.append(i)\n",
    "   \n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([points,points[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def get_barycenter(contour):\n",
    "    return np.array([contour[:,0].sum()/contour.shape[0],contour[:,1].sum()/contour.shape[0]])\n",
    "\n",
    "\n",
    "def apply_procrustes(polygon_points,plot=False):  \n",
    "    \n",
    "    # Get reference polygona and adjust any random poygon to that\n",
    "    ref_polygon=get_reference_polygon(polygon_points.shape[0])\n",
    "    \n",
    "    \n",
    "    #Mean of each coordinate\n",
    "    mu_polygon = polygon_points.mean(0)\n",
    "    mu_ref_polygon = ref_polygon.mean(0)\n",
    "    \n",
    "    #Centralize data to the mean \n",
    "    centralised_ref_polygon_points = ref_polygon-mu_ref_polygon\n",
    "    centralised_polygon_points = polygon_points-mu_polygon\n",
    "    \n",
    "    #Squared sum of X-mean(X)\n",
    "    ss_ref_polygon_points = (centralised_ref_polygon_points**2.).sum()\n",
    "    ss_polygon_points = (centralised_polygon_points**2.).sum()\n",
    "\n",
    "       \n",
    "    #Frobenius norm of X\n",
    "    norm_ss_ref_polygon_points = np.sqrt(ss_ref_polygon_points)\n",
    "    norm_ss_polygon_points = np.sqrt(ss_polygon_points)\n",
    "\n",
    "    \n",
    "    # scale to equal (unit) norm\n",
    "    centralised_ref_polygon_points /=norm_ss_ref_polygon_points     \n",
    "    centralised_polygon_points /=norm_ss_polygon_points\n",
    "        \n",
    "    \n",
    "    #Finding best rotation to superimpose on regular triangle\n",
    "    #Applying SVD to the  matrix \n",
    "    A = np.dot(centralised_ref_polygon_points.T, centralised_polygon_points)\n",
    "    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "    V=Vt.T\n",
    "    R = np.dot(V,U.T)\n",
    "    \n",
    "  \n",
    "    traceTA = s.sum()\n",
    "    d = 1 - traceTA**2\n",
    "    b = traceTA * norm_ss_ref_polygon_points / norm_ss_polygon_points    \n",
    "    indices=[i for i in range(polygon_points.shape[0])]\n",
    "    \n",
    "   \n",
    "\n",
    "    polygon_transformed =norm_ss_ref_polygon_points*traceTA*np.dot(centralised_polygon_points,R)+mu_ref_polygon\n",
    "\n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([polygon_transformed,polygon_transformed[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return polygon_transformed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contains_points(triangle,polygon):\n",
    "    hull=ConvexHull(triangle)\n",
    "    hull_path=Path(triangle[hull.vertices])\n",
    "    set_polygon=set(tuple(i) for i in polygon)\n",
    "    set_triangle=set(tuple(i) for i in triangle)\n",
    "    #print(set_polygon,set_triangle)\n",
    "    difference=set_polygon-set_triangle\n",
    "    \n",
    "    if len(difference)==0:\n",
    "        return False\n",
    "\n",
    "    for i in difference:\n",
    "        if hull_path.contains_point(i):\n",
    "            return True\n",
    "            break\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_edge_lengths(pt1,pt2):\n",
    "    return np.linalg.norm(pt1-pt2)\n",
    "\n",
    "def compute_edge_lengths2(triangle):\n",
    "    edgelengths2=np.empty([2,3])\n",
    "    for i in range(2):\n",
    "        for j in range(i+1,3):\n",
    "            eij=triangle[j]-triangle[i]\n",
    "            edgelengths2[i][j]=np.dot(eij,eij)\n",
    "    return edgelengths2\n",
    "    \n",
    "\n",
    "\n",
    "def compute_triangle_normals(triangle):\n",
    "   \n",
    "    e01=triangle[1]-triangle[0]\n",
    "    e02=triangle[2]-triangle[0]\n",
    "    \n",
    "    e01_cross_e02=np.cross(e01,e02)\n",
    "    \n",
    "    return e01_cross_e02\n",
    "\n",
    "\n",
    "def compute_triangle_area(triangle):\n",
    "   \n",
    "    e01=triangle[1]-triangle[0]\n",
    "    e02=triangle[2]-triangle[0]\n",
    "    \n",
    "    e01_cross_e02=np.cross(e01,e02)\n",
    "    \n",
    "    # Omit triangles that are inverted (out of the domain)\n",
    "    if e01_cross_e02<0:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    e01_cross_e02_norm=np.linalg.norm(e01_cross_e02)\n",
    "\n",
    "        \n",
    "    return e01_cross_e02_norm/2\n",
    "\n",
    "def compute_triangle_quality(triangle,polygon=None):\n",
    "    \n",
    "    if polygon is None:\n",
    "        polygon=triangle\n",
    "    \n",
    "    # The incoming triangle has edged [p0,p1] which is an edge and p2 is the connection\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    \n",
    "    indices=[]\n",
    "\n",
    "\n",
    "    for point in triangle: \n",
    "        for index,point_in_polygon in enumerate(polygon):\n",
    "              if np.allclose(point,point_in_polygon):\n",
    "                    indices.append(index)\n",
    "                \n",
    "    p0,p1,p2=indices[0],indices[1],indices[2]\n",
    "    \n",
    "    neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "    # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "    if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "        or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "    ):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    factor=4/sqrt(3)\n",
    "    area=compute_triangle_area(triangle)\n",
    "   \n",
    "    if area==0:\n",
    "        return 0\n",
    "    \n",
    "    if contains_points(triangle,polygon):\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    #edgelengths=np.empty([triangle.shape[0],1])\n",
    "    \n",
    "    #for i,_ in enumerate(triangle):\n",
    "     #   edgelengths[i]=compute_edge_length(triangle[i],triangle[(i+1)%3])\n",
    "    \n",
    "    #sum_edge_lengths=edgelengths.sum() \n",
    "    #factor=4/sqrt(3)*3/sum_edge_lengths\n",
    "    \n",
    "    sum_edge_lengths=0\n",
    "    edge_length2=compute_edge_lengths2(triangle)\n",
    "    for i in range(2):\n",
    "        for j in range(i+1,3):\n",
    "            sum_edge_lengths+=edge_length2[i][j]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lrms=sqrt(sum_edge_lengths/3)\n",
    "    lrms2=lrms**2\n",
    "    quality=area/lrms2\n",
    "    \n",
    "    return quality*factor\n",
    "\n",
    "\n",
    "\n",
    "def compute_minimum_quality_triangle(triangle,polygon=None,get_mean=False):\n",
    "    if polygon is None:\n",
    "        polygon=triangle\n",
    "    \n",
    "    # The incoming triangle has edged [p0,p1] which is an edge and p2 is the connection\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    \n",
    "    indices=[]\n",
    "\n",
    "\n",
    "    for point in triangle: \n",
    "        for index,point_in_polygon in enumerate(polygon):\n",
    "              if np.allclose(point,point_in_polygon):\n",
    "                    indices.append(index)\n",
    "                \n",
    "    p0,p1,p2=indices[0],indices[1],indices[2]\n",
    "    \n",
    "    neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "    # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "    if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "        or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "    ):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    area=compute_triangle_area(triangle)\n",
    "   \n",
    "    if area==0:\n",
    "        return 0\n",
    "    \n",
    "    if contains_points(triangle,polygon):\n",
    "        return 0\n",
    "    \n",
    "    triangles_in_mesh=[]\n",
    "    triangles_in_mesh.append(triangle)\n",
    "    contour_connectivity=get_contour_edges(polygon)\n",
    "    contour_connectivity=np.vstack([contour_connectivity,[p0,p2],[p1,p2]])\n",
    "    hole=np.array([(triangle.sum(0))/3])\n",
    "    shape=dict(holes=hole,vertices=polygon,segments=contour_connectivity)\n",
    "    t = tri.triangulate(shape, 'pq0')\n",
    "    \n",
    "    Invalid_triangulation=False\n",
    "\n",
    "    try:   \n",
    "        for triangle_index in t['triangles']:\n",
    "            triangles_in_mesh.append(polygon[np.asarray([triangle_index])])\n",
    "    except :\n",
    "        print(\"Invalid triangulation\",p0,p1,p2)\n",
    "        Invalid_triangulation=True\n",
    "        \n",
    "    triangle_qualities=[]\n",
    "    for triangle in triangles_in_mesh:\n",
    "        triangle.resize(3,2)\n",
    "        triangle_quality=compute_triangle_quality(triangle)\n",
    "        triangle_qualities.append(triangle_quality)\n",
    "    \n",
    "    if Invalid_triangulation:\n",
    "        mean_quality,minimum_quality=0,0\n",
    "    else:\n",
    "        triangle_qualities=np.array(triangle_qualities)\n",
    "        mean_quality=triangle_qualities.mean()\n",
    "        minimum_quality=triangle_qualities.min()\n",
    "\n",
    "    \n",
    "\n",
    "    if get_mean:\n",
    "        return mean_quality\n",
    "    else:\n",
    "        return minimum_quality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Quality of elements formed by connecting each edge with one of the other points of the contour\n",
    "\n",
    "def quality_matrix(polygon,compute_minimum=True ,normalize=False,mean=False):\n",
    "    polygon=apply_procrustes(polygon,False)\n",
    "\n",
    "    contour_connectivity=np.array(list(tuple(i) for i in get_contour_edges(polygon)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    quality_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "    area_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "    normals_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "\n",
    "    list_of_triangles=[]\n",
    "    \n",
    "    for index,edge in enumerate(contour_connectivity):\n",
    "        # Not omitting non triangles because either way their quality is zero\n",
    "        triangles_to_edge_indices=[[*edge,i] for i in range(polygon.shape[0]) ]\n",
    "        \n",
    "        \n",
    "\n",
    "        #print(triangles_to_edge_indices)\n",
    "        triangles_to_edge_indices=np.asarray(triangles_to_edge_indices)\n",
    "        triangles=polygon[triangles_to_edge_indices]\n",
    "        list_of_triangles.append(triangles)\n",
    "        \n",
    "    \n",
    "    list_of_triangles=np.array(list_of_triangles)\n",
    "    \n",
    "    if compute_minimum:\n",
    "        for i,triangles in enumerate(list_of_triangles):\n",
    "            for j,triangle in enumerate(triangles):\n",
    "                quality_matrix[i,j]=compute_minimum_quality_triangle(triangle,polygon)\n",
    "    else:\n",
    "         for i,triangles in enumerate(list_of_triangles):\n",
    "            for j,triangle in enumerate(triangles):\n",
    "                if mean:\n",
    "                    quality_matrix[i,j]=compute_triangle_quality(triangle,polygon,get_mean=True)\n",
    "                else: \n",
    "                    quality_matrix[i,j]=compute_triangle_quality(triangle,polygon)\n",
    "    \n",
    "            #area_matrix[i,j]=compute_triangle_area(triangle)\n",
    "            #normals_matrix[i,j]=compute_triangle_normals(triangle)\n",
    "\n",
    "            \n",
    "    \n",
    "    sum_of_qualities=quality_matrix.sum(1)\n",
    "\n",
    "    if normalize is True:\n",
    "        for i,_ in enumerate(quality_matrix):\n",
    "            quality_matrix[i]/=sum_of_qualities[i]\n",
    "    \n",
    "    return quality_matrix,normals_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_edge_validity(edge,polygon,set_edges,interior_edges):        \n",
    "    # Check if new edges are already in the set\n",
    "    found_in_set=False\n",
    "    found_in_interior_set=False\n",
    "    for index in range(len(polygon)):\n",
    "        occuring_index=index\n",
    "\n",
    "        edge1,edge2=tuple(permutations((edge[0],index))),tuple(permutations((edge[1],index)))\n",
    "        condition1= edge1[0] in set_edges or edge1[1] in set_edges\n",
    "        condition2= edge2[0] in set_edges or edge2[1] in set_edges\n",
    "        condition3= edge1[0] in interior_edges or edge1[1] in interior_edges\n",
    "        condition4= edge2[0] in interior_edges or edge2[1] in interior_edges\n",
    "    \n",
    "        \n",
    "            # both edges are found in the list of set of edges (Invalid)\n",
    "        if (condition1 and condition2): \n",
    "            found_in_set=True\n",
    "            occuring_index=index\n",
    "            \n",
    "        \n",
    "        # both edges are found in the list of interior edges created\n",
    "        if (condition3 and condition4):\n",
    "            found_in_interior_set=True\n",
    "            occuring_index=index\n",
    "            \n",
    "        if found_in_interior_set or found_in_set:\n",
    "            break\n",
    "    return found_in_interior_set,found_in_set,occuring_index\n",
    "\n",
    "def triangulate(polygon,ordered_quality_matrix,recursive=True):\n",
    "    set_edges=set(tuple(i) for i in get_contour_edges(polygon))\n",
    "    interior_edges=set()\n",
    "    set_elements=set()\n",
    "    set_locked_vertices=set()\n",
    "    set_forbidden_intersections=set()\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "\n",
    "    print(\"initial set edges:\", set_edges)\n",
    "    \n",
    "\n",
    "\n",
    "    for edge in ordered_quality_matrix.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        found_in_interior_set,found_in_set,index=check_edge_validity(edge,polygon,set_edges,interior_edges)\n",
    "\n",
    "        for qualities_with_edges in ordered_quality_matrix[edge][0]:\n",
    "            \n",
    "            element_created=False\n",
    "           \n",
    "            target_vtx=qualities_with_edges[1]\n",
    "            \n",
    "            if target_vtx==edge[0] or target_vtx==edge[1]:\n",
    "                continue\n",
    "           \n",
    "            print(\"Edge:\",edge,\"targeting:\",target_vtx)\n",
    "        \n",
    "            if found_in_interior_set:\n",
    "                element=(edge[0],edge[1],index)  \n",
    "                set_elements.add(element)\n",
    "                print(\"Element inserted:\",element)\n",
    "                continue\n",
    "        \n",
    "            if found_in_set and not found_in_interior_set:    \n",
    "                if(index != target_vtx):\n",
    "                    print('found',(edge[0],index),(edge[1],index),\"Canceling creation\")\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            # Passed edges checking \n",
    "            # Proceed to check vertices\n",
    "            temp_element=(edge[0],edge[1],target_vtx)\n",
    "            \n",
    "            # Cehcking element normals to avoid inverted elements\n",
    "            element_indices=np.asarray([edge[0],edge[1],target_vtx])\n",
    "            if compute_triangle_normals(polygon[element_indices])<0:\n",
    "                continue\n",
    "            \n",
    "            # Checking if the element contains other points of the polygon inside\n",
    "            if contains_points(polygon[element_indices],polygon):\n",
    "                continue\n",
    "            \n",
    "                            \n",
    "            p0,p1,p2=edge[0],edge[1],target_vtx\n",
    "    \n",
    "            neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "            # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "            if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "            or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "            #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "                 continue\n",
    "    \n",
    "    \n",
    "            if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "           or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "            ):\n",
    "                continue\n",
    "    \n",
    "            if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "            or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "            #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "                continue\n",
    "         \n",
    "        \n",
    "        \n",
    "            quality=compute_minimum_quality_triangle(polygon[element_indices],polygon)\n",
    "            if quality==0 or quality<1e-2:\n",
    "                continue\n",
    "            \n",
    "            print(temp_element)\n",
    "            existing_element=False\n",
    "            for element in set_elements:\n",
    "                if set(temp_element)==set(element):\n",
    "                    print(\"Element {} already in set\".format(element))\n",
    "                    existing_element=True\n",
    "                    break\n",
    "            if existing_element:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "            if target_vtx in set_locked_vertices:\n",
    "                print(\" Target vertex {} is locked\".format(target_vtx))\n",
    "                continue\n",
    "            set_elements.add(temp_element)\n",
    "\n",
    "        \n",
    "    \n",
    "            # Check if a locked vertex was created after the creation of the element\n",
    "            # If so, add it to the list\n",
    "            #Tracer()()\n",
    "            Found_locked_vertex=False\n",
    "            for vertex in temp_element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                    print(\"Vertex locked:\",vertex)\n",
    "                    Found_locked_vertex=True\n",
    "                    set_locked_vertices.add(vertex)\n",
    "            set_elements.remove(temp_element)\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Locking the vertices and checking if the connection is with a locked vertex has been checked/\n",
    "            # Proceeding to check if both internal edges intersect with other internal edges\n",
    "            internal_edge1=(edge[0],target_vtx)\n",
    "            internal_edge2=(edge[1],target_vtx)\n",
    "            set_a,set_b=get_intermediate_indices(target_vtx,polygon,edge[0],edge[1])\n",
    "        \n",
    "            internal_condition1= internal_edge1 in set_forbidden_intersections or tuple(reversed(internal_edge1)) in set_forbidden_intersections\n",
    "                                                                        \n",
    "            internal_condition2=internal_edge2 in set_forbidden_intersections or tuple(reversed(internal_edge2)) in set_forbidden_intersections\n",
    "                                                                            \n",
    "    \n",
    "                                                                                   \n",
    "            internal_intersection=False\n",
    "            if internal_condition1 or  internal_condition2:\n",
    "                print(\"edges :\",internal_edge1, \"and\",internal_edge2,\"intersecting\")\n",
    "                print(\"Abandoning creation of element\",temp_element)\n",
    "                internal_intersection=True\n",
    "        \n",
    "     \n",
    "            if internal_intersection:\n",
    "                for vtx in temp_element:\n",
    "                    if Found_locked_vertex and vtx in set_locked_vertices:\n",
    "                        print(\"Unlocking vertex\",vtx)\n",
    "                        set_locked_vertices.remove(vtx)                    \n",
    "                continue\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Create the element\n",
    "            element=temp_element\n",
    "        \n",
    "            # Add to set of edges all the forbidden intersections after the creation of the element\n",
    "            \n",
    "            for i in set_a:\n",
    "                for j in set_b:\n",
    "                    set_forbidden_intersections.add((i,j))\n",
    "            #print(\"set of forbidden inter section edges updated:\",set_forbidden_intersections)\n",
    "    \n",
    "                \n",
    "        \n",
    "        \n",
    "        # New edges after creation of the element\n",
    "   \n",
    "            new_edge1=(edge[0],target_vtx)\n",
    "            new_edge2=(edge[1],target_vtx)\n",
    "        \n",
    "            if new_edge1 not in set_edges and tuple(reversed(new_edge1)) not in set_edges:\n",
    "                set_edges.add(new_edge1)\n",
    "                interior_edges.add(new_edge1)\n",
    "                print(\"edges inserted:\",new_edge1)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            if new_edge2 not in set_edges and tuple(reversed(new_edge2)) not in set_edges:    \n",
    "                set_edges.add(new_edge2)\n",
    "                interior_edges.add(new_edge2)\n",
    "                print(\"edges inserted:\",new_edge2)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "            # Checking list of elements to see whether the were created or were already there\n",
    "            #for element_permutation in tuple(permutations(element)):\n",
    "                #if element_permutation in set_elements:\n",
    "                 #   print(\"Element {} already in set\".format(element))\n",
    "                  #  break\n",
    "            #else:\n",
    "            set_elements.add(element)\n",
    "            indices=np.asarray(element)\n",
    "            print(\"element inserted:\",element)\n",
    "            element_created=True\n",
    "            break\n",
    "        if element_created:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    triangulated={'segment_markers': np.ones([polygon.shape[0]]), 'segments':np.array(get_contour_edges(polygon)), 'triangles': np.array(list( list(i) for i in set_elements)),\n",
    "                  'vertex_markers': np.ones([polygon.shape[0]]), 'vertices': polygon}\n",
    "    plot.plot(plt.axes(), **triangulated)\n",
    "    print(\"Final edges:\",set_edges)\n",
    "    print(\"Elements created:\",set_elements)\n",
    "    print(\"Set of locked vertices:\", set_locked_vertices)\n",
    "    \n",
    "    \n",
    "    # find open vertices\n",
    "    for element in set_elements:\n",
    "        for vertex in  element:\n",
    "                    _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                    if isclosed and vertex not in set_locked_vertices:\n",
    "                        print(\"Vertex locked:\",vertex)\n",
    "                        Found_locked_vertex=True\n",
    "                        set_locked_vertices.add(vertex)\n",
    "    set_open_vertices=set(range(len(polygon)))-set_locked_vertices\n",
    "    print(\"Set of open vertices:\", set_open_vertices)\n",
    "    set_edges.clear(),set_locked_vertices.clear(),set_forbidden_intersections.clear\n",
    "    if recursive:\n",
    "        sub_polygon_list=check_for_sub_polygon(set_open_vertices,interior_edges,set_elements,polygon)\n",
    "\n",
    "        for sub_polygon_indices in sub_polygon_list:\n",
    "            if len(sub_polygon_indices)>=4:\n",
    "                polygon_copy=polygon.copy()\n",
    "                sub_polygon=np.array(polygon_copy[sub_polygon_indices])\n",
    "                sub_quality,_=quality_matrix(sub_polygon,compute_minimum=True,normalize=False)\n",
    "                sub_order_matrix=order_quality_matrix(sub_quality,sub_polygon)\n",
    "                print(sub_quality,sub_order_matrix)\n",
    "                triangulate(sub_polygon,sub_order_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def pure_triangulate(polygon,ordered_quality_matrix,recursive=True):\n",
    "    set_edges=set(tuple(i) for i in get_contour_edges(polygon))\n",
    "    interior_edges=set()\n",
    "    set_elements=set()\n",
    "    set_locked_vertices=set()\n",
    "    set_forbidden_intersections=set()\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    check_edge_validity_time,check_for_existing_element=0,0\n",
    "    #print(\"initial set edges:\", set_edges)\n",
    "    \n",
    "\n",
    "   \n",
    "    for edge in ordered_quality_matrix.keys():\n",
    "        \n",
    "        start=timer()\n",
    "        found_in_interior_set,found_in_set,index=check_edge_validity(edge,polygon,set_edges,interior_edges)\n",
    "        stop=timer()\n",
    "        check_edge_validity_time+=stop-start\n",
    "        \n",
    "        for qualities_with_edges in ordered_quality_matrix[edge][0]:\n",
    "            \n",
    "            element_created=False\n",
    "           \n",
    "            target_vtx=qualities_with_edges[1]\n",
    "            \n",
    "\n",
    "        \n",
    "            if found_in_interior_set:\n",
    "                element=(edge[0],edge[1],index)  \n",
    "                set_elements.add(element)\n",
    "                #print(\"Element inserted:\",element)\n",
    "                continue\n",
    "        \n",
    "            if found_in_set and not found_in_interior_set:    \n",
    "                if(index != target_vtx):\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        \n",
    "            # Passed edges checking \n",
    "            # Proceed to check vertices\n",
    "            temp_element=(edge[0],edge[1],target_vtx)\n",
    "\n",
    "            existing_element=False\n",
    "           \n",
    "            for element in set_elements:\n",
    "                if set(temp_element)==set(element):\n",
    "                    existing_element=True\n",
    "                    break\n",
    "            \n",
    "            if existing_element:\n",
    "                break\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            if target_vtx in set_locked_vertices:\n",
    "                #print(\" Target vertex {} is locked\".format(target_vtx))\n",
    "                continue\n",
    "            set_elements.add(temp_element)\n",
    "\n",
    "        \n",
    "    \n",
    "            # Check if a locked vertex was created after the creation of the element\n",
    "            # If so, add it to the list\n",
    "            #Tracer()()\n",
    "            \n",
    "            Found_locked_vertex=False\n",
    "#            timer1=timer()\n",
    "            for vertex in temp_element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                    #print(\"Vertex locked:\",vertex)\n",
    "                    Found_locked_vertex=True\n",
    "                    set_locked_vertices.add(vertex)\n",
    "  #          timer2=timer()\n",
    "            checking_locked_vertices=time2-time1\n",
    "            set_elements.remove(temp_element)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Locking the vertices and checking if the connection is with a locked vertex has been checked/\n",
    "            # Proceeding to check if both internal edges intersect with other internal edges\n",
    "            internal_edge1=(edge[0],target_vtx)\n",
    "            internal_edge2=(edge[1],target_vtx)\n",
    "            set_a,set_b=get_intermediate_indices(target_vtx,polygon,edge[0],edge[1])\n",
    "        \n",
    "            internal_condition1= internal_edge1 in set_forbidden_intersections or tuple(reversed(internal_edge1)) in set_forbidden_intersections\n",
    "                                                                        \n",
    "            internal_condition2=internal_edge2 in set_forbidden_intersections or tuple(reversed(internal_edge2)) in set_forbidden_intersections\n",
    "                                                                            \n",
    "    \n",
    "                                                                                   \n",
    "            internal_intersection=False\n",
    "            \n",
    "\n",
    "            if internal_condition1 or  internal_condition2:\n",
    "                #print(\"edges :\",internal_edge1, \"and\",internal_edge2,\"intersecting\")\n",
    "                #print(\"Abandoning creation of element\",temp_element)\n",
    "                internal_intersection=True\n",
    "        \n",
    "     \n",
    "            if internal_intersection:\n",
    "                for vtx in temp_element:\n",
    "                    if Found_locked_vertex and vtx in set_locked_vertices:\n",
    "                        #print(\"Unlocking vertex\",vtx)\n",
    "                        set_locked_vertices.remove(vtx)                    \n",
    "                continue\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Create the element\n",
    "            element=temp_element\n",
    "        \n",
    "            # Add to set of edges all the forbidden intersections after the creation of the element\n",
    "            \n",
    "            for i in set_a:\n",
    "                for j in set_b:\n",
    "                    set_forbidden_intersections.add((i,j))\n",
    "            #print(\"set of forbidden inter section edges updated:\",set_forbidden_intersections)\n",
    "    \n",
    "                \n",
    "        \n",
    "        \n",
    "        # New edges after creation of the element\n",
    "   \n",
    "            new_edge1=(edge[0],target_vtx)\n",
    "            new_edge2=(edge[1],target_vtx)\n",
    "        \n",
    "            if new_edge1 not in set_edges and tuple(reversed(new_edge1)) not in set_edges:\n",
    "                set_edges.add(new_edge1)\n",
    "                interior_edges.add(new_edge1)\n",
    "                #print(\"edges inserted:\",new_edge1)\n",
    "                #print(\"set of interior edges updated:\",interior_edges)\n",
    "                #print(\"set of edges updated:\",set_edges)\n",
    "            if new_edge2 not in set_edges and tuple(reversed(new_edge2)) not in set_edges:    \n",
    "                set_edges.add(new_edge2)\n",
    "                interior_edges.add(new_edge2)\n",
    "                #print(\"edges inserted:\",new_edge2)\n",
    "                #print(\"set of interior edges updated:\",interior_edges)\n",
    "                #print(\"set of edges updated:\",set_edges)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "         \n",
    "            set_elements.add(element)\n",
    "            indices=np.asarray(element)\n",
    "                #print(\"element inserted:\",element)\n",
    "            element_created=True\n",
    "            \n",
    "            if element_created:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "           \n",
    "    \n",
    "#    triangulated={'segment_markers': np.ones([polygon.shape[0]]), 'segments':np.array(get_contour_edges(polygon)), 'triangles': np.array(list( list(i) for i in set_elements)),\n",
    "#                  'vertex_markers': np.ones([polygon.shape[0]]), 'vertices': polygon}\n",
    "#    plot.plot(plt.axes(), **triangulated)\n",
    "    #print(\"Final edges:\",set_edges)\n",
    "    #print(\"Elements created:\",set_elements)\n",
    "    #print(\"Set of locked vertices:\", set_locked_vertices)\n",
    "    \n",
    "    \n",
    "    # find open vertices\n",
    "    for element in set_elements:\n",
    "        for vertex in  element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                        #print(\"Vertex locked:\",vertex)\n",
    "                        Found_locked_vertex=True\n",
    "                        set_locked_vertices.add(vertex)\n",
    "    set_open_vertices=set(range(len(polygon)))-set_locked_vertices\n",
    "    #print(\"Set of open vertices:\", set_open_vertices)\n",
    "    set_edges.clear(),set_locked_vertices.clear(),set_forbidden_intersections.clear\n",
    "    if recursive and len(set_open_vertices)>=4:\n",
    "        sub_polygon_list=check_for_sub_polygon(set_open_vertices,interior_edges,set_elements,polygon)\n",
    "#\n",
    "        for sub_polygon_indices in sub_polygon_list:\n",
    "            if len(sub_polygon_indices)>=4:\n",
    "                polygon_copy=polygon\n",
    "                sub_polygon=np.array(polygon_copy[sub_polygon_indices])\n",
    "                sub_quality,_=quality_matrix(sub_polygon,compute_minimum=True,normalize=False)\n",
    "                sub_order_matrix=order_quality_matrix(sub_quality,sub_polygon)\n",
    "                #print(sub_quality,sub_order_matrix)\n",
    "                pure_triangulate(sub_polygon,sub_order_matrix)\n",
    "#\n",
    "\n",
    "    \n",
    "def order_quality_matrix(_quality_matrix,_polygon):\n",
    "\n",
    "    #  Create the quality matrix in accordance with the edges\n",
    "    quality_board=[(q,index)  for qualities in _quality_matrix for index,q in enumerate(qualities)]\n",
    "    quality_board=np.array(quality_board)\n",
    "    #print(\"Quality board not resized:\",quality_board)\n",
    "\n",
    "    quality_board.resize(len(get_contour_edges(_polygon)),len(_polygon),2)\n",
    "    quality_board=dict(zip(list(tuple(i) for i in get_contour_edges(_polygon)),quality_board))\n",
    "    \n",
    "    \n",
    "    #sorted_quality_board={i[0]:i[1] for i in sorted(board.items(),key=lambda x: max(x[1]),reverse=True)}\n",
    "    #print(\"Quality board\")\n",
    "    #for keys,items in quality_board.items():\n",
    "    #    print(keys,items)\n",
    "    edge_quality=quality_board[(0,1)]\n",
    "    edge_quality=edge_quality[np.lexsort(np.fliplr(edge_quality).T)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in quality_board.keys():\n",
    "        quality_board[i]=quality_board[i][np.lexsort(np.fliplr(quality_board[i]).T)]\n",
    "        quality_board[i]=quality_board[i][::-1]\n",
    "        quality_board[i][:,1]=quality_board[i][:,1].astype(int)\n",
    "    \n",
    "    edge=[]\n",
    "    max_quality=[]\n",
    "    listing=[]\n",
    "    for keys,values in quality_board.items():\n",
    "        listing.append([keys,max(values[:,0])])\n",
    "    \n",
    "    listing=np.array(listing)\n",
    "    listing=listing[np.lexsort(np.transpose(listing)[::-3]).T]\n",
    "    listing=listing[::-1]\n",
    "    ordered_indices=listing[:,0]\n",
    "\n",
    "    ordered_quality_matrix={}\n",
    "\n",
    "    for i in ordered_indices:\n",
    "        ordered_quality_matrix[i]=[tuple(zip(quality_board[i][:,0],quality_board[i][:,1].astype(int)))]\n",
    "    \n",
    "    return ordered_quality_matrix    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the list of edges of a polygon\n",
    "def get_contour_edges(polygon):\n",
    "    contour_connectivity=np.array([[i,(i+1)%polygon.shape[0]] for i in range(polygon.shape[0])])\n",
    "    return contour_connectivity\n",
    "\n",
    "\n",
    "\n",
    "# Function to return indices that are connected to a vertex\n",
    "def connection_indices(vertex,edges):   \n",
    "    indices=[]\n",
    "    for edge in edges:\n",
    "        if vertex in edge:\n",
    "\n",
    "            if edge[0] == vertex:\n",
    "                indices.append(edge[1])\n",
    "            else:\n",
    "                indices.append(edge[0])\n",
    "\n",
    "    return indices\n",
    "\n",
    "# Function to calculate and angle:\n",
    "def calculate_angle(p0,p1,p2):\n",
    "    v0 = p1 - p0\n",
    "    v1 = p2 - p0\n",
    "    \n",
    "    \n",
    "    normal=compute_triangle_normals([p0,p1,p2])\n",
    "    angle = np.math.atan2(np.linalg.det([v0,v1]),np.dot(v0,v1))\n",
    "    angle=abs(angle)\n",
    "    #unit_v0=v0 / np.linalg.norm(v0)\n",
    "    #unit_v1=v1 / np.linalg.norm(v1)\n",
    "    #angle=np.arccos(np.clip(np.dot(unit_v0, unit_v1), -1.0, 1.0))\n",
    "    \n",
    "    return np.degrees(angle)\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate the angles of a polygon\n",
    "def get_polygon_angles(polygon):\n",
    "    angles=[]\n",
    "    for index,point in enumerate(polygon):\n",
    "        p0=point        \n",
    "        neighbor_points=connection_indices(index,get_contour_edges(polygon))\n",
    "        #print(\"neighbor points\",neighbor_points)\n",
    "        indices=np.asarray(neighbor_points)\n",
    "        p1,p2=polygon[indices]\n",
    "        angle=calculate_angle(p0,p1,p2)\n",
    "        if index !=0:\n",
    "            triangle_normal=compute_triangle_normals([p0,p1,p2])\n",
    "        else:\n",
    "            triangle_normal=compute_triangle_normals([p1,p0,p2])\n",
    "\n",
    "            \n",
    "        if triangle_normal>0:\n",
    "            angle=360-angle\n",
    "        \n",
    "        angles.append(angle)\n",
    "    return angles\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def is_closed_ring(vtx,set_of_elements,*adj_vtx):\n",
    "    contour_edge1=(vtx,adj_vtx[0])\n",
    "    contour_edge2=(vtx,adj_vtx[1])\n",
    "    visited_elements=set_of_elements.copy()\n",
    "    \n",
    "    target_edge=contour_edge1\n",
    "    \n",
    "    edges_found=[]\n",
    "    edges_found.append(contour_edge1)\n",
    "\n",
    "    proceed=True\n",
    "    \n",
    "    while proceed:\n",
    "        \n",
    "        if not visited_elements:\n",
    "            break\n",
    "        \n",
    "        remaining_edge,found_element=edge2elem(target_edge,visited_elements)\n",
    "        \n",
    "        if found_element is None:\n",
    "            #print(\"stopped\")\n",
    "            proceed=False\n",
    "            break\n",
    "            \n",
    "        visited_elements.remove(found_element)\n",
    "        edges_found.append(remaining_edge)\n",
    "        target_edge=remaining_edge\n",
    "    \n",
    "    \n",
    "                \n",
    "    #print(set(edges_found))\n",
    "    found_contour_edge1,found_contour_edge2=False,False\n",
    "    found_contour_edges=False\n",
    "\n",
    "    # Checking if both contour edges area contained in the set of edges acquired\n",
    "    \n",
    "    for edge in edges_found:\n",
    "        condition1= contour_edge1[0] in set(edge) and contour_edge1[1] in set((edge))\n",
    "        condition2= contour_edge2[0] in set(edge) and contour_edge2[1] in set((edge))\n",
    "        if condition1:\n",
    "            #print(\"found \",contour_edge1)\n",
    "            found_contour_edge1=True\n",
    "        if condition2:\n",
    "            #print(\"found\",contour_edge2)\n",
    "            found_contour_edge2=True\n",
    "            \n",
    "    if found_contour_edge1 and found_contour_edge2:\n",
    "        found_contour_edges=True\n",
    "        #print(\"found both of contour edges in set\")\n",
    "    \n",
    "    visited_elements.clear()\n",
    "    return edges_found,found_contour_edges\n",
    "    \n",
    "    \n",
    "    \n",
    "# Finds element containing the edge and exits (does not give the full list of elements)   \n",
    "# Serve is_one_ring function\n",
    "def edge2elem(edge,set_of_elements):\n",
    "    found=False\n",
    "    Found_element=()\n",
    "    Remaining_edge=()\n",
    "  \n",
    "    for element in set_of_elements.copy():\n",
    "        \n",
    "        if edge[0] in  set(element) and edge[1] in set(element): \n",
    "            #print(\"Edge {} is part of element {}\".format(edge,element))\n",
    "            Found_element=element\n",
    "            Remaining_index=set(element)-set(edge)\n",
    "            Remaining_index=list(Remaining_index)\n",
    "            Remaining_edge=(edge[0],Remaining_index[0])\n",
    "            #print(\" Remaining edge is {}\".format(Remaining_edge))\n",
    "            break \n",
    "        else:\n",
    "            Found_element=None\n",
    "            Remaining_edge=None\n",
    "    return  Remaining_edge,Found_element \n",
    "\n",
    "# Departing from a target vertex connected with and edge get all intermediate  indices from one side and other\n",
    "def get_intermediate_indices(target_vtx,polygon,*edge):\n",
    "    \n",
    "    set_1=set()\n",
    "    set_2=set()\n",
    "    \n",
    "    \n",
    "    contour_edges=get_contour_edges(polygon)\n",
    "    \n",
    "    \n",
    "    # Depart from target vertex and get neighbor indices\n",
    "    neighbors=connection_indices(target_vtx,contour_edges)\n",
    "    found_vertex1,found_vertex2=neighbors[0],neighbors[1]\n",
    "    #print(\"found vertices:\",found_vertex1,found_vertex2)\n",
    "\n",
    "    \n",
    "    # Include them into seperate lists\n",
    "    set_1.add(found_vertex1)\n",
    "    set_2.add(found_vertex2)\n",
    "    \n",
    "    visited_vertex=target_vtx\n",
    "      \n",
    "    \n",
    "    while found_vertex1!=edge[0] and found_vertex1!=edge[1]:\n",
    "        visiting_vertex=found_vertex1\n",
    "        neighbors=connection_indices(visiting_vertex,contour_edges)\n",
    "        for index in neighbors:\n",
    "            if index !=  visited_vertex:\n",
    "                set_1.add(index)\n",
    "                found_vertex1=index\n",
    "                #print(\"Found vertex:\",found_vertex1)     \n",
    "        visited_vertex=visiting_vertex\n",
    "        \n",
    "    #print(\"Start  looking the other way\")\n",
    "    \n",
    "    # Resetting to go the other way\n",
    "    visited_vertex=target_vtx\n",
    "\n",
    "    while found_vertex2!=edge[0] and found_vertex2!=edge[1]:\n",
    "        visiting_vertex=found_vertex2\n",
    "        neighbors=connection_indices(visiting_vertex,contour_edges)\n",
    "        for index in neighbors:\n",
    "            if index !=  visited_vertex:\n",
    "                set_2.add(index)\n",
    "                found_vertex2=index\n",
    "                #print(\"Found vertex:\",found_vertex2)     \n",
    "        visited_vertex=visiting_vertex\n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "  \n",
    "    return set_1,set_2\n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_2_vtx(starting_vertex,edges_to_visit):\n",
    "    from  more_itertools import unique_everseen\n",
    "    \n",
    "    if not edges_to_visit:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    closed=False\n",
    "\n",
    "                \n",
    "    #print(\"Edges to visit:\",edges_to_visit)\n",
    "    subpolygon=[]\n",
    "    \n",
    "                \n",
    "    found_vertex=starting_vertex\n",
    "    \n",
    "    while not closed:\n",
    "        for index,edge in enumerate(edges_to_visit.copy()):\n",
    "            visiting_vertex=found_vertex\n",
    "            \n",
    "           \n",
    "            #if visiting_vertex not in set(edge) and index==len(edges_to_visit.copy()):\n",
    "               # Tracer()()\n",
    "                #print(\"Not found in list of edges\")\n",
    "                #closed=True\n",
    "                #break\n",
    "            if visiting_vertex not in set(edge):\n",
    "                continue\n",
    "            subpolygon.append(visiting_vertex)\n",
    "                \n",
    "                                \n",
    "            #print(\"Visiting vertex\",visiting_vertex)\n",
    "            \n",
    "            found_starting_vtx=False\n",
    "            subpolygon.append(found_vertex)\n",
    "            \n",
    "            \n",
    "            #print(visiting_vertex,\" in \", edge)\n",
    "                \n",
    "                \n",
    "            for index in set(edge):\n",
    "                if visiting_vertex!= index:\n",
    "                    found_vertex=index\n",
    "                    #print(\"Found vertex:\",found_vertex)\n",
    "                    subpolygon.append(found_vertex)\n",
    "                    \n",
    "                    \n",
    "            #print(\"Removing edge\",edge)\n",
    "            edges_to_visit.discard(edge)\n",
    "            #print(edges_to_visit)\n",
    "            if found_vertex==starting_vertex:\n",
    "                subpolygon=list(unique_everseen(subpolygon))\n",
    "                #print(\"Back to starting vertex\")    \n",
    "                closed=True\n",
    "                break\n",
    "                \n",
    "    if  len(subpolygon)<=3:\n",
    "        return \n",
    "    else:\n",
    "        return subpolygon\n",
    "     \n",
    "\n",
    "                       \n",
    "\n",
    "def check_for_sub_polygon(set_of_open_vertices,set_of_interior_edges,set_of_elements,polygon):\n",
    "\n",
    "    \n",
    "    \n",
    "    if not set_of_open_vertices or  len(set_of_open_vertices)<3:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    sub_polygon_list=[]\n",
    "    modified_interior_edge_set=set_of_interior_edges.copy()\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    polygon_connectivity=[tuple(i) for i in get_contour_edges(polygon)]\n",
    "    \n",
    "    for edge in modified_interior_edge_set.copy():\n",
    "        if edge[0] not in set_of_open_vertices or edge[1] not in set_of_open_vertices:\n",
    "            modified_interior_edge_set.discard(edge)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Taking care of vertices that are locked but the element is not seen\n",
    "    \n",
    "    set_of_unfound_locked_vertices=set()\n",
    "    continue_looking=True\n",
    "\n",
    "    \n",
    "    while continue_looking:\n",
    "        \n",
    "        if not set_of_open_vertices:\n",
    "            continue_looking=False\n",
    "            \n",
    "        for vtx in set_of_open_vertices.copy():\n",
    "                vtx1,vtx2 =connection_indices(vtx,get_contour_edges(polygon))\n",
    "                found_edges1,isclosed1=is_closed_ring(vtx,set_of_elements,vtx2,vtx1)\n",
    "                found_edges2,isclosed2=is_closed_ring(vtx,set_of_elements,vtx1,vtx2)\n",
    "                #print(\"Examining if vtx {} is locked\".format(vtx))\n",
    "                \n",
    "                if isclosed1 or isclosed2:\n",
    "                    #print(vtx,\"locked after all\")\n",
    "                    set_of_open_vertices.discard(vtx)\n",
    "                    for edge in modified_interior_edge_set.copy():\n",
    "                        if vtx in edge:\n",
    "                            modified_interior_edge_set.discard(edge)\n",
    "                    break\n",
    "                \n",
    "                for edge in found_edges1:\n",
    "                    if edge in polygon_connectivity or edge[::-1] in polygon_connectivity:\n",
    "                        found_edges1.remove(edge)\n",
    "                for edge in found_edges2:\n",
    "                    if edge in polygon_connectivity or edge[::-1] in polygon_connectivity:\n",
    "                        found_edges2.remove(edge)\n",
    "                between_edges=[]\n",
    "                for edge in found_edges1:\n",
    "                    for indices in edge:\n",
    "                        if indices==vtx:\n",
    "                            continue\n",
    "                    between_edges.append(indices)\n",
    "                for edge in found_edges2:\n",
    "                    for indices in edge:\n",
    "                        if indices==vtx:\n",
    "                            continue\n",
    "                    between_edges.append(indices)\n",
    "                for edge in set_of_interior_edges.copy():\n",
    "                    found_locked_vtx=False\n",
    "                    if set(between_edges)==set(edge):\n",
    "                       # print(vtx,\"locked after all\")\n",
    "                        found_locked_vtx=True\n",
    "                        set_of_unfound_locked_vertices.add(vtx)\n",
    "                        #Tracer()()\n",
    "                        if edge in set_of_interior_edges or edge[::-1] in set_of_interior_edges:                 \n",
    "                            #modified_interior_edge_set.discard(edge)\n",
    "                            #print(edge,\"removed\")               \n",
    "                            #modified_interior_edge_set.discard(edge[::-1])\n",
    "                            modified_interior_edge_set.discard((vtx,between_edges[0]))\n",
    "                            modified_interior_edge_set.discard((between_edges[0],vtx))\n",
    "                        \n",
    "    \n",
    "                            modified_interior_edge_set.discard((vtx,between_edges[1]))\n",
    "                            modified_interior_edge_set.discard((between_edges[1],vtx))\n",
    "                            element=(vtx,between_edges[0],between_edges[1])\n",
    "                            #print(\"Removed:\",(vtx),\"from set of open vertices\")\n",
    "    \n",
    "                            #print(\"Added new element:\",element)\n",
    "                            #print(\"Removed:\",(vtx,between_edges[0]),\"from set of edges\")\n",
    "                            #print(\"Removed:\",(vtx,between_edges[1]),\"from set of edges\")\n",
    "    \n",
    "                            set_of_elements.add(element) \n",
    "                            #print(\"New set of elements\",set_of_elements)\n",
    "                            set_of_open_vertices.discard(vtx)\n",
    "                            \n",
    "                    if found_locked_vtx:\n",
    "                        #Tracer()()\n",
    "                        continue_looking=True\n",
    "                        #print(\"Re-evaluting set of open vertices\")\n",
    "                        break\n",
    "                        \n",
    "                    else: continue_looking=False\n",
    "                        \n",
    "                        \n",
    "    #    for edge in modified_interior_edge_set.copy():\n",
    "    #        if set(edge).issubset(set_of_unfound_locked_vertices):\n",
    "    #            modified_interior_edge_set.discard(edge)\n",
    "#            modified_interior_edge_set.discard(edge[::-1])\n",
    "#            print(\"removed\",edge)\n",
    "            \n",
    "            #print(\"inbetween\",between_edges)\n",
    "                \n",
    "    #print(\"set of open vertices\",set_of_open_vertices)\n",
    "    \n",
    "    if not set_of_open_vertices or  len(set_of_open_vertices)<3:\n",
    "        return []\n",
    "    \n",
    "    # In the set of open vertices there may be vertices that are part of  of multiple polygon\n",
    "    found_common_vertex=False\n",
    "    set_of_common_vertices=set()\n",
    "    nb_of_polygon=0\n",
    "    for vertex in set_of_open_vertices:\n",
    "        count=0\n",
    "        for edge in modified_interior_edge_set.copy():\n",
    "            if vertex in set(edge):\n",
    "                count+=1\n",
    "        if count>=3:\n",
    "            nb_of_polygon=count-2\n",
    "            #print(\"Vertex {} is a common vtx of multiple polygons\".format(vertex))\n",
    "            set_of_common_vertices.add(vertex)\n",
    "        found_common_vertex=True\n",
    "        \n",
    "    # An edge could be part of more than one polygons. This means that the vertices of this edge\n",
    "    # are already in the set of common vertices and the edges is inside the set of the of modi\n",
    "    # fied interior edges\n",
    "    set_of_common_edges=set()\n",
    "    for vtx1 in  set_of_common_vertices:\n",
    "        for vtx2 in set_of_common_vertices:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # if the set found is les than 4 then now polygon is formed\n",
    "    if len(set_of_open_vertices)<4:\n",
    "        return []\n",
    "    \n",
    "    edges_to_visit=modified_interior_edge_set\n",
    "\n",
    "    \n",
    "    sub_polygon_list=[]\n",
    "    if len(edges_to_visit)<3:\n",
    "        return sub_polygon_list\n",
    "    if set_of_common_vertices:\n",
    "        while edges_to_visit:  \n",
    "            for vtx in set_of_common_vertices:\n",
    "                subpolygon=polygon_2_vtx(vtx,edges_to_visit)\n",
    "                if subpolygon is not None:\n",
    "                    sub_polygon_list.append(subpolygon)\n",
    "                    #print(sub_polygon_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Removing eges where one vertex is locked and the other is not:\n",
    "    for edge in edges_to_visit.copy():\n",
    "        if (edge[0] in set_of_open_vertices and edge[1] not in set_of_open_vertices) or (edge[1] in set_of_open_vertices and edge[0] not in set_of_open_vertices):\n",
    "            edges_to_visit.discard(edge)\n",
    "            #print(\"Removing\",edge,\"from edges to visit\")\n",
    "            #print(\"Edges to visit are now\",edges_to_visit)\n",
    "    \n",
    "    \n",
    "    while edges_to_visit:          \n",
    "        for vtx in set_of_open_vertices.copy():\n",
    "            #print(\"Starting with vertex\",vtx)\n",
    "            subpolygon=polygon_2_vtx(vtx,edges_to_visit)\n",
    "\n",
    "            if subpolygon is not None:\n",
    "                sub_polygon_list.append(subpolygon)\n",
    "                print(sub_polygon_list)\n",
    "        \n",
    "                                    \n",
    "    for sub_polygon in sub_polygon_list:\n",
    "        if len(sub_polygon)>3:\n",
    "            pass\n",
    "            #print(\"found polygon\",sub_polygon)\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"found element\",sub_polygon)\n",
    "    return sub_polygon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_contour(filename,contour):\n",
    "    path=os.path.join('contour_cases',filename+'.txt')\n",
    "    file=open(path,'w')\n",
    "    for i in contour:\n",
    "        file.write(np.array2string(i)+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "def read_contour(filename):\n",
    "    path=os.path.join('contour_cases',filename+'.txt')\n",
    "    contour=[]\n",
    "    file=open(path,'r')\n",
    "    for line in file:\n",
    "        coord=np.fromstring(line.strip('[\\n]'), dtype=float, sep=' ')\n",
    "        contour.append(coord)\n",
    "    file.close()\n",
    "    return np.array(contour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(nb_of_contours,nb_of_points):\n",
    "    Polygons=np.empty([nb_of_contours,nb_of_points,2])\n",
    "    Labels_min=np.empty([nb_of_contours,nb_of_points,nb_of_points]) # Labels for polygons\n",
    "    Labels_mean=np.empty([nb_of_contours,nb_of_points,nb_of_points]) # Labels for polygons\n",
    "\n",
    "    count=0\n",
    "    for i in range(nb_of_contours):\n",
    "        Polygons[i] = apply_procrustes(generate_contour(nb_of_points,False),False)\n",
    "        Labels_min[i]=quality_matrix( Polygons[i],compute_minimum=True,mean=False)[0]\n",
    "        Labels_mean[i]=quality_matrix( Polygons[i],compute_minimum=True,mean=True)[0]\n",
    "\n",
    "        count+=1\n",
    "        print(count, \" out of \", nb_of_contours, \"calculated\",nb_of_points)\n",
    "    return Polygons,Labels_min,Labels_mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(filename,dataset):\n",
    "    path=os.path.join('../polygon_datasets/',filename)\n",
    "    \n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(dataset,output)\n",
    "        \n",
    "def load_dataset(filename):\n",
    "    path=os.path.join('../polygon_datasets/',filename)\n",
    "\n",
    "    with open(path,'rb') as input:\n",
    "        dataset=pickle.load(input)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(filename,net):\n",
    "    path=os.path.join('../network_datasets/',filename)\n",
    "\n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(net,output)\n",
    "        \n",
    "def load_network(filename):\n",
    "    path=os.path.join('../network_datasets/',filename)\n",
    "        \n",
    "    with open(path,'rb') as input:\n",
    "        net=pickle.load(input)\n",
    "        \n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def run_time_test(start,finish):\n",
    "    \n",
    "    times=np.empty([int(((finish+1)-start)),9])\n",
    "\n",
    "    for index,i in enumerate(range(start,finish+1)):\n",
    "        \n",
    "        # train to project with pca\n",
    "        Polygons=load_dataset(str(i)+'_polygons.pkl')\n",
    "    \n",
    "        Polygons_reshaped=[]\n",
    "        for j in range(Polygons.shape[0]):\n",
    "            Polygons_reshaped.append(Polygons[j].reshape(2*i))\n",
    "\n",
    "        Polygons_reshaped=np.array(Polygons_reshaped) \n",
    "        \n",
    "                        # PCA   #\n",
    "        pca=PCA(.999)\n",
    "        pca.fit(Polygons_reshaped)\n",
    "        Polygons_projected=pca.transform(Polygons_reshaped)\n",
    "\n",
    "        \n",
    "        contour=generate_contour(i)\n",
    "        \n",
    "        network=load_network(str(i)+'_neural_net.pkl')\n",
    "        start_time=time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Perform procrustes\n",
    "        time1=timer()\n",
    "        procrustes_contour=apply_procrustes(contour)\n",
    "        procrustes_time=timer() - time1\n",
    "        print(\"Elapsed time for applying procrustes --- %s seconds ---\" % (procrustes_time))\n",
    "        \n",
    "        \n",
    "        procrustes_contour=procrustes_contour.reshape(2*contour.shape[0]).reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Project to space \n",
    "        time2=timer()\n",
    "        contour_transformed=pca.transform(procrustes_contour)\n",
    "        projection_time=timer() - time2\n",
    "        print(\"Elapsed time for projection --- %s seconds ---\" % (projection_time))\n",
    "        \n",
    "        \n",
    "        # Fire to network\n",
    "        time3=timer()\n",
    "        input_contour=Variable(torch.from_numpy(contour_transformed).type(torch.FloatTensor)).cuda()\n",
    "        predicted_quality_matrix=network(input_contour)\n",
    "        network_time=timer() - time3\n",
    "        print(\"Elapsed time for getting quality matrix from neural net --- %s seconds ---\" % (network_time))\n",
    "        \n",
    "        predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "        predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(i,i)\n",
    "        procrustes_contour=procrustes_contour.reshape(i,2)\n",
    "        \n",
    "        \n",
    "        # Calculating quality matrix\n",
    "        time4=timer()\n",
    "        original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "        calculation_quality_matrix_time=timer() - time4\n",
    "        print(\"Elapsed time for calculating original quality matrix  --- %s seconds ---\" % (calculation_quality_matrix_time))\n",
    "        \n",
    "        \n",
    "        # Ordering quality matrix\n",
    "        time5=timer()\n",
    "        ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "        ordering_matrix_time=timer() - time5\n",
    "        print(\"Elapsed time for ordering the quality matrix  --- %s seconds ---\" % (ordering_matrix_time))\n",
    "        \n",
    "        \n",
    "        # Triangulation\n",
    "        time6=timer()\n",
    "        pure_triangulate(procrustes_contour,ordered_matrix,recursive=True)\n",
    "        triangulation_time=timer() - time6\n",
    "        print(\"Elapsed time to triangulate according to matrix   --- %s seconds ---\" % (triangulation_time))\n",
    "        \n",
    "        \n",
    "        total_time_with_calculation=(procrustes_time+calculation_quality_matrix_time+ordering_matrix_time+triangulation_time)\n",
    "        print(\"Total time  of triangulation with calculation of matrix :   --- %s seconds ---\" % (total_time_with_calculation ))\n",
    "        total_time_with_NN=(procrustes_time+projection_time+network_time+triangulation_time)\n",
    "        print(\"Total time  of triangulation using neural network :   --- %s seconds ---\" % ( total_time_with_NN))\n",
    "\n",
    "\n",
    "        \n",
    "        # Triangulate using the library \n",
    "        shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "        time7=timer()\n",
    "        for i in range(10000):\n",
    "            t = triangle.triangulate(shape, 'pq0')\n",
    "        remeshing_time=timer()-time7\n",
    "        remeshing_time/=10000\n",
    "\n",
    "        #plot.plot(plt.axes(), **t) \n",
    "        print(\"Elapsed time to triangulate using triangle module   --- %s seconds ---\" % (remeshing_time))\n",
    "        times[index]=[procrustes_time,projection_time,network_time,calculation_quality_matrix_time,ordering_matrix_time,triangulation_time,total_time_with_calculation,total_time_with_NN,remeshing_time]\n",
    "    return times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "\n",
    "Polygons,quality_matrices=load_dataset('12_polygons.pkl'),load_dataset('12_polygons_qualities_min.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data are usually 20 percent of the whole\n",
    "\n",
    "nb_of_points=12\n",
    "nb_of_contours=int(Polygons.shape[0])\n",
    "\n",
    "nb_test_data=int(0.2*Polygons.shape[0])\n",
    "nb_training_data=int(Polygons.shape[0])-nb_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nb_of_contours):\n",
    "    Polygons_reshaped.append(Polygons[i].reshape(2*nb_of_points))\n",
    "\n",
    "Polygons_reshaped=np.array(Polygons_reshaped) \n",
    "\n",
    "\n",
    "\n",
    "#nb_of_components=12\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "#pca=PCA(.999)\n",
    "#pca.fit(Polygons_reshaped)\n",
    "#Polygons_projected=pca.transform(Polygons_reshaped)\n",
    "#nb_components=int(pca.n_components_)\n",
    "#print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing data \n",
    "training_data = Polygons_reshaped[:nb_training_data]\n",
    "testing_data  = Polygons_reshaped[nb_training_data:]\n",
    "\n",
    "training_labels=quality_matrices[:nb_training_data]\n",
    "testing_labels=quality_matrices[nb_training_data:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert to pytorch tennsors\n",
    "\n",
    "x_tensor=torch.from_numpy(training_data).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(testing_data).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "y_tensor=torch.from_numpy(training_labels).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(testing_labels).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_variable=Variable(x_tensor)\n",
    "x_variable_test=Variable(x_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "y_variable=Variable(y_tensor)\n",
    "y_variable=y_variable.resize(nb_training_data,nb_of_points*nb_of_points)\n",
    "\n",
    "\n",
    "y_variable_test=Variable(y_tensor_test)\n",
    "y_variable_test=y_variable_test.resize(nb_test_data,nb_of_points*nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B_INIT = -0.2 # use a bad bias constant initializer\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_features_dimension,out_features_dimension,nb_of_hidden_layers,nb_of_hidden_nodes,\n",
    "                 batch_normalization=False):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.nb_hidden_layers=nb_of_hidden_layers\n",
    "        self.do_bn=batch_normalization\n",
    "        self.fcs=[]\n",
    "        self.bns=[]\n",
    "        self.bn_input=nn.BatchNorm1d(in_features_dimension,momentum=0.5) #for input data\n",
    "        \n",
    "        for i in range(nb_of_hidden_layers):                              # build hidden layers and BN layers\n",
    "            \n",
    "            input_size=in_features_dimension if i==0 else nb_of_hidden_nodes\n",
    "            fc=nn.Linear(input_size,nb_of_hidden_nodes)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(nb_of_hidden_nodes, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)                         # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "    \n",
    "            self.predict = nn.Linear(nb_of_hidden_nodes,out_features_dimension)         # output layer\n",
    "            self._set_init(self.predict)                                              # parameters initialization\n",
    "    \n",
    "    \n",
    "    def _set_init(self, layer):\n",
    "            init.normal(layer.weight, mean=0., std=.1)\n",
    "            init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ACTIVATION=F.relu\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_net=Net(Polygons_reshaped.shape[1],nb_of_points*nb_of_points,nb_of_hidden_layers=3,\n",
    "           nb_of_hidden_nodes=160,batch_normalization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=.1)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    my_net.cuda()\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable = x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test , y_variable_test = x_variable_test.cuda(), y_variable_test.cuda()\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 38.213303080240884 Test Loss: 25.189190104166666\n",
      "Epoch: 1 Training Loss: 18.290825937906902 Test Loss: 13.275432291666666\n",
      "Epoch: 2 Training Loss: 10.328123077392577 Test Loss: 8.1914296875\n",
      "Epoch: 3 Training Loss: 6.826279276529948 Test Loss: 5.835490885416666\n",
      "Epoch: 4 Training Loss: 5.1374686991373695 Test Loss: 4.620953125\n",
      "Epoch: 5 Training Loss: 4.229574546813965 Test Loss: 3.9317229817708332\n",
      "Epoch: 6 Training Loss: 3.6916663411458335 Test Loss: 3.5024378255208335\n",
      "Epoch: 7 Training Loss: 3.3428022181193033 Test Loss: 3.211515625\n",
      "Epoch: 8 Training Loss: 3.097181241353353 Test Loss: 2.9988313802083333\n",
      "Epoch: 9 Training Loss: 2.9120091501871745 Test Loss: 2.8332239583333334\n",
      "Epoch: 10 Training Loss: 2.763537668863932 Test Loss: 2.6965843098958335\n",
      "Epoch: 11 Training Loss: 2.638071365356445 Test Loss: 2.57854345703125\n",
      "Epoch: 12 Training Loss: 2.5271553649902345 Test Loss: 2.472734375\n",
      "Epoch: 13 Training Loss: 2.4259733606974283 Test Loss: 2.375308268229167\n",
      "Epoch: 14 Training Loss: 2.331876927693685 Test Loss: 2.2842145182291667\n",
      "Epoch: 15 Training Loss: 2.2435203272501627 Test Loss: 2.1986689453125\n",
      "Epoch: 16 Training Loss: 2.1603700408935547 Test Loss: 2.1184130859375\n",
      "Epoch: 17 Training Loss: 2.0825184694925944 Test Loss: 2.043553873697917\n",
      "Epoch: 18 Training Loss: 2.0102088673909506 Test Loss: 1.9743639322916666\n",
      "Epoch: 19 Training Loss: 1.9438248023986817 Test Loss: 1.9112771809895834\n",
      "Epoch: 20 Training Loss: 1.8836750272115073 Test Loss: 1.8544405924479166\n",
      "Epoch: 21 Training Loss: 1.8296479771931966 Test Loss: 1.8034119466145833\n",
      "Epoch: 22 Training Loss: 1.781244223276774 Test Loss: 1.75767578125\n",
      "Epoch: 23 Training Loss: 1.737705342610677 Test Loss: 1.71639111328125\n",
      "Epoch: 24 Training Loss: 1.6982947476704915 Test Loss: 1.6789173177083334\n",
      "Epoch: 25 Training Loss: 1.662445598602295 Test Loss: 1.6448107096354168\n",
      "Epoch: 26 Training Loss: 1.6296936073303223 Test Loss: 1.6135545247395833\n",
      "Epoch: 27 Training Loss: 1.5995840098063152 Test Loss: 1.5848131510416668\n",
      "Epoch: 28 Training Loss: 1.5717913703918458 Test Loss: 1.55819189453125\n",
      "Epoch: 29 Training Loss: 1.5459732780456543 Test Loss: 1.5333740234375\n",
      "Epoch: 30 Training Loss: 1.5218966522216797 Test Loss: 1.5101077473958333\n",
      "Epoch: 31 Training Loss: 1.49930184173584 Test Loss: 1.4881861979166666\n",
      "Epoch: 32 Training Loss: 1.477986557006836 Test Loss: 1.4675084635416666\n",
      "Epoch: 33 Training Loss: 1.4578208351135253 Test Loss: 1.44795556640625\n",
      "Epoch: 34 Training Loss: 1.438643278757731 Test Loss: 1.4293058268229166\n",
      "Epoch: 35 Training Loss: 1.4203648211161295 Test Loss: 1.4114781901041666\n",
      "Epoch: 36 Training Loss: 1.4029031880696614 Test Loss: 1.3943653971354166\n",
      "Epoch: 37 Training Loss: 1.3861379318237306 Test Loss: 1.3779225260416668\n",
      "Epoch: 38 Training Loss: 1.370003615061442 Test Loss: 1.3620958658854168\n",
      "Epoch: 39 Training Loss: 1.354511806488037 Test Loss: 1.3468717447916667\n",
      "Epoch: 40 Training Loss: 1.339602081298828 Test Loss: 1.3322391764322916\n",
      "Epoch: 41 Training Loss: 1.325196393330892 Test Loss: 1.3180364583333333\n",
      "Epoch: 42 Training Loss: 1.3112247937520345 Test Loss: 1.304319580078125\n",
      "Epoch: 43 Training Loss: 1.2976475868225097 Test Loss: 1.29096142578125\n",
      "Epoch: 44 Training Loss: 1.2844190152486166 Test Loss: 1.2779161783854167\n",
      "Epoch: 45 Training Loss: 1.2715442352294921 Test Loss: 1.2652157389322916\n",
      "Epoch: 46 Training Loss: 1.2589972610473632 Test Loss: 1.2528639322916666\n",
      "Epoch: 47 Training Loss: 1.2467438468933105 Test Loss: 1.2408119303385416\n",
      "Epoch: 48 Training Loss: 1.2347331555684407 Test Loss: 1.22904931640625\n",
      "Epoch: 49 Training Loss: 1.222959051767985 Test Loss: 1.2175377604166666\n",
      "Epoch: 50 Training Loss: 1.2114136848449708 Test Loss: 1.2062317708333334\n",
      "Epoch: 51 Training Loss: 1.2000619799296062 Test Loss: 1.1951289876302083\n",
      "Epoch: 52 Training Loss: 1.1888879178365073 Test Loss: 1.1842505696614583\n",
      "Epoch: 53 Training Loss: 1.1778937848409017 Test Loss: 1.1735450032552084\n",
      "Epoch: 54 Training Loss: 1.1670566164652507 Test Loss: 1.1629883626302084\n",
      "Epoch: 55 Training Loss: 1.1563896268208822 Test Loss: 1.1525804036458334\n",
      "Epoch: 56 Training Loss: 1.1458977292378743 Test Loss: 1.1423216959635416\n",
      "Epoch: 57 Training Loss: 1.1355667190551757 Test Loss: 1.132274169921875\n",
      "Epoch: 58 Training Loss: 1.1253824475606282 Test Loss: 1.1223517252604167\n",
      "Epoch: 59 Training Loss: 1.115332717895508 Test Loss: 1.1125601399739584\n",
      "Epoch: 60 Training Loss: 1.1054169044494628 Test Loss: 1.1029236653645833\n",
      "Epoch: 61 Training Loss: 1.0956429640452068 Test Loss: 1.09337890625\n",
      "Epoch: 62 Training Loss: 1.0860025596618652 Test Loss: 1.0840152180989584\n",
      "Epoch: 63 Training Loss: 1.0765259278615316 Test Loss: 1.074790771484375\n",
      "Epoch: 64 Training Loss: 1.0671988194783528 Test Loss: 1.0657008463541666\n",
      "Epoch: 65 Training Loss: 1.0580282096862792 Test Loss: 1.0567799479166666\n",
      "Epoch: 66 Training Loss: 1.0489927978515625 Test Loss: 1.0480150553385417\n",
      "Epoch: 67 Training Loss: 1.0401178175608317 Test Loss: 1.0394186197916666\n",
      "Epoch: 68 Training Loss: 1.0313833955128988 Test Loss: 1.0309363606770834\n",
      "Epoch: 69 Training Loss: 1.0228037122090659 Test Loss: 1.0226353352864583\n",
      "Epoch: 70 Training Loss: 1.0143686866760253 Test Loss: 1.0144152018229167\n",
      "Epoch: 71 Training Loss: 1.0060811106363932 Test Loss: 1.0063312174479166\n",
      "Epoch: 72 Training Loss: 0.9979699509938558 Test Loss: 0.9983893229166667\n",
      "Epoch: 73 Training Loss: 0.9900271453857422 Test Loss: 0.9906111653645834\n",
      "Epoch: 74 Training Loss: 0.9822362772623698 Test Loss: 0.9829930013020833\n",
      "Epoch: 75 Training Loss: 0.9745767631530762 Test Loss: 0.9755059407552084\n",
      "Epoch: 76 Training Loss: 0.9670576718648275 Test Loss: 0.9681110026041667\n",
      "Epoch: 77 Training Loss: 0.9596683870951335 Test Loss: 0.9608497721354167\n",
      "Epoch: 78 Training Loss: 0.9524261474609375 Test Loss: 0.9537283528645834\n",
      "Epoch: 79 Training Loss: 0.9453227138519287 Test Loss: 0.9467151692708333\n",
      "Epoch: 80 Training Loss: 0.938357608795166 Test Loss: 0.9398475748697916\n",
      "Epoch: 81 Training Loss: 0.9315298652648926 Test Loss: 0.9330888671875\n",
      "Epoch: 82 Training Loss: 0.9248271878560385 Test Loss: 0.9264609375\n",
      "Epoch: 83 Training Loss: 0.9182425079345703 Test Loss: 0.9199476725260417\n",
      "Epoch: 84 Training Loss: 0.9117679971059164 Test Loss: 0.9135667317708334\n",
      "Epoch: 85 Training Loss: 0.9053984209696452 Test Loss: 0.9073372395833333\n",
      "Epoch: 86 Training Loss: 0.8991509323120117 Test Loss: 0.9011902669270834\n",
      "Epoch: 87 Training Loss: 0.893031105041504 Test Loss: 0.8951778971354166\n",
      "Epoch: 88 Training Loss: 0.8870383224487305 Test Loss: 0.8892583821614584\n",
      "Epoch: 89 Training Loss: 0.8811626370747884 Test Loss: 0.8834357096354166\n",
      "Epoch: 90 Training Loss: 0.8754029165903727 Test Loss: 0.877751708984375\n",
      "Epoch: 91 Training Loss: 0.8697476634979248 Test Loss: 0.8721497395833333\n",
      "Epoch: 92 Training Loss: 0.8641991017659505 Test Loss: 0.8666829427083333\n",
      "Epoch: 93 Training Loss: 0.8587495085398356 Test Loss: 0.8612967936197916\n",
      "Epoch: 94 Training Loss: 0.8533938802083333 Test Loss: 0.8560091145833333\n",
      "Epoch: 95 Training Loss: 0.8481345933278401 Test Loss: 0.8508181966145834\n",
      "Epoch: 96 Training Loss: 0.8429746996561687 Test Loss: 0.8456978352864584\n",
      "Epoch: 97 Training Loss: 0.8379061234792073 Test Loss: 0.8406464029947917\n",
      "Epoch: 98 Training Loss: 0.8329171988169353 Test Loss: 0.8356875\n",
      "Epoch: 99 Training Loss: 0.8280173066457113 Test Loss: 0.8308319498697917\n",
      "Epoch: 100 Training Loss: 0.8232034187316895 Test Loss: 0.8260152994791666\n",
      "Epoch: 101 Training Loss: 0.8184621289571127 Test Loss: 0.82130126953125\n",
      "Epoch: 102 Training Loss: 0.8137979227701823 Test Loss: 0.8166263834635417\n",
      "Epoch: 103 Training Loss: 0.8092121404012044 Test Loss: 0.8120419108072917\n",
      "Epoch: 104 Training Loss: 0.8047057075500488 Test Loss: 0.8075469563802083\n",
      "Epoch: 105 Training Loss: 0.8002758464813232 Test Loss: 0.8031453450520833\n",
      "Epoch: 106 Training Loss: 0.7959217249552409 Test Loss: 0.798808837890625\n",
      "Epoch: 107 Training Loss: 0.7916411768595377 Test Loss: 0.7945281575520833\n",
      "Epoch: 108 Training Loss: 0.7874316914876303 Test Loss: 0.7903419596354166\n",
      "Epoch: 109 Training Loss: 0.7833031133015951 Test Loss: 0.7862146809895834\n",
      "Epoch: 110 Training Loss: 0.7792446969350179 Test Loss: 0.7821519368489583\n",
      "Epoch: 111 Training Loss: 0.7752605431874593 Test Loss: 0.778165283203125\n",
      "Epoch: 112 Training Loss: 0.7713506132761637 Test Loss: 0.7742626139322917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 Training Loss: 0.7675120010375976 Test Loss: 0.7703946940104167\n",
      "Epoch: 114 Training Loss: 0.763731580734253 Test Loss: 0.7666112467447916\n",
      "Epoch: 115 Training Loss: 0.7600087312062581 Test Loss: 0.7628699544270834\n",
      "Epoch: 116 Training Loss: 0.756336109161377 Test Loss: 0.7591904296875\n",
      "Epoch: 117 Training Loss: 0.7527126522064209 Test Loss: 0.755569580078125\n",
      "Epoch: 118 Training Loss: 0.7491384665171306 Test Loss: 0.7520106608072916\n",
      "Epoch: 119 Training Loss: 0.7456222012837728 Test Loss: 0.748506591796875\n",
      "Epoch: 120 Training Loss: 0.7421633205413818 Test Loss: 0.7450680338541666\n",
      "Epoch: 121 Training Loss: 0.7387571481068929 Test Loss: 0.7416932779947917\n",
      "Epoch: 122 Training Loss: 0.7354070657094319 Test Loss: 0.73836962890625\n",
      "Epoch: 123 Training Loss: 0.7321102848052978 Test Loss: 0.7350853678385416\n",
      "Epoch: 124 Training Loss: 0.7288597742716472 Test Loss: 0.7318478190104166\n",
      "Epoch: 125 Training Loss: 0.7256508585611979 Test Loss: 0.7286542154947917\n",
      "Epoch: 126 Training Loss: 0.7224841677347819 Test Loss: 0.7254794921875\n",
      "Epoch: 127 Training Loss: 0.7193644046783447 Test Loss: 0.72235107421875\n",
      "Epoch: 128 Training Loss: 0.7162852853139241 Test Loss: 0.7192696126302084\n",
      "Epoch: 129 Training Loss: 0.7132370325724284 Test Loss: 0.71620751953125\n",
      "Epoch: 130 Training Loss: 0.7102249762217204 Test Loss: 0.7131870930989583\n",
      "Epoch: 131 Training Loss: 0.7072452863057455 Test Loss: 0.7102138671875\n",
      "Epoch: 132 Training Loss: 0.7042871831258138 Test Loss: 0.7073033854166667\n",
      "Epoch: 133 Training Loss: 0.7013846499125163 Test Loss: 0.704421875\n",
      "Epoch: 134 Training Loss: 0.698519526163737 Test Loss: 0.70159716796875\n",
      "Epoch: 135 Training Loss: 0.6956916809082031 Test Loss: 0.6987857259114584\n",
      "Epoch: 136 Training Loss: 0.6929109694163005 Test Loss: 0.6960293782552084\n",
      "Epoch: 137 Training Loss: 0.6901791801452637 Test Loss: 0.6933414713541667\n",
      "Epoch: 138 Training Loss: 0.6874793116251627 Test Loss: 0.6906668294270834\n",
      "Epoch: 139 Training Loss: 0.6848381830851237 Test Loss: 0.6880347493489584\n",
      "Epoch: 140 Training Loss: 0.682236853281657 Test Loss: 0.6854290364583333\n",
      "Epoch: 141 Training Loss: 0.679677064259847 Test Loss: 0.6829115397135417\n",
      "Epoch: 142 Training Loss: 0.677151211420695 Test Loss: 0.6803759765625\n",
      "Epoch: 143 Training Loss: 0.6746680444081624 Test Loss: 0.677892578125\n",
      "Epoch: 144 Training Loss: 0.6722128302256266 Test Loss: 0.6754525146484375\n",
      "Epoch: 145 Training Loss: 0.6697916240692139 Test Loss: 0.6730382893880208\n",
      "Epoch: 146 Training Loss: 0.6673945655822754 Test Loss: 0.6706429850260417\n",
      "Epoch: 147 Training Loss: 0.6650332813262939 Test Loss: 0.6682659505208334\n",
      "Epoch: 148 Training Loss: 0.6626995035807292 Test Loss: 0.66591064453125\n",
      "Epoch: 149 Training Loss: 0.6603995221455892 Test Loss: 0.6636117350260416\n",
      "Epoch: 150 Training Loss: 0.6581270999908447 Test Loss: 0.6613370768229166\n",
      "Epoch: 151 Training Loss: 0.6558845291137695 Test Loss: 0.6590654703776042\n",
      "Epoch: 152 Training Loss: 0.6536668980916341 Test Loss: 0.6568396402994792\n",
      "Epoch: 153 Training Loss: 0.6514777571360271 Test Loss: 0.6546483561197917\n",
      "Epoch: 154 Training Loss: 0.6493163026173909 Test Loss: 0.6524812418619792\n",
      "Epoch: 155 Training Loss: 0.6471800098419189 Test Loss: 0.6503618977864584\n",
      "Epoch: 156 Training Loss: 0.6450651327768961 Test Loss: 0.6482605794270834\n",
      "Epoch: 157 Training Loss: 0.642982276280721 Test Loss: 0.6461962890625\n",
      "Epoch: 158 Training Loss: 0.6409378242492676 Test Loss: 0.6441505940755209\n",
      "Epoch: 159 Training Loss: 0.6389119656880696 Test Loss: 0.6421228841145833\n",
      "Epoch: 160 Training Loss: 0.6369020074208578 Test Loss: 0.6401461181640625\n",
      "Epoch: 161 Training Loss: 0.6349213581085205 Test Loss: 0.6381645100911458\n",
      "Epoch: 162 Training Loss: 0.632973108291626 Test Loss: 0.6362399088541667\n",
      "Epoch: 163 Training Loss: 0.6310504182179769 Test Loss: 0.6343231201171875\n",
      "Epoch: 164 Training Loss: 0.6291483936309814 Test Loss: 0.6324249267578125\n",
      "Epoch: 165 Training Loss: 0.6272702929178874 Test Loss: 0.6305567220052083\n",
      "Epoch: 166 Training Loss: 0.6254118283589681 Test Loss: 0.628727294921875\n",
      "Epoch: 167 Training Loss: 0.6235775197347005 Test Loss: 0.626896484375\n",
      "Epoch: 168 Training Loss: 0.6217583020528158 Test Loss: 0.6250729166666666\n",
      "Epoch: 169 Training Loss: 0.619960304260254 Test Loss: 0.6233062744140625\n",
      "Epoch: 170 Training Loss: 0.6181737785339355 Test Loss: 0.6215391438802084\n",
      "Epoch: 171 Training Loss: 0.6164117698669433 Test Loss: 0.6197997233072917\n",
      "Epoch: 172 Training Loss: 0.6146732934315999 Test Loss: 0.6180950113932292\n",
      "Epoch: 173 Training Loss: 0.6129547570546469 Test Loss: 0.6164031575520833\n",
      "Epoch: 174 Training Loss: 0.611259339650472 Test Loss: 0.6147151692708334\n",
      "Epoch: 175 Training Loss: 0.6095889167785644 Test Loss: 0.6130521240234375\n",
      "Epoch: 176 Training Loss: 0.6079334557851156 Test Loss: 0.611415771484375\n",
      "Epoch: 177 Training Loss: 0.6063006343841553 Test Loss: 0.6097931315104167\n",
      "Epoch: 178 Training Loss: 0.6046869449615478 Test Loss: 0.6082167154947916\n",
      "Epoch: 179 Training Loss: 0.6030960222880045 Test Loss: 0.6066261393229166\n",
      "Epoch: 180 Training Loss: 0.6015133266448974 Test Loss: 0.6050538736979166\n",
      "Epoch: 181 Training Loss: 0.5999501349131267 Test Loss: 0.6034995930989583\n",
      "Epoch: 182 Training Loss: 0.5984016183217367 Test Loss: 0.6019656168619791\n",
      "Epoch: 183 Training Loss: 0.5968689409891764 Test Loss: 0.6004404296875\n",
      "Epoch: 184 Training Loss: 0.5953484522501628 Test Loss: 0.5989325764973958\n",
      "Epoch: 185 Training Loss: 0.5938418776194254 Test Loss: 0.59743359375\n",
      "Epoch: 186 Training Loss: 0.592354320526123 Test Loss: 0.5959777018229167\n",
      "Epoch: 187 Training Loss: 0.5908802935282389 Test Loss: 0.5945163167317709\n",
      "Epoch: 188 Training Loss: 0.5894200382232666 Test Loss: 0.5930668131510417\n",
      "Epoch: 189 Training Loss: 0.5879717585245768 Test Loss: 0.59165087890625\n",
      "Epoch: 190 Training Loss: 0.5865362103780111 Test Loss: 0.5902280680338542\n",
      "Epoch: 191 Training Loss: 0.5851216519673665 Test Loss: 0.5888232828776042\n",
      "Epoch: 192 Training Loss: 0.583711675008138 Test Loss: 0.5874281819661459\n",
      "Epoch: 193 Training Loss: 0.5823157850901286 Test Loss: 0.5860498046875\n",
      "Epoch: 194 Training Loss: 0.5809232088724772 Test Loss: 0.5846964518229166\n",
      "Epoch: 195 Training Loss: 0.5795543181101481 Test Loss: 0.5833525797526041\n",
      "Epoch: 196 Training Loss: 0.5781930885314941 Test Loss: 0.5820032552083333\n",
      "Epoch: 197 Training Loss: 0.576839049021403 Test Loss: 0.5806617024739583\n",
      "Epoch: 198 Training Loss: 0.5755009740193685 Test Loss: 0.5793583577473959\n",
      "Epoch: 199 Training Loss: 0.5741726144154866 Test Loss: 0.5780550130208333\n",
      "Epoch: 200 Training Loss: 0.5728601665496826 Test Loss: 0.5767674560546875\n",
      "Epoch: 201 Training Loss: 0.5715551509857177 Test Loss: 0.5755059000651042\n",
      "Epoch: 202 Training Loss: 0.5702664629618327 Test Loss: 0.5742694905598958\n",
      "Epoch: 203 Training Loss: 0.5689890918731689 Test Loss: 0.573001708984375\n",
      "Epoch: 204 Training Loss: 0.5677184130350749 Test Loss: 0.5717620442708333\n",
      "Epoch: 205 Training Loss: 0.5664512564341228 Test Loss: 0.5705083821614584\n",
      "Epoch: 206 Training Loss: 0.5651998112996419 Test Loss: 0.569294921875\n",
      "Epoch: 207 Training Loss: 0.5639499696095784 Test Loss: 0.5680572916666666\n",
      "Epoch: 208 Training Loss: 0.5627113393147787 Test Loss: 0.5668414306640625\n",
      "Epoch: 209 Training Loss: 0.5614872182210286 Test Loss: 0.5656376139322916\n",
      "Epoch: 210 Training Loss: 0.5602623805999756 Test Loss: 0.5644349772135416\n",
      "Epoch: 211 Training Loss: 0.5590547714233398 Test Loss: 0.563216552734375\n",
      "Epoch: 212 Training Loss: 0.5578499870300293 Test Loss: 0.5620476481119792\n",
      "Epoch: 213 Training Loss: 0.5566554609934489 Test Loss: 0.5608299967447916\n",
      "Epoch: 214 Training Loss: 0.5554707927703857 Test Loss: 0.559703857421875\n",
      "Epoch: 215 Training Loss: 0.5542980658213298 Test Loss: 0.5585775960286459\n",
      "Epoch: 216 Training Loss: 0.5531362698872884 Test Loss: 0.5574623616536458\n",
      "Epoch: 217 Training Loss: 0.5519762023289998 Test Loss: 0.5563249104817708\n",
      "Epoch: 218 Training Loss: 0.5508315629959106 Test Loss: 0.5552314453125\n",
      "Epoch: 219 Training Loss: 0.5496984812418619 Test Loss: 0.55414111328125\n",
      "Epoch: 220 Training Loss: 0.5485800530115763 Test Loss: 0.5530417887369792\n",
      "Epoch: 221 Training Loss: 0.5474719813664755 Test Loss: 0.5519665934244792\n",
      "Epoch: 222 Training Loss: 0.5463722381591797 Test Loss: 0.550877197265625\n",
      "Epoch: 223 Training Loss: 0.5452818698883056 Test Loss: 0.54980078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 Training Loss: 0.5442018102010091 Test Loss: 0.5487489420572916\n",
      "Epoch: 225 Training Loss: 0.5431285196940104 Test Loss: 0.5477146809895833\n",
      "Epoch: 226 Training Loss: 0.5420672874450684 Test Loss: 0.5466990559895833\n",
      "Epoch: 227 Training Loss: 0.541014635403951 Test Loss: 0.5456641031901042\n",
      "Epoch: 228 Training Loss: 0.5399774802525839 Test Loss: 0.5446683756510416\n",
      "Epoch: 229 Training Loss: 0.5389357929229737 Test Loss: 0.5436642659505209\n",
      "Epoch: 230 Training Loss: 0.5379030990600586 Test Loss: 0.5426538899739584\n",
      "Epoch: 231 Training Loss: 0.5368880850474039 Test Loss: 0.5416524251302083\n",
      "Epoch: 232 Training Loss: 0.5358684142430623 Test Loss: 0.5406420491536459\n",
      "Epoch: 233 Training Loss: 0.5348597822189332 Test Loss: 0.5396738688151042\n",
      "Epoch: 234 Training Loss: 0.5338632555007935 Test Loss: 0.5387263997395834\n",
      "Epoch: 235 Training Loss: 0.5328678661982218 Test Loss: 0.537758544921875\n",
      "Epoch: 236 Training Loss: 0.5318850717544555 Test Loss: 0.5367714029947916\n",
      "Epoch: 237 Training Loss: 0.5309074659347535 Test Loss: 0.5357874755859375\n",
      "Epoch: 238 Training Loss: 0.5299330005645752 Test Loss: 0.5348345540364583\n",
      "Epoch: 239 Training Loss: 0.5289647849400838 Test Loss: 0.5338944091796874\n",
      "Epoch: 240 Training Loss: 0.5280072552363078 Test Loss: 0.5329497884114583\n",
      "Epoch: 241 Training Loss: 0.5270580129623413 Test Loss: 0.5320187174479166\n",
      "Epoch: 242 Training Loss: 0.526119699160258 Test Loss: 0.5310635579427083\n",
      "Epoch: 243 Training Loss: 0.525183762550354 Test Loss: 0.5301509602864584\n",
      "Epoch: 244 Training Loss: 0.5242508862813314 Test Loss: 0.5292288411458334\n",
      "Epoch: 245 Training Loss: 0.5233322900136311 Test Loss: 0.5283109130859375\n",
      "Epoch: 246 Training Loss: 0.5224157266616821 Test Loss: 0.5274199625651042\n",
      "Epoch: 247 Training Loss: 0.5215031604766845 Test Loss: 0.5265277913411458\n",
      "Epoch: 248 Training Loss: 0.5206013428370158 Test Loss: 0.5256341552734375\n",
      "Epoch: 249 Training Loss: 0.5196988817850748 Test Loss: 0.5247976888020833\n",
      "Epoch: 250 Training Loss: 0.5188070116043091 Test Loss: 0.523905517578125\n",
      "Epoch: 251 Training Loss: 0.5179171501795451 Test Loss: 0.5230223388671875\n",
      "Epoch: 252 Training Loss: 0.5170276816685995 Test Loss: 0.5221150716145834\n",
      "Epoch: 253 Training Loss: 0.5161444180806478 Test Loss: 0.5212428385416666\n",
      "Epoch: 254 Training Loss: 0.5152689297993978 Test Loss: 0.5203918863932292\n",
      "Epoch: 255 Training Loss: 0.5143996537526448 Test Loss: 0.5195351969401042\n",
      "Epoch: 256 Training Loss: 0.5135370632807413 Test Loss: 0.5186776529947916\n",
      "Epoch: 257 Training Loss: 0.5126792297363282 Test Loss: 0.51784521484375\n",
      "Epoch: 258 Training Loss: 0.5118279027938842 Test Loss: 0.5169883626302083\n",
      "Epoch: 259 Training Loss: 0.5109807415008545 Test Loss: 0.5161472981770834\n",
      "Epoch: 260 Training Loss: 0.5101398855845134 Test Loss: 0.5152827555338542\n",
      "Epoch: 261 Training Loss: 0.5093055076599121 Test Loss: 0.5144449462890625\n",
      "Epoch: 262 Training Loss: 0.5084788465499878 Test Loss: 0.5136078694661458\n",
      "Epoch: 263 Training Loss: 0.5076569738388061 Test Loss: 0.5127906087239583\n",
      "Epoch: 264 Training Loss: 0.5068421541849772 Test Loss: 0.511974365234375\n",
      "Epoch: 265 Training Loss: 0.5060395720799764 Test Loss: 0.5111654459635416\n",
      "Epoch: 266 Training Loss: 0.5052318741480509 Test Loss: 0.5103748779296875\n",
      "Epoch: 267 Training Loss: 0.5044376211166381 Test Loss: 0.5096046142578124\n",
      "Epoch: 268 Training Loss: 0.5036484597524007 Test Loss: 0.5088274332682292\n",
      "Epoch: 269 Training Loss: 0.5028626044591268 Test Loss: 0.5080458984375\n",
      "Epoch: 270 Training Loss: 0.5020810194015503 Test Loss: 0.5072473551432292\n",
      "Epoch: 271 Training Loss: 0.5013069267272949 Test Loss: 0.5064768880208333\n",
      "Epoch: 272 Training Loss: 0.5005339034398397 Test Loss: 0.5057005208333333\n",
      "Epoch: 273 Training Loss: 0.49977454471588134 Test Loss: 0.5049686686197916\n",
      "Epoch: 274 Training Loss: 0.4990181976954142 Test Loss: 0.5042183430989583\n",
      "Epoch: 275 Training Loss: 0.4982679828008016 Test Loss: 0.5034925944010417\n",
      "Epoch: 276 Training Loss: 0.4975192092259725 Test Loss: 0.5027490641276041\n",
      "Epoch: 277 Training Loss: 0.49677605883280435 Test Loss: 0.5020147705078125\n",
      "Epoch: 278 Training Loss: 0.49603958733876546 Test Loss: 0.50128759765625\n",
      "Epoch: 279 Training Loss: 0.4953050133387248 Test Loss: 0.500576904296875\n",
      "Epoch: 280 Training Loss: 0.49457682196299235 Test Loss: 0.499863525390625\n",
      "Epoch: 281 Training Loss: 0.49385052363077797 Test Loss: 0.4991753743489583\n",
      "Epoch: 282 Training Loss: 0.49313096364339193 Test Loss: 0.4984420166015625\n",
      "Epoch: 283 Training Loss: 0.49241682720184327 Test Loss: 0.49776302083333335\n",
      "Epoch: 284 Training Loss: 0.4917130460739136 Test Loss: 0.4970731201171875\n",
      "Epoch: 285 Training Loss: 0.49101432418823243 Test Loss: 0.49639090983072914\n",
      "Epoch: 286 Training Loss: 0.49031349404652913 Test Loss: 0.4956885986328125\n",
      "Epoch: 287 Training Loss: 0.489622652053833 Test Loss: 0.495012939453125\n",
      "Epoch: 288 Training Loss: 0.48893370151519777 Test Loss: 0.4943367513020833\n",
      "Epoch: 289 Training Loss: 0.4882526572545369 Test Loss: 0.4936702473958333\n",
      "Epoch: 290 Training Loss: 0.4875795087814331 Test Loss: 0.4930165608723958\n",
      "Epoch: 291 Training Loss: 0.4869013598759969 Test Loss: 0.49233333333333335\n",
      "Epoch: 292 Training Loss: 0.4862322276433309 Test Loss: 0.4916754150390625\n",
      "Epoch: 293 Training Loss: 0.48556903266906737 Test Loss: 0.49101090494791666\n",
      "Epoch: 294 Training Loss: 0.4849068552652995 Test Loss: 0.49037337239583334\n",
      "Epoch: 295 Training Loss: 0.4842551657358805 Test Loss: 0.48972013346354165\n",
      "Epoch: 296 Training Loss: 0.4836075045267741 Test Loss: 0.48909318033854166\n",
      "Epoch: 297 Training Loss: 0.48296206951141357 Test Loss: 0.48842333984375\n",
      "Epoch: 298 Training Loss: 0.4823241999944051 Test Loss: 0.48777498372395833\n",
      "Epoch: 299 Training Loss: 0.4816855386098226 Test Loss: 0.4871593017578125\n",
      "Epoch: 300 Training Loss: 0.48105364513397214 Test Loss: 0.4865380859375\n",
      "Epoch: 301 Training Loss: 0.4804269641240438 Test Loss: 0.485917236328125\n",
      "Epoch: 302 Training Loss: 0.47980342515309654 Test Loss: 0.48527315266927085\n",
      "Epoch: 303 Training Loss: 0.4791800266901652 Test Loss: 0.484649169921875\n",
      "Epoch: 304 Training Loss: 0.47856939951578775 Test Loss: 0.4840633951822917\n",
      "Epoch: 305 Training Loss: 0.4779601936340332 Test Loss: 0.48349625651041667\n",
      "Epoch: 306 Training Loss: 0.47735991032918296 Test Loss: 0.4829170328776042\n",
      "Epoch: 307 Training Loss: 0.476752682685852 Test Loss: 0.48230167643229166\n",
      "Epoch: 308 Training Loss: 0.4761523847579956 Test Loss: 0.4817223714192708\n",
      "Epoch: 309 Training Loss: 0.47555911827087405 Test Loss: 0.4811242268880208\n",
      "Epoch: 310 Training Loss: 0.474969646135966 Test Loss: 0.48054313151041667\n",
      "Epoch: 311 Training Loss: 0.4743836622238159 Test Loss: 0.47996573893229166\n",
      "Epoch: 312 Training Loss: 0.47379896926879883 Test Loss: 0.4793707682291667\n",
      "Epoch: 313 Training Loss: 0.4732178522745768 Test Loss: 0.4788177490234375\n",
      "Epoch: 314 Training Loss: 0.47264023145039874 Test Loss: 0.478252685546875\n",
      "Epoch: 315 Training Loss: 0.4720661211013794 Test Loss: 0.477703857421875\n",
      "Epoch: 316 Training Loss: 0.4715035851796468 Test Loss: 0.47716349283854165\n",
      "Epoch: 317 Training Loss: 0.47093994267781575 Test Loss: 0.47658935546875\n",
      "Epoch: 318 Training Loss: 0.47038144334157306 Test Loss: 0.4760509847005208\n",
      "Epoch: 319 Training Loss: 0.4698262357711792 Test Loss: 0.47552994791666664\n",
      "Epoch: 320 Training Loss: 0.46927268886566165 Test Loss: 0.47497505696614584\n",
      "Epoch: 321 Training Loss: 0.4687193492253621 Test Loss: 0.47447416178385415\n",
      "Epoch: 322 Training Loss: 0.4681762285232544 Test Loss: 0.47391227213541665\n",
      "Epoch: 323 Training Loss: 0.4676327797571818 Test Loss: 0.4733988444010417\n",
      "Epoch: 324 Training Loss: 0.467097105662028 Test Loss: 0.472906005859375\n",
      "Epoch: 325 Training Loss: 0.46655818049112957 Test Loss: 0.4723878173828125\n",
      "Epoch: 326 Training Loss: 0.46602218691507974 Test Loss: 0.4718824869791667\n",
      "Epoch: 327 Training Loss: 0.4654945659637451 Test Loss: 0.471385009765625\n",
      "Epoch: 328 Training Loss: 0.4649657408396403 Test Loss: 0.47087996419270833\n",
      "Epoch: 329 Training Loss: 0.4644426952997843 Test Loss: 0.4703561197916667\n",
      "Epoch: 330 Training Loss: 0.46392499732971193 Test Loss: 0.46984806315104166\n",
      "Epoch: 331 Training Loss: 0.46340731779734295 Test Loss: 0.46934635416666665\n",
      "Epoch: 332 Training Loss: 0.46288996346791583 Test Loss: 0.4688302815755208\n",
      "Epoch: 333 Training Loss: 0.4623769648869832 Test Loss: 0.4683466796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 Training Loss: 0.46187434577941894 Test Loss: 0.46786197916666666\n",
      "Epoch: 335 Training Loss: 0.461368927637736 Test Loss: 0.46733878580729166\n",
      "Epoch: 336 Training Loss: 0.46086645921071373 Test Loss: 0.46685176595052086\n",
      "Epoch: 337 Training Loss: 0.46036612542470295 Test Loss: 0.4664010416666667\n",
      "Epoch: 338 Training Loss: 0.4598686606089274 Test Loss: 0.4659164632161458\n",
      "Epoch: 339 Training Loss: 0.4593755350112915 Test Loss: 0.4654668782552083\n",
      "Epoch: 340 Training Loss: 0.45888131427764894 Test Loss: 0.4650078125\n",
      "Epoch: 341 Training Loss: 0.4583953901926676 Test Loss: 0.4645235188802083\n",
      "Epoch: 342 Training Loss: 0.45790807469685874 Test Loss: 0.4640828857421875\n",
      "Epoch: 343 Training Loss: 0.45742477862040204 Test Loss: 0.46360611979166666\n",
      "Epoch: 344 Training Loss: 0.4569436632792155 Test Loss: 0.46314705403645834\n",
      "Epoch: 345 Training Loss: 0.4564623807271322 Test Loss: 0.46267338053385415\n",
      "Epoch: 346 Training Loss: 0.4559824851353963 Test Loss: 0.462253662109375\n",
      "Epoch: 347 Training Loss: 0.4555118128458659 Test Loss: 0.4617977294921875\n",
      "Epoch: 348 Training Loss: 0.45503934637705484 Test Loss: 0.4613448486328125\n",
      "Epoch: 349 Training Loss: 0.4545677941640218 Test Loss: 0.4609074300130208\n",
      "Epoch: 350 Training Loss: 0.45409959634145103 Test Loss: 0.46043851725260415\n",
      "Epoch: 351 Training Loss: 0.4536317777633667 Test Loss: 0.45997021484375\n",
      "Epoch: 352 Training Loss: 0.4531602128346761 Test Loss: 0.4595487467447917\n",
      "Epoch: 353 Training Loss: 0.4526991532643636 Test Loss: 0.4590916341145833\n",
      "Epoch: 354 Training Loss: 0.452240216255188 Test Loss: 0.4586563720703125\n",
      "Epoch: 355 Training Loss: 0.45178516737620034 Test Loss: 0.458219970703125\n",
      "Epoch: 356 Training Loss: 0.45132420190175376 Test Loss: 0.457796630859375\n",
      "Epoch: 357 Training Loss: 0.450870512008667 Test Loss: 0.45734842936197917\n",
      "Epoch: 358 Training Loss: 0.450416446685791 Test Loss: 0.4569326578776042\n",
      "Epoch: 359 Training Loss: 0.4499714021682739 Test Loss: 0.45650931803385414\n",
      "Epoch: 360 Training Loss: 0.44952327315012613 Test Loss: 0.456058349609375\n",
      "Epoch: 361 Training Loss: 0.4490773639678955 Test Loss: 0.4555865478515625\n",
      "Epoch: 362 Training Loss: 0.44863239987691245 Test Loss: 0.4551915690104167\n",
      "Epoch: 363 Training Loss: 0.4481868321100871 Test Loss: 0.4547646484375\n",
      "Epoch: 364 Training Loss: 0.44775001112620033 Test Loss: 0.4543580322265625\n",
      "Epoch: 365 Training Loss: 0.4473057902654012 Test Loss: 0.45392057291666665\n",
      "Epoch: 366 Training Loss: 0.44686913617451984 Test Loss: 0.4534996337890625\n",
      "Epoch: 367 Training Loss: 0.44643537553151447 Test Loss: 0.4530765380859375\n",
      "Epoch: 368 Training Loss: 0.4460002161661784 Test Loss: 0.45262736002604165\n",
      "Epoch: 369 Training Loss: 0.4455641120274862 Test Loss: 0.4522272135416667\n",
      "Epoch: 370 Training Loss: 0.4451388028462728 Test Loss: 0.451804931640625\n",
      "Epoch: 371 Training Loss: 0.4447131722768148 Test Loss: 0.4513846028645833\n",
      "Epoch: 372 Training Loss: 0.44428564484914146 Test Loss: 0.45094535319010415\n",
      "Epoch: 373 Training Loss: 0.44385605049133303 Test Loss: 0.4505431722005208\n",
      "Epoch: 374 Training Loss: 0.44343513520558675 Test Loss: 0.45011027018229166\n",
      "Epoch: 375 Training Loss: 0.44301546478271486 Test Loss: 0.4497005615234375\n",
      "Epoch: 376 Training Loss: 0.4425951919555664 Test Loss: 0.4493109944661458\n",
      "Epoch: 377 Training Loss: 0.4421713339487712 Test Loss: 0.44887939453125\n",
      "Epoch: 378 Training Loss: 0.4417556130091349 Test Loss: 0.44846549479166664\n",
      "Epoch: 379 Training Loss: 0.44133754062652586 Test Loss: 0.44811140950520834\n",
      "Epoch: 380 Training Loss: 0.440927801767985 Test Loss: 0.4477079264322917\n",
      "Epoch: 381 Training Loss: 0.44051974709828695 Test Loss: 0.447320068359375\n",
      "Epoch: 382 Training Loss: 0.4401077817281087 Test Loss: 0.4469208984375\n",
      "Epoch: 383 Training Loss: 0.4397017682393392 Test Loss: 0.4465264892578125\n",
      "Epoch: 384 Training Loss: 0.43929106839497883 Test Loss: 0.44611263020833336\n",
      "Epoch: 385 Training Loss: 0.43888817342122394 Test Loss: 0.44572526041666666\n",
      "Epoch: 386 Training Loss: 0.43848275343577064 Test Loss: 0.445348388671875\n",
      "Epoch: 387 Training Loss: 0.43807976945241295 Test Loss: 0.44496146647135415\n",
      "Epoch: 388 Training Loss: 0.4376798769632975 Test Loss: 0.4446199951171875\n",
      "Epoch: 389 Training Loss: 0.43727810605367023 Test Loss: 0.4442355143229167\n",
      "Epoch: 390 Training Loss: 0.4368768253326416 Test Loss: 0.4438582763671875\n",
      "Epoch: 391 Training Loss: 0.4364772504170736 Test Loss: 0.44347806803385414\n",
      "Epoch: 392 Training Loss: 0.4360804615020752 Test Loss: 0.44310270182291667\n",
      "Epoch: 393 Training Loss: 0.43568598047892254 Test Loss: 0.442748291015625\n",
      "Epoch: 394 Training Loss: 0.4352919635772705 Test Loss: 0.44239705403645835\n",
      "Epoch: 395 Training Loss: 0.43490143648783364 Test Loss: 0.4420227864583333\n",
      "Epoch: 396 Training Loss: 0.4345131622950236 Test Loss: 0.4416748046875\n",
      "Epoch: 397 Training Loss: 0.4341245603561401 Test Loss: 0.44128963216145833\n",
      "Epoch: 398 Training Loss: 0.43373563385009767 Test Loss: 0.440911376953125\n",
      "Epoch: 399 Training Loss: 0.4333508014678955 Test Loss: 0.440541748046875\n",
      "Epoch: 400 Training Loss: 0.43296482022603355 Test Loss: 0.440189453125\n",
      "Epoch: 401 Training Loss: 0.4325767726898193 Test Loss: 0.43981595865885414\n",
      "Epoch: 402 Training Loss: 0.43218799591064455 Test Loss: 0.43944840494791665\n",
      "Epoch: 403 Training Loss: 0.4318051350911458 Test Loss: 0.43908573404947915\n",
      "Epoch: 404 Training Loss: 0.43141996892293294 Test Loss: 0.43871280924479167\n",
      "Epoch: 405 Training Loss: 0.4310417887369792 Test Loss: 0.4383821614583333\n",
      "Epoch: 406 Training Loss: 0.43066317844390867 Test Loss: 0.43799812825520834\n",
      "Epoch: 407 Training Loss: 0.4302814032236735 Test Loss: 0.437666259765625\n",
      "Epoch: 408 Training Loss: 0.4299040600458781 Test Loss: 0.43726021321614583\n",
      "Epoch: 409 Training Loss: 0.429530886332194 Test Loss: 0.4369004720052083\n",
      "Epoch: 410 Training Loss: 0.42915263748168947 Test Loss: 0.4365362955729167\n",
      "Epoch: 411 Training Loss: 0.4287816508611043 Test Loss: 0.4362235921223958\n",
      "Epoch: 412 Training Loss: 0.42841034730275473 Test Loss: 0.43582137044270836\n",
      "Epoch: 413 Training Loss: 0.4280386323928833 Test Loss: 0.43550370279947914\n",
      "Epoch: 414 Training Loss: 0.4276682449976603 Test Loss: 0.43511226399739583\n",
      "Epoch: 415 Training Loss: 0.4272949256896973 Test Loss: 0.4348037923177083\n",
      "Epoch: 416 Training Loss: 0.4269253886540731 Test Loss: 0.4344208984375\n",
      "Epoch: 417 Training Loss: 0.42656130727132163 Test Loss: 0.43406770833333336\n",
      "Epoch: 418 Training Loss: 0.4261928243637085 Test Loss: 0.43374112955729166\n",
      "Epoch: 419 Training Loss: 0.4258301394780477 Test Loss: 0.4333942464192708\n",
      "Epoch: 420 Training Loss: 0.4254666748046875 Test Loss: 0.43304756673177086\n",
      "Epoch: 421 Training Loss: 0.4251044158935547 Test Loss: 0.4327001953125\n",
      "Epoch: 422 Training Loss: 0.4247397969563802 Test Loss: 0.43234134928385415\n",
      "Epoch: 423 Training Loss: 0.42437907695770266 Test Loss: 0.43203926595052083\n",
      "Epoch: 424 Training Loss: 0.4240242900848389 Test Loss: 0.4316970621744792\n",
      "Epoch: 425 Training Loss: 0.4236663360595703 Test Loss: 0.4313763834635417\n",
      "Epoch: 426 Training Loss: 0.42330732186635334 Test Loss: 0.431043701171875\n",
      "Epoch: 427 Training Loss: 0.42295164680480957 Test Loss: 0.43068888346354167\n",
      "Epoch: 428 Training Loss: 0.4225942017237345 Test Loss: 0.4303604736328125\n",
      "Epoch: 429 Training Loss: 0.4222414286931356 Test Loss: 0.43001416015625\n",
      "Epoch: 430 Training Loss: 0.4218899405797323 Test Loss: 0.42968448893229166\n",
      "Epoch: 431 Training Loss: 0.4215403973261515 Test Loss: 0.4294031982421875\n",
      "Epoch: 432 Training Loss: 0.4211946045557658 Test Loss: 0.42903059895833334\n",
      "Epoch: 433 Training Loss: 0.42084331639607747 Test Loss: 0.4287182210286458\n",
      "Epoch: 434 Training Loss: 0.42049195925394695 Test Loss: 0.428362060546875\n",
      "Epoch: 435 Training Loss: 0.42014569632212323 Test Loss: 0.42805338541666665\n",
      "Epoch: 436 Training Loss: 0.4197998622258504 Test Loss: 0.4277370198567708\n",
      "Epoch: 437 Training Loss: 0.4194572490056356 Test Loss: 0.427435791015625\n",
      "Epoch: 438 Training Loss: 0.4191141974131266 Test Loss: 0.4271209716796875\n",
      "Epoch: 439 Training Loss: 0.4187740882237752 Test Loss: 0.4268015950520833\n",
      "Epoch: 440 Training Loss: 0.4184281088511149 Test Loss: 0.4264559733072917\n",
      "Epoch: 441 Training Loss: 0.41808341630299883 Test Loss: 0.4261456705729167\n",
      "Epoch: 442 Training Loss: 0.4177415466308594 Test Loss: 0.425811767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 443 Training Loss: 0.41740789826711017 Test Loss: 0.4255026041666667\n",
      "Epoch: 444 Training Loss: 0.41706933816274006 Test Loss: 0.4251857503255208\n",
      "Epoch: 445 Training Loss: 0.4167355457941691 Test Loss: 0.42487691243489584\n",
      "Epoch: 446 Training Loss: 0.4164035237630208 Test Loss: 0.424559814453125\n",
      "Epoch: 447 Training Loss: 0.41607265663146975 Test Loss: 0.424246337890625\n",
      "Epoch: 448 Training Loss: 0.4157440430323283 Test Loss: 0.4239068196614583\n",
      "Epoch: 449 Training Loss: 0.41541403738657634 Test Loss: 0.4236280110677083\n",
      "Epoch: 450 Training Loss: 0.415090137163798 Test Loss: 0.42331095377604167\n",
      "Epoch: 451 Training Loss: 0.4147654145558675 Test Loss: 0.42298042805989583\n",
      "Epoch: 452 Training Loss: 0.4144431765874227 Test Loss: 0.4226470133463542\n",
      "Epoch: 453 Training Loss: 0.4141162748336792 Test Loss: 0.4223199462890625\n",
      "Epoch: 454 Training Loss: 0.41379471524556477 Test Loss: 0.42203173828125\n",
      "Epoch: 455 Training Loss: 0.4134751482009888 Test Loss: 0.42170992024739584\n",
      "Epoch: 456 Training Loss: 0.4131599397659302 Test Loss: 0.42141646321614584\n",
      "Epoch: 457 Training Loss: 0.4128399499257406 Test Loss: 0.42113956705729166\n",
      "Epoch: 458 Training Loss: 0.412521603902181 Test Loss: 0.4208187255859375\n",
      "Epoch: 459 Training Loss: 0.41220691649119057 Test Loss: 0.42049007161458335\n",
      "Epoch: 460 Training Loss: 0.41189294497172035 Test Loss: 0.42018827311197915\n",
      "Epoch: 461 Training Loss: 0.4115822903315226 Test Loss: 0.41992667643229165\n",
      "Epoch: 462 Training Loss: 0.41127279154459634 Test Loss: 0.4195753580729167\n",
      "Epoch: 463 Training Loss: 0.41096386973063154 Test Loss: 0.41929654947916667\n",
      "Epoch: 464 Training Loss: 0.41065257994333904 Test Loss: 0.41901261393229167\n",
      "Epoch: 465 Training Loss: 0.41034274514516195 Test Loss: 0.41868534342447916\n",
      "Epoch: 466 Training Loss: 0.4100337209701538 Test Loss: 0.4184156901041667\n",
      "Epoch: 467 Training Loss: 0.40973044617970783 Test Loss: 0.41811576334635414\n",
      "Epoch: 468 Training Loss: 0.40942245642344155 Test Loss: 0.41781852213541665\n",
      "Epoch: 469 Training Loss: 0.40911980978647866 Test Loss: 0.4175008544921875\n",
      "Epoch: 470 Training Loss: 0.4088202393849691 Test Loss: 0.417216796875\n",
      "Epoch: 471 Training Loss: 0.4085178826649984 Test Loss: 0.4169119466145833\n",
      "Epoch: 472 Training Loss: 0.40822002601623536 Test Loss: 0.416635498046875\n",
      "Epoch: 473 Training Loss: 0.40792216046651203 Test Loss: 0.4163395589192708\n",
      "Epoch: 474 Training Loss: 0.4076268138885498 Test Loss: 0.416077392578125\n",
      "Epoch: 475 Training Loss: 0.40733126068115233 Test Loss: 0.41575130208333333\n",
      "Epoch: 476 Training Loss: 0.40703667736053467 Test Loss: 0.41549003092447917\n",
      "Epoch: 477 Training Loss: 0.40674559529622395 Test Loss: 0.41517647298177085\n",
      "Epoch: 478 Training Loss: 0.4064490172068278 Test Loss: 0.41491227213541665\n",
      "Epoch: 479 Training Loss: 0.4061589584350586 Test Loss: 0.41459733072916666\n",
      "Epoch: 480 Training Loss: 0.4058662462234497 Test Loss: 0.41432511393229166\n",
      "Epoch: 481 Training Loss: 0.40557498264312747 Test Loss: 0.4140377197265625\n",
      "Epoch: 482 Training Loss: 0.40528521410624185 Test Loss: 0.4137459716796875\n",
      "Epoch: 483 Training Loss: 0.40500241152445476 Test Loss: 0.413500732421875\n",
      "Epoch: 484 Training Loss: 0.40471794732411703 Test Loss: 0.41321138509114586\n",
      "Epoch: 485 Training Loss: 0.4044286467234294 Test Loss: 0.41291536458333333\n",
      "Epoch: 486 Training Loss: 0.4041436529159546 Test Loss: 0.41262581380208335\n",
      "Epoch: 487 Training Loss: 0.40385747941335043 Test Loss: 0.41233902994791666\n",
      "Epoch: 488 Training Loss: 0.4035721966425578 Test Loss: 0.41207747395833333\n",
      "Epoch: 489 Training Loss: 0.4032903814315796 Test Loss: 0.4117777099609375\n",
      "Epoch: 490 Training Loss: 0.40301199340820315 Test Loss: 0.41149715169270834\n",
      "Epoch: 491 Training Loss: 0.40272918955485026 Test Loss: 0.41121785481770834\n",
      "Epoch: 492 Training Loss: 0.402448743502299 Test Loss: 0.41090938313802083\n",
      "Epoch: 493 Training Loss: 0.4021723648707072 Test Loss: 0.41062076822916665\n",
      "Epoch: 494 Training Loss: 0.4018927415211995 Test Loss: 0.41035432942708333\n",
      "Epoch: 495 Training Loss: 0.4016137714385986 Test Loss: 0.41003849283854166\n",
      "Epoch: 496 Training Loss: 0.4013392251332601 Test Loss: 0.40981591796875\n",
      "Epoch: 497 Training Loss: 0.4010667117436727 Test Loss: 0.4095305989583333\n",
      "Epoch: 498 Training Loss: 0.400789755821228 Test Loss: 0.4092737223307292\n",
      "Epoch: 499 Training Loss: 0.40051723035176595 Test Loss: 0.4090225016276042\n",
      "Epoch: 500 Training Loss: 0.4002454140981038 Test Loss: 0.40871781412760416\n",
      "Epoch: 501 Training Loss: 0.3999683561325073 Test Loss: 0.40846622721354164\n",
      "Epoch: 502 Training Loss: 0.39970204734802245 Test Loss: 0.4081995849609375\n",
      "Epoch: 503 Training Loss: 0.3994343382517497 Test Loss: 0.40794632975260414\n",
      "Epoch: 504 Training Loss: 0.39916331322987875 Test Loss: 0.4076761881510417\n",
      "Epoch: 505 Training Loss: 0.3988939781188965 Test Loss: 0.40742928059895833\n",
      "Epoch: 506 Training Loss: 0.39862797419230145 Test Loss: 0.40716276041666666\n",
      "Epoch: 507 Training Loss: 0.398360857963562 Test Loss: 0.4069016927083333\n",
      "Epoch: 508 Training Loss: 0.3980927314758301 Test Loss: 0.406635498046875\n",
      "Epoch: 509 Training Loss: 0.39782138029734293 Test Loss: 0.40637190755208336\n",
      "Epoch: 510 Training Loss: 0.3975564022064209 Test Loss: 0.40610835774739584\n",
      "Epoch: 511 Training Loss: 0.3972903807957967 Test Loss: 0.40583642578125\n",
      "Epoch: 512 Training Loss: 0.397021915435791 Test Loss: 0.40554361979166664\n",
      "Epoch: 513 Training Loss: 0.3967578337987264 Test Loss: 0.40529890950520836\n",
      "Epoch: 514 Training Loss: 0.3964938936233521 Test Loss: 0.405039306640625\n",
      "Epoch: 515 Training Loss: 0.3962321761449178 Test Loss: 0.4047867024739583\n",
      "Epoch: 516 Training Loss: 0.39597233645121255 Test Loss: 0.40452726236979164\n",
      "Epoch: 517 Training Loss: 0.3957127313613892 Test Loss: 0.4042568359375\n",
      "Epoch: 518 Training Loss: 0.3954566561381022 Test Loss: 0.4039945475260417\n",
      "Epoch: 519 Training Loss: 0.3951973333358765 Test Loss: 0.403727294921875\n",
      "Epoch: 520 Training Loss: 0.39493995761871337 Test Loss: 0.4034375406901042\n",
      "Epoch: 521 Training Loss: 0.3946908276875814 Test Loss: 0.403201904296875\n",
      "Epoch: 522 Training Loss: 0.3944352785746256 Test Loss: 0.4029594319661458\n",
      "Epoch: 523 Training Loss: 0.39418268807729084 Test Loss: 0.4026920166015625\n",
      "Epoch: 524 Training Loss: 0.393923770904541 Test Loss: 0.40244905598958336\n",
      "Epoch: 525 Training Loss: 0.3936729618708293 Test Loss: 0.4021866455078125\n",
      "Epoch: 526 Training Loss: 0.39341667079925535 Test Loss: 0.40191642252604165\n",
      "Epoch: 527 Training Loss: 0.39315673859914146 Test Loss: 0.4016214599609375\n",
      "Epoch: 528 Training Loss: 0.39288472398122154 Test Loss: 0.40134521484375\n",
      "Epoch: 529 Training Loss: 0.39261192162831626 Test Loss: 0.401088134765625\n",
      "Epoch: 530 Training Loss: 0.3923345028559367 Test Loss: 0.40081245930989584\n",
      "Epoch: 531 Training Loss: 0.3920625982284546 Test Loss: 0.40051298014322917\n",
      "Epoch: 532 Training Loss: 0.3917867835362752 Test Loss: 0.40026888020833334\n",
      "Epoch: 533 Training Loss: 0.3915166034698486 Test Loss: 0.39997705078125\n",
      "Epoch: 534 Training Loss: 0.39124253908793133 Test Loss: 0.39969287109375\n",
      "Epoch: 535 Training Loss: 0.3909694617589315 Test Loss: 0.39941634114583335\n",
      "Epoch: 536 Training Loss: 0.3906946147282918 Test Loss: 0.39914835611979166\n",
      "Epoch: 537 Training Loss: 0.3904296515782674 Test Loss: 0.3988764241536458\n",
      "Epoch: 538 Training Loss: 0.39015916442871096 Test Loss: 0.3985989583333333\n",
      "Epoch: 539 Training Loss: 0.38989340591430666 Test Loss: 0.3983430989583333\n",
      "Epoch: 540 Training Loss: 0.3896315603256226 Test Loss: 0.3980772705078125\n",
      "Epoch: 541 Training Loss: 0.3893656415939331 Test Loss: 0.39779121907552084\n",
      "Epoch: 542 Training Loss: 0.38910274537404377 Test Loss: 0.3975330403645833\n",
      "Epoch: 543 Training Loss: 0.38884197870890297 Test Loss: 0.3972960205078125\n",
      "Epoch: 544 Training Loss: 0.38858784929911294 Test Loss: 0.39701517740885417\n",
      "Epoch: 545 Training Loss: 0.38833131663004555 Test Loss: 0.3967542724609375\n",
      "Epoch: 546 Training Loss: 0.3880750764211019 Test Loss: 0.3964932454427083\n",
      "Epoch: 547 Training Loss: 0.3878209988276164 Test Loss: 0.3962344563802083\n",
      "Epoch: 548 Training Loss: 0.38756854565938315 Test Loss: 0.39594677734375\n",
      "Epoch: 549 Training Loss: 0.387322701772054 Test Loss: 0.39572574869791666\n",
      "Epoch: 550 Training Loss: 0.38707117875417074 Test Loss: 0.39546329752604165\n",
      "Epoch: 551 Training Loss: 0.38682675584157306 Test Loss: 0.3952225748697917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 552 Training Loss: 0.38658370049794516 Test Loss: 0.3950018717447917\n",
      "Epoch: 553 Training Loss: 0.38633919207255046 Test Loss: 0.39475537109375\n",
      "Epoch: 554 Training Loss: 0.3860962047576904 Test Loss: 0.39451460774739583\n",
      "Epoch: 555 Training Loss: 0.3858591210047404 Test Loss: 0.3943172200520833\n",
      "Epoch: 556 Training Loss: 0.3856165132522583 Test Loss: 0.3940420735677083\n",
      "Epoch: 557 Training Loss: 0.38538214842478435 Test Loss: 0.39380375162760417\n",
      "Epoch: 558 Training Loss: 0.385146728515625 Test Loss: 0.39359505208333334\n",
      "Epoch: 559 Training Loss: 0.38491295973459877 Test Loss: 0.39335404459635415\n",
      "Epoch: 560 Training Loss: 0.38467660649617513 Test Loss: 0.3931280517578125\n",
      "Epoch: 561 Training Loss: 0.3844514598846436 Test Loss: 0.39288997395833336\n",
      "Epoch: 562 Training Loss: 0.38421805572509765 Test Loss: 0.3926287434895833\n",
      "Epoch: 563 Training Loss: 0.3839892864227295 Test Loss: 0.39243400065104167\n",
      "Epoch: 564 Training Loss: 0.38376285807291666 Test Loss: 0.3922289632161458\n",
      "Epoch: 565 Training Loss: 0.38353417746225993 Test Loss: 0.39202962239583333\n",
      "Epoch: 566 Training Loss: 0.38330740642547606 Test Loss: 0.39179402669270835\n",
      "Epoch: 567 Training Loss: 0.3830763336817424 Test Loss: 0.39157861328125\n",
      "Epoch: 568 Training Loss: 0.38285151799519856 Test Loss: 0.39133414713541664\n",
      "Epoch: 569 Training Loss: 0.38262997023264567 Test Loss: 0.39113004557291664\n",
      "Epoch: 570 Training Loss: 0.38240465195973716 Test Loss: 0.3908966064453125\n",
      "Epoch: 571 Training Loss: 0.38217872714996337 Test Loss: 0.39066451009114583\n",
      "Epoch: 572 Training Loss: 0.3819526586532593 Test Loss: 0.3904422607421875\n",
      "Epoch: 573 Training Loss: 0.38173067156473794 Test Loss: 0.390220947265625\n",
      "Epoch: 574 Training Loss: 0.38151307932535805 Test Loss: 0.3899990234375\n",
      "Epoch: 575 Training Loss: 0.38129037539164223 Test Loss: 0.38976265462239584\n",
      "Epoch: 576 Training Loss: 0.3810721632639567 Test Loss: 0.38954833984375\n",
      "Epoch: 577 Training Loss: 0.3808530829747518 Test Loss: 0.38933162434895835\n",
      "Epoch: 578 Training Loss: 0.3806360737482707 Test Loss: 0.3891194661458333\n",
      "Epoch: 579 Training Loss: 0.3804201488494873 Test Loss: 0.3889376627604167\n",
      "Epoch: 580 Training Loss: 0.38020659573872884 Test Loss: 0.3886731770833333\n",
      "Epoch: 581 Training Loss: 0.37998524379730225 Test Loss: 0.3884716796875\n",
      "Epoch: 582 Training Loss: 0.3797759920756022 Test Loss: 0.3882769368489583\n",
      "Epoch: 583 Training Loss: 0.37956106185913085 Test Loss: 0.3880705973307292\n",
      "Epoch: 584 Training Loss: 0.37934587478637694 Test Loss: 0.38787101236979166\n",
      "Epoch: 585 Training Loss: 0.37913321145375567 Test Loss: 0.38761893717447915\n",
      "Epoch: 586 Training Loss: 0.37891960366566974 Test Loss: 0.3874381510416667\n",
      "Epoch: 587 Training Loss: 0.37870575618743896 Test Loss: 0.3872141520182292\n",
      "Epoch: 588 Training Loss: 0.37849991607666017 Test Loss: 0.3870077718098958\n",
      "Epoch: 589 Training Loss: 0.37829113801320396 Test Loss: 0.38680464680989585\n",
      "Epoch: 590 Training Loss: 0.37808647060394285 Test Loss: 0.38661600748697916\n",
      "Epoch: 591 Training Loss: 0.377876514116923 Test Loss: 0.38637679036458333\n",
      "Epoch: 592 Training Loss: 0.37766897360483803 Test Loss: 0.3861954345703125\n",
      "Epoch: 593 Training Loss: 0.3774683361053467 Test Loss: 0.38603446451822915\n",
      "Epoch: 594 Training Loss: 0.37726135540008543 Test Loss: 0.38581571451822916\n",
      "Epoch: 595 Training Loss: 0.3770589205423991 Test Loss: 0.385667724609375\n",
      "Epoch: 596 Training Loss: 0.37685649998982745 Test Loss: 0.38542049153645835\n",
      "Epoch: 597 Training Loss: 0.3766492172876994 Test Loss: 0.38523343912760416\n",
      "Epoch: 598 Training Loss: 0.3764447348912557 Test Loss: 0.38503715006510414\n",
      "Epoch: 599 Training Loss: 0.3762444610595703 Test Loss: 0.38488199869791667\n",
      "Epoch: 600 Training Loss: 0.3760420370101929 Test Loss: 0.38465250651041666\n",
      "Epoch: 601 Training Loss: 0.37583442878723144 Test Loss: 0.38445035807291666\n",
      "Epoch: 602 Training Loss: 0.375634867032369 Test Loss: 0.384252685546875\n",
      "Epoch: 603 Training Loss: 0.37543590863545734 Test Loss: 0.3840544840494792\n",
      "Epoch: 604 Training Loss: 0.3752354237238566 Test Loss: 0.3838658447265625\n",
      "Epoch: 605 Training Loss: 0.3750369421641032 Test Loss: 0.38366792805989586\n",
      "Epoch: 606 Training Loss: 0.3748365691502889 Test Loss: 0.3834761555989583\n",
      "Epoch: 607 Training Loss: 0.3746330509185791 Test Loss: 0.3832252197265625\n",
      "Epoch: 608 Training Loss: 0.3744311208724976 Test Loss: 0.3830935465494792\n",
      "Epoch: 609 Training Loss: 0.374237779935201 Test Loss: 0.38284513346354165\n",
      "Epoch: 610 Training Loss: 0.37404270458221434 Test Loss: 0.3826415608723958\n",
      "Epoch: 611 Training Loss: 0.3738450037638346 Test Loss: 0.38244441731770834\n",
      "Epoch: 612 Training Loss: 0.373651047706604 Test Loss: 0.38228011067708334\n",
      "Epoch: 613 Training Loss: 0.373461353302002 Test Loss: 0.38210782877604166\n",
      "Epoch: 614 Training Loss: 0.373268409093221 Test Loss: 0.38190128580729166\n",
      "Epoch: 615 Training Loss: 0.37307839171091717 Test Loss: 0.38171415201822917\n",
      "Epoch: 616 Training Loss: 0.37288671906789145 Test Loss: 0.3815115152994792\n",
      "Epoch: 617 Training Loss: 0.37269413661956785 Test Loss: 0.38138008626302083\n",
      "Epoch: 618 Training Loss: 0.3725032294591268 Test Loss: 0.38118009440104167\n",
      "Epoch: 619 Training Loss: 0.3723124993642171 Test Loss: 0.380948974609375\n",
      "Epoch: 620 Training Loss: 0.37212497266133626 Test Loss: 0.38076334635416664\n",
      "Epoch: 621 Training Loss: 0.37192838923136395 Test Loss: 0.3805986328125\n",
      "Epoch: 622 Training Loss: 0.3717471590042114 Test Loss: 0.38041923014322915\n",
      "Epoch: 623 Training Loss: 0.3715619894663493 Test Loss: 0.38022517903645836\n",
      "Epoch: 624 Training Loss: 0.3713699884414673 Test Loss: 0.380076904296875\n",
      "Epoch: 625 Training Loss: 0.37118724727630614 Test Loss: 0.3798997802734375\n",
      "Epoch: 626 Training Loss: 0.3710010871887207 Test Loss: 0.3796864013671875\n",
      "Epoch: 627 Training Loss: 0.3708148120244344 Test Loss: 0.3795019938151042\n",
      "Epoch: 628 Training Loss: 0.3706302890777588 Test Loss: 0.37933272298177084\n",
      "Epoch: 629 Training Loss: 0.3704466247558594 Test Loss: 0.37915470377604166\n",
      "Epoch: 630 Training Loss: 0.37025887139638264 Test Loss: 0.37898116048177083\n",
      "Epoch: 631 Training Loss: 0.3700777972539266 Test Loss: 0.37878369140625\n",
      "Epoch: 632 Training Loss: 0.3698962227503459 Test Loss: 0.378610595703125\n",
      "Epoch: 633 Training Loss: 0.36971176020304364 Test Loss: 0.3783816731770833\n",
      "Epoch: 634 Training Loss: 0.36953414154052733 Test Loss: 0.37822705078125\n",
      "Epoch: 635 Training Loss: 0.3693523906071981 Test Loss: 0.37805322265625\n",
      "Epoch: 636 Training Loss: 0.3691703961690267 Test Loss: 0.3778828531901042\n",
      "Epoch: 637 Training Loss: 0.36899031448364256 Test Loss: 0.3777108154296875\n",
      "Epoch: 638 Training Loss: 0.36881268374125165 Test Loss: 0.3775581868489583\n",
      "Epoch: 639 Training Loss: 0.36863694254557294 Test Loss: 0.3773609619140625\n",
      "Epoch: 640 Training Loss: 0.36845850690205895 Test Loss: 0.37720536295572915\n",
      "Epoch: 641 Training Loss: 0.3682823689778646 Test Loss: 0.37704878743489584\n",
      "Epoch: 642 Training Loss: 0.3681053949991862 Test Loss: 0.37688883463541667\n",
      "Epoch: 643 Training Loss: 0.36792341168721515 Test Loss: 0.37672111002604164\n",
      "Epoch: 644 Training Loss: 0.3677547279993693 Test Loss: 0.3765694173177083\n",
      "Epoch: 645 Training Loss: 0.3675779523849487 Test Loss: 0.37638704427083336\n",
      "Epoch: 646 Training Loss: 0.3674099938074748 Test Loss: 0.37619645182291667\n",
      "Epoch: 647 Training Loss: 0.36723367277781166 Test Loss: 0.3760762939453125\n",
      "Epoch: 648 Training Loss: 0.36705668512980144 Test Loss: 0.3758720703125\n",
      "Epoch: 649 Training Loss: 0.36688879776000977 Test Loss: 0.3757286376953125\n",
      "Epoch: 650 Training Loss: 0.36671325492858886 Test Loss: 0.3755687255859375\n",
      "Epoch: 651 Training Loss: 0.3665407292048136 Test Loss: 0.3754491373697917\n",
      "Epoch: 652 Training Loss: 0.36637252108256024 Test Loss: 0.375208251953125\n",
      "Epoch: 653 Training Loss: 0.36619756603240966 Test Loss: 0.3750732014973958\n",
      "Epoch: 654 Training Loss: 0.3660299727121989 Test Loss: 0.3749486083984375\n",
      "Epoch: 655 Training Loss: 0.36586182785034177 Test Loss: 0.3747474772135417\n",
      "Epoch: 656 Training Loss: 0.36569336700439453 Test Loss: 0.37462019856770834\n",
      "Epoch: 657 Training Loss: 0.36552414576212566 Test Loss: 0.374415283203125\n",
      "Epoch: 658 Training Loss: 0.3653590030670166 Test Loss: 0.37427482096354164\n",
      "Epoch: 659 Training Loss: 0.36519312286376954 Test Loss: 0.3741162109375\n",
      "Epoch: 660 Training Loss: 0.3650258591969808 Test Loss: 0.3739518229166667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 661 Training Loss: 0.3648635918299357 Test Loss: 0.3738204345703125\n",
      "Epoch: 662 Training Loss: 0.36470140870412193 Test Loss: 0.3736406656901042\n",
      "Epoch: 663 Training Loss: 0.3645335054397583 Test Loss: 0.3735082194010417\n",
      "Epoch: 664 Training Loss: 0.3643662627538045 Test Loss: 0.3733564860026042\n",
      "Epoch: 665 Training Loss: 0.3642005608876546 Test Loss: 0.37319340006510415\n",
      "Epoch: 666 Training Loss: 0.3640363661448161 Test Loss: 0.37303861490885415\n",
      "Epoch: 667 Training Loss: 0.36387720521291095 Test Loss: 0.37289510091145833\n",
      "Epoch: 668 Training Loss: 0.3637124401728312 Test Loss: 0.372789306640625\n",
      "Epoch: 669 Training Loss: 0.36355278301239013 Test Loss: 0.3725732421875\n",
      "Epoch: 670 Training Loss: 0.3633905814488729 Test Loss: 0.37245943196614584\n",
      "Epoch: 671 Training Loss: 0.363232019106547 Test Loss: 0.3723009033203125\n",
      "Epoch: 672 Training Loss: 0.36306945514678957 Test Loss: 0.3721579182942708\n",
      "Epoch: 673 Training Loss: 0.3629091265996297 Test Loss: 0.3720257568359375\n",
      "Epoch: 674 Training Loss: 0.3627484474182129 Test Loss: 0.3718436686197917\n",
      "Epoch: 675 Training Loss: 0.3625920689900716 Test Loss: 0.3717066650390625\n",
      "Epoch: 676 Training Loss: 0.3624305925369263 Test Loss: 0.3715308837890625\n",
      "Epoch: 677 Training Loss: 0.3622740182876587 Test Loss: 0.37137801106770835\n",
      "Epoch: 678 Training Loss: 0.3621147123972575 Test Loss: 0.3712233479817708\n",
      "Epoch: 679 Training Loss: 0.3619551347096761 Test Loss: 0.3711255696614583\n",
      "Epoch: 680 Training Loss: 0.36180262883504233 Test Loss: 0.370953857421875\n",
      "Epoch: 681 Training Loss: 0.3616510915756226 Test Loss: 0.3708202311197917\n",
      "Epoch: 682 Training Loss: 0.36149073028564455 Test Loss: 0.3706529947916667\n",
      "Epoch: 683 Training Loss: 0.3613384682337443 Test Loss: 0.37053861490885415\n",
      "Epoch: 684 Training Loss: 0.3611766481399536 Test Loss: 0.37034639485677084\n",
      "Epoch: 685 Training Loss: 0.36102589130401613 Test Loss: 0.3702412109375\n",
      "Epoch: 686 Training Loss: 0.36087028630574547 Test Loss: 0.37008650716145836\n",
      "Epoch: 687 Training Loss: 0.3607172390619914 Test Loss: 0.3699670003255208\n",
      "Epoch: 688 Training Loss: 0.3605597111384074 Test Loss: 0.3697799479166667\n",
      "Epoch: 689 Training Loss: 0.36040565236409505 Test Loss: 0.3696813151041667\n",
      "Epoch: 690 Training Loss: 0.36025675423940023 Test Loss: 0.3695042724609375\n",
      "Epoch: 691 Training Loss: 0.3601034282048543 Test Loss: 0.36936258951822915\n",
      "Epoch: 692 Training Loss: 0.3599549868901571 Test Loss: 0.3692384033203125\n",
      "Epoch: 693 Training Loss: 0.359806892712911 Test Loss: 0.36907478841145835\n",
      "Epoch: 694 Training Loss: 0.35964987754821776 Test Loss: 0.3689398600260417\n",
      "Epoch: 695 Training Loss: 0.3594979578653971 Test Loss: 0.3687711181640625\n",
      "Epoch: 696 Training Loss: 0.3593456042607625 Test Loss: 0.36862841796875\n",
      "Epoch: 697 Training Loss: 0.35919776471455894 Test Loss: 0.3684700927734375\n",
      "Epoch: 698 Training Loss: 0.35904921213785806 Test Loss: 0.36839680989583334\n",
      "Epoch: 699 Training Loss: 0.358895840326945 Test Loss: 0.36822281901041665\n",
      "Epoch: 700 Training Loss: 0.3587478132247925 Test Loss: 0.368047607421875\n",
      "Epoch: 701 Training Loss: 0.35859902318318687 Test Loss: 0.367945068359375\n",
      "Epoch: 702 Training Loss: 0.3584520689646403 Test Loss: 0.3678089192708333\n",
      "Epoch: 703 Training Loss: 0.3583065722783407 Test Loss: 0.36766353352864584\n",
      "Epoch: 704 Training Loss: 0.35816201464335123 Test Loss: 0.367544189453125\n",
      "Epoch: 705 Training Loss: 0.3580149914423625 Test Loss: 0.36738789876302086\n",
      "Epoch: 706 Training Loss: 0.35786936251322427 Test Loss: 0.3672437744140625\n",
      "Epoch: 707 Training Loss: 0.35771964009602863 Test Loss: 0.3671582845052083\n",
      "Epoch: 708 Training Loss: 0.35758097712198894 Test Loss: 0.3669560546875\n",
      "Epoch: 709 Training Loss: 0.3574377177556356 Test Loss: 0.36684334309895833\n",
      "Epoch: 710 Training Loss: 0.35729424381256103 Test Loss: 0.366697265625\n",
      "Epoch: 711 Training Loss: 0.35714508883158363 Test Loss: 0.36656306966145835\n",
      "Epoch: 712 Training Loss: 0.35700490601857504 Test Loss: 0.36642472330729164\n",
      "Epoch: 713 Training Loss: 0.35686167812347414 Test Loss: 0.36631229654947917\n",
      "Epoch: 714 Training Loss: 0.3567186269760132 Test Loss: 0.36612654622395835\n",
      "Epoch: 715 Training Loss: 0.3565750322341919 Test Loss: 0.36602156575520833\n",
      "Epoch: 716 Training Loss: 0.3564326575597127 Test Loss: 0.36590804036458335\n",
      "Epoch: 717 Training Loss: 0.3562921263376872 Test Loss: 0.36576680501302083\n",
      "Epoch: 718 Training Loss: 0.35614963658650717 Test Loss: 0.3656335042317708\n",
      "Epoch: 719 Training Loss: 0.35600319163004557 Test Loss: 0.36550809733072914\n",
      "Epoch: 720 Training Loss: 0.3558659515380859 Test Loss: 0.3653743896484375\n",
      "Epoch: 721 Training Loss: 0.35572400665283205 Test Loss: 0.3652025553385417\n",
      "Epoch: 722 Training Loss: 0.35558403364817304 Test Loss: 0.3651011962890625\n",
      "Epoch: 723 Training Loss: 0.355442987759908 Test Loss: 0.3649068196614583\n",
      "Epoch: 724 Training Loss: 0.35530030155181885 Test Loss: 0.364783203125\n",
      "Epoch: 725 Training Loss: 0.35516239643096925 Test Loss: 0.36467960611979167\n",
      "Epoch: 726 Training Loss: 0.35501927661895755 Test Loss: 0.36453572591145833\n",
      "Epoch: 727 Training Loss: 0.35487965488433837 Test Loss: 0.364386962890625\n",
      "Epoch: 728 Training Loss: 0.3547362616856893 Test Loss: 0.36424609375\n",
      "Epoch: 729 Training Loss: 0.3545959901809692 Test Loss: 0.364072998046875\n",
      "Epoch: 730 Training Loss: 0.35446096165974933 Test Loss: 0.36397355143229165\n",
      "Epoch: 731 Training Loss: 0.3543178618748983 Test Loss: 0.3638214925130208\n",
      "Epoch: 732 Training Loss: 0.3541764605840047 Test Loss: 0.3637032063802083\n",
      "Epoch: 733 Training Loss: 0.35403555997212727 Test Loss: 0.36358492024739586\n",
      "Epoch: 734 Training Loss: 0.35389253044128416 Test Loss: 0.3634349772135417\n",
      "Epoch: 735 Training Loss: 0.3537545747756958 Test Loss: 0.36328202311197916\n",
      "Epoch: 736 Training Loss: 0.3536124175389608 Test Loss: 0.36317814127604164\n",
      "Epoch: 737 Training Loss: 0.3534706455866496 Test Loss: 0.3630257568359375\n",
      "Epoch: 738 Training Loss: 0.3533320913314819 Test Loss: 0.36289237467447916\n",
      "Epoch: 739 Training Loss: 0.3531949335734049 Test Loss: 0.36277620442708336\n",
      "Epoch: 740 Training Loss: 0.35305750878651937 Test Loss: 0.36261751302083334\n",
      "Epoch: 741 Training Loss: 0.3529172226587931 Test Loss: 0.36250618489583336\n",
      "Epoch: 742 Training Loss: 0.35277884419759115 Test Loss: 0.36240096028645835\n",
      "Epoch: 743 Training Loss: 0.35264340750376383 Test Loss: 0.36226790364583333\n",
      "Epoch: 744 Training Loss: 0.3525051679611206 Test Loss: 0.3621405029296875\n",
      "Epoch: 745 Training Loss: 0.3523733088175456 Test Loss: 0.36197261555989585\n",
      "Epoch: 746 Training Loss: 0.3522353054682414 Test Loss: 0.3618291015625\n",
      "Epoch: 747 Training Loss: 0.3520990260442098 Test Loss: 0.36173885091145835\n",
      "Epoch: 748 Training Loss: 0.35196787865956625 Test Loss: 0.3616248779296875\n",
      "Epoch: 749 Training Loss: 0.35183435440063476 Test Loss: 0.36146272786458333\n",
      "Epoch: 750 Training Loss: 0.3517024393081665 Test Loss: 0.361343994140625\n",
      "Epoch: 751 Training Loss: 0.35157332388559975 Test Loss: 0.3612164306640625\n",
      "Epoch: 752 Training Loss: 0.35143876711527505 Test Loss: 0.36109049479166666\n",
      "Epoch: 753 Training Loss: 0.3513099168141683 Test Loss: 0.360912353515625\n",
      "Epoch: 754 Training Loss: 0.3511768763860067 Test Loss: 0.360792236328125\n",
      "Epoch: 755 Training Loss: 0.3510433464050293 Test Loss: 0.3606821695963542\n",
      "Epoch: 756 Training Loss: 0.35091366004943847 Test Loss: 0.36054833984375\n",
      "Epoch: 757 Training Loss: 0.35078380393981934 Test Loss: 0.3604212646484375\n",
      "Epoch: 758 Training Loss: 0.35065474096934 Test Loss: 0.36026033528645834\n",
      "Epoch: 759 Training Loss: 0.35052170848846437 Test Loss: 0.36016288248697914\n",
      "Epoch: 760 Training Loss: 0.3503923371632894 Test Loss: 0.360015625\n",
      "Epoch: 761 Training Loss: 0.3502638982137044 Test Loss: 0.3599179280598958\n",
      "Epoch: 762 Training Loss: 0.35013381640116376 Test Loss: 0.3597927652994792\n",
      "Epoch: 763 Training Loss: 0.35000303077697753 Test Loss: 0.359663330078125\n",
      "Epoch: 764 Training Loss: 0.34987879753112794 Test Loss: 0.35953938802083335\n",
      "Epoch: 765 Training Loss: 0.3497488489151001 Test Loss: 0.35946818033854167\n",
      "Epoch: 766 Training Loss: 0.34962005519866945 Test Loss: 0.35929264322916665\n",
      "Epoch: 767 Training Loss: 0.3494975725809733 Test Loss: 0.3591544189453125\n",
      "Epoch: 768 Training Loss: 0.3493677965799967 Test Loss: 0.3590461832682292\n",
      "Epoch: 769 Training Loss: 0.349244410832723 Test Loss: 0.3589289957682292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 770 Training Loss: 0.349115740776062 Test Loss: 0.35880594889322914\n",
      "Epoch: 771 Training Loss: 0.34899004618326823 Test Loss: 0.35868672688802083\n",
      "Epoch: 772 Training Loss: 0.34885914262135825 Test Loss: 0.35857853190104166\n",
      "Epoch: 773 Training Loss: 0.34873470465342205 Test Loss: 0.35845210774739583\n",
      "Epoch: 774 Training Loss: 0.3486126686731974 Test Loss: 0.3583522135416667\n",
      "Epoch: 775 Training Loss: 0.3484853041966756 Test Loss: 0.358215576171875\n",
      "Epoch: 776 Training Loss: 0.34836113739013674 Test Loss: 0.3581003011067708\n",
      "Epoch: 777 Training Loss: 0.34823744964599607 Test Loss: 0.35797465006510415\n",
      "Epoch: 778 Training Loss: 0.34811409123738607 Test Loss: 0.35785172526041664\n",
      "Epoch: 779 Training Loss: 0.34798667589823407 Test Loss: 0.35772566731770833\n",
      "Epoch: 780 Training Loss: 0.34786161708831786 Test Loss: 0.3576175944010417\n",
      "Epoch: 781 Training Loss: 0.347740966796875 Test Loss: 0.357467529296875\n",
      "Epoch: 782 Training Loss: 0.34762082386016846 Test Loss: 0.35737996419270834\n",
      "Epoch: 783 Training Loss: 0.34749980862935387 Test Loss: 0.3572418212890625\n",
      "Epoch: 784 Training Loss: 0.34737423578898113 Test Loss: 0.3571869710286458\n",
      "Epoch: 785 Training Loss: 0.3472517229715983 Test Loss: 0.35700996907552085\n",
      "Epoch: 786 Training Loss: 0.3471325559616089 Test Loss: 0.3569241943359375\n",
      "Epoch: 787 Training Loss: 0.34701101620992025 Test Loss: 0.35679899088541667\n",
      "Epoch: 788 Training Loss: 0.34689014689127606 Test Loss: 0.35668977864583334\n",
      "Epoch: 789 Training Loss: 0.34676373036702474 Test Loss: 0.3566078287760417\n",
      "Epoch: 790 Training Loss: 0.34664110247294105 Test Loss: 0.35644563802083334\n",
      "Epoch: 791 Training Loss: 0.3465244274139404 Test Loss: 0.3563931477864583\n",
      "Epoch: 792 Training Loss: 0.34640136337280275 Test Loss: 0.3562371826171875\n",
      "Epoch: 793 Training Loss: 0.34628465620676674 Test Loss: 0.3561488037109375\n",
      "Epoch: 794 Training Loss: 0.3461635996500651 Test Loss: 0.3560262858072917\n",
      "Epoch: 795 Training Loss: 0.34604259554545086 Test Loss: 0.35587890625\n",
      "Epoch: 796 Training Loss: 0.34591994190216063 Test Loss: 0.3557918294270833\n",
      "Epoch: 797 Training Loss: 0.34580068588256835 Test Loss: 0.35567232259114584\n",
      "Epoch: 798 Training Loss: 0.34567895921071373 Test Loss: 0.3555430908203125\n",
      "Epoch: 799 Training Loss: 0.34556418005625406 Test Loss: 0.3554291585286458\n",
      "Epoch: 800 Training Loss: 0.34544591554005943 Test Loss: 0.3553451334635417\n",
      "Epoch: 801 Training Loss: 0.3453274625142415 Test Loss: 0.3552292887369792\n",
      "Epoch: 802 Training Loss: 0.3452036650975545 Test Loss: 0.35510465494791665\n",
      "Epoch: 803 Training Loss: 0.3450838743845622 Test Loss: 0.35500390625\n",
      "Epoch: 804 Training Loss: 0.34496038977305093 Test Loss: 0.3548924967447917\n",
      "Epoch: 805 Training Loss: 0.3448421850204468 Test Loss: 0.35481803385416666\n",
      "Epoch: 806 Training Loss: 0.34471770032246907 Test Loss: 0.35466845703125\n",
      "Epoch: 807 Training Loss: 0.3446001726786296 Test Loss: 0.354550048828125\n",
      "Epoch: 808 Training Loss: 0.34448108609517414 Test Loss: 0.3544842122395833\n",
      "Epoch: 809 Training Loss: 0.3443593552907308 Test Loss: 0.3543837890625\n",
      "Epoch: 810 Training Loss: 0.34424025344848636 Test Loss: 0.3542550862630208\n",
      "Epoch: 811 Training Loss: 0.34412269592285155 Test Loss: 0.3541514078776042\n",
      "Epoch: 812 Training Loss: 0.34400207360585533 Test Loss: 0.354031494140625\n",
      "Epoch: 813 Training Loss: 0.3438883778254191 Test Loss: 0.35391756184895834\n",
      "Epoch: 814 Training Loss: 0.3437683782577515 Test Loss: 0.35384785970052085\n",
      "Epoch: 815 Training Loss: 0.3436512943903605 Test Loss: 0.35369482421875\n",
      "Epoch: 816 Training Loss: 0.3435318752924601 Test Loss: 0.35357633463541666\n",
      "Epoch: 817 Training Loss: 0.34341744232177734 Test Loss: 0.35345975748697916\n",
      "Epoch: 818 Training Loss: 0.34329350980122886 Test Loss: 0.353378662109375\n",
      "Epoch: 819 Training Loss: 0.3431812375386556 Test Loss: 0.3532362874348958\n",
      "Epoch: 820 Training Loss: 0.3430618251164754 Test Loss: 0.353136474609375\n",
      "Epoch: 821 Training Loss: 0.3429460426966349 Test Loss: 0.3530339762369792\n",
      "Epoch: 822 Training Loss: 0.3428301385243734 Test Loss: 0.35290519205729165\n",
      "Epoch: 823 Training Loss: 0.3427159163157145 Test Loss: 0.3527953694661458\n",
      "Epoch: 824 Training Loss: 0.3426018161773682 Test Loss: 0.3526677652994792\n",
      "Epoch: 825 Training Loss: 0.3424871899286906 Test Loss: 0.35258455403645833\n",
      "Epoch: 826 Training Loss: 0.34237083784739175 Test Loss: 0.35247566731770835\n",
      "Epoch: 827 Training Loss: 0.3422509660720825 Test Loss: 0.35237178548177084\n",
      "Epoch: 828 Training Loss: 0.3421364911397298 Test Loss: 0.3522724609375\n",
      "Epoch: 829 Training Loss: 0.34202566623687747 Test Loss: 0.35212215169270833\n",
      "Epoch: 830 Training Loss: 0.34191161855061847 Test Loss: 0.3520326334635417\n",
      "Epoch: 831 Training Loss: 0.34179389413197836 Test Loss: 0.35192610677083336\n",
      "Epoch: 832 Training Loss: 0.34168288739522296 Test Loss: 0.35180745442708333\n",
      "Epoch: 833 Training Loss: 0.34157644367218015 Test Loss: 0.35170076497395836\n",
      "Epoch: 834 Training Loss: 0.3414615999857585 Test Loss: 0.35164156087239584\n",
      "Epoch: 835 Training Loss: 0.3413477792739868 Test Loss: 0.35151082356770835\n",
      "Epoch: 836 Training Loss: 0.34123761240641276 Test Loss: 0.3513833414713542\n",
      "Epoch: 837 Training Loss: 0.3411220308939616 Test Loss: 0.35125732421875\n",
      "Epoch: 838 Training Loss: 0.3410120277404785 Test Loss: 0.3511708577473958\n",
      "Epoch: 839 Training Loss: 0.34090061696370444 Test Loss: 0.3510835367838542\n",
      "Epoch: 840 Training Loss: 0.3407933187484741 Test Loss: 0.35096512858072915\n",
      "Epoch: 841 Training Loss: 0.34067691294352215 Test Loss: 0.3508882649739583\n",
      "Epoch: 842 Training Loss: 0.3405609245300293 Test Loss: 0.3507396647135417\n",
      "Epoch: 843 Training Loss: 0.34045147864023845 Test Loss: 0.3506106770833333\n",
      "Epoch: 844 Training Loss: 0.3403455645243327 Test Loss: 0.35050443522135416\n",
      "Epoch: 845 Training Loss: 0.3402303787867228 Test Loss: 0.35039371744791664\n",
      "Epoch: 846 Training Loss: 0.3401206569671631 Test Loss: 0.350327880859375\n",
      "Epoch: 847 Training Loss: 0.34000985844930015 Test Loss: 0.3501962483723958\n",
      "Epoch: 848 Training Loss: 0.3399051462809245 Test Loss: 0.3501124267578125\n",
      "Epoch: 849 Training Loss: 0.3397952178319295 Test Loss: 0.34999169921875\n",
      "Epoch: 850 Training Loss: 0.3396882708867391 Test Loss: 0.3498986002604167\n",
      "Epoch: 851 Training Loss: 0.3395793113708496 Test Loss: 0.34979191080729166\n",
      "Epoch: 852 Training Loss: 0.3394633156458537 Test Loss: 0.3496728515625\n",
      "Epoch: 853 Training Loss: 0.33935419368743897 Test Loss: 0.3495171305338542\n",
      "Epoch: 854 Training Loss: 0.3392433268229167 Test Loss: 0.3494857584635417\n",
      "Epoch: 855 Training Loss: 0.33913238588968914 Test Loss: 0.3493446858723958\n",
      "Epoch: 856 Training Loss: 0.33902174250284833 Test Loss: 0.34925826009114586\n",
      "Epoch: 857 Training Loss: 0.3389097000757853 Test Loss: 0.34916031901041666\n",
      "Epoch: 858 Training Loss: 0.3387998472849528 Test Loss: 0.34902591959635415\n",
      "Epoch: 859 Training Loss: 0.33869343598683677 Test Loss: 0.3489745279947917\n",
      "Epoch: 860 Training Loss: 0.3385858726501465 Test Loss: 0.348884765625\n",
      "Epoch: 861 Training Loss: 0.3384769811630249 Test Loss: 0.3487395833333333\n",
      "Epoch: 862 Training Loss: 0.3383694906234741 Test Loss: 0.3486582438151042\n",
      "Epoch: 863 Training Loss: 0.338259827931722 Test Loss: 0.3485561116536458\n",
      "Epoch: 864 Training Loss: 0.33814545631408693 Test Loss: 0.34842903645833334\n",
      "Epoch: 865 Training Loss: 0.3380372330347697 Test Loss: 0.34834830729166666\n",
      "Epoch: 866 Training Loss: 0.3379257647196452 Test Loss: 0.3482163899739583\n",
      "Epoch: 867 Training Loss: 0.3378170468012492 Test Loss: 0.34810017903645835\n",
      "Epoch: 868 Training Loss: 0.33770396614074705 Test Loss: 0.348021728515625\n",
      "Epoch: 869 Training Loss: 0.3375935494105021 Test Loss: 0.34791943359375\n",
      "Epoch: 870 Training Loss: 0.3374886455535889 Test Loss: 0.34778287760416665\n",
      "Epoch: 871 Training Loss: 0.337378142674764 Test Loss: 0.3476568603515625\n",
      "Epoch: 872 Training Loss: 0.33726786422729493 Test Loss: 0.3475948486328125\n",
      "Epoch: 873 Training Loss: 0.3371578645706177 Test Loss: 0.34748213704427083\n",
      "Epoch: 874 Training Loss: 0.3370502611796061 Test Loss: 0.3473638509114583\n",
      "Epoch: 875 Training Loss: 0.33694299920399984 Test Loss: 0.347271240234375\n",
      "Epoch: 876 Training Loss: 0.3368333339691162 Test Loss: 0.3471749267578125\n",
      "Epoch: 877 Training Loss: 0.33672931226094566 Test Loss: 0.34707405598958335\n",
      "Epoch: 878 Training Loss: 0.3366206502914429 Test Loss: 0.34695930989583335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 879 Training Loss: 0.33651435407002767 Test Loss: 0.3468774007161458\n",
      "Epoch: 880 Training Loss: 0.33641140524546304 Test Loss: 0.3467765706380208\n",
      "Epoch: 881 Training Loss: 0.3363010959625244 Test Loss: 0.3466474609375\n",
      "Epoch: 882 Training Loss: 0.3361934986114502 Test Loss: 0.3465230712890625\n",
      "Epoch: 883 Training Loss: 0.3360826981862386 Test Loss: 0.34647880045572915\n",
      "Epoch: 884 Training Loss: 0.33597449493408205 Test Loss: 0.34636336263020834\n",
      "Epoch: 885 Training Loss: 0.3358652048110962 Test Loss: 0.3462002766927083\n",
      "Epoch: 886 Training Loss: 0.33576016330718994 Test Loss: 0.34614750162760416\n",
      "Epoch: 887 Training Loss: 0.3356563243865967 Test Loss: 0.34604736328125\n",
      "Epoch: 888 Training Loss: 0.33555182043711346 Test Loss: 0.3459114583333333\n",
      "Epoch: 889 Training Loss: 0.335441388130188 Test Loss: 0.345791015625\n",
      "Epoch: 890 Training Loss: 0.33533590443929034 Test Loss: 0.3457210693359375\n",
      "Epoch: 891 Training Loss: 0.33522727807362873 Test Loss: 0.34564876302083336\n",
      "Epoch: 892 Training Loss: 0.3351194349924723 Test Loss: 0.3455173746744792\n",
      "Epoch: 893 Training Loss: 0.3350153694152832 Test Loss: 0.345411376953125\n",
      "Epoch: 894 Training Loss: 0.3349057534535726 Test Loss: 0.3453124186197917\n",
      "Epoch: 895 Training Loss: 0.3348013696670532 Test Loss: 0.34521272786458335\n",
      "Epoch: 896 Training Loss: 0.3346968361536662 Test Loss: 0.3451184488932292\n",
      "Epoch: 897 Training Loss: 0.3345931593577067 Test Loss: 0.3449618326822917\n",
      "Epoch: 898 Training Loss: 0.33448486296335855 Test Loss: 0.344831787109375\n",
      "Epoch: 899 Training Loss: 0.33438047154744466 Test Loss: 0.3447579752604167\n",
      "Epoch: 900 Training Loss: 0.3342778466542562 Test Loss: 0.34467740885416664\n",
      "Epoch: 901 Training Loss: 0.3341740729014079 Test Loss: 0.3445689697265625\n",
      "Epoch: 902 Training Loss: 0.334067058245341 Test Loss: 0.34448258463541664\n",
      "Epoch: 903 Training Loss: 0.3339660987854004 Test Loss: 0.34438321940104166\n",
      "Epoch: 904 Training Loss: 0.33386513106028237 Test Loss: 0.34429060872395834\n",
      "Epoch: 905 Training Loss: 0.33376161257425946 Test Loss: 0.34423282877604167\n",
      "Epoch: 906 Training Loss: 0.3336632194519043 Test Loss: 0.34408097330729165\n",
      "Epoch: 907 Training Loss: 0.3335513219833374 Test Loss: 0.34400313313802083\n",
      "Epoch: 908 Training Loss: 0.3334478022257487 Test Loss: 0.343871826171875\n",
      "Epoch: 909 Training Loss: 0.3333450361887614 Test Loss: 0.34380318196614584\n",
      "Epoch: 910 Training Loss: 0.33324621645609537 Test Loss: 0.3436632893880208\n",
      "Epoch: 911 Training Loss: 0.33314129734039305 Test Loss: 0.3436108805338542\n",
      "Epoch: 912 Training Loss: 0.33303442001342776 Test Loss: 0.343460205078125\n",
      "Epoch: 913 Training Loss: 0.3329369239807129 Test Loss: 0.34339583333333334\n",
      "Epoch: 914 Training Loss: 0.3328248987197876 Test Loss: 0.34326554361979167\n",
      "Epoch: 915 Training Loss: 0.33272308349609375 Test Loss: 0.34314591471354167\n",
      "Epoch: 916 Training Loss: 0.3326142822901408 Test Loss: 0.34306852213541666\n",
      "Epoch: 917 Training Loss: 0.3325141480763753 Test Loss: 0.34298828125\n",
      "Epoch: 918 Training Loss: 0.3324137881596883 Test Loss: 0.34290376790364585\n",
      "Epoch: 919 Training Loss: 0.33231014823913574 Test Loss: 0.3427562255859375\n",
      "Epoch: 920 Training Loss: 0.3322050352096558 Test Loss: 0.3426788330078125\n",
      "Epoch: 921 Training Loss: 0.3321000855763753 Test Loss: 0.3425635986328125\n",
      "Epoch: 922 Training Loss: 0.33200385061899823 Test Loss: 0.3424710693359375\n",
      "Epoch: 923 Training Loss: 0.3319014368057251 Test Loss: 0.3423428548177083\n",
      "Epoch: 924 Training Loss: 0.3318029890060425 Test Loss: 0.3423064371744792\n",
      "Epoch: 925 Training Loss: 0.33170299053192137 Test Loss: 0.3421754150390625\n",
      "Epoch: 926 Training Loss: 0.3316011864344279 Test Loss: 0.3420509033203125\n",
      "Epoch: 927 Training Loss: 0.3315008742014567 Test Loss: 0.3420045572916667\n",
      "Epoch: 928 Training Loss: 0.33139877891540526 Test Loss: 0.3419188232421875\n",
      "Epoch: 929 Training Loss: 0.3313003069559733 Test Loss: 0.341803955078125\n",
      "Epoch: 930 Training Loss: 0.33120207977294924 Test Loss: 0.3417071533203125\n",
      "Epoch: 931 Training Loss: 0.33110190645853677 Test Loss: 0.3416031901041667\n",
      "Epoch: 932 Training Loss: 0.33100232887268066 Test Loss: 0.34155550130208334\n",
      "Epoch: 933 Training Loss: 0.3309054931004842 Test Loss: 0.3413997395833333\n",
      "Epoch: 934 Training Loss: 0.33080781332651776 Test Loss: 0.341379638671875\n",
      "Epoch: 935 Training Loss: 0.33070840454101563 Test Loss: 0.34122957356770833\n",
      "Epoch: 936 Training Loss: 0.3306127023696899 Test Loss: 0.341139404296875\n",
      "Epoch: 937 Training Loss: 0.3305109281539917 Test Loss: 0.34107476806640624\n",
      "Epoch: 938 Training Loss: 0.3304177153905233 Test Loss: 0.3409560953776042\n",
      "Epoch: 939 Training Loss: 0.33031491343180336 Test Loss: 0.34089208984375\n",
      "Epoch: 940 Training Loss: 0.33021911525726316 Test Loss: 0.3408092447916667\n",
      "Epoch: 941 Training Loss: 0.33011950365702314 Test Loss: 0.34068617757161457\n",
      "Epoch: 942 Training Loss: 0.33002155367533365 Test Loss: 0.340617919921875\n",
      "Epoch: 943 Training Loss: 0.32992248090108234 Test Loss: 0.3405115763346354\n",
      "Epoch: 944 Training Loss: 0.32982506561279296 Test Loss: 0.34045048014322915\n",
      "Epoch: 945 Training Loss: 0.3297262423833211 Test Loss: 0.34030165608723956\n",
      "Epoch: 946 Training Loss: 0.32963550853729245 Test Loss: 0.3402415364583333\n",
      "Epoch: 947 Training Loss: 0.3295344155629476 Test Loss: 0.34018990071614585\n",
      "Epoch: 948 Training Loss: 0.3294350032806396 Test Loss: 0.3400335693359375\n",
      "Epoch: 949 Training Loss: 0.32933703263600667 Test Loss: 0.33993166097005206\n",
      "Epoch: 950 Training Loss: 0.3292387495040894 Test Loss: 0.3398350830078125\n",
      "Epoch: 951 Training Loss: 0.3291354548136393 Test Loss: 0.3397424723307292\n",
      "Epoch: 952 Training Loss: 0.3290371751785278 Test Loss: 0.33966259765625\n",
      "Epoch: 953 Training Loss: 0.3289475399653117 Test Loss: 0.3395545654296875\n",
      "Epoch: 954 Training Loss: 0.3288457514444987 Test Loss: 0.33949397786458335\n",
      "Epoch: 955 Training Loss: 0.32875719960530597 Test Loss: 0.3394158121744792\n",
      "Epoch: 956 Training Loss: 0.3286542936960856 Test Loss: 0.3393313802083333\n",
      "Epoch: 957 Training Loss: 0.328560066541036 Test Loss: 0.3392244873046875\n",
      "Epoch: 958 Training Loss: 0.3284632689158122 Test Loss: 0.33909769694010417\n",
      "Epoch: 959 Training Loss: 0.3283636458714803 Test Loss: 0.33900341796875\n",
      "Epoch: 960 Training Loss: 0.328268030166626 Test Loss: 0.3389321899414062\n",
      "Epoch: 961 Training Loss: 0.32817346445719403 Test Loss: 0.33880423990885417\n",
      "Epoch: 962 Training Loss: 0.3280801264444987 Test Loss: 0.3386971028645833\n",
      "Epoch: 963 Training Loss: 0.3279840745925903 Test Loss: 0.33859639485677084\n",
      "Epoch: 964 Training Loss: 0.32788809553782144 Test Loss: 0.3385330403645833\n",
      "Epoch: 965 Training Loss: 0.32779322465260824 Test Loss: 0.33846746826171875\n",
      "Epoch: 966 Training Loss: 0.32769323857625327 Test Loss: 0.3383569946289062\n",
      "Epoch: 967 Training Loss: 0.3275996545155843 Test Loss: 0.33828352864583333\n",
      "Epoch: 968 Training Loss: 0.3275086921056112 Test Loss: 0.33824745686848956\n",
      "Epoch: 969 Training Loss: 0.327409774462382 Test Loss: 0.3381232503255208\n",
      "Epoch: 970 Training Loss: 0.3273208977381388 Test Loss: 0.3380332845052083\n",
      "Epoch: 971 Training Loss: 0.32722290229797363 Test Loss: 0.33793511962890627\n",
      "Epoch: 972 Training Loss: 0.32712844022115073 Test Loss: 0.3378211669921875\n",
      "Epoch: 973 Training Loss: 0.32702911376953125 Test Loss: 0.33774039713541665\n",
      "Epoch: 974 Training Loss: 0.3269376598993937 Test Loss: 0.3376720987955729\n",
      "Epoch: 975 Training Loss: 0.3268464117050171 Test Loss: 0.33753531901041667\n",
      "Epoch: 976 Training Loss: 0.3267489347457886 Test Loss: 0.33747233072916666\n",
      "Epoch: 977 Training Loss: 0.326653013865153 Test Loss: 0.33736397298177084\n",
      "Epoch: 978 Training Loss: 0.32656184609731037 Test Loss: 0.3372745361328125\n",
      "Epoch: 979 Training Loss: 0.32646692053476967 Test Loss: 0.33719673665364586\n",
      "Epoch: 980 Training Loss: 0.32637447357177735 Test Loss: 0.33710209147135417\n",
      "Epoch: 981 Training Loss: 0.3262754004796346 Test Loss: 0.3370489095052083\n",
      "Epoch: 982 Training Loss: 0.3261849025090536 Test Loss: 0.33695747884114585\n",
      "Epoch: 983 Training Loss: 0.32609139537811277 Test Loss: 0.33685198974609376\n",
      "Epoch: 984 Training Loss: 0.32599538135528566 Test Loss: 0.336762939453125\n",
      "Epoch: 985 Training Loss: 0.32590673033396406 Test Loss: 0.3366983642578125\n",
      "Epoch: 986 Training Loss: 0.32580887349446613 Test Loss: 0.33657362874348956\n",
      "Epoch: 987 Training Loss: 0.3257124417622884 Test Loss: 0.33646988932291666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 988 Training Loss: 0.3256188999811808 Test Loss: 0.3364083251953125\n",
      "Epoch: 989 Training Loss: 0.32552330843607585 Test Loss: 0.3362745361328125\n",
      "Epoch: 990 Training Loss: 0.32542549069722493 Test Loss: 0.3361701253255208\n",
      "Epoch: 991 Training Loss: 0.32533824412027995 Test Loss: 0.33610331217447914\n",
      "Epoch: 992 Training Loss: 0.3252486842473348 Test Loss: 0.33601922607421875\n",
      "Epoch: 993 Training Loss: 0.3251504144668579 Test Loss: 0.335905517578125\n",
      "Epoch: 994 Training Loss: 0.32506045150756835 Test Loss: 0.33579390462239583\n",
      "Epoch: 995 Training Loss: 0.32496484883626303 Test Loss: 0.3357125447591146\n",
      "Epoch: 996 Training Loss: 0.3248750549952189 Test Loss: 0.33562982177734374\n",
      "Epoch: 997 Training Loss: 0.32477953179677327 Test Loss: 0.33555635579427084\n",
      "Epoch: 998 Training Loss: 0.32468983459472656 Test Loss: 0.33547835286458333\n",
      "Epoch: 999 Training Loss: 0.32459361203511555 Test Loss: 0.33538944498697915\n",
      "Epoch: 1000 Training Loss: 0.32450362205505373 Test Loss: 0.33532462565104165\n",
      "Epoch: 1001 Training Loss: 0.3244112542470296 Test Loss: 0.3352652791341146\n",
      "Epoch: 1002 Training Loss: 0.32431197293599445 Test Loss: 0.3351673583984375\n",
      "Epoch: 1003 Training Loss: 0.3242222960789998 Test Loss: 0.33508418782552085\n",
      "Epoch: 1004 Training Loss: 0.3241250893274943 Test Loss: 0.3350135701497396\n",
      "Epoch: 1005 Training Loss: 0.3240318660736084 Test Loss: 0.3349041341145833\n",
      "Epoch: 1006 Training Loss: 0.32394105847676596 Test Loss: 0.3348262939453125\n",
      "Epoch: 1007 Training Loss: 0.32385404777526855 Test Loss: 0.334798583984375\n",
      "Epoch: 1008 Training Loss: 0.323760760307312 Test Loss: 0.3346898600260417\n",
      "Epoch: 1009 Training Loss: 0.3236671632130941 Test Loss: 0.33457295735677084\n",
      "Epoch: 1010 Training Loss: 0.32357635498046877 Test Loss: 0.33450004069010414\n",
      "Epoch: 1011 Training Loss: 0.3234832804997762 Test Loss: 0.334379150390625\n",
      "Epoch: 1012 Training Loss: 0.3233904759089152 Test Loss: 0.33428875732421875\n",
      "Epoch: 1013 Training Loss: 0.32329697132110596 Test Loss: 0.33420745849609373\n",
      "Epoch: 1014 Training Loss: 0.32320655695597333 Test Loss: 0.3340624186197917\n",
      "Epoch: 1015 Training Loss: 0.3231097421646118 Test Loss: 0.333990234375\n",
      "Epoch: 1016 Training Loss: 0.32302628580729165 Test Loss: 0.3339465128580729\n",
      "Epoch: 1017 Training Loss: 0.32292916742960615 Test Loss: 0.33382926432291665\n",
      "Epoch: 1018 Training Loss: 0.3228385044733683 Test Loss: 0.33377042643229166\n",
      "Epoch: 1019 Training Loss: 0.32274770927429197 Test Loss: 0.33372735595703124\n",
      "Epoch: 1020 Training Loss: 0.32265671793619793 Test Loss: 0.333582763671875\n",
      "Epoch: 1021 Training Loss: 0.32256483236948646 Test Loss: 0.33349690755208333\n",
      "Epoch: 1022 Training Loss: 0.3224687728881836 Test Loss: 0.33336112467447915\n",
      "Epoch: 1023 Training Loss: 0.32237669881184894 Test Loss: 0.3333001302083333\n",
      "Epoch: 1024 Training Loss: 0.322282719930013 Test Loss: 0.33314607747395836\n",
      "Epoch: 1025 Training Loss: 0.3221948804855347 Test Loss: 0.3330913899739583\n",
      "Epoch: 1026 Training Loss: 0.32209925587972005 Test Loss: 0.3329920654296875\n",
      "Epoch: 1027 Training Loss: 0.32200182596842447 Test Loss: 0.3329151204427083\n",
      "Epoch: 1028 Training Loss: 0.3219137398401896 Test Loss: 0.33284672037760415\n",
      "Epoch: 1029 Training Loss: 0.3218197685877482 Test Loss: 0.33278304036458334\n",
      "Epoch: 1030 Training Loss: 0.3217304169336955 Test Loss: 0.3326792399088542\n",
      "Epoch: 1031 Training Loss: 0.3216329304377238 Test Loss: 0.3326177978515625\n",
      "Epoch: 1032 Training Loss: 0.32155128955841067 Test Loss: 0.3325066935221354\n",
      "Epoch: 1033 Training Loss: 0.3214507846832275 Test Loss: 0.3323720499674479\n",
      "Epoch: 1034 Training Loss: 0.3213578074773153 Test Loss: 0.33226509602864585\n",
      "Epoch: 1035 Training Loss: 0.3212683312098185 Test Loss: 0.33221624755859375\n",
      "Epoch: 1036 Training Loss: 0.32117454465230305 Test Loss: 0.3321146647135417\n",
      "Epoch: 1037 Training Loss: 0.3210848382314046 Test Loss: 0.33199576822916665\n",
      "Epoch: 1038 Training Loss: 0.32099257373809814 Test Loss: 0.3319273274739583\n",
      "Epoch: 1039 Training Loss: 0.320901948928833 Test Loss: 0.3318770751953125\n",
      "Epoch: 1040 Training Loss: 0.3208096135457357 Test Loss: 0.33174857584635414\n",
      "Epoch: 1041 Training Loss: 0.32072059154510496 Test Loss: 0.3316728108723958\n",
      "Epoch: 1042 Training Loss: 0.32062658309936526 Test Loss: 0.3316026611328125\n",
      "Epoch: 1043 Training Loss: 0.32053275299072265 Test Loss: 0.3315072021484375\n",
      "Epoch: 1044 Training Loss: 0.3204418551127116 Test Loss: 0.33140116373697914\n",
      "Epoch: 1045 Training Loss: 0.3203492046991984 Test Loss: 0.3313076375325521\n",
      "Epoch: 1046 Training Loss: 0.32025831000010174 Test Loss: 0.3311750691731771\n",
      "Epoch: 1047 Training Loss: 0.3201643133163452 Test Loss: 0.33111844889322917\n",
      "Epoch: 1048 Training Loss: 0.3200798486073812 Test Loss: 0.3310578002929688\n",
      "Epoch: 1049 Training Loss: 0.3199775238037109 Test Loss: 0.33093424479166667\n",
      "Epoch: 1050 Training Loss: 0.31989028326670327 Test Loss: 0.33086879475911457\n",
      "Epoch: 1051 Training Loss: 0.31979957485198973 Test Loss: 0.3307443644205729\n",
      "Epoch: 1052 Training Loss: 0.31970250129699707 Test Loss: 0.3306650797526042\n",
      "Epoch: 1053 Training Loss: 0.3196128543217977 Test Loss: 0.33059818522135415\n",
      "Epoch: 1054 Training Loss: 0.3195279989242554 Test Loss: 0.33051253255208335\n",
      "Epoch: 1055 Training Loss: 0.31943690903981525 Test Loss: 0.3304009602864583\n",
      "Epoch: 1056 Training Loss: 0.31934058062235515 Test Loss: 0.3303164876302083\n",
      "Epoch: 1057 Training Loss: 0.3192529961268107 Test Loss: 0.3302703043619792\n",
      "Epoch: 1058 Training Loss: 0.3191624577840169 Test Loss: 0.330166748046875\n",
      "Epoch: 1059 Training Loss: 0.31907511615753176 Test Loss: 0.33006490071614586\n",
      "Epoch: 1060 Training Loss: 0.31898453998565673 Test Loss: 0.3300482788085938\n",
      "Epoch: 1061 Training Loss: 0.3188959108988444 Test Loss: 0.3299320068359375\n",
      "Epoch: 1062 Training Loss: 0.3188026704788208 Test Loss: 0.32987522379557294\n",
      "Epoch: 1063 Training Loss: 0.31871880213419596 Test Loss: 0.3298024088541667\n",
      "Epoch: 1064 Training Loss: 0.318628067334493 Test Loss: 0.3297373046875\n",
      "Epoch: 1065 Training Loss: 0.3185344727834066 Test Loss: 0.32964302571614584\n",
      "Epoch: 1066 Training Loss: 0.31844912974039713 Test Loss: 0.329551025390625\n",
      "Epoch: 1067 Training Loss: 0.3183570909500122 Test Loss: 0.3295057373046875\n",
      "Epoch: 1068 Training Loss: 0.3182672239939372 Test Loss: 0.32944462076822917\n",
      "Epoch: 1069 Training Loss: 0.31818975989023845 Test Loss: 0.32929939778645834\n",
      "Epoch: 1070 Training Loss: 0.3181000146865845 Test Loss: 0.3292501017252604\n",
      "Epoch: 1071 Training Loss: 0.3180126969019572 Test Loss: 0.32916119384765624\n",
      "Epoch: 1072 Training Loss: 0.31791925779978436 Test Loss: 0.3290950927734375\n",
      "Epoch: 1073 Training Loss: 0.3178341655731201 Test Loss: 0.3289991455078125\n",
      "Epoch: 1074 Training Loss: 0.31774865500132243 Test Loss: 0.32889558919270834\n",
      "Epoch: 1075 Training Loss: 0.31766148312886555 Test Loss: 0.3288438720703125\n",
      "Epoch: 1076 Training Loss: 0.3175728073120117 Test Loss: 0.3287781982421875\n",
      "Epoch: 1077 Training Loss: 0.317481156984965 Test Loss: 0.3287054850260417\n",
      "Epoch: 1078 Training Loss: 0.3173971983591716 Test Loss: 0.32861539713541665\n",
      "Epoch: 1079 Training Loss: 0.31730831241607665 Test Loss: 0.32852091471354167\n",
      "Epoch: 1080 Training Loss: 0.3172199010848999 Test Loss: 0.3284445393880208\n",
      "Epoch: 1081 Training Loss: 0.3171329952875773 Test Loss: 0.32838985188802083\n",
      "Epoch: 1082 Training Loss: 0.3170489091873169 Test Loss: 0.32830403645833334\n",
      "Epoch: 1083 Training Loss: 0.31696285915374756 Test Loss: 0.32824167887369793\n",
      "Epoch: 1084 Training Loss: 0.316871213277181 Test Loss: 0.32810404459635417\n",
      "Epoch: 1085 Training Loss: 0.31678280703226724 Test Loss: 0.3280421142578125\n",
      "Epoch: 1086 Training Loss: 0.31669618956247964 Test Loss: 0.3279617513020833\n",
      "Epoch: 1087 Training Loss: 0.3166163803736369 Test Loss: 0.3278943888346354\n",
      "Epoch: 1088 Training Loss: 0.3165264705022176 Test Loss: 0.3278240966796875\n",
      "Epoch: 1089 Training Loss: 0.31643792152404787 Test Loss: 0.3277278238932292\n",
      "Epoch: 1090 Training Loss: 0.31635285091400145 Test Loss: 0.3275714111328125\n",
      "Epoch: 1091 Training Loss: 0.31626601219177247 Test Loss: 0.32754532877604164\n",
      "Epoch: 1092 Training Loss: 0.31617806434631346 Test Loss: 0.32746097819010417\n",
      "Epoch: 1093 Training Loss: 0.316089635848999 Test Loss: 0.32737394205729164\n",
      "Epoch: 1094 Training Loss: 0.3160019172032674 Test Loss: 0.3272896728515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1095 Training Loss: 0.31591430759429934 Test Loss: 0.32716691080729166\n",
      "Epoch: 1096 Training Loss: 0.31583119837443036 Test Loss: 0.3271026204427083\n",
      "Epoch: 1097 Training Loss: 0.31574689547220863 Test Loss: 0.3270235188802083\n",
      "Epoch: 1098 Training Loss: 0.3156551411946615 Test Loss: 0.32694618733723957\n",
      "Epoch: 1099 Training Loss: 0.3155747283299764 Test Loss: 0.32687872314453126\n",
      "Epoch: 1100 Training Loss: 0.3154835004806519 Test Loss: 0.32679823811848957\n",
      "Epoch: 1101 Training Loss: 0.3154012778600057 Test Loss: 0.3266900227864583\n",
      "Epoch: 1102 Training Loss: 0.3153179572423299 Test Loss: 0.32660654703776043\n",
      "Epoch: 1103 Training Loss: 0.3152322187423706 Test Loss: 0.32656278483072915\n",
      "Epoch: 1104 Training Loss: 0.31515109157562254 Test Loss: 0.3264908447265625\n",
      "Epoch: 1105 Training Loss: 0.31506560802459715 Test Loss: 0.32642982991536457\n",
      "Epoch: 1106 Training Loss: 0.3149787848790487 Test Loss: 0.32635135904947915\n",
      "Epoch: 1107 Training Loss: 0.31489293479919434 Test Loss: 0.3262530721028646\n",
      "Epoch: 1108 Training Loss: 0.3148086808522542 Test Loss: 0.3262099202473958\n",
      "Epoch: 1109 Training Loss: 0.3147306976318359 Test Loss: 0.3261073404947917\n",
      "Epoch: 1110 Training Loss: 0.3146428025563558 Test Loss: 0.3260150553385417\n",
      "Epoch: 1111 Training Loss: 0.31455961100260416 Test Loss: 0.325978759765625\n",
      "Epoch: 1112 Training Loss: 0.3144774589538574 Test Loss: 0.32587367757161456\n",
      "Epoch: 1113 Training Loss: 0.3143958266576131 Test Loss: 0.3258046061197917\n",
      "Epoch: 1114 Training Loss: 0.31430763403574624 Test Loss: 0.32572794596354165\n",
      "Epoch: 1115 Training Loss: 0.3142232729593913 Test Loss: 0.3255763346354167\n",
      "Epoch: 1116 Training Loss: 0.3141423616409302 Test Loss: 0.32554486083984374\n",
      "Epoch: 1117 Training Loss: 0.3140577023824056 Test Loss: 0.325442138671875\n",
      "Epoch: 1118 Training Loss: 0.3139685961405436 Test Loss: 0.32539398193359376\n",
      "Epoch: 1119 Training Loss: 0.31388949616750084 Test Loss: 0.325318603515625\n",
      "Epoch: 1120 Training Loss: 0.31380226389567056 Test Loss: 0.3252225138346354\n",
      "Epoch: 1121 Training Loss: 0.3137229267756144 Test Loss: 0.3251444498697917\n",
      "Epoch: 1122 Training Loss: 0.313639097849528 Test Loss: 0.3250807698567708\n",
      "Epoch: 1123 Training Loss: 0.3135569372177124 Test Loss: 0.3250197347005208\n",
      "Epoch: 1124 Training Loss: 0.3134761724472046 Test Loss: 0.3249029541015625\n",
      "Epoch: 1125 Training Loss: 0.3133947893778483 Test Loss: 0.32483154296875\n",
      "Epoch: 1126 Training Loss: 0.3133095827102661 Test Loss: 0.324754638671875\n",
      "Epoch: 1127 Training Loss: 0.31322544733683266 Test Loss: 0.32468341064453127\n",
      "Epoch: 1128 Training Loss: 0.3131470305124919 Test Loss: 0.3245913696289062\n",
      "Epoch: 1129 Training Loss: 0.3130621678034465 Test Loss: 0.32457511393229166\n",
      "Epoch: 1130 Training Loss: 0.31298530197143554 Test Loss: 0.32437762451171875\n",
      "Epoch: 1131 Training Loss: 0.3128982934951782 Test Loss: 0.3243877360026042\n",
      "Epoch: 1132 Training Loss: 0.3128181718190511 Test Loss: 0.3242797444661458\n",
      "Epoch: 1133 Training Loss: 0.31273717657725014 Test Loss: 0.3241858113606771\n",
      "Epoch: 1134 Training Loss: 0.3126516091028849 Test Loss: 0.3240918172200521\n",
      "Epoch: 1135 Training Loss: 0.31256827640533447 Test Loss: 0.32401786295572915\n",
      "Epoch: 1136 Training Loss: 0.3124860264460246 Test Loss: 0.3239142862955729\n",
      "Epoch: 1137 Training Loss: 0.3124088821411133 Test Loss: 0.3238830973307292\n",
      "Epoch: 1138 Training Loss: 0.3123216149012248 Test Loss: 0.3238008219401042\n",
      "Epoch: 1139 Training Loss: 0.31224669806162514 Test Loss: 0.3237228800455729\n",
      "Epoch: 1140 Training Loss: 0.3121629540125529 Test Loss: 0.3236029052734375\n",
      "Epoch: 1141 Training Loss: 0.31208363564809166 Test Loss: 0.32350885009765623\n",
      "Epoch: 1142 Training Loss: 0.31199720859527585 Test Loss: 0.3234400634765625\n",
      "Epoch: 1143 Training Loss: 0.31191474469502767 Test Loss: 0.3234235432942708\n",
      "Epoch: 1144 Training Loss: 0.31183716042836507 Test Loss: 0.3233063761393229\n",
      "Epoch: 1145 Training Loss: 0.31175439262390137 Test Loss: 0.32325358072916666\n",
      "Epoch: 1146 Training Loss: 0.31167481422424315 Test Loss: 0.32321756998697915\n",
      "Epoch: 1147 Training Loss: 0.31159444681803383 Test Loss: 0.3231088460286458\n",
      "Epoch: 1148 Training Loss: 0.3115142800013224 Test Loss: 0.3230774332682292\n",
      "Epoch: 1149 Training Loss: 0.31142626508076987 Test Loss: 0.32293668619791666\n",
      "Epoch: 1150 Training Loss: 0.3113512674967448 Test Loss: 0.3229040934244792\n",
      "Epoch: 1151 Training Loss: 0.3112740968068441 Test Loss: 0.32282834879557293\n",
      "Epoch: 1152 Training Loss: 0.3111963011423747 Test Loss: 0.32281341552734377\n",
      "Epoch: 1153 Training Loss: 0.31111103598276774 Test Loss: 0.32267041015625\n",
      "Epoch: 1154 Training Loss: 0.31103590806325276 Test Loss: 0.3226017252604167\n",
      "Epoch: 1155 Training Loss: 0.3109574960072835 Test Loss: 0.32253373209635416\n",
      "Epoch: 1156 Training Loss: 0.31087715562184653 Test Loss: 0.3224267578125\n",
      "Epoch: 1157 Training Loss: 0.31079855569203696 Test Loss: 0.32243162027994793\n",
      "Epoch: 1158 Training Loss: 0.3107105588912964 Test Loss: 0.3223038126627604\n",
      "Epoch: 1159 Training Loss: 0.31063487656911215 Test Loss: 0.32229620361328126\n",
      "Epoch: 1160 Training Loss: 0.31055180390675863 Test Loss: 0.3221336669921875\n",
      "Epoch: 1161 Training Loss: 0.3104723968505859 Test Loss: 0.32206227620442707\n",
      "Epoch: 1162 Training Loss: 0.3103920946121216 Test Loss: 0.32198933919270833\n",
      "Epoch: 1163 Training Loss: 0.3103151534398397 Test Loss: 0.3219124755859375\n",
      "Epoch: 1164 Training Loss: 0.31023222001393636 Test Loss: 0.3218267822265625\n",
      "Epoch: 1165 Training Loss: 0.3101548515955607 Test Loss: 0.32174934895833335\n",
      "Epoch: 1166 Training Loss: 0.31007868226369223 Test Loss: 0.32168479410807294\n",
      "Epoch: 1167 Training Loss: 0.309996444384257 Test Loss: 0.3215776774088542\n",
      "Epoch: 1168 Training Loss: 0.309919589360555 Test Loss: 0.32149110921223956\n",
      "Epoch: 1169 Training Loss: 0.30984401162465414 Test Loss: 0.3214515380859375\n",
      "Epoch: 1170 Training Loss: 0.3097591562271118 Test Loss: 0.3213624267578125\n",
      "Epoch: 1171 Training Loss: 0.3096819524765015 Test Loss: 0.3212384440104167\n",
      "Epoch: 1172 Training Loss: 0.30960409196217853 Test Loss: 0.32127018229166665\n",
      "Epoch: 1173 Training Loss: 0.3095228748321533 Test Loss: 0.3211238810221354\n",
      "Epoch: 1174 Training Loss: 0.3094412031173706 Test Loss: 0.3210909423828125\n",
      "Epoch: 1175 Training Loss: 0.3093621959686279 Test Loss: 0.32098828125\n",
      "Epoch: 1176 Training Loss: 0.30928561846415203 Test Loss: 0.3209007771809896\n",
      "Epoch: 1177 Training Loss: 0.3092031450271606 Test Loss: 0.320775634765625\n",
      "Epoch: 1178 Training Loss: 0.3091251996358236 Test Loss: 0.3207475179036458\n",
      "Epoch: 1179 Training Loss: 0.3090491921106974 Test Loss: 0.3206849161783854\n",
      "Epoch: 1180 Training Loss: 0.30896973578135173 Test Loss: 0.32063671875\n",
      "Epoch: 1181 Training Loss: 0.30889542325337727 Test Loss: 0.32056256103515623\n",
      "Epoch: 1182 Training Loss: 0.3088197940190633 Test Loss: 0.32051780192057294\n",
      "Epoch: 1183 Training Loss: 0.3087381871541341 Test Loss: 0.3204178263346354\n",
      "Epoch: 1184 Training Loss: 0.30865894476572675 Test Loss: 0.3203411865234375\n",
      "Epoch: 1185 Training Loss: 0.3085871486663818 Test Loss: 0.3202859293619792\n",
      "Epoch: 1186 Training Loss: 0.3085043198267619 Test Loss: 0.32024361165364584\n",
      "Epoch: 1187 Training Loss: 0.3084321807225545 Test Loss: 0.32011446126302084\n",
      "Epoch: 1188 Training Loss: 0.3083545411427816 Test Loss: 0.32011336263020834\n",
      "Epoch: 1189 Training Loss: 0.3082773987452189 Test Loss: 0.32000390625\n",
      "Epoch: 1190 Training Loss: 0.3082016417185465 Test Loss: 0.31993288167317707\n",
      "Epoch: 1191 Training Loss: 0.30812703863779706 Test Loss: 0.31984999593098956\n",
      "Epoch: 1192 Training Loss: 0.30804600874582927 Test Loss: 0.31979600016276044\n",
      "Epoch: 1193 Training Loss: 0.30796765677134197 Test Loss: 0.31968363444010417\n",
      "Epoch: 1194 Training Loss: 0.30789194043477375 Test Loss: 0.31960099283854165\n",
      "Epoch: 1195 Training Loss: 0.3078166399002075 Test Loss: 0.31959326171875\n",
      "Epoch: 1196 Training Loss: 0.30773784732818604 Test Loss: 0.3194533894856771\n",
      "Epoch: 1197 Training Loss: 0.30766040007273354 Test Loss: 0.31938096110026043\n",
      "Epoch: 1198 Training Loss: 0.3075830014546712 Test Loss: 0.31934151204427086\n",
      "Epoch: 1199 Training Loss: 0.3075057341257731 Test Loss: 0.31929707845052085\n",
      "Epoch: 1200 Training Loss: 0.30743373616536457 Test Loss: 0.3192197265625\n",
      "Epoch: 1201 Training Loss: 0.30735425154368085 Test Loss: 0.319073486328125\n",
      "Epoch: 1202 Training Loss: 0.30728133233388266 Test Loss: 0.31901236979166664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1203 Training Loss: 0.30720476277669273 Test Loss: 0.3189574991861979\n",
      "Epoch: 1204 Training Loss: 0.30713341585795084 Test Loss: 0.31886592610677084\n",
      "Epoch: 1205 Training Loss: 0.30705461661020916 Test Loss: 0.3188690388997396\n",
      "Epoch: 1206 Training Loss: 0.3069822146097819 Test Loss: 0.3187749226888021\n",
      "Epoch: 1207 Training Loss: 0.30690858999888104 Test Loss: 0.3187525431315104\n",
      "Epoch: 1208 Training Loss: 0.3068369293212891 Test Loss: 0.31865301513671873\n",
      "Epoch: 1209 Training Loss: 0.3067618586222331 Test Loss: 0.31861285400390627\n",
      "Epoch: 1210 Training Loss: 0.30668373489379885 Test Loss: 0.318512451171875\n",
      "Epoch: 1211 Training Loss: 0.30661053816477457 Test Loss: 0.31845107014973956\n",
      "Epoch: 1212 Training Loss: 0.3065389019648234 Test Loss: 0.31842295328776044\n",
      "Epoch: 1213 Training Loss: 0.3064639905293783 Test Loss: 0.3183333536783854\n",
      "Epoch: 1214 Training Loss: 0.3063851617177327 Test Loss: 0.31828336588541667\n",
      "Epoch: 1215 Training Loss: 0.3063141202926636 Test Loss: 0.31819891357421876\n",
      "Epoch: 1216 Training Loss: 0.30623354339599607 Test Loss: 0.3181344604492187\n",
      "Epoch: 1217 Training Loss: 0.30616898473103843 Test Loss: 0.3180628255208333\n",
      "Epoch: 1218 Training Loss: 0.3060882209142049 Test Loss: 0.3179606730143229\n",
      "Epoch: 1219 Training Loss: 0.3060166676839193 Test Loss: 0.31789933268229165\n",
      "Epoch: 1220 Training Loss: 0.30594305324554444 Test Loss: 0.31788787841796873\n",
      "Epoch: 1221 Training Loss: 0.305866992632548 Test Loss: 0.3178505655924479\n",
      "Epoch: 1222 Training Loss: 0.30579881223042804 Test Loss: 0.31777827962239585\n",
      "Epoch: 1223 Training Loss: 0.3057201191584269 Test Loss: 0.31768994140625\n",
      "Epoch: 1224 Training Loss: 0.3056469666163127 Test Loss: 0.31763393147786456\n",
      "Epoch: 1225 Training Loss: 0.3055718517303467 Test Loss: 0.31759012858072916\n",
      "Epoch: 1226 Training Loss: 0.30550316874186195 Test Loss: 0.3175341796875\n",
      "Epoch: 1227 Training Loss: 0.30543184852600097 Test Loss: 0.3174713134765625\n",
      "Epoch: 1228 Training Loss: 0.3053532126744588 Test Loss: 0.3173600667317708\n",
      "Epoch: 1229 Training Loss: 0.3052853603363037 Test Loss: 0.3172964070638021\n",
      "Epoch: 1230 Training Loss: 0.30521430365244545 Test Loss: 0.31725465901692707\n",
      "Epoch: 1231 Training Loss: 0.3051421632766724 Test Loss: 0.3171723022460938\n",
      "Epoch: 1232 Training Loss: 0.30506726741790774 Test Loss: 0.3171025390625\n",
      "Epoch: 1233 Training Loss: 0.3049979680379232 Test Loss: 0.3170378824869792\n",
      "Epoch: 1234 Training Loss: 0.3049255609512329 Test Loss: 0.31701509602864586\n",
      "Epoch: 1235 Training Loss: 0.304848552385966 Test Loss: 0.3169278971354167\n",
      "Epoch: 1236 Training Loss: 0.304778395652771 Test Loss: 0.31686055501302085\n",
      "Epoch: 1237 Training Loss: 0.30469869740804034 Test Loss: 0.3167883911132813\n",
      "Epoch: 1238 Training Loss: 0.30463246917724607 Test Loss: 0.31674204508463544\n",
      "Epoch: 1239 Training Loss: 0.3045649693806966 Test Loss: 0.31667238362630207\n",
      "Epoch: 1240 Training Loss: 0.3044923741022746 Test Loss: 0.31663826497395836\n",
      "Epoch: 1241 Training Loss: 0.30441377353668214 Test Loss: 0.3165605265299479\n",
      "Epoch: 1242 Training Loss: 0.30433977890014646 Test Loss: 0.3165201416015625\n",
      "Epoch: 1243 Training Loss: 0.30427349694569905 Test Loss: 0.3164018758138021\n",
      "Epoch: 1244 Training Loss: 0.30420024077097574 Test Loss: 0.3163934733072917\n",
      "Epoch: 1245 Training Loss: 0.30412983703613283 Test Loss: 0.31630354817708334\n",
      "Epoch: 1246 Training Loss: 0.3040614983240763 Test Loss: 0.3162892049153646\n",
      "Epoch: 1247 Training Loss: 0.3039924341837565 Test Loss: 0.31624178059895836\n",
      "Epoch: 1248 Training Loss: 0.30392259375254316 Test Loss: 0.31616819254557293\n",
      "Epoch: 1249 Training Loss: 0.30385274251302086 Test Loss: 0.31606361897786456\n",
      "Epoch: 1250 Training Loss: 0.3037830384572347 Test Loss: 0.31603043619791665\n",
      "Epoch: 1251 Training Loss: 0.3037120745976766 Test Loss: 0.3159365030924479\n",
      "Epoch: 1252 Training Loss: 0.3036380631128947 Test Loss: 0.31591552734375\n",
      "Epoch: 1253 Training Loss: 0.3035718685785929 Test Loss: 0.3158317057291667\n",
      "Epoch: 1254 Training Loss: 0.30350045680999754 Test Loss: 0.3157428385416667\n",
      "Epoch: 1255 Training Loss: 0.3034345919291178 Test Loss: 0.3157153727213542\n",
      "Epoch: 1256 Training Loss: 0.30336106777191163 Test Loss: 0.3156639404296875\n",
      "Epoch: 1257 Training Loss: 0.3032935593922933 Test Loss: 0.3155914103190104\n",
      "Epoch: 1258 Training Loss: 0.30322884782155357 Test Loss: 0.31552616373697917\n",
      "Epoch: 1259 Training Loss: 0.30316023127237957 Test Loss: 0.31544158935546873\n",
      "Epoch: 1260 Training Loss: 0.3030832217534383 Test Loss: 0.3154056193033854\n",
      "Epoch: 1261 Training Loss: 0.3030163923899333 Test Loss: 0.3153292236328125\n",
      "Epoch: 1262 Training Loss: 0.3029469871520996 Test Loss: 0.3152701416015625\n",
      "Epoch: 1263 Training Loss: 0.30288733609517415 Test Loss: 0.3152233479817708\n",
      "Epoch: 1264 Training Loss: 0.3028119109471639 Test Loss: 0.3151798095703125\n",
      "Epoch: 1265 Training Loss: 0.30274813524882 Test Loss: 0.31510599772135417\n",
      "Epoch: 1266 Training Loss: 0.30267755444844563 Test Loss: 0.31505411783854165\n",
      "Epoch: 1267 Training Loss: 0.3026147394180298 Test Loss: 0.3150099283854167\n",
      "Epoch: 1268 Training Loss: 0.30254707876841225 Test Loss: 0.31490171305338543\n",
      "Epoch: 1269 Training Loss: 0.3024798536300659 Test Loss: 0.31486702473958333\n",
      "Epoch: 1270 Training Loss: 0.3024099753697713 Test Loss: 0.3147923990885417\n",
      "Epoch: 1271 Training Loss: 0.30234642124176025 Test Loss: 0.31478696695963543\n",
      "Epoch: 1272 Training Loss: 0.30227746518452964 Test Loss: 0.31466119384765623\n",
      "Epoch: 1273 Training Loss: 0.3022093855539958 Test Loss: 0.31464794921875\n",
      "Epoch: 1274 Training Loss: 0.3021429713567098 Test Loss: 0.31454850260416667\n",
      "Epoch: 1275 Training Loss: 0.3020712022781372 Test Loss: 0.31448779296875\n",
      "Epoch: 1276 Training Loss: 0.30200787512461347 Test Loss: 0.31444403076171873\n",
      "Epoch: 1277 Training Loss: 0.3019380588531494 Test Loss: 0.3144009602864583\n",
      "Epoch: 1278 Training Loss: 0.3018719170888265 Test Loss: 0.3143551025390625\n",
      "Epoch: 1279 Training Loss: 0.3018005965550741 Test Loss: 0.3142257893880208\n",
      "Epoch: 1280 Training Loss: 0.3017326669692993 Test Loss: 0.31420003255208334\n",
      "Epoch: 1281 Training Loss: 0.30166462643941244 Test Loss: 0.3141178385416667\n",
      "Epoch: 1282 Training Loss: 0.3016021633148193 Test Loss: 0.3140601399739583\n",
      "Epoch: 1283 Training Loss: 0.3015336850484212 Test Loss: 0.31402467854817706\n",
      "Epoch: 1284 Training Loss: 0.3014672555923462 Test Loss: 0.3139508056640625\n",
      "Epoch: 1285 Training Loss: 0.30140298302968344 Test Loss: 0.31388663736979167\n",
      "Epoch: 1286 Training Loss: 0.3013441031773885 Test Loss: 0.31388533528645834\n",
      "Epoch: 1287 Training Loss: 0.3012751410802205 Test Loss: 0.31378474934895834\n",
      "Epoch: 1288 Training Loss: 0.30121032301584877 Test Loss: 0.3137539265950521\n",
      "Epoch: 1289 Training Loss: 0.3011478900909424 Test Loss: 0.3136695556640625\n",
      "Epoch: 1290 Training Loss: 0.30107901414235433 Test Loss: 0.3136439412434896\n",
      "Epoch: 1291 Training Loss: 0.30101524511973066 Test Loss: 0.31356913248697915\n",
      "Epoch: 1292 Training Loss: 0.30095349915822345 Test Loss: 0.3135274251302083\n",
      "Epoch: 1293 Training Loss: 0.30088500849405925 Test Loss: 0.31343988037109377\n",
      "Epoch: 1294 Training Loss: 0.30081919733683266 Test Loss: 0.3133463134765625\n",
      "Epoch: 1295 Training Loss: 0.3007532682418823 Test Loss: 0.3133638102213542\n",
      "Epoch: 1296 Training Loss: 0.3006936575571696 Test Loss: 0.31334578450520834\n",
      "Epoch: 1297 Training Loss: 0.3006340430577596 Test Loss: 0.31325040690104167\n",
      "Epoch: 1298 Training Loss: 0.3005637140274048 Test Loss: 0.3132600708007813\n",
      "Epoch: 1299 Training Loss: 0.30049861907958986 Test Loss: 0.3131460164388021\n",
      "Epoch: 1300 Training Loss: 0.3004295593897502 Test Loss: 0.31320318603515623\n",
      "Epoch: 1301 Training Loss: 0.3003663314183553 Test Loss: 0.31314300537109374\n",
      "Epoch: 1302 Training Loss: 0.3002961638768514 Test Loss: 0.31299275716145836\n",
      "Epoch: 1303 Training Loss: 0.30023390038808184 Test Loss: 0.31300341796875\n",
      "Epoch: 1304 Training Loss: 0.3001728620529175 Test Loss: 0.3129320068359375\n",
      "Epoch: 1305 Training Loss: 0.3001076240539551 Test Loss: 0.312809326171875\n",
      "Epoch: 1306 Training Loss: 0.3000447069803874 Test Loss: 0.3128096720377604\n",
      "Epoch: 1307 Training Loss: 0.29998376592000325 Test Loss: 0.31276373291015624\n",
      "Epoch: 1308 Training Loss: 0.29992318884531655 Test Loss: 0.31272076416015626\n",
      "Epoch: 1309 Training Loss: 0.2998573694229126 Test Loss: 0.3125997314453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1310 Training Loss: 0.29979412810007733 Test Loss: 0.31259027099609377\n",
      "Epoch: 1311 Training Loss: 0.2997267100016276 Test Loss: 0.3125220743815104\n",
      "Epoch: 1312 Training Loss: 0.2996635955174764 Test Loss: 0.3124141642252604\n",
      "Epoch: 1313 Training Loss: 0.2996004098256429 Test Loss: 0.31241569010416664\n",
      "Epoch: 1314 Training Loss: 0.2995378278096517 Test Loss: 0.312353515625\n",
      "Epoch: 1315 Training Loss: 0.2994706087112427 Test Loss: 0.31229911295572915\n",
      "Epoch: 1316 Training Loss: 0.2994172223409017 Test Loss: 0.31226200358072914\n",
      "Epoch: 1317 Training Loss: 0.2993518368403117 Test Loss: 0.3121912434895833\n",
      "Epoch: 1318 Training Loss: 0.29929419072469077 Test Loss: 0.31215130615234377\n",
      "Epoch: 1319 Training Loss: 0.2992219616572062 Test Loss: 0.3120646158854167\n",
      "Epoch: 1320 Training Loss: 0.2991656084060669 Test Loss: 0.3120458984375\n",
      "Epoch: 1321 Training Loss: 0.2991012137730916 Test Loss: 0.31199662272135414\n",
      "Epoch: 1322 Training Loss: 0.299044633547465 Test Loss: 0.31193707275390625\n",
      "Epoch: 1323 Training Loss: 0.2989805113474528 Test Loss: 0.3118500569661458\n",
      "Epoch: 1324 Training Loss: 0.29891397221883137 Test Loss: 0.31179752604166666\n",
      "Epoch: 1325 Training Loss: 0.2988585182825724 Test Loss: 0.3117746785481771\n",
      "Epoch: 1326 Training Loss: 0.2987949676513672 Test Loss: 0.31166170247395836\n",
      "Epoch: 1327 Training Loss: 0.2987374693552653 Test Loss: 0.3116497192382813\n",
      "Epoch: 1328 Training Loss: 0.2986729440689087 Test Loss: 0.31165273030598956\n",
      "Epoch: 1329 Training Loss: 0.2986184059778849 Test Loss: 0.31157084147135417\n",
      "Epoch: 1330 Training Loss: 0.2985526466369629 Test Loss: 0.31151338704427084\n",
      "Epoch: 1331 Training Loss: 0.29849238554636637 Test Loss: 0.31148583984375\n",
      "Epoch: 1332 Training Loss: 0.29842811584472656 Test Loss: 0.3113677571614583\n",
      "Epoch: 1333 Training Loss: 0.29836763159434 Test Loss: 0.3113770345052083\n",
      "Epoch: 1334 Training Loss: 0.29830070877075193 Test Loss: 0.3113020426432292\n",
      "Epoch: 1335 Training Loss: 0.2982399177551269 Test Loss: 0.31121103922526044\n",
      "Epoch: 1336 Training Loss: 0.29818484592437744 Test Loss: 0.31117787679036457\n",
      "Epoch: 1337 Training Loss: 0.29811638259887696 Test Loss: 0.3111506551106771\n",
      "Epoch: 1338 Training Loss: 0.2980563596089681 Test Loss: 0.3110539957682292\n",
      "Epoch: 1339 Training Loss: 0.29799881744384765 Test Loss: 0.31098876953125\n",
      "Epoch: 1340 Training Loss: 0.2979312826792399 Test Loss: 0.31097310384114585\n",
      "Epoch: 1341 Training Loss: 0.2978805176417033 Test Loss: 0.3109283854166667\n",
      "Epoch: 1342 Training Loss: 0.29781715138753256 Test Loss: 0.310888427734375\n",
      "Epoch: 1343 Training Loss: 0.29775490538279215 Test Loss: 0.31087422688802085\n",
      "Epoch: 1344 Training Loss: 0.29769199148813885 Test Loss: 0.3108119913736979\n",
      "Epoch: 1345 Training Loss: 0.2976361627578735 Test Loss: 0.31079862467447916\n",
      "Epoch: 1346 Training Loss: 0.2975748519897461 Test Loss: 0.31071280924479167\n",
      "Epoch: 1347 Training Loss: 0.2975138568878174 Test Loss: 0.310680908203125\n",
      "Epoch: 1348 Training Loss: 0.29744605541229246 Test Loss: 0.3106346842447917\n",
      "Epoch: 1349 Training Loss: 0.29738377634684243 Test Loss: 0.3106061604817708\n",
      "Epoch: 1350 Training Loss: 0.2973238312403361 Test Loss: 0.3104984537760417\n",
      "Epoch: 1351 Training Loss: 0.29725989564259847 Test Loss: 0.31043373616536457\n",
      "Epoch: 1352 Training Loss: 0.29719989649454753 Test Loss: 0.3104031982421875\n",
      "Epoch: 1353 Training Loss: 0.2971334679921468 Test Loss: 0.310369140625\n",
      "Epoch: 1354 Training Loss: 0.2970782947540283 Test Loss: 0.31024625651041665\n",
      "Epoch: 1355 Training Loss: 0.2970186754862467 Test Loss: 0.310226806640625\n",
      "Epoch: 1356 Training Loss: 0.2969596293767293 Test Loss: 0.31017154947916664\n",
      "Epoch: 1357 Training Loss: 0.2969042835235596 Test Loss: 0.31017696126302086\n",
      "Epoch: 1358 Training Loss: 0.29683949184417724 Test Loss: 0.31003857421875\n",
      "Epoch: 1359 Training Loss: 0.29678165340423585 Test Loss: 0.3099937947591146\n",
      "Epoch: 1360 Training Loss: 0.29672173500061033 Test Loss: 0.3099580078125\n",
      "Epoch: 1361 Training Loss: 0.29665728378295897 Test Loss: 0.30987910970052085\n",
      "Epoch: 1362 Training Loss: 0.29660036563873293 Test Loss: 0.3098519287109375\n",
      "Epoch: 1363 Training Loss: 0.2965389827092489 Test Loss: 0.3098604736328125\n",
      "Epoch: 1364 Training Loss: 0.2964823300043742 Test Loss: 0.30982657877604164\n",
      "Epoch: 1365 Training Loss: 0.29642437585194903 Test Loss: 0.30974609375\n",
      "Epoch: 1366 Training Loss: 0.29636614418029783 Test Loss: 0.3096968180338542\n",
      "Epoch: 1367 Training Loss: 0.2963038012186686 Test Loss: 0.30964335123697917\n",
      "Epoch: 1368 Training Loss: 0.296242301940918 Test Loss: 0.3095799560546875\n",
      "Epoch: 1369 Training Loss: 0.29618180910746256 Test Loss: 0.3095152180989583\n",
      "Epoch: 1370 Training Loss: 0.2961213191350301 Test Loss: 0.3094652506510417\n",
      "Epoch: 1371 Training Loss: 0.2960598824818929 Test Loss: 0.30940610758463544\n",
      "Epoch: 1372 Training Loss: 0.2959936424891154 Test Loss: 0.3093456013997396\n",
      "Epoch: 1373 Training Loss: 0.2959447603225708 Test Loss: 0.3093155314127604\n",
      "Epoch: 1374 Training Loss: 0.295882261912028 Test Loss: 0.3091545817057292\n",
      "Epoch: 1375 Training Loss: 0.29582628218332924 Test Loss: 0.3090823974609375\n",
      "Epoch: 1376 Training Loss: 0.29576511414845785 Test Loss: 0.30916389973958336\n",
      "Epoch: 1377 Training Loss: 0.2957066551844279 Test Loss: 0.30905118815104166\n",
      "Epoch: 1378 Training Loss: 0.2956435883839925 Test Loss: 0.30897932942708334\n",
      "Epoch: 1379 Training Loss: 0.2955888586044311 Test Loss: 0.3089144287109375\n",
      "Epoch: 1380 Training Loss: 0.29553099505106606 Test Loss: 0.3088830362955729\n",
      "Epoch: 1381 Training Loss: 0.2954747234980265 Test Loss: 0.30881392415364584\n",
      "Epoch: 1382 Training Loss: 0.2954163049062093 Test Loss: 0.30875577799479165\n",
      "Epoch: 1383 Training Loss: 0.2953641055425008 Test Loss: 0.30878377278645835\n",
      "Epoch: 1384 Training Loss: 0.2953034750620524 Test Loss: 0.3087239990234375\n",
      "Epoch: 1385 Training Loss: 0.2952491238911947 Test Loss: 0.30866731770833333\n",
      "Epoch: 1386 Training Loss: 0.29518504651387534 Test Loss: 0.30858321126302085\n",
      "Epoch: 1387 Training Loss: 0.2951316086451213 Test Loss: 0.3085274251302083\n",
      "Epoch: 1388 Training Loss: 0.29506789906819664 Test Loss: 0.3084698689778646\n",
      "Epoch: 1389 Training Loss: 0.29500861899058023 Test Loss: 0.30845780436197917\n",
      "Epoch: 1390 Training Loss: 0.294947478612264 Test Loss: 0.3083905029296875\n",
      "Epoch: 1391 Training Loss: 0.29489031569163004 Test Loss: 0.3083693033854167\n",
      "Epoch: 1392 Training Loss: 0.2948316977818807 Test Loss: 0.3082826131184896\n",
      "Epoch: 1393 Training Loss: 0.29477360407511394 Test Loss: 0.3082317911783854\n",
      "Epoch: 1394 Training Loss: 0.2947146081924438 Test Loss: 0.30815152994791667\n",
      "Epoch: 1395 Training Loss: 0.2946516755421956 Test Loss: 0.30816617838541666\n",
      "Epoch: 1396 Training Loss: 0.29459400526682533 Test Loss: 0.30804296875\n",
      "Epoch: 1397 Training Loss: 0.29453616523742676 Test Loss: 0.30806514485677083\n",
      "Epoch: 1398 Training Loss: 0.29447141488393147 Test Loss: 0.30798042805989584\n",
      "Epoch: 1399 Training Loss: 0.2944166523615519 Test Loss: 0.30793988037109377\n",
      "Epoch: 1400 Training Loss: 0.29435813236236574 Test Loss: 0.3078511759440104\n",
      "Epoch: 1401 Training Loss: 0.294298246383667 Test Loss: 0.30776387532552085\n",
      "Epoch: 1402 Training Loss: 0.2942302977244059 Test Loss: 0.3077398681640625\n",
      "Epoch: 1403 Training Loss: 0.29416858132680257 Test Loss: 0.30773486328125\n",
      "Epoch: 1404 Training Loss: 0.2941110827128092 Test Loss: 0.30761568196614586\n",
      "Epoch: 1405 Training Loss: 0.2940419260660807 Test Loss: 0.3075657958984375\n",
      "Epoch: 1406 Training Loss: 0.2939841833114624 Test Loss: 0.307530517578125\n",
      "Epoch: 1407 Training Loss: 0.29392324956258137 Test Loss: 0.3074347330729167\n",
      "Epoch: 1408 Training Loss: 0.2938564561208089 Test Loss: 0.30744207763671877\n",
      "Epoch: 1409 Training Loss: 0.2937997630437215 Test Loss: 0.30739984130859377\n",
      "Epoch: 1410 Training Loss: 0.2937303603490194 Test Loss: 0.30732364908854165\n",
      "Epoch: 1411 Training Loss: 0.29367356904347736 Test Loss: 0.3071974487304687\n",
      "Epoch: 1412 Training Loss: 0.2936149886449178 Test Loss: 0.307155029296875\n",
      "Epoch: 1413 Training Loss: 0.2935489282608032 Test Loss: 0.30716817220052084\n",
      "Epoch: 1414 Training Loss: 0.2934925769170125 Test Loss: 0.3070269368489583\n",
      "Epoch: 1415 Training Loss: 0.2934255278905233 Test Loss: 0.30701363118489583\n",
      "Epoch: 1416 Training Loss: 0.29337071323394776 Test Loss: 0.30695760091145835\n",
      "Epoch: 1417 Training Loss: 0.2933050193786621 Test Loss: 0.30692313639322916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1418 Training Loss: 0.2932441151936849 Test Loss: 0.3068389689127604\n",
      "Epoch: 1419 Training Loss: 0.29318757184346517 Test Loss: 0.3068300984700521\n",
      "Epoch: 1420 Training Loss: 0.29311890443166094 Test Loss: 0.3067962443033854\n",
      "Epoch: 1421 Training Loss: 0.2930618848800659 Test Loss: 0.30669624837239584\n",
      "Epoch: 1422 Training Loss: 0.29299680296579994 Test Loss: 0.3067003377278646\n",
      "Epoch: 1423 Training Loss: 0.29293664582570395 Test Loss: 0.30665043131510417\n",
      "Epoch: 1424 Training Loss: 0.29287703927357994 Test Loss: 0.3066077067057292\n",
      "Epoch: 1425 Training Loss: 0.2928158982594808 Test Loss: 0.3065081787109375\n",
      "Epoch: 1426 Training Loss: 0.29275306288401287 Test Loss: 0.30649210611979166\n",
      "Epoch: 1427 Training Loss: 0.29269040711720784 Test Loss: 0.30643532307942706\n",
      "Epoch: 1428 Training Loss: 0.2926290836334228 Test Loss: 0.3063732706705729\n",
      "Epoch: 1429 Training Loss: 0.29257444953918454 Test Loss: 0.3062989501953125\n",
      "Epoch: 1430 Training Loss: 0.29250654888153077 Test Loss: 0.30630938720703127\n",
      "Epoch: 1431 Training Loss: 0.2924490617116292 Test Loss: 0.306228515625\n",
      "Epoch: 1432 Training Loss: 0.29238800080617267 Test Loss: 0.30615863037109375\n",
      "Epoch: 1433 Training Loss: 0.29233038107554116 Test Loss: 0.3062105712890625\n",
      "Epoch: 1434 Training Loss: 0.2922617588043213 Test Loss: 0.30615531412760416\n",
      "Epoch: 1435 Training Loss: 0.29220212904612225 Test Loss: 0.3060296630859375\n",
      "Epoch: 1436 Training Loss: 0.29214324537913006 Test Loss: 0.3059830322265625\n",
      "Epoch: 1437 Training Loss: 0.2920856612523397 Test Loss: 0.30591048177083335\n",
      "Epoch: 1438 Training Loss: 0.2920226078033447 Test Loss: 0.3059287516276042\n",
      "Epoch: 1439 Training Loss: 0.291969118754069 Test Loss: 0.3058863118489583\n",
      "Epoch: 1440 Training Loss: 0.29190657202402753 Test Loss: 0.3058304443359375\n",
      "Epoch: 1441 Training Loss: 0.2918494965235392 Test Loss: 0.30578466796875\n",
      "Epoch: 1442 Training Loss: 0.2917902628580729 Test Loss: 0.3057232869466146\n",
      "Epoch: 1443 Training Loss: 0.29173152192433677 Test Loss: 0.3056456909179687\n",
      "Epoch: 1444 Training Loss: 0.29167392349243165 Test Loss: 0.30559611002604165\n",
      "Epoch: 1445 Training Loss: 0.29161008008321126 Test Loss: 0.30557216389973957\n",
      "Epoch: 1446 Training Loss: 0.2915514030456543 Test Loss: 0.30552400716145833\n",
      "Epoch: 1447 Training Loss: 0.2914955145517985 Test Loss: 0.30551643880208335\n",
      "Epoch: 1448 Training Loss: 0.2914416561126709 Test Loss: 0.30540380859375\n",
      "Epoch: 1449 Training Loss: 0.2913840964635213 Test Loss: 0.30538712565104165\n",
      "Epoch: 1450 Training Loss: 0.29132909361521403 Test Loss: 0.3053329671223958\n",
      "Epoch: 1451 Training Loss: 0.29126989618937177 Test Loss: 0.30532607014973956\n",
      "Epoch: 1452 Training Loss: 0.29121339893341064 Test Loss: 0.30529066975911456\n",
      "Epoch: 1453 Training Loss: 0.291151912689209 Test Loss: 0.3051966552734375\n",
      "Epoch: 1454 Training Loss: 0.2910979668299357 Test Loss: 0.3051395263671875\n",
      "Epoch: 1455 Training Loss: 0.2910343141555786 Test Loss: 0.30512044270833333\n",
      "Epoch: 1456 Training Loss: 0.29097753715515134 Test Loss: 0.3051040852864583\n",
      "Epoch: 1457 Training Loss: 0.2909183874130249 Test Loss: 0.3050889078776042\n",
      "Epoch: 1458 Training Loss: 0.2908613545099894 Test Loss: 0.3050225830078125\n",
      "Epoch: 1459 Training Loss: 0.29080022939046224 Test Loss: 0.3049720458984375\n",
      "Epoch: 1460 Training Loss: 0.29074106566111246 Test Loss: 0.3049599405924479\n",
      "Epoch: 1461 Training Loss: 0.29068156878153484 Test Loss: 0.30491097005208334\n",
      "Epoch: 1462 Training Loss: 0.2906270147959391 Test Loss: 0.30483793131510417\n",
      "Epoch: 1463 Training Loss: 0.29056893444061277 Test Loss: 0.3048263753255208\n",
      "Epoch: 1464 Training Loss: 0.2905124559402466 Test Loss: 0.3047580159505208\n",
      "Epoch: 1465 Training Loss: 0.2904594653447469 Test Loss: 0.30470039876302085\n",
      "Epoch: 1466 Training Loss: 0.29040183321634927 Test Loss: 0.30461710611979165\n",
      "Epoch: 1467 Training Loss: 0.2903473129272461 Test Loss: 0.3046151529947917\n",
      "Epoch: 1468 Training Loss: 0.29028610515594483 Test Loss: 0.30453009033203127\n",
      "Epoch: 1469 Training Loss: 0.29023086865743003 Test Loss: 0.304523193359375\n",
      "Epoch: 1470 Training Loss: 0.29017259883880614 Test Loss: 0.3044479573567708\n",
      "Epoch: 1471 Training Loss: 0.2901182959874471 Test Loss: 0.30441888427734376\n",
      "Epoch: 1472 Training Loss: 0.29005933221181235 Test Loss: 0.30430657958984375\n",
      "Epoch: 1473 Training Loss: 0.290001062075297 Test Loss: 0.30436476643880206\n",
      "Epoch: 1474 Training Loss: 0.28994853115081787 Test Loss: 0.30429351806640625\n",
      "Epoch: 1475 Training Loss: 0.2898930231730143 Test Loss: 0.3041850179036458\n",
      "Epoch: 1476 Training Loss: 0.2898338762919108 Test Loss: 0.30414798990885417\n",
      "Epoch: 1477 Training Loss: 0.289780286471049 Test Loss: 0.304145263671875\n",
      "Epoch: 1478 Training Loss: 0.2897231019337972 Test Loss: 0.30410087076822917\n",
      "Epoch: 1479 Training Loss: 0.2896669807434082 Test Loss: 0.3040931396484375\n",
      "Epoch: 1480 Training Loss: 0.28961429627736407 Test Loss: 0.303967041015625\n",
      "Epoch: 1481 Training Loss: 0.28955873648325603 Test Loss: 0.30392069498697916\n",
      "Epoch: 1482 Training Loss: 0.2895030558904012 Test Loss: 0.3039132080078125\n",
      "Epoch: 1483 Training Loss: 0.28945073127746584 Test Loss: 0.3037653605143229\n",
      "Epoch: 1484 Training Loss: 0.28939047622680664 Test Loss: 0.30381868489583336\n",
      "Epoch: 1485 Training Loss: 0.2893327236175537 Test Loss: 0.3037486572265625\n",
      "Epoch: 1486 Training Loss: 0.28927714252471926 Test Loss: 0.3037640380859375\n",
      "Epoch: 1487 Training Loss: 0.28922427972157794 Test Loss: 0.30364298502604165\n",
      "Epoch: 1488 Training Loss: 0.2891614376703898 Test Loss: 0.30357572428385415\n",
      "Epoch: 1489 Training Loss: 0.2891051305135091 Test Loss: 0.30351485188802085\n",
      "Epoch: 1490 Training Loss: 0.2890523977279663 Test Loss: 0.3034878336588542\n",
      "Epoch: 1491 Training Loss: 0.28898885599772134 Test Loss: 0.30339202880859373\n",
      "Epoch: 1492 Training Loss: 0.28893980820973714 Test Loss: 0.3033748575846354\n",
      "Epoch: 1493 Training Loss: 0.28887624009450275 Test Loss: 0.30334657796223957\n",
      "Epoch: 1494 Training Loss: 0.28881304105122885 Test Loss: 0.3032123006184896\n",
      "Epoch: 1495 Training Loss: 0.28875963656107584 Test Loss: 0.3032057902018229\n",
      "Epoch: 1496 Training Loss: 0.2887030725479126 Test Loss: 0.303240478515625\n",
      "Epoch: 1497 Training Loss: 0.28865036328633625 Test Loss: 0.30306418863932294\n",
      "Epoch: 1498 Training Loss: 0.2885941416422526 Test Loss: 0.3031061197916667\n",
      "Epoch: 1499 Training Loss: 0.2885417753855387 Test Loss: 0.30302237955729167\n",
      "Epoch: 1500 Training Loss: 0.28847864691416425 Test Loss: 0.3029731852213542\n",
      "Epoch: 1501 Training Loss: 0.28843008931477865 Test Loss: 0.30294287109375\n",
      "Epoch: 1502 Training Loss: 0.28837895901997884 Test Loss: 0.3028809814453125\n",
      "Epoch: 1503 Training Loss: 0.2883223876953125 Test Loss: 0.30283028157552083\n",
      "Epoch: 1504 Training Loss: 0.2882686735788981 Test Loss: 0.3027430013020833\n",
      "Epoch: 1505 Training Loss: 0.2882126871744792 Test Loss: 0.30272412109375\n",
      "Epoch: 1506 Training Loss: 0.28814898363749186 Test Loss: 0.30272281901041664\n",
      "Epoch: 1507 Training Loss: 0.2880960454940796 Test Loss: 0.30268316650390625\n",
      "Epoch: 1508 Training Loss: 0.28803946018218995 Test Loss: 0.3026455078125\n",
      "Epoch: 1509 Training Loss: 0.28798051675160724 Test Loss: 0.3025447998046875\n",
      "Epoch: 1510 Training Loss: 0.28792889054616294 Test Loss: 0.3025317789713542\n",
      "Epoch: 1511 Training Loss: 0.2878698565165202 Test Loss: 0.30246927897135417\n",
      "Epoch: 1512 Training Loss: 0.2878086840311686 Test Loss: 0.3023871866861979\n",
      "Epoch: 1513 Training Loss: 0.28775874932607015 Test Loss: 0.30239060465494794\n",
      "Epoch: 1514 Training Loss: 0.28769776566823324 Test Loss: 0.30230570475260415\n",
      "Epoch: 1515 Training Loss: 0.2876430966059367 Test Loss: 0.3023494873046875\n",
      "Epoch: 1516 Training Loss: 0.287584729830424 Test Loss: 0.30222672526041666\n",
      "Epoch: 1517 Training Loss: 0.28753309694925944 Test Loss: 0.30222389729817706\n",
      "Epoch: 1518 Training Loss: 0.28747944831848143 Test Loss: 0.30220113118489583\n",
      "Epoch: 1519 Training Loss: 0.2874194132486979 Test Loss: 0.3021168212890625\n",
      "Epoch: 1520 Training Loss: 0.28736223284403484 Test Loss: 0.30209517415364584\n",
      "Epoch: 1521 Training Loss: 0.28731182098388675 Test Loss: 0.302049560546875\n",
      "Epoch: 1522 Training Loss: 0.2872547731399536 Test Loss: 0.30200826009114584\n",
      "Epoch: 1523 Training Loss: 0.2871999225616455 Test Loss: 0.3019552001953125\n",
      "Epoch: 1524 Training Loss: 0.28714522647857665 Test Loss: 0.30188924153645835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1525 Training Loss: 0.28708962217966716 Test Loss: 0.30192197672526044\n",
      "Epoch: 1526 Training Loss: 0.2870446926752726 Test Loss: 0.3018129069010417\n",
      "Epoch: 1527 Training Loss: 0.2869813060760498 Test Loss: 0.30177996826171877\n",
      "Epoch: 1528 Training Loss: 0.2869207979838053 Test Loss: 0.3017803955078125\n",
      "Epoch: 1529 Training Loss: 0.28686748758951824 Test Loss: 0.3017403564453125\n",
      "Epoch: 1530 Training Loss: 0.2868137111663818 Test Loss: 0.30166505940755206\n",
      "Epoch: 1531 Training Loss: 0.28675297768910724 Test Loss: 0.30161397298177084\n",
      "Epoch: 1532 Training Loss: 0.28670579783121747 Test Loss: 0.3016598714192708\n",
      "Epoch: 1533 Training Loss: 0.2866488097508748 Test Loss: 0.3015720011393229\n",
      "Epoch: 1534 Training Loss: 0.28659380213419594 Test Loss: 0.30146754964192707\n",
      "Epoch: 1535 Training Loss: 0.28654178778330486 Test Loss: 0.30144099934895835\n",
      "Epoch: 1536 Training Loss: 0.2864822130203247 Test Loss: 0.30140848795572917\n",
      "Epoch: 1537 Training Loss: 0.2864330908457438 Test Loss: 0.3013865966796875\n",
      "Epoch: 1538 Training Loss: 0.28637683900197347 Test Loss: 0.30129311116536456\n",
      "Epoch: 1539 Training Loss: 0.2863185625076294 Test Loss: 0.30125374348958334\n",
      "Epoch: 1540 Training Loss: 0.2862693049112956 Test Loss: 0.3012654825846354\n",
      "Epoch: 1541 Training Loss: 0.28620967610677084 Test Loss: 0.30118233235677083\n",
      "Epoch: 1542 Training Loss: 0.2861564105351766 Test Loss: 0.3011483154296875\n",
      "Epoch: 1543 Training Loss: 0.2861013644536336 Test Loss: 0.3011077067057292\n",
      "Epoch: 1544 Training Loss: 0.28604182291030883 Test Loss: 0.301091552734375\n",
      "Epoch: 1545 Training Loss: 0.28599310779571535 Test Loss: 0.30096160888671875\n",
      "Epoch: 1546 Training Loss: 0.2859312675793966 Test Loss: 0.3010008138020833\n",
      "Epoch: 1547 Training Loss: 0.28587309551239015 Test Loss: 0.3009484049479167\n",
      "Epoch: 1548 Training Loss: 0.28581676133473716 Test Loss: 0.300848388671875\n",
      "Epoch: 1549 Training Loss: 0.2857621332804362 Test Loss: 0.3007982177734375\n",
      "Epoch: 1550 Training Loss: 0.2857140652338664 Test Loss: 0.3007622477213542\n",
      "Epoch: 1551 Training Loss: 0.28565506013234454 Test Loss: 0.30075193277994794\n",
      "Epoch: 1552 Training Loss: 0.2856049642562866 Test Loss: 0.3006740315755208\n",
      "Epoch: 1553 Training Loss: 0.28554594739278155 Test Loss: 0.300663330078125\n",
      "Epoch: 1554 Training Loss: 0.28549365170796714 Test Loss: 0.30061859130859375\n",
      "Epoch: 1555 Training Loss: 0.28543919134140017 Test Loss: 0.3005040283203125\n",
      "Epoch: 1556 Training Loss: 0.285380573908488 Test Loss: 0.30054142252604166\n",
      "Epoch: 1557 Training Loss: 0.28532581265767415 Test Loss: 0.30049654134114584\n",
      "Epoch: 1558 Training Loss: 0.28526613728205363 Test Loss: 0.30044816080729164\n",
      "Epoch: 1559 Training Loss: 0.28521228122711184 Test Loss: 0.30026472981770835\n",
      "Epoch: 1560 Training Loss: 0.2851591353416443 Test Loss: 0.3003088175455729\n",
      "Epoch: 1561 Training Loss: 0.2851053064664205 Test Loss: 0.30024381510416664\n",
      "Epoch: 1562 Training Loss: 0.28504980182647705 Test Loss: 0.30021415201822915\n",
      "Epoch: 1563 Training Loss: 0.28499554840723673 Test Loss: 0.3001552327473958\n",
      "Epoch: 1564 Training Loss: 0.2849389055569967 Test Loss: 0.3001370849609375\n",
      "Epoch: 1565 Training Loss: 0.28488365650177 Test Loss: 0.3000618693033854\n",
      "Epoch: 1566 Training Loss: 0.2848279201189677 Test Loss: 0.30007942708333335\n",
      "Epoch: 1567 Training Loss: 0.2847771889368693 Test Loss: 0.2999802449544271\n",
      "Epoch: 1568 Training Loss: 0.2847266664505005 Test Loss: 0.299951904296875\n",
      "Epoch: 1569 Training Loss: 0.2846602398554484 Test Loss: 0.29991912841796875\n",
      "Epoch: 1570 Training Loss: 0.28461114676793414 Test Loss: 0.29983203125\n",
      "Epoch: 1571 Training Loss: 0.28455594873428347 Test Loss: 0.29986124674479164\n",
      "Epoch: 1572 Training Loss: 0.28450421619415284 Test Loss: 0.29981144205729165\n",
      "Epoch: 1573 Training Loss: 0.28445031690597533 Test Loss: 0.2997462158203125\n",
      "Epoch: 1574 Training Loss: 0.2843913942972819 Test Loss: 0.299685546875\n",
      "Epoch: 1575 Training Loss: 0.2843381543159485 Test Loss: 0.2996412353515625\n",
      "Epoch: 1576 Training Loss: 0.2842868553797404 Test Loss: 0.2995848795572917\n",
      "Epoch: 1577 Training Loss: 0.2842268048922221 Test Loss: 0.29955904134114586\n",
      "Epoch: 1578 Training Loss: 0.28417656310399375 Test Loss: 0.29949735514322917\n",
      "Epoch: 1579 Training Loss: 0.2841239334742228 Test Loss: 0.29948199462890623\n",
      "Epoch: 1580 Training Loss: 0.2840724458694458 Test Loss: 0.2994163818359375\n",
      "Epoch: 1581 Training Loss: 0.2840103635787964 Test Loss: 0.29943861897786456\n",
      "Epoch: 1582 Training Loss: 0.28397101354598997 Test Loss: 0.29936100260416665\n",
      "Epoch: 1583 Training Loss: 0.28391342385609947 Test Loss: 0.2992942708333333\n",
      "Epoch: 1584 Training Loss: 0.2838666842778524 Test Loss: 0.29927742513020833\n",
      "Epoch: 1585 Training Loss: 0.2838032704989115 Test Loss: 0.2992487386067708\n",
      "Epoch: 1586 Training Loss: 0.2837529578208923 Test Loss: 0.29922273763020835\n",
      "Epoch: 1587 Training Loss: 0.2836939949989319 Test Loss: 0.2991068115234375\n",
      "Epoch: 1588 Training Loss: 0.2836450510025024 Test Loss: 0.2990794677734375\n",
      "Epoch: 1589 Training Loss: 0.28358761755625406 Test Loss: 0.29904803466796875\n",
      "Epoch: 1590 Training Loss: 0.28353598435719807 Test Loss: 0.2989972941080729\n",
      "Epoch: 1591 Training Loss: 0.2834768195152283 Test Loss: 0.2990057373046875\n",
      "Epoch: 1592 Training Loss: 0.2834265486399333 Test Loss: 0.2989244384765625\n",
      "Epoch: 1593 Training Loss: 0.28336976925532026 Test Loss: 0.2988846638997396\n",
      "Epoch: 1594 Training Loss: 0.2833130941390991 Test Loss: 0.29884147135416667\n",
      "Epoch: 1595 Training Loss: 0.28325934171676637 Test Loss: 0.29878218587239586\n",
      "Epoch: 1596 Training Loss: 0.2832034600575765 Test Loss: 0.29873360188802084\n",
      "Epoch: 1597 Training Loss: 0.2831534907023112 Test Loss: 0.29876810709635415\n",
      "Epoch: 1598 Training Loss: 0.28310039107004803 Test Loss: 0.2987435302734375\n",
      "Epoch: 1599 Training Loss: 0.2830512816111247 Test Loss: 0.2986238199869792\n",
      "Epoch: 1600 Training Loss: 0.28299427127838134 Test Loss: 0.29857881673177084\n",
      "Epoch: 1601 Training Loss: 0.2829359259605408 Test Loss: 0.2985113525390625\n",
      "Epoch: 1602 Training Loss: 0.2828888125419617 Test Loss: 0.29850803629557293\n",
      "Epoch: 1603 Training Loss: 0.28283483282725014 Test Loss: 0.29840250651041667\n",
      "Epoch: 1604 Training Loss: 0.2827768778800964 Test Loss: 0.298429931640625\n",
      "Epoch: 1605 Training Loss: 0.282730620543162 Test Loss: 0.2984261271158854\n",
      "Epoch: 1606 Training Loss: 0.28266366656621295 Test Loss: 0.2983135986328125\n",
      "Epoch: 1607 Training Loss: 0.28261777067184446 Test Loss: 0.29831490071614586\n",
      "Epoch: 1608 Training Loss: 0.28256755542755124 Test Loss: 0.2982053629557292\n",
      "Epoch: 1609 Training Loss: 0.2825173306465149 Test Loss: 0.2981516520182292\n",
      "Epoch: 1610 Training Loss: 0.28246683851877846 Test Loss: 0.2981678670247396\n",
      "Epoch: 1611 Training Loss: 0.28241853777567544 Test Loss: 0.29810353597005207\n",
      "Epoch: 1612 Training Loss: 0.2823595220247904 Test Loss: 0.29809090169270835\n",
      "Epoch: 1613 Training Loss: 0.2823089063962301 Test Loss: 0.29803190104166666\n",
      "Epoch: 1614 Training Loss: 0.28225209697087605 Test Loss: 0.297999267578125\n",
      "Epoch: 1615 Training Loss: 0.2822016736666361 Test Loss: 0.2979337158203125\n",
      "Epoch: 1616 Training Loss: 0.28215409262975055 Test Loss: 0.2979308064778646\n",
      "Epoch: 1617 Training Loss: 0.28210180044174193 Test Loss: 0.297870849609375\n",
      "Epoch: 1618 Training Loss: 0.28204519605636597 Test Loss: 0.2978111572265625\n",
      "Epoch: 1619 Training Loss: 0.2819888598124186 Test Loss: 0.2977672932942708\n",
      "Epoch: 1620 Training Loss: 0.28194247134526573 Test Loss: 0.29776517740885416\n",
      "Epoch: 1621 Training Loss: 0.28188632281621295 Test Loss: 0.29764449055989584\n",
      "Epoch: 1622 Training Loss: 0.2818346217473348 Test Loss: 0.2976713460286458\n",
      "Epoch: 1623 Training Loss: 0.28178243001302083 Test Loss: 0.2975605265299479\n",
      "Epoch: 1624 Training Loss: 0.28173211352030436 Test Loss: 0.297533203125\n",
      "Epoch: 1625 Training Loss: 0.2816741366386414 Test Loss: 0.2975028483072917\n",
      "Epoch: 1626 Training Loss: 0.28162653477986654 Test Loss: 0.29745137532552085\n",
      "Epoch: 1627 Training Loss: 0.28157312409083046 Test Loss: 0.2974353230794271\n",
      "Epoch: 1628 Training Loss: 0.28152018547058105 Test Loss: 0.29735823567708336\n",
      "Epoch: 1629 Training Loss: 0.2814712506930033 Test Loss: 0.2973548990885417\n",
      "Epoch: 1630 Training Loss: 0.2814127883911133 Test Loss: 0.2972613525390625\n",
      "Epoch: 1631 Training Loss: 0.28136479870478315 Test Loss: 0.29722920735677083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1632 Training Loss: 0.28130501000086466 Test Loss: 0.29719580078125\n",
      "Epoch: 1633 Training Loss: 0.2812571975390116 Test Loss: 0.29721280924479165\n",
      "Epoch: 1634 Training Loss: 0.28120875310897825 Test Loss: 0.29708475748697916\n",
      "Epoch: 1635 Training Loss: 0.2811546597480774 Test Loss: 0.2970503336588542\n",
      "Epoch: 1636 Training Loss: 0.2811075873374939 Test Loss: 0.297025390625\n",
      "Epoch: 1637 Training Loss: 0.28105344597498577 Test Loss: 0.29690474446614584\n",
      "Epoch: 1638 Training Loss: 0.280997792561849 Test Loss: 0.29691825358072915\n",
      "Epoch: 1639 Training Loss: 0.2809504156112671 Test Loss: 0.2969043782552083\n",
      "Epoch: 1640 Training Loss: 0.280891211827596 Test Loss: 0.29675856526692707\n",
      "Epoch: 1641 Training Loss: 0.2808411652247111 Test Loss: 0.29670170084635417\n",
      "Epoch: 1642 Training Loss: 0.280782527923584 Test Loss: 0.29665083821614585\n",
      "Epoch: 1643 Training Loss: 0.2807354885737101 Test Loss: 0.2966888631184896\n",
      "Epoch: 1644 Training Loss: 0.2806810943285624 Test Loss: 0.2966300048828125\n",
      "Epoch: 1645 Training Loss: 0.2806256699562073 Test Loss: 0.2965810546875\n",
      "Epoch: 1646 Training Loss: 0.28058066860834757 Test Loss: 0.2965041910807292\n",
      "Epoch: 1647 Training Loss: 0.2805277338027954 Test Loss: 0.2964738566080729\n",
      "Epoch: 1648 Training Loss: 0.28047523625691734 Test Loss: 0.2964620361328125\n",
      "Epoch: 1649 Training Loss: 0.2804198673566182 Test Loss: 0.2964216715494792\n",
      "Epoch: 1650 Training Loss: 0.28035668659210206 Test Loss: 0.2963303426106771\n",
      "Epoch: 1651 Training Loss: 0.28031918970743813 Test Loss: 0.296407958984375\n",
      "Epoch: 1652 Training Loss: 0.2802633514404297 Test Loss: 0.2963013916015625\n",
      "Epoch: 1653 Training Loss: 0.2802074621518453 Test Loss: 0.29631136067708336\n",
      "Epoch: 1654 Training Loss: 0.280150554339091 Test Loss: 0.2962454427083333\n",
      "Epoch: 1655 Training Loss: 0.2801066376368205 Test Loss: 0.2962452392578125\n",
      "Epoch: 1656 Training Loss: 0.2800541677474976 Test Loss: 0.29624796549479165\n",
      "Epoch: 1657 Training Loss: 0.2799949256579081 Test Loss: 0.29614866129557293\n",
      "Epoch: 1658 Training Loss: 0.2799445017178853 Test Loss: 0.2960925699869792\n",
      "Epoch: 1659 Training Loss: 0.27988679949442546 Test Loss: 0.2960879313151042\n",
      "Epoch: 1660 Training Loss: 0.27983633454640705 Test Loss: 0.2960367431640625\n",
      "Epoch: 1661 Training Loss: 0.27978489303588866 Test Loss: 0.2959539388020833\n",
      "Epoch: 1662 Training Loss: 0.2797302842140198 Test Loss: 0.2958670247395833\n",
      "Epoch: 1663 Training Loss: 0.27967988538742067 Test Loss: 0.29587933349609374\n",
      "Epoch: 1664 Training Loss: 0.27962207539876305 Test Loss: 0.29585697428385416\n",
      "Epoch: 1665 Training Loss: 0.2795725288391113 Test Loss: 0.29575177001953123\n",
      "Epoch: 1666 Training Loss: 0.27952248620986936 Test Loss: 0.2957203369140625\n",
      "Epoch: 1667 Training Loss: 0.27945945517222087 Test Loss: 0.2956622314453125\n",
      "Epoch: 1668 Training Loss: 0.27940727440516155 Test Loss: 0.2955535888671875\n",
      "Epoch: 1669 Training Loss: 0.27934637101491294 Test Loss: 0.2956135864257812\n",
      "Epoch: 1670 Training Loss: 0.2793007308642069 Test Loss: 0.29553059895833333\n",
      "Epoch: 1671 Training Loss: 0.2792484254837036 Test Loss: 0.2954659016927083\n",
      "Epoch: 1672 Training Loss: 0.27919621324539184 Test Loss: 0.29547218831380206\n",
      "Epoch: 1673 Training Loss: 0.2791486306190491 Test Loss: 0.2954796142578125\n",
      "Epoch: 1674 Training Loss: 0.27908972136179605 Test Loss: 0.2953743896484375\n",
      "Epoch: 1675 Training Loss: 0.27903542216618854 Test Loss: 0.29529439290364584\n",
      "Epoch: 1676 Training Loss: 0.27898815790812176 Test Loss: 0.29530080159505206\n",
      "Epoch: 1677 Training Loss: 0.2789324769973755 Test Loss: 0.2952835693359375\n",
      "Epoch: 1678 Training Loss: 0.2788824044863383 Test Loss: 0.2952066853841146\n",
      "Epoch: 1679 Training Loss: 0.27882840808232623 Test Loss: 0.2951697998046875\n",
      "Epoch: 1680 Training Loss: 0.2787792843182882 Test Loss: 0.2951146240234375\n",
      "Epoch: 1681 Training Loss: 0.27872724310557045 Test Loss: 0.29505025227864584\n",
      "Epoch: 1682 Training Loss: 0.27867218192418414 Test Loss: 0.2950888468424479\n",
      "Epoch: 1683 Training Loss: 0.2786177245775859 Test Loss: 0.2950901285807292\n",
      "Epoch: 1684 Training Loss: 0.2785646096865336 Test Loss: 0.2950198567708333\n",
      "Epoch: 1685 Training Loss: 0.2785145729382833 Test Loss: 0.294994140625\n",
      "Epoch: 1686 Training Loss: 0.2784596021970113 Test Loss: 0.29494287109375\n",
      "Epoch: 1687 Training Loss: 0.27840618658065797 Test Loss: 0.294913818359375\n",
      "Epoch: 1688 Training Loss: 0.2783589383761088 Test Loss: 0.29486653645833333\n",
      "Epoch: 1689 Training Loss: 0.27829778925577797 Test Loss: 0.29480537923177086\n",
      "Epoch: 1690 Training Loss: 0.27824400758743284 Test Loss: 0.2948210042317708\n",
      "Epoch: 1691 Training Loss: 0.2781952543258667 Test Loss: 0.2946427815755208\n",
      "Epoch: 1692 Training Loss: 0.2781401154200236 Test Loss: 0.29463812255859373\n",
      "Epoch: 1693 Training Loss: 0.27808964188893637 Test Loss: 0.2946576334635417\n",
      "Epoch: 1694 Training Loss: 0.2780332188606262 Test Loss: 0.2946363525390625\n",
      "Epoch: 1695 Training Loss: 0.27799069674809773 Test Loss: 0.294541748046875\n",
      "Epoch: 1696 Training Loss: 0.2779293845494588 Test Loss: 0.2945352783203125\n",
      "Epoch: 1697 Training Loss: 0.27787730248769127 Test Loss: 0.2944680582682292\n",
      "Epoch: 1698 Training Loss: 0.2778251937230428 Test Loss: 0.29444938151041666\n",
      "Epoch: 1699 Training Loss: 0.27777525266011555 Test Loss: 0.2943492431640625\n",
      "Epoch: 1700 Training Loss: 0.27771952184041343 Test Loss: 0.2943251953125\n",
      "Epoch: 1701 Training Loss: 0.27766659609476724 Test Loss: 0.2942902425130208\n",
      "Epoch: 1702 Training Loss: 0.27760976139704385 Test Loss: 0.2942337443033854\n",
      "Epoch: 1703 Training Loss: 0.2775515505472819 Test Loss: 0.2942043050130208\n",
      "Epoch: 1704 Training Loss: 0.2774970752398173 Test Loss: 0.29415041097005207\n",
      "Epoch: 1705 Training Loss: 0.27744021368026733 Test Loss: 0.29404823811848957\n",
      "Epoch: 1706 Training Loss: 0.27739655987421674 Test Loss: 0.29407283528645833\n",
      "Epoch: 1707 Training Loss: 0.27735051918029785 Test Loss: 0.29400492350260415\n",
      "Epoch: 1708 Training Loss: 0.2772944622039795 Test Loss: 0.29395501708984373\n",
      "Epoch: 1709 Training Loss: 0.2772400550842285 Test Loss: 0.29391158040364584\n",
      "Epoch: 1710 Training Loss: 0.27719232606887817 Test Loss: 0.29385945638020833\n",
      "Epoch: 1711 Training Loss: 0.2771355013847351 Test Loss: 0.293785400390625\n",
      "Epoch: 1712 Training Loss: 0.2770862787564596 Test Loss: 0.2938480428059896\n",
      "Epoch: 1713 Training Loss: 0.27703001801172894 Test Loss: 0.2937584228515625\n",
      "Epoch: 1714 Training Loss: 0.27697730080286664 Test Loss: 0.2937137654622396\n",
      "Epoch: 1715 Training Loss: 0.2769243421554565 Test Loss: 0.2936040852864583\n",
      "Epoch: 1716 Training Loss: 0.27687847836812335 Test Loss: 0.2935810953776042\n",
      "Epoch: 1717 Training Loss: 0.2768209613164266 Test Loss: 0.29354005940755207\n",
      "Epoch: 1718 Training Loss: 0.27677187490463256 Test Loss: 0.29352376302083333\n",
      "Epoch: 1719 Training Loss: 0.2767142176628113 Test Loss: 0.29342578125\n",
      "Epoch: 1720 Training Loss: 0.27666286420822145 Test Loss: 0.2933553670247396\n",
      "Epoch: 1721 Training Loss: 0.27660741058985394 Test Loss: 0.2933345540364583\n",
      "Epoch: 1722 Training Loss: 0.2765543944040934 Test Loss: 0.29334527587890624\n",
      "Epoch: 1723 Training Loss: 0.27650056791305544 Test Loss: 0.29326123046875\n",
      "Epoch: 1724 Training Loss: 0.2764491278330485 Test Loss: 0.2932467041015625\n",
      "Epoch: 1725 Training Loss: 0.27639649709065756 Test Loss: 0.29324188232421877\n",
      "Epoch: 1726 Training Loss: 0.27634587971369423 Test Loss: 0.2930777791341146\n",
      "Epoch: 1727 Training Loss: 0.27629094854990643 Test Loss: 0.2930939127604167\n",
      "Epoch: 1728 Training Loss: 0.27624169127146403 Test Loss: 0.2930590006510417\n",
      "Epoch: 1729 Training Loss: 0.27618461100260416 Test Loss: 0.29292970784505207\n",
      "Epoch: 1730 Training Loss: 0.2761353491147359 Test Loss: 0.293\n",
      "Epoch: 1731 Training Loss: 0.27608180967966717 Test Loss: 0.2929141031901042\n",
      "Epoch: 1732 Training Loss: 0.2760260893503825 Test Loss: 0.2928194580078125\n",
      "Epoch: 1733 Training Loss: 0.2759647881189982 Test Loss: 0.2927984619140625\n",
      "Epoch: 1734 Training Loss: 0.2759136946996053 Test Loss: 0.29282938639322914\n",
      "Epoch: 1735 Training Loss: 0.2758635640144348 Test Loss: 0.2927171223958333\n",
      "Epoch: 1736 Training Loss: 0.27580990314483644 Test Loss: 0.29266145833333335\n",
      "Epoch: 1737 Training Loss: 0.2757547845840454 Test Loss: 0.29263299560546874\n",
      "Epoch: 1738 Training Loss: 0.2757021811803182 Test Loss: 0.292630126953125\n",
      "Epoch: 1739 Training Loss: 0.27564945046106976 Test Loss: 0.2925321044921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1740 Training Loss: 0.2756028100649516 Test Loss: 0.29250333658854166\n",
      "Epoch: 1741 Training Loss: 0.2755441703796387 Test Loss: 0.2924446818033854\n",
      "Epoch: 1742 Training Loss: 0.27549500211079914 Test Loss: 0.29240618896484377\n",
      "Epoch: 1743 Training Loss: 0.2754457268714905 Test Loss: 0.29239261881510414\n",
      "Epoch: 1744 Training Loss: 0.2753862167994181 Test Loss: 0.2923648885091146\n",
      "Epoch: 1745 Training Loss: 0.27534651120503745 Test Loss: 0.2922933756510417\n",
      "Epoch: 1746 Training Loss: 0.27529141648610433 Test Loss: 0.292260986328125\n",
      "Epoch: 1747 Training Loss: 0.2752346207300822 Test Loss: 0.2923096923828125\n",
      "Epoch: 1748 Training Loss: 0.2751880192756653 Test Loss: 0.29223555501302084\n",
      "Epoch: 1749 Training Loss: 0.2751312305132548 Test Loss: 0.2921838989257812\n",
      "Epoch: 1750 Training Loss: 0.275081289768219 Test Loss: 0.2921653849283854\n",
      "Epoch: 1751 Training Loss: 0.2750188919703166 Test Loss: 0.2920587565104167\n",
      "Epoch: 1752 Training Loss: 0.27496947574615477 Test Loss: 0.2920081380208333\n",
      "Epoch: 1753 Training Loss: 0.2749174470901489 Test Loss: 0.2919715576171875\n",
      "Epoch: 1754 Training Loss: 0.2748617952664693 Test Loss: 0.29188771565755206\n",
      "Epoch: 1755 Training Loss: 0.2748128679593404 Test Loss: 0.29193973795572914\n",
      "Epoch: 1756 Training Loss: 0.27475911744435627 Test Loss: 0.2918414306640625\n",
      "Epoch: 1757 Training Loss: 0.27470680793126423 Test Loss: 0.2918247884114583\n",
      "Epoch: 1758 Training Loss: 0.2746542862256368 Test Loss: 0.29175484212239583\n",
      "Epoch: 1759 Training Loss: 0.27460058466593423 Test Loss: 0.2917005208333333\n",
      "Epoch: 1760 Training Loss: 0.2745458006858826 Test Loss: 0.29171498616536456\n",
      "Epoch: 1761 Training Loss: 0.274491898059845 Test Loss: 0.2916467692057292\n",
      "Epoch: 1762 Training Loss: 0.27444246530532834 Test Loss: 0.29159273274739583\n",
      "Epoch: 1763 Training Loss: 0.27438910166422525 Test Loss: 0.29158837890625\n",
      "Epoch: 1764 Training Loss: 0.27434110816319784 Test Loss: 0.29149910481770835\n",
      "Epoch: 1765 Training Loss: 0.27429292583465575 Test Loss: 0.29144563802083334\n",
      "Epoch: 1766 Training Loss: 0.2742431557973226 Test Loss: 0.29137860107421876\n",
      "Epoch: 1767 Training Loss: 0.2741829466819763 Test Loss: 0.29136690266927084\n",
      "Epoch: 1768 Training Loss: 0.2741323246955872 Test Loss: 0.2912856241861979\n",
      "Epoch: 1769 Training Loss: 0.2740759104092916 Test Loss: 0.2912146606445313\n",
      "Epoch: 1770 Training Loss: 0.2740260057449341 Test Loss: 0.2911923421223958\n",
      "Epoch: 1771 Training Loss: 0.2739667778015137 Test Loss: 0.29107474772135417\n",
      "Epoch: 1772 Training Loss: 0.27390942271550495 Test Loss: 0.2910645548502604\n",
      "Epoch: 1773 Training Loss: 0.2738593152364095 Test Loss: 0.2910379842122396\n",
      "Epoch: 1774 Training Loss: 0.27380733426411946 Test Loss: 0.2910034383138021\n",
      "Epoch: 1775 Training Loss: 0.2737522147496541 Test Loss: 0.2909231974283854\n",
      "Epoch: 1776 Training Loss: 0.27370324881871544 Test Loss: 0.29083154296875\n",
      "Epoch: 1777 Training Loss: 0.2736484475135803 Test Loss: 0.2908486531575521\n",
      "Epoch: 1778 Training Loss: 0.27360183652242026 Test Loss: 0.29074930826822915\n",
      "Epoch: 1779 Training Loss: 0.27354991722106936 Test Loss: 0.29075236002604166\n",
      "Epoch: 1780 Training Loss: 0.2734852313995361 Test Loss: 0.29061617024739583\n",
      "Epoch: 1781 Training Loss: 0.2734397080739339 Test Loss: 0.29059684244791667\n",
      "Epoch: 1782 Training Loss: 0.27338518476486207 Test Loss: 0.29054718017578124\n",
      "Epoch: 1783 Training Loss: 0.2733308204015096 Test Loss: 0.2904846598307292\n",
      "Epoch: 1784 Training Loss: 0.2732775026957194 Test Loss: 0.29047355143229164\n",
      "Epoch: 1785 Training Loss: 0.2732252542177836 Test Loss: 0.29042138671875\n",
      "Epoch: 1786 Training Loss: 0.2731713698705037 Test Loss: 0.29027294921875\n",
      "Epoch: 1787 Training Loss: 0.2731132907867432 Test Loss: 0.290286376953125\n",
      "Epoch: 1788 Training Loss: 0.27307114124298093 Test Loss: 0.290243896484375\n",
      "Epoch: 1789 Training Loss: 0.27301401901245115 Test Loss: 0.29022021484375\n",
      "Epoch: 1790 Training Loss: 0.2729637641906738 Test Loss: 0.29018937174479165\n",
      "Epoch: 1791 Training Loss: 0.27290643692016603 Test Loss: 0.2901080322265625\n",
      "Epoch: 1792 Training Loss: 0.27285548496246337 Test Loss: 0.29011747233072915\n",
      "Epoch: 1793 Training Loss: 0.2728004754384359 Test Loss: 0.28993794759114583\n",
      "Epoch: 1794 Training Loss: 0.27275084670384725 Test Loss: 0.28989251708984376\n",
      "Epoch: 1795 Training Loss: 0.2726944316228231 Test Loss: 0.2898629150390625\n",
      "Epoch: 1796 Training Loss: 0.27263445186614993 Test Loss: 0.28984126790364584\n",
      "Epoch: 1797 Training Loss: 0.27257807191212974 Test Loss: 0.289779541015625\n",
      "Epoch: 1798 Training Loss: 0.27253506596883137 Test Loss: 0.28981182861328125\n",
      "Epoch: 1799 Training Loss: 0.27247459650039674 Test Loss: 0.28972292073567707\n",
      "Epoch: 1800 Training Loss: 0.27242327960332235 Test Loss: 0.2896346232096354\n",
      "Epoch: 1801 Training Loss: 0.2723675764401754 Test Loss: 0.28956510416666664\n",
      "Epoch: 1802 Training Loss: 0.2723101474444071 Test Loss: 0.28951595052083334\n",
      "Epoch: 1803 Training Loss: 0.27226136430104575 Test Loss: 0.28953053792317707\n",
      "Epoch: 1804 Training Loss: 0.2722059435844421 Test Loss: 0.2894311116536458\n",
      "Epoch: 1805 Training Loss: 0.2721469488143921 Test Loss: 0.2893137410481771\n",
      "Epoch: 1806 Training Loss: 0.2720983338356018 Test Loss: 0.2893646240234375\n",
      "Epoch: 1807 Training Loss: 0.27204663038253785 Test Loss: 0.28933746337890626\n",
      "Epoch: 1808 Training Loss: 0.271993369102478 Test Loss: 0.28928727213541666\n",
      "Epoch: 1809 Training Loss: 0.271945734500885 Test Loss: 0.28917919921875\n",
      "Epoch: 1810 Training Loss: 0.271886422475179 Test Loss: 0.2891905314127604\n",
      "Epoch: 1811 Training Loss: 0.2718334188461304 Test Loss: 0.28906129964192706\n",
      "Epoch: 1812 Training Loss: 0.2717717645963033 Test Loss: 0.2890144856770833\n",
      "Epoch: 1813 Training Loss: 0.271723270257314 Test Loss: 0.2890534464518229\n",
      "Epoch: 1814 Training Loss: 0.2716666159629822 Test Loss: 0.2889315592447917\n",
      "Epoch: 1815 Training Loss: 0.271623149394989 Test Loss: 0.28889475504557294\n",
      "Epoch: 1816 Training Loss: 0.2715701322555542 Test Loss: 0.2888907063802083\n",
      "Epoch: 1817 Training Loss: 0.2715142628351847 Test Loss: 0.2888458251953125\n",
      "Epoch: 1818 Training Loss: 0.2714642400741577 Test Loss: 0.2888199259440104\n",
      "Epoch: 1819 Training Loss: 0.2714092508951823 Test Loss: 0.2887279459635417\n",
      "Epoch: 1820 Training Loss: 0.2713499345779419 Test Loss: 0.28866984049479166\n",
      "Epoch: 1821 Training Loss: 0.2713034421602885 Test Loss: 0.2886419677734375\n",
      "Epoch: 1822 Training Loss: 0.2712472864786784 Test Loss: 0.28855330403645835\n",
      "Epoch: 1823 Training Loss: 0.27120265595118204 Test Loss: 0.2884713134765625\n",
      "Epoch: 1824 Training Loss: 0.2711474998792012 Test Loss: 0.2884362996419271\n",
      "Epoch: 1825 Training Loss: 0.27108776648839317 Test Loss: 0.2884581095377604\n",
      "Epoch: 1826 Training Loss: 0.27103824122746784 Test Loss: 0.28834944661458334\n",
      "Epoch: 1827 Training Loss: 0.2709809414545695 Test Loss: 0.2883152262369792\n",
      "Epoch: 1828 Training Loss: 0.27092848889033 Test Loss: 0.2882944742838542\n",
      "Epoch: 1829 Training Loss: 0.27087761004765826 Test Loss: 0.2882438354492188\n",
      "Epoch: 1830 Training Loss: 0.2708183143933614 Test Loss: 0.2881628824869792\n",
      "Epoch: 1831 Training Loss: 0.2707646147410075 Test Loss: 0.2881104736328125\n",
      "Epoch: 1832 Training Loss: 0.27071977186203006 Test Loss: 0.2881387736002604\n",
      "Epoch: 1833 Training Loss: 0.2706635775566101 Test Loss: 0.2880792846679687\n",
      "Epoch: 1834 Training Loss: 0.2706100147565206 Test Loss: 0.2879993896484375\n",
      "Epoch: 1835 Training Loss: 0.27055619208017984 Test Loss: 0.28795458984375\n",
      "Epoch: 1836 Training Loss: 0.2705048565864563 Test Loss: 0.28793387858072916\n",
      "Epoch: 1837 Training Loss: 0.2704516444206238 Test Loss: 0.2879002685546875\n",
      "Epoch: 1838 Training Loss: 0.2703998012542725 Test Loss: 0.28782600911458334\n",
      "Epoch: 1839 Training Loss: 0.2703449807167053 Test Loss: 0.2877438557942708\n",
      "Epoch: 1840 Training Loss: 0.27029411172866824 Test Loss: 0.28781917317708333\n",
      "Epoch: 1841 Training Loss: 0.2702431106567383 Test Loss: 0.287736328125\n",
      "Epoch: 1842 Training Loss: 0.2701919803619385 Test Loss: 0.287658203125\n",
      "Epoch: 1843 Training Loss: 0.27013622760772704 Test Loss: 0.2876931966145833\n",
      "Epoch: 1844 Training Loss: 0.27008844725290937 Test Loss: 0.2875811767578125\n",
      "Epoch: 1845 Training Loss: 0.2700339860916138 Test Loss: 0.2875452473958333\n",
      "Epoch: 1846 Training Loss: 0.26998459323247276 Test Loss: 0.2874530436197917\n",
      "Epoch: 1847 Training Loss: 0.2699332838058472 Test Loss: 0.2874338175455729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1848 Training Loss: 0.2698783702850342 Test Loss: 0.2874258422851563\n",
      "Epoch: 1849 Training Loss: 0.26982672135035196 Test Loss: 0.2873247477213542\n",
      "Epoch: 1850 Training Loss: 0.269776637395223 Test Loss: 0.28734212239583334\n",
      "Epoch: 1851 Training Loss: 0.26971842495600384 Test Loss: 0.28727402750651043\n",
      "Epoch: 1852 Training Loss: 0.269671294371287 Test Loss: 0.2872244059244792\n",
      "Epoch: 1853 Training Loss: 0.26962110471725464 Test Loss: 0.28716105143229165\n",
      "Epoch: 1854 Training Loss: 0.2695711172421773 Test Loss: 0.28710101318359377\n",
      "Epoch: 1855 Training Loss: 0.26952326742808025 Test Loss: 0.28709639485677085\n",
      "Epoch: 1856 Training Loss: 0.2694656451543172 Test Loss: 0.28704034423828123\n",
      "Epoch: 1857 Training Loss: 0.2694127117792765 Test Loss: 0.28693546549479165\n",
      "Epoch: 1858 Training Loss: 0.26936700916290285 Test Loss: 0.28689095052083335\n",
      "Epoch: 1859 Training Loss: 0.2693134428660075 Test Loss: 0.28676875813802083\n",
      "Epoch: 1860 Training Loss: 0.2692584587732951 Test Loss: 0.2868441975911458\n",
      "Epoch: 1861 Training Loss: 0.2692101747194926 Test Loss: 0.28676847330729166\n",
      "Epoch: 1862 Training Loss: 0.2691582449277242 Test Loss: 0.28672222900390626\n",
      "Epoch: 1863 Training Loss: 0.26912005758285523 Test Loss: 0.286652099609375\n",
      "Epoch: 1864 Training Loss: 0.26906377061208087 Test Loss: 0.2865723876953125\n",
      "Epoch: 1865 Training Loss: 0.26901054430007937 Test Loss: 0.28664227294921873\n",
      "Epoch: 1866 Training Loss: 0.2689586494763692 Test Loss: 0.28646506754557294\n",
      "Epoch: 1867 Training Loss: 0.26890473715464275 Test Loss: 0.2864738362630208\n",
      "Epoch: 1868 Training Loss: 0.2688595887819926 Test Loss: 0.2864784545898437\n",
      "Epoch: 1869 Training Loss: 0.2688112293879191 Test Loss: 0.2864399820963542\n",
      "Epoch: 1870 Training Loss: 0.26875647830963134 Test Loss: 0.2862968953450521\n",
      "Epoch: 1871 Training Loss: 0.2687022992769877 Test Loss: 0.28635933430989585\n",
      "Epoch: 1872 Training Loss: 0.2686528582572937 Test Loss: 0.2862765909830729\n",
      "Epoch: 1873 Training Loss: 0.26860478782653807 Test Loss: 0.28624216715494794\n",
      "Epoch: 1874 Training Loss: 0.26855527798334755 Test Loss: 0.2862192586263021\n",
      "Epoch: 1875 Training Loss: 0.2685063894589742 Test Loss: 0.2861920776367187\n",
      "Epoch: 1876 Training Loss: 0.2684604147275289 Test Loss: 0.2861651204427083\n",
      "Epoch: 1877 Training Loss: 0.2684101457595825 Test Loss: 0.2861027425130208\n",
      "Epoch: 1878 Training Loss: 0.2683567851384481 Test Loss: 0.28603542073567706\n",
      "Epoch: 1879 Training Loss: 0.26830298964182536 Test Loss: 0.2861031290690104\n",
      "Epoch: 1880 Training Loss: 0.26825505622227985 Test Loss: 0.28597283935546874\n",
      "Epoch: 1881 Training Loss: 0.26820411650339765 Test Loss: 0.2859307861328125\n",
      "Epoch: 1882 Training Loss: 0.2681565103530884 Test Loss: 0.2859309285481771\n",
      "Epoch: 1883 Training Loss: 0.26811025889714557 Test Loss: 0.28584855143229165\n",
      "Epoch: 1884 Training Loss: 0.26806018702189127 Test Loss: 0.2858154296875\n",
      "Epoch: 1885 Training Loss: 0.268015087445577 Test Loss: 0.2858140665690104\n",
      "Epoch: 1886 Training Loss: 0.26795802895228066 Test Loss: 0.28576802571614585\n",
      "Epoch: 1887 Training Loss: 0.26791113837560016 Test Loss: 0.28576300048828124\n",
      "Epoch: 1888 Training Loss: 0.26786204703648886 Test Loss: 0.2855886027018229\n",
      "Epoch: 1889 Training Loss: 0.26781588220596314 Test Loss: 0.285689453125\n",
      "Epoch: 1890 Training Loss: 0.2677623858451843 Test Loss: 0.2855843302408854\n",
      "Epoch: 1891 Training Loss: 0.267722731590271 Test Loss: 0.2854618326822917\n",
      "Epoch: 1892 Training Loss: 0.2676725387573242 Test Loss: 0.2854994913736979\n",
      "Epoch: 1893 Training Loss: 0.26761769088109333 Test Loss: 0.2854869384765625\n",
      "Epoch: 1894 Training Loss: 0.26758150466283165 Test Loss: 0.2853703816731771\n",
      "Epoch: 1895 Training Loss: 0.26753093846639 Test Loss: 0.28537398274739584\n",
      "Epoch: 1896 Training Loss: 0.26748433923721315 Test Loss: 0.28533304850260416\n",
      "Epoch: 1897 Training Loss: 0.2674298803011576 Test Loss: 0.28522774251302085\n",
      "Epoch: 1898 Training Loss: 0.2673840400377909 Test Loss: 0.28520703125\n",
      "Epoch: 1899 Training Loss: 0.2673345273335775 Test Loss: 0.28521561686197916\n",
      "Epoch: 1900 Training Loss: 0.26728489065170286 Test Loss: 0.2851778767903646\n",
      "Epoch: 1901 Training Loss: 0.2672458521525065 Test Loss: 0.28511234537760416\n",
      "Epoch: 1902 Training Loss: 0.2671932061513265 Test Loss: 0.28509765625\n",
      "Epoch: 1903 Training Loss: 0.2671461181640625 Test Loss: 0.28509228515625\n",
      "Epoch: 1904 Training Loss: 0.2671072247823079 Test Loss: 0.2849539388020833\n",
      "Epoch: 1905 Training Loss: 0.26704072252909344 Test Loss: 0.28488033040364585\n",
      "Epoch: 1906 Training Loss: 0.26700021839141846 Test Loss: 0.28485093180338544\n",
      "Epoch: 1907 Training Loss: 0.26694800074895225 Test Loss: 0.28486319986979164\n",
      "Epoch: 1908 Training Loss: 0.26689660803476967 Test Loss: 0.28479280598958334\n",
      "Epoch: 1909 Training Loss: 0.26685106341044107 Test Loss: 0.2847626953125\n",
      "Epoch: 1910 Training Loss: 0.26679735438028973 Test Loss: 0.2847402954101563\n",
      "Epoch: 1911 Training Loss: 0.2667564094861348 Test Loss: 0.28470454915364585\n",
      "Epoch: 1912 Training Loss: 0.26670212999979653 Test Loss: 0.28463431803385414\n",
      "Epoch: 1913 Training Loss: 0.2666509563128153 Test Loss: 0.2845961100260417\n",
      "Epoch: 1914 Training Loss: 0.266607616742452 Test Loss: 0.28451861572265624\n",
      "Epoch: 1915 Training Loss: 0.26655881945292154 Test Loss: 0.2845713500976563\n",
      "Epoch: 1916 Training Loss: 0.2665074116388957 Test Loss: 0.28450931803385415\n",
      "Epoch: 1917 Training Loss: 0.2664670755068461 Test Loss: 0.2845008138020833\n",
      "Epoch: 1918 Training Loss: 0.26641949113210045 Test Loss: 0.2844386393229167\n",
      "Epoch: 1919 Training Loss: 0.2663645216623942 Test Loss: 0.28437089029947915\n",
      "Epoch: 1920 Training Loss: 0.26632581663131716 Test Loss: 0.2843184814453125\n",
      "Epoch: 1921 Training Loss: 0.2662816454569499 Test Loss: 0.2843628336588542\n",
      "Epoch: 1922 Training Loss: 0.2662342467308044 Test Loss: 0.2842809855143229\n",
      "Epoch: 1923 Training Loss: 0.26617443513870237 Test Loss: 0.2842401123046875\n",
      "Epoch: 1924 Training Loss: 0.2661306707064311 Test Loss: 0.28415706380208333\n",
      "Epoch: 1925 Training Loss: 0.2660828609466553 Test Loss: 0.28414534505208333\n",
      "Epoch: 1926 Training Loss: 0.26603714577356974 Test Loss: 0.28412386067708334\n",
      "Epoch: 1927 Training Loss: 0.265986044883728 Test Loss: 0.2840737711588542\n",
      "Epoch: 1928 Training Loss: 0.2659437017440796 Test Loss: 0.2840087890625\n",
      "Epoch: 1929 Training Loss: 0.2658946531613668 Test Loss: 0.2839925333658854\n",
      "Epoch: 1930 Training Loss: 0.2658449401855469 Test Loss: 0.28393717447916667\n",
      "Epoch: 1931 Training Loss: 0.2658032808303833 Test Loss: 0.2839324340820312\n",
      "Epoch: 1932 Training Loss: 0.2657514772415161 Test Loss: 0.28378507486979165\n",
      "Epoch: 1933 Training Loss: 0.2657043972015381 Test Loss: 0.2838163452148437\n",
      "Epoch: 1934 Training Loss: 0.2656573920249939 Test Loss: 0.2836569417317708\n",
      "Epoch: 1935 Training Loss: 0.26561063051223754 Test Loss: 0.2837159627278646\n",
      "Epoch: 1936 Training Loss: 0.26556754302978514 Test Loss: 0.283686767578125\n",
      "Epoch: 1937 Training Loss: 0.26551295693715415 Test Loss: 0.28368695068359373\n",
      "Epoch: 1938 Training Loss: 0.2654697257677714 Test Loss: 0.28353511555989586\n",
      "Epoch: 1939 Training Loss: 0.2654241240819295 Test Loss: 0.2835654296875\n",
      "Epoch: 1940 Training Loss: 0.26537978331247963 Test Loss: 0.28355531819661456\n",
      "Epoch: 1941 Training Loss: 0.2653387254079183 Test Loss: 0.28348848470052085\n",
      "Epoch: 1942 Training Loss: 0.2652936256726583 Test Loss: 0.2834441731770833\n",
      "Epoch: 1943 Training Loss: 0.26524226713180543 Test Loss: 0.2833514811197917\n",
      "Epoch: 1944 Training Loss: 0.26519514703750613 Test Loss: 0.28337288411458333\n",
      "Epoch: 1945 Training Loss: 0.2651516938209534 Test Loss: 0.28328397623697915\n",
      "Epoch: 1946 Training Loss: 0.26510662492116294 Test Loss: 0.2832250162760417\n",
      "Epoch: 1947 Training Loss: 0.2650565152168274 Test Loss: 0.2831319173177083\n",
      "Epoch: 1948 Training Loss: 0.2650100205739339 Test Loss: 0.283126220703125\n",
      "Epoch: 1949 Training Loss: 0.26495889616012575 Test Loss: 0.28304427083333333\n",
      "Epoch: 1950 Training Loss: 0.2649177026748657 Test Loss: 0.2830693766276042\n",
      "Epoch: 1951 Training Loss: 0.26487331819534304 Test Loss: 0.2831253662109375\n",
      "Epoch: 1952 Training Loss: 0.2648335305849711 Test Loss: 0.2830416259765625\n",
      "Epoch: 1953 Training Loss: 0.2647836446762085 Test Loss: 0.2830160115559896\n",
      "Epoch: 1954 Training Loss: 0.2647415817578634 Test Loss: 0.2829090576171875\n",
      "Epoch: 1955 Training Loss: 0.2646972336769104 Test Loss: 0.2829348958333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1956 Training Loss: 0.26464785385131834 Test Loss: 0.28290287272135417\n",
      "Epoch: 1957 Training Loss: 0.2646032721201579 Test Loss: 0.2828327433268229\n",
      "Epoch: 1958 Training Loss: 0.26455254046122234 Test Loss: 0.28279130045572914\n",
      "Epoch: 1959 Training Loss: 0.2645115602811178 Test Loss: 0.28270304361979165\n",
      "Epoch: 1960 Training Loss: 0.2644676651954651 Test Loss: 0.28271337890625\n",
      "Epoch: 1961 Training Loss: 0.26442331457138063 Test Loss: 0.282662353515625\n",
      "Epoch: 1962 Training Loss: 0.2643794212341309 Test Loss: 0.28260992431640625\n",
      "Epoch: 1963 Training Loss: 0.2643323335647583 Test Loss: 0.28258418782552086\n",
      "Epoch: 1964 Training Loss: 0.2642953988711039 Test Loss: 0.2826163330078125\n",
      "Epoch: 1965 Training Loss: 0.2642461775143941 Test Loss: 0.2824870402018229\n",
      "Epoch: 1966 Training Loss: 0.2642030774752299 Test Loss: 0.2824711303710937\n",
      "Epoch: 1967 Training Loss: 0.2641570431391398 Test Loss: 0.28245528157552086\n",
      "Epoch: 1968 Training Loss: 0.26411548248926797 Test Loss: 0.2824089558919271\n",
      "Epoch: 1969 Training Loss: 0.2640657475789388 Test Loss: 0.2824078776041667\n",
      "Epoch: 1970 Training Loss: 0.2640273251533508 Test Loss: 0.28236529541015626\n",
      "Epoch: 1971 Training Loss: 0.263983771165212 Test Loss: 0.2823164265950521\n",
      "Epoch: 1972 Training Loss: 0.26393518590927123 Test Loss: 0.28233894856770836\n",
      "Epoch: 1973 Training Loss: 0.26389158725738526 Test Loss: 0.28225946044921874\n",
      "Epoch: 1974 Training Loss: 0.26384659560521445 Test Loss: 0.28233919270833335\n",
      "Epoch: 1975 Training Loss: 0.2638075671195984 Test Loss: 0.28222406005859374\n",
      "Epoch: 1976 Training Loss: 0.26376177819569907 Test Loss: 0.2821690673828125\n",
      "Epoch: 1977 Training Loss: 0.2637142251332601 Test Loss: 0.2820704752604167\n",
      "Epoch: 1978 Training Loss: 0.2636685841878255 Test Loss: 0.2820795084635417\n",
      "Epoch: 1979 Training Loss: 0.2636184032758077 Test Loss: 0.28207613118489583\n",
      "Epoch: 1980 Training Loss: 0.26357965898513797 Test Loss: 0.2820746459960938\n",
      "Epoch: 1981 Training Loss: 0.2635335954030355 Test Loss: 0.28194970703125\n",
      "Epoch: 1982 Training Loss: 0.26347757228215535 Test Loss: 0.281858154296875\n",
      "Epoch: 1983 Training Loss: 0.2634489237467448 Test Loss: 0.2819186808268229\n",
      "Epoch: 1984 Training Loss: 0.2633912247021993 Test Loss: 0.2818961588541667\n",
      "Epoch: 1985 Training Loss: 0.2633548053105672 Test Loss: 0.28189876302083333\n",
      "Epoch: 1986 Training Loss: 0.2633201684951782 Test Loss: 0.28184029134114585\n",
      "Epoch: 1987 Training Loss: 0.26326628621419274 Test Loss: 0.2818069864908854\n",
      "Epoch: 1988 Training Loss: 0.263224893728892 Test Loss: 0.2817586263020833\n",
      "Epoch: 1989 Training Loss: 0.2631859722137451 Test Loss: 0.28170369466145834\n",
      "Epoch: 1990 Training Loss: 0.26313645362854005 Test Loss: 0.28159488932291665\n",
      "Epoch: 1991 Training Loss: 0.26309229040145876 Test Loss: 0.28154555257161457\n",
      "Epoch: 1992 Training Loss: 0.26304427957534793 Test Loss: 0.2815616455078125\n",
      "Epoch: 1993 Training Loss: 0.26300548283259073 Test Loss: 0.2815498657226562\n",
      "Epoch: 1994 Training Loss: 0.26295566733678183 Test Loss: 0.2815162556966146\n",
      "Epoch: 1995 Training Loss: 0.26291137758890787 Test Loss: 0.2813999226888021\n",
      "Epoch: 1996 Training Loss: 0.2628718811670939 Test Loss: 0.2814386189778646\n",
      "Epoch: 1997 Training Loss: 0.26282421191533406 Test Loss: 0.2813517049153646\n",
      "Epoch: 1998 Training Loss: 0.26278162908554076 Test Loss: 0.28136238606770836\n",
      "Epoch: 1999 Training Loss: 0.26274401346842446 Test Loss: 0.28129191080729166\n",
      "Epoch: 2000 Training Loss: 0.2626959872245789 Test Loss: 0.28133441162109374\n",
      "Epoch: 2001 Training Loss: 0.26264283164342245 Test Loss: 0.2813257853190104\n",
      "Epoch: 2002 Training Loss: 0.26260339498519897 Test Loss: 0.28117154947916667\n",
      "Epoch: 2003 Training Loss: 0.26256647109985354 Test Loss: 0.28122178141276044\n",
      "Epoch: 2004 Training Loss: 0.2625213894844055 Test Loss: 0.2811683756510417\n",
      "Epoch: 2005 Training Loss: 0.2624710144996643 Test Loss: 0.28112674967447915\n",
      "Epoch: 2006 Training Loss: 0.26243428309758504 Test Loss: 0.2811202596028646\n",
      "Epoch: 2007 Training Loss: 0.26238534116744994 Test Loss: 0.28108223470052085\n",
      "Epoch: 2008 Training Loss: 0.26234883499145506 Test Loss: 0.2809873250325521\n",
      "Epoch: 2009 Training Loss: 0.26229480330149335 Test Loss: 0.2809949747721354\n",
      "Epoch: 2010 Training Loss: 0.2622518439292908 Test Loss: 0.2809583333333333\n",
      "Epoch: 2011 Training Loss: 0.2622088001569112 Test Loss: 0.28086842854817706\n",
      "Epoch: 2012 Training Loss: 0.26215936930974326 Test Loss: 0.28091794840494794\n",
      "Epoch: 2013 Training Loss: 0.26211866092681885 Test Loss: 0.28084051513671876\n",
      "Epoch: 2014 Training Loss: 0.26207146565119427 Test Loss: 0.28075520833333334\n",
      "Epoch: 2015 Training Loss: 0.2620317842165629 Test Loss: 0.28076932779947916\n",
      "Epoch: 2016 Training Loss: 0.26198483276367185 Test Loss: 0.2807411295572917\n",
      "Epoch: 2017 Training Loss: 0.26195202859242755 Test Loss: 0.2807373046875\n",
      "Epoch: 2018 Training Loss: 0.2618956384658814 Test Loss: 0.2806385294596354\n",
      "Epoch: 2019 Training Loss: 0.26185455290476484 Test Loss: 0.2806144002278646\n",
      "Epoch: 2020 Training Loss: 0.2618134641647339 Test Loss: 0.28061185709635417\n",
      "Epoch: 2021 Training Loss: 0.26175801785786945 Test Loss: 0.28051261393229165\n",
      "Epoch: 2022 Training Loss: 0.2617205549875895 Test Loss: 0.28051590983072916\n",
      "Epoch: 2023 Training Loss: 0.2616816693941752 Test Loss: 0.28043570963541664\n",
      "Epoch: 2024 Training Loss: 0.26163308572769167 Test Loss: 0.28051607259114586\n",
      "Epoch: 2025 Training Loss: 0.26158131647109983 Test Loss: 0.28040645345052084\n",
      "Epoch: 2026 Training Loss: 0.26154763158162436 Test Loss: 0.28043648274739585\n",
      "Epoch: 2027 Training Loss: 0.2615046113332113 Test Loss: 0.2803804524739583\n",
      "Epoch: 2028 Training Loss: 0.2614588095347087 Test Loss: 0.2803531494140625\n",
      "Epoch: 2029 Training Loss: 0.261420272509257 Test Loss: 0.2802848917643229\n",
      "Epoch: 2030 Training Loss: 0.26138330141703287 Test Loss: 0.2802733154296875\n",
      "Epoch: 2031 Training Loss: 0.2613383316993713 Test Loss: 0.28029960123697917\n",
      "Epoch: 2032 Training Loss: 0.2612972033818563 Test Loss: 0.28018170166015627\n",
      "Epoch: 2033 Training Loss: 0.26125157610575356 Test Loss: 0.28023323567708336\n",
      "Epoch: 2034 Training Loss: 0.26120679505666095 Test Loss: 0.28015901692708334\n",
      "Epoch: 2035 Training Loss: 0.26116351191202797 Test Loss: 0.2801595052083333\n",
      "Epoch: 2036 Training Loss: 0.2611196775436401 Test Loss: 0.2800537109375\n",
      "Epoch: 2037 Training Loss: 0.26107627948125206 Test Loss: 0.2800551961263021\n",
      "Epoch: 2038 Training Loss: 0.2610404645601908 Test Loss: 0.28009326171875\n",
      "Epoch: 2039 Training Loss: 0.26099670966466265 Test Loss: 0.2800062662760417\n",
      "Epoch: 2040 Training Loss: 0.260958021958669 Test Loss: 0.279990966796875\n",
      "Epoch: 2041 Training Loss: 0.2609160561561584 Test Loss: 0.2799686686197917\n",
      "Epoch: 2042 Training Loss: 0.2608719112078349 Test Loss: 0.2798963623046875\n",
      "Epoch: 2043 Training Loss: 0.26083356205622354 Test Loss: 0.27992972819010414\n",
      "Epoch: 2044 Training Loss: 0.2607878424326579 Test Loss: 0.2798827311197917\n",
      "Epoch: 2045 Training Loss: 0.26074858411153157 Test Loss: 0.2798078816731771\n",
      "Epoch: 2046 Training Loss: 0.2607057787577311 Test Loss: 0.2797803141276042\n",
      "Epoch: 2047 Training Loss: 0.2606627998352051 Test Loss: 0.27980672200520834\n",
      "Epoch: 2048 Training Loss: 0.26063167174657187 Test Loss: 0.27972782389322914\n",
      "Epoch: 2049 Training Loss: 0.260591210047404 Test Loss: 0.27970076497395835\n",
      "Epoch: 2050 Training Loss: 0.26054696544011435 Test Loss: 0.2797022298177083\n",
      "Epoch: 2051 Training Loss: 0.26051376485824584 Test Loss: 0.27970770263671874\n",
      "Epoch: 2052 Training Loss: 0.260471923828125 Test Loss: 0.27962677001953123\n",
      "Epoch: 2053 Training Loss: 0.26043257999420166 Test Loss: 0.27958048502604166\n",
      "Epoch: 2054 Training Loss: 0.26038452529907224 Test Loss: 0.27960223388671873\n",
      "Epoch: 2055 Training Loss: 0.26034799353281657 Test Loss: 0.2795091349283854\n",
      "Epoch: 2056 Training Loss: 0.2603096588452657 Test Loss: 0.2794839680989583\n",
      "Epoch: 2057 Training Loss: 0.2602708773612976 Test Loss: 0.2794834798177083\n",
      "Epoch: 2058 Training Loss: 0.2602277870178223 Test Loss: 0.2794021199544271\n",
      "Epoch: 2059 Training Loss: 0.26017953650156656 Test Loss: 0.27942108154296874\n",
      "Epoch: 2060 Training Loss: 0.26014561764399213 Test Loss: 0.27940378824869794\n",
      "Epoch: 2061 Training Loss: 0.2600973062515259 Test Loss: 0.27936474609375\n",
      "Epoch: 2062 Training Loss: 0.2600584683418274 Test Loss: 0.2793040771484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2063 Training Loss: 0.26001937850316365 Test Loss: 0.2793459879557292\n",
      "Epoch: 2064 Training Loss: 0.2599840734799703 Test Loss: 0.27925764973958334\n",
      "Epoch: 2065 Training Loss: 0.25993265835444135 Test Loss: 0.2792294514973958\n",
      "Epoch: 2066 Training Loss: 0.25989777088165283 Test Loss: 0.2791859130859375\n",
      "Epoch: 2067 Training Loss: 0.25985458978017173 Test Loss: 0.27916984049479165\n",
      "Epoch: 2068 Training Loss: 0.25981193590164187 Test Loss: 0.27911226399739586\n",
      "Epoch: 2069 Training Loss: 0.25977371152242024 Test Loss: 0.2791511433919271\n",
      "Epoch: 2070 Training Loss: 0.25973503112792967 Test Loss: 0.2790416463216146\n",
      "Epoch: 2071 Training Loss: 0.2596930859883626 Test Loss: 0.2790576171875\n",
      "Epoch: 2072 Training Loss: 0.2596550957361857 Test Loss: 0.27899861653645835\n",
      "Epoch: 2073 Training Loss: 0.25961369943618773 Test Loss: 0.278914306640625\n",
      "Epoch: 2074 Training Loss: 0.25957501792907717 Test Loss: 0.2788419799804687\n",
      "Epoch: 2075 Training Loss: 0.2595327339172363 Test Loss: 0.27891975911458333\n",
      "Epoch: 2076 Training Loss: 0.2594932783444722 Test Loss: 0.2788969930013021\n",
      "Epoch: 2077 Training Loss: 0.25945839182535807 Test Loss: 0.27884055582682293\n",
      "Epoch: 2078 Training Loss: 0.25940889565149944 Test Loss: 0.2788395385742187\n",
      "Epoch: 2079 Training Loss: 0.25937122933069867 Test Loss: 0.27879142252604167\n",
      "Epoch: 2080 Training Loss: 0.25933317391077676 Test Loss: 0.27889501953125\n",
      "Epoch: 2081 Training Loss: 0.25929478645324705 Test Loss: 0.2787707722981771\n",
      "Epoch: 2082 Training Loss: 0.25925604104995725 Test Loss: 0.2787980143229167\n",
      "Epoch: 2083 Training Loss: 0.25921042935053507 Test Loss: 0.2786982421875\n",
      "Epoch: 2084 Training Loss: 0.25916912603378295 Test Loss: 0.27870430501302085\n",
      "Epoch: 2085 Training Loss: 0.25912886460622153 Test Loss: 0.2786565958658854\n",
      "Epoch: 2086 Training Loss: 0.2590928166707357 Test Loss: 0.27858052571614583\n",
      "Epoch: 2087 Training Loss: 0.25904441595077515 Test Loss: 0.2785882568359375\n",
      "Epoch: 2088 Training Loss: 0.25899657742182414 Test Loss: 0.27852591959635414\n",
      "Epoch: 2089 Training Loss: 0.2589680260022481 Test Loss: 0.27853055826822914\n",
      "Epoch: 2090 Training Loss: 0.2589143158594767 Test Loss: 0.2785184122721354\n",
      "Epoch: 2091 Training Loss: 0.25888041067123413 Test Loss: 0.2784500935872396\n",
      "Epoch: 2092 Training Loss: 0.2588418925603231 Test Loss: 0.2784092203776042\n",
      "Epoch: 2093 Training Loss: 0.2588113473256429 Test Loss: 0.27829254150390625\n",
      "Epoch: 2094 Training Loss: 0.2587597133318583 Test Loss: 0.2783581339518229\n",
      "Epoch: 2095 Training Loss: 0.2587226147651672 Test Loss: 0.27835164388020833\n",
      "Epoch: 2096 Training Loss: 0.2586928248405457 Test Loss: 0.27832368977864586\n",
      "Epoch: 2097 Training Loss: 0.25864390420913697 Test Loss: 0.27827360026041664\n",
      "Epoch: 2098 Training Loss: 0.2586112000147502 Test Loss: 0.2782216389973958\n",
      "Epoch: 2099 Training Loss: 0.2585626686414083 Test Loss: 0.2782406005859375\n",
      "Epoch: 2100 Training Loss: 0.2585260593096415 Test Loss: 0.27818802897135414\n",
      "Epoch: 2101 Training Loss: 0.25849088191986086 Test Loss: 0.2780748087565104\n",
      "Epoch: 2102 Training Loss: 0.25844916979471844 Test Loss: 0.27805126953125\n",
      "Epoch: 2103 Training Loss: 0.2584098529815674 Test Loss: 0.2780435791015625\n",
      "Epoch: 2104 Training Loss: 0.2583719714482625 Test Loss: 0.2780198974609375\n",
      "Epoch: 2105 Training Loss: 0.25833518187204996 Test Loss: 0.27805704752604166\n",
      "Epoch: 2106 Training Loss: 0.2582902285257975 Test Loss: 0.27797383626302086\n",
      "Epoch: 2107 Training Loss: 0.2582546315193176 Test Loss: 0.27805598958333333\n",
      "Epoch: 2108 Training Loss: 0.25821055698394774 Test Loss: 0.2780020548502604\n",
      "Epoch: 2109 Training Loss: 0.25818084303538 Test Loss: 0.2779527384440104\n",
      "Epoch: 2110 Training Loss: 0.2581413154602051 Test Loss: 0.27790669759114583\n",
      "Epoch: 2111 Training Loss: 0.2581098920504252 Test Loss: 0.27790804036458333\n",
      "Epoch: 2112 Training Loss: 0.2580643038749695 Test Loss: 0.27781050618489583\n",
      "Epoch: 2113 Training Loss: 0.2580390173594157 Test Loss: 0.2777738850911458\n",
      "Epoch: 2114 Training Loss: 0.25798995939890546 Test Loss: 0.27779825846354167\n",
      "Epoch: 2115 Training Loss: 0.2579594949086507 Test Loss: 0.27774495442708336\n",
      "Epoch: 2116 Training Loss: 0.25792364343007407 Test Loss: 0.2776613566080729\n",
      "Epoch: 2117 Training Loss: 0.25789064121246336 Test Loss: 0.2777742919921875\n",
      "Epoch: 2118 Training Loss: 0.25784702618916827 Test Loss: 0.2776509806315104\n",
      "Epoch: 2119 Training Loss: 0.25780758015314736 Test Loss: 0.2776024373372396\n",
      "Epoch: 2120 Training Loss: 0.25777012507120767 Test Loss: 0.27761140950520835\n",
      "Epoch: 2121 Training Loss: 0.25773108784357707 Test Loss: 0.2776409505208333\n",
      "Epoch: 2122 Training Loss: 0.2576922917366028 Test Loss: 0.2776266276041667\n",
      "Epoch: 2123 Training Loss: 0.25766229232152305 Test Loss: 0.2774740193684896\n",
      "Epoch: 2124 Training Loss: 0.25762874364852906 Test Loss: 0.2775105183919271\n",
      "Epoch: 2125 Training Loss: 0.2575862243970235 Test Loss: 0.2774667765299479\n",
      "Epoch: 2126 Training Loss: 0.2575545504887899 Test Loss: 0.27742964680989585\n",
      "Epoch: 2127 Training Loss: 0.25752090311050413 Test Loss: 0.27745220947265625\n",
      "Epoch: 2128 Training Loss: 0.25748480463027956 Test Loss: 0.27746533203125\n",
      "Epoch: 2129 Training Loss: 0.25744069449106854 Test Loss: 0.27729854329427084\n",
      "Epoch: 2130 Training Loss: 0.25739917103449506 Test Loss: 0.27726023356119794\n",
      "Epoch: 2131 Training Loss: 0.25737281767527265 Test Loss: 0.2773371785481771\n",
      "Epoch: 2132 Training Loss: 0.25732932790120444 Test Loss: 0.2772471923828125\n",
      "Epoch: 2133 Training Loss: 0.25729428656895953 Test Loss: 0.27733076985677085\n",
      "Epoch: 2134 Training Loss: 0.25726002343495685 Test Loss: 0.27718924967447917\n",
      "Epoch: 2135 Training Loss: 0.25722743209203086 Test Loss: 0.2772650146484375\n",
      "Epoch: 2136 Training Loss: 0.2571881229082743 Test Loss: 0.27731585693359373\n",
      "Epoch: 2137 Training Loss: 0.2571405968666077 Test Loss: 0.2771131184895833\n",
      "Epoch: 2138 Training Loss: 0.2571162377993266 Test Loss: 0.27716436767578123\n",
      "Epoch: 2139 Training Loss: 0.25707963943481443 Test Loss: 0.27713273111979164\n",
      "Epoch: 2140 Training Loss: 0.2570486591657003 Test Loss: 0.2771082967122396\n",
      "Epoch: 2141 Training Loss: 0.25701834551493324 Test Loss: 0.27717268880208334\n",
      "Epoch: 2142 Training Loss: 0.2569739648501078 Test Loss: 0.27708223470052085\n",
      "Epoch: 2143 Training Loss: 0.2569419520696004 Test Loss: 0.27710856119791666\n",
      "Epoch: 2144 Training Loss: 0.25690425729751587 Test Loss: 0.27697037760416665\n",
      "Epoch: 2145 Training Loss: 0.25686706415812177 Test Loss: 0.2769704996744792\n",
      "Epoch: 2146 Training Loss: 0.2568322256406148 Test Loss: 0.27698234049479165\n",
      "Epoch: 2147 Training Loss: 0.2567995818456014 Test Loss: 0.27689798990885417\n",
      "Epoch: 2148 Training Loss: 0.2567610023816427 Test Loss: 0.27686846923828123\n",
      "Epoch: 2149 Training Loss: 0.25673262325922647 Test Loss: 0.27684206136067707\n",
      "Epoch: 2150 Training Loss: 0.2566900080045064 Test Loss: 0.27684676106770834\n",
      "Epoch: 2151 Training Loss: 0.2566576116879781 Test Loss: 0.276824951171875\n",
      "Epoch: 2152 Training Loss: 0.25662498887379964 Test Loss: 0.2768330078125\n",
      "Epoch: 2153 Training Loss: 0.2565942079226176 Test Loss: 0.27679146321614584\n",
      "Epoch: 2154 Training Loss: 0.256551100730896 Test Loss: 0.27667818196614585\n",
      "Epoch: 2155 Training Loss: 0.25651506980260214 Test Loss: 0.2767426350911458\n",
      "Epoch: 2156 Training Loss: 0.2564804118474325 Test Loss: 0.2766136474609375\n",
      "Epoch: 2157 Training Loss: 0.2564492891629537 Test Loss: 0.27665397135416664\n",
      "Epoch: 2158 Training Loss: 0.2564244550069173 Test Loss: 0.2765631306966146\n",
      "Epoch: 2159 Training Loss: 0.2563781609535217 Test Loss: 0.27645792643229167\n",
      "Epoch: 2160 Training Loss: 0.25635056114196775 Test Loss: 0.2765355428059896\n",
      "Epoch: 2161 Training Loss: 0.256314031124115 Test Loss: 0.27644405110677084\n",
      "Epoch: 2162 Training Loss: 0.2562833070755005 Test Loss: 0.2763984375\n",
      "Epoch: 2163 Training Loss: 0.25624624490737913 Test Loss: 0.27641743977864586\n",
      "Epoch: 2164 Training Loss: 0.2562118412653605 Test Loss: 0.2763541259765625\n",
      "Epoch: 2165 Training Loss: 0.25617085440953574 Test Loss: 0.2763600667317708\n",
      "Epoch: 2166 Training Loss: 0.2561403414408366 Test Loss: 0.2764144490559896\n",
      "Epoch: 2167 Training Loss: 0.2561063586870829 Test Loss: 0.2762999674479167\n",
      "Epoch: 2168 Training Loss: 0.256070250193278 Test Loss: 0.27631414794921877\n",
      "Epoch: 2169 Training Loss: 0.25603552532196044 Test Loss: 0.2762216389973958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2170 Training Loss: 0.256006826877594 Test Loss: 0.27619856770833334\n",
      "Epoch: 2171 Training Loss: 0.2559673827489217 Test Loss: 0.2761658732096354\n",
      "Epoch: 2172 Training Loss: 0.2559404486020406 Test Loss: 0.2760781453450521\n",
      "Epoch: 2173 Training Loss: 0.2558980293273926 Test Loss: 0.2760719807942708\n",
      "Epoch: 2174 Training Loss: 0.2558675125439962 Test Loss: 0.275942626953125\n",
      "Epoch: 2175 Training Loss: 0.2558324424425761 Test Loss: 0.2760436808268229\n",
      "Epoch: 2176 Training Loss: 0.25579362614949547 Test Loss: 0.2760709228515625\n",
      "Epoch: 2177 Training Loss: 0.25576885890960693 Test Loss: 0.275920166015625\n",
      "Epoch: 2178 Training Loss: 0.25572958167394 Test Loss: 0.275968505859375\n",
      "Epoch: 2179 Training Loss: 0.25569699144363406 Test Loss: 0.27598722330729164\n",
      "Epoch: 2180 Training Loss: 0.25565322160720827 Test Loss: 0.2759267374674479\n",
      "Epoch: 2181 Training Loss: 0.2556319637298584 Test Loss: 0.27591178385416665\n",
      "Epoch: 2182 Training Loss: 0.2555954065322876 Test Loss: 0.2757749837239583\n",
      "Epoch: 2183 Training Loss: 0.2555618453025818 Test Loss: 0.2758830973307292\n",
      "Epoch: 2184 Training Loss: 0.25552584807078044 Test Loss: 0.2758200887044271\n",
      "Epoch: 2185 Training Loss: 0.2554966484705607 Test Loss: 0.2757408854166667\n",
      "Epoch: 2186 Training Loss: 0.25546715370814005 Test Loss: 0.27580865478515626\n",
      "Epoch: 2187 Training Loss: 0.25543238512674965 Test Loss: 0.2757842814127604\n",
      "Epoch: 2188 Training Loss: 0.2554032678604126 Test Loss: 0.27568929036458334\n",
      "Epoch: 2189 Training Loss: 0.25536443424224853 Test Loss: 0.2756884765625\n",
      "Epoch: 2190 Training Loss: 0.25533067957560224 Test Loss: 0.2756461181640625\n",
      "Epoch: 2191 Training Loss: 0.2552956765492757 Test Loss: 0.2755578816731771\n",
      "Epoch: 2192 Training Loss: 0.2552609890302022 Test Loss: 0.2755188802083333\n",
      "Epoch: 2193 Training Loss: 0.255231005191803 Test Loss: 0.27547119140625\n",
      "Epoch: 2194 Training Loss: 0.25519100125630695 Test Loss: 0.27557051595052084\n",
      "Epoch: 2195 Training Loss: 0.25515938584009806 Test Loss: 0.2755555216471354\n",
      "Epoch: 2196 Training Loss: 0.25511819473902386 Test Loss: 0.2755191040039062\n",
      "Epoch: 2197 Training Loss: 0.2550919176737467 Test Loss: 0.2754242553710938\n",
      "Epoch: 2198 Training Loss: 0.25505710411071775 Test Loss: 0.27546744791666666\n",
      "Epoch: 2199 Training Loss: 0.2550257511138916 Test Loss: 0.275435302734375\n",
      "Epoch: 2200 Training Loss: 0.25499955415725706 Test Loss: 0.27532747395833335\n",
      "Epoch: 2201 Training Loss: 0.2549561462402344 Test Loss: 0.2753212890625\n",
      "Epoch: 2202 Training Loss: 0.2549336468378703 Test Loss: 0.27532686360677083\n",
      "Epoch: 2203 Training Loss: 0.254894261042277 Test Loss: 0.2753236490885417\n",
      "Epoch: 2204 Training Loss: 0.2548659602801005 Test Loss: 0.27525655110677083\n",
      "Epoch: 2205 Training Loss: 0.2548270109494527 Test Loss: 0.2752701009114583\n",
      "Epoch: 2206 Training Loss: 0.25479576285680133 Test Loss: 0.27518575032552084\n",
      "Epoch: 2207 Training Loss: 0.25476516389846804 Test Loss: 0.2751658732096354\n",
      "Epoch: 2208 Training Loss: 0.2547310220400492 Test Loss: 0.2751128946940104\n",
      "Epoch: 2209 Training Loss: 0.2547018500963847 Test Loss: 0.2750906982421875\n",
      "Epoch: 2210 Training Loss: 0.25466956933339435 Test Loss: 0.275158447265625\n",
      "Epoch: 2211 Training Loss: 0.2546403282483419 Test Loss: 0.27521382649739584\n",
      "Epoch: 2212 Training Loss: 0.2546064774195353 Test Loss: 0.2750485636393229\n",
      "Epoch: 2213 Training Loss: 0.25457979599634806 Test Loss: 0.27500067138671874\n",
      "Epoch: 2214 Training Loss: 0.25454473225275676 Test Loss: 0.27505255126953126\n",
      "Epoch: 2215 Training Loss: 0.2545146120389303 Test Loss: 0.2751146240234375\n",
      "Epoch: 2216 Training Loss: 0.25447917302449546 Test Loss: 0.27498543294270833\n",
      "Epoch: 2217 Training Loss: 0.2544413188298543 Test Loss: 0.27493428548177085\n",
      "Epoch: 2218 Training Loss: 0.2544143261909485 Test Loss: 0.27490443929036457\n",
      "Epoch: 2219 Training Loss: 0.25438402366638185 Test Loss: 0.2749769287109375\n",
      "Epoch: 2220 Training Loss: 0.25434770981470745 Test Loss: 0.274840087890625\n",
      "Epoch: 2221 Training Loss: 0.2543165187835693 Test Loss: 0.2749303792317708\n",
      "Epoch: 2222 Training Loss: 0.25428490416208904 Test Loss: 0.27483186848958335\n",
      "Epoch: 2223 Training Loss: 0.25425161933898927 Test Loss: 0.27491668701171873\n",
      "Epoch: 2224 Training Loss: 0.2542177209854126 Test Loss: 0.2747961629231771\n",
      "Epoch: 2225 Training Loss: 0.2541878361701965 Test Loss: 0.2748226318359375\n",
      "Epoch: 2226 Training Loss: 0.2541572243372599 Test Loss: 0.2748048502604167\n",
      "Epoch: 2227 Training Loss: 0.2541227280298869 Test Loss: 0.2748006184895833\n",
      "Epoch: 2228 Training Loss: 0.254100240389506 Test Loss: 0.2746744384765625\n",
      "Epoch: 2229 Training Loss: 0.2540698668162028 Test Loss: 0.27466731770833336\n",
      "Epoch: 2230 Training Loss: 0.25403532600402834 Test Loss: 0.27476153564453126\n",
      "Epoch: 2231 Training Loss: 0.2540022101402283 Test Loss: 0.2746058349609375\n",
      "Epoch: 2232 Training Loss: 0.25397213220596315 Test Loss: 0.2746220703125\n",
      "Epoch: 2233 Training Loss: 0.253945219039917 Test Loss: 0.27456850179036457\n",
      "Epoch: 2234 Training Loss: 0.2539099225997925 Test Loss: 0.2745089314778646\n",
      "Epoch: 2235 Training Loss: 0.25388398885726926 Test Loss: 0.274456787109375\n",
      "Epoch: 2236 Training Loss: 0.2538552722930908 Test Loss: 0.27456166585286457\n",
      "Epoch: 2237 Training Loss: 0.2538176975250244 Test Loss: 0.27456783040364585\n",
      "Epoch: 2238 Training Loss: 0.2537920018831889 Test Loss: 0.2744660441080729\n",
      "Epoch: 2239 Training Loss: 0.25375574572881066 Test Loss: 0.27445503743489585\n",
      "Epoch: 2240 Training Loss: 0.2537383631070455 Test Loss: 0.27450661214192706\n",
      "Epoch: 2241 Training Loss: 0.25369275919596357 Test Loss: 0.27439398193359377\n",
      "Epoch: 2242 Training Loss: 0.2536651577949524 Test Loss: 0.2744932047526042\n",
      "Epoch: 2243 Training Loss: 0.2536351521809896 Test Loss: 0.2743797607421875\n",
      "Epoch: 2244 Training Loss: 0.25359887472788495 Test Loss: 0.27438187662760416\n",
      "Epoch: 2245 Training Loss: 0.25356219482421877 Test Loss: 0.27436553955078125\n",
      "Epoch: 2246 Training Loss: 0.2535350669225057 Test Loss: 0.27440244547526044\n",
      "Epoch: 2247 Training Loss: 0.2535045121510824 Test Loss: 0.274255859375\n",
      "Epoch: 2248 Training Loss: 0.2534830722808838 Test Loss: 0.27428255208333335\n",
      "Epoch: 2249 Training Loss: 0.25344069449106854 Test Loss: 0.2741513264973958\n",
      "Epoch: 2250 Training Loss: 0.2534037373860677 Test Loss: 0.2742146402994792\n",
      "Epoch: 2251 Training Loss: 0.25337620226542157 Test Loss: 0.274111572265625\n",
      "Epoch: 2252 Training Loss: 0.2533454287846883 Test Loss: 0.2742135009765625\n",
      "Epoch: 2253 Training Loss: 0.25331123224894203 Test Loss: 0.2741155192057292\n",
      "Epoch: 2254 Training Loss: 0.25327575572331745 Test Loss: 0.2741259765625\n",
      "Epoch: 2255 Training Loss: 0.253249306678772 Test Loss: 0.2740498046875\n",
      "Epoch: 2256 Training Loss: 0.2532089236577352 Test Loss: 0.27406978352864586\n",
      "Epoch: 2257 Training Loss: 0.25318428039550783 Test Loss: 0.2740473836263021\n",
      "Epoch: 2258 Training Loss: 0.2531396425565084 Test Loss: 0.27392454020182294\n",
      "Epoch: 2259 Training Loss: 0.25311775890986127 Test Loss: 0.274001953125\n",
      "Epoch: 2260 Training Loss: 0.2530859196980794 Test Loss: 0.2739737955729167\n",
      "Epoch: 2261 Training Loss: 0.25305315128962197 Test Loss: 0.2739328816731771\n",
      "Epoch: 2262 Training Loss: 0.25301937023798626 Test Loss: 0.27401717122395836\n",
      "Epoch: 2263 Training Loss: 0.25298957792917887 Test Loss: 0.27390948486328126\n",
      "Epoch: 2264 Training Loss: 0.2529533708890279 Test Loss: 0.273904296875\n",
      "Epoch: 2265 Training Loss: 0.25293244981765745 Test Loss: 0.27385396321614586\n",
      "Epoch: 2266 Training Loss: 0.25289399496714277 Test Loss: 0.27378287760416664\n",
      "Epoch: 2267 Training Loss: 0.252860889116923 Test Loss: 0.27382790120442707\n",
      "Epoch: 2268 Training Loss: 0.25283988666534424 Test Loss: 0.27379010009765625\n",
      "Epoch: 2269 Training Loss: 0.25280688619613645 Test Loss: 0.27381561279296873\n",
      "Epoch: 2270 Training Loss: 0.2527813884417216 Test Loss: 0.273693603515625\n",
      "Epoch: 2271 Training Loss: 0.25274450381596886 Test Loss: 0.2737445068359375\n",
      "Epoch: 2272 Training Loss: 0.2527192653020223 Test Loss: 0.2736575317382812\n",
      "Epoch: 2273 Training Loss: 0.2526954755783081 Test Loss: 0.2737139078776042\n",
      "Epoch: 2274 Training Loss: 0.25265988222757974 Test Loss: 0.27355391438802085\n",
      "Epoch: 2275 Training Loss: 0.25262346744537356 Test Loss: 0.27358378092447916\n",
      "Epoch: 2276 Training Loss: 0.25258979241053264 Test Loss: 0.2735188802083333\n",
      "Epoch: 2277 Training Loss: 0.25255726607640583 Test Loss: 0.2735005289713542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2278 Training Loss: 0.25253204854329425 Test Loss: 0.2734823811848958\n",
      "Epoch: 2279 Training Loss: 0.25250247240066526 Test Loss: 0.2735238037109375\n",
      "Epoch: 2280 Training Loss: 0.25246720123291017 Test Loss: 0.27346040852864584\n",
      "Epoch: 2281 Training Loss: 0.2524377376238505 Test Loss: 0.2734282633463542\n",
      "Epoch: 2282 Training Loss: 0.25241015895207725 Test Loss: 0.27337333170572914\n",
      "Epoch: 2283 Training Loss: 0.2523834662437439 Test Loss: 0.2734052734375\n",
      "Epoch: 2284 Training Loss: 0.2523562118212382 Test Loss: 0.2733578694661458\n",
      "Epoch: 2285 Training Loss: 0.2523309146563212 Test Loss: 0.2732395629882812\n",
      "Epoch: 2286 Training Loss: 0.2522919586499532 Test Loss: 0.27325537109375\n",
      "Epoch: 2287 Training Loss: 0.2522661442756653 Test Loss: 0.2733806762695313\n",
      "Epoch: 2288 Training Loss: 0.2522324209213257 Test Loss: 0.2733161214192708\n",
      "Epoch: 2289 Training Loss: 0.25219899797439577 Test Loss: 0.2732563273111979\n",
      "Epoch: 2290 Training Loss: 0.2521640431086222 Test Loss: 0.2731774495442708\n",
      "Epoch: 2291 Training Loss: 0.25214957650502523 Test Loss: 0.27310481770833334\n",
      "Epoch: 2292 Training Loss: 0.25211242167154946 Test Loss: 0.27315165201822916\n",
      "Epoch: 2293 Training Loss: 0.2520819718043009 Test Loss: 0.2731672159830729\n",
      "Epoch: 2294 Training Loss: 0.25205601104100545 Test Loss: 0.2730911458333333\n",
      "Epoch: 2295 Training Loss: 0.2520244539578756 Test Loss: 0.27297823079427086\n",
      "Epoch: 2296 Training Loss: 0.2519952767690023 Test Loss: 0.2731134847005208\n",
      "Epoch: 2297 Training Loss: 0.25196603711446125 Test Loss: 0.2729145711263021\n",
      "Epoch: 2298 Training Loss: 0.25193561585744223 Test Loss: 0.27297469075520836\n",
      "Epoch: 2299 Training Loss: 0.2519032491048177 Test Loss: 0.27294661458333336\n",
      "Epoch: 2300 Training Loss: 0.2518720324834188 Test Loss: 0.2728785400390625\n",
      "Epoch: 2301 Training Loss: 0.2518398947715759 Test Loss: 0.27286612955729167\n",
      "Epoch: 2302 Training Loss: 0.25180807638168334 Test Loss: 0.2728123168945312\n",
      "Epoch: 2303 Training Loss: 0.2517839473088582 Test Loss: 0.2728258463541667\n",
      "Epoch: 2304 Training Loss: 0.2517628788948059 Test Loss: 0.2728321533203125\n",
      "Epoch: 2305 Training Loss: 0.2517180314064026 Test Loss: 0.27275162760416666\n",
      "Epoch: 2306 Training Loss: 0.2516999336878459 Test Loss: 0.2727713623046875\n",
      "Epoch: 2307 Training Loss: 0.2516672347386678 Test Loss: 0.27274210611979166\n",
      "Epoch: 2308 Training Loss: 0.2516332731246948 Test Loss: 0.27279290771484377\n",
      "Epoch: 2309 Training Loss: 0.25160539674758914 Test Loss: 0.27273095703125\n",
      "Epoch: 2310 Training Loss: 0.25158200709025064 Test Loss: 0.2727289632161458\n",
      "Epoch: 2311 Training Loss: 0.25154722325007123 Test Loss: 0.27266361490885416\n",
      "Epoch: 2312 Training Loss: 0.25152016830444335 Test Loss: 0.2727706298828125\n",
      "Epoch: 2313 Training Loss: 0.2514867140452067 Test Loss: 0.2725972086588542\n",
      "Epoch: 2314 Training Loss: 0.2514632666905721 Test Loss: 0.27267472330729164\n",
      "Epoch: 2315 Training Loss: 0.25143479712804157 Test Loss: 0.2726683553059896\n",
      "Epoch: 2316 Training Loss: 0.2514048164685567 Test Loss: 0.27272047932942706\n",
      "Epoch: 2317 Training Loss: 0.2513728316624959 Test Loss: 0.272588134765625\n",
      "Epoch: 2318 Training Loss: 0.2513489449818929 Test Loss: 0.27264839680989583\n",
      "Epoch: 2319 Training Loss: 0.25130836296081543 Test Loss: 0.2725737508138021\n",
      "Epoch: 2320 Training Loss: 0.2512845233281453 Test Loss: 0.27255279541015626\n",
      "Epoch: 2321 Training Loss: 0.25124901040395103 Test Loss: 0.27252632649739583\n",
      "Epoch: 2322 Training Loss: 0.25122151629130046 Test Loss: 0.27255122884114585\n",
      "Epoch: 2323 Training Loss: 0.2511935698191325 Test Loss: 0.2725442708333333\n",
      "Epoch: 2324 Training Loss: 0.25116958379745485 Test Loss: 0.2725129191080729\n",
      "Epoch: 2325 Training Loss: 0.2511373709042867 Test Loss: 0.2723763427734375\n",
      "Epoch: 2326 Training Loss: 0.2511053916613261 Test Loss: 0.2723525187174479\n",
      "Epoch: 2327 Training Loss: 0.25108075761795046 Test Loss: 0.2723733317057292\n",
      "Epoch: 2328 Training Loss: 0.25104965098698934 Test Loss: 0.27245416259765626\n",
      "Epoch: 2329 Training Loss: 0.25101536575953165 Test Loss: 0.27236083984375\n",
      "Epoch: 2330 Training Loss: 0.25098885742823285 Test Loss: 0.2722868855794271\n",
      "Epoch: 2331 Training Loss: 0.25096135409673054 Test Loss: 0.27232110595703124\n",
      "Epoch: 2332 Training Loss: 0.2509313012758891 Test Loss: 0.27222369384765627\n",
      "Epoch: 2333 Training Loss: 0.2508971892992655 Test Loss: 0.2722400309244792\n",
      "Epoch: 2334 Training Loss: 0.25088308699925743 Test Loss: 0.2722302652994792\n",
      "Epoch: 2335 Training Loss: 0.2508425798416138 Test Loss: 0.2721794026692708\n",
      "Epoch: 2336 Training Loss: 0.25081743574142457 Test Loss: 0.2722254638671875\n",
      "Epoch: 2337 Training Loss: 0.2507884620030721 Test Loss: 0.27210628255208336\n",
      "Epoch: 2338 Training Loss: 0.2507526418368022 Test Loss: 0.272226318359375\n",
      "Epoch: 2339 Training Loss: 0.2507117077509562 Test Loss: 0.2721623331705729\n",
      "Epoch: 2340 Training Loss: 0.2506581411361694 Test Loss: 0.2719971720377604\n",
      "Epoch: 2341 Training Loss: 0.2505946811040243 Test Loss: 0.2719660847981771\n",
      "Epoch: 2342 Training Loss: 0.25053829050064086 Test Loss: 0.27198075358072915\n",
      "Epoch: 2343 Training Loss: 0.2504746233622233 Test Loss: 0.27185791015625\n",
      "Epoch: 2344 Training Loss: 0.250414928595225 Test Loss: 0.27189756266276044\n",
      "Epoch: 2345 Training Loss: 0.25036382659276324 Test Loss: 0.27177701822916667\n",
      "Epoch: 2346 Training Loss: 0.25031670157114666 Test Loss: 0.2717406616210937\n",
      "Epoch: 2347 Training Loss: 0.2502657647132874 Test Loss: 0.27162361653645833\n",
      "Epoch: 2348 Training Loss: 0.25021783129374187 Test Loss: 0.27172574869791666\n",
      "Epoch: 2349 Training Loss: 0.25018008263905844 Test Loss: 0.2715997517903646\n",
      "Epoch: 2350 Training Loss: 0.25013743845621744 Test Loss: 0.2715987752278646\n",
      "Epoch: 2351 Training Loss: 0.2500942341486613 Test Loss: 0.27155462646484374\n",
      "Epoch: 2352 Training Loss: 0.25005513858795164 Test Loss: 0.2714868570963542\n",
      "Epoch: 2353 Training Loss: 0.25001942650477094 Test Loss: 0.2715855712890625\n",
      "Epoch: 2354 Training Loss: 0.24996983512242635 Test Loss: 0.2715733642578125\n",
      "Epoch: 2355 Training Loss: 0.24994221623738608 Test Loss: 0.2713930257161458\n",
      "Epoch: 2356 Training Loss: 0.24989503796895346 Test Loss: 0.271453125\n",
      "Epoch: 2357 Training Loss: 0.2498635458946228 Test Loss: 0.27135660807291667\n",
      "Epoch: 2358 Training Loss: 0.24983732350667318 Test Loss: 0.2714983113606771\n",
      "Epoch: 2359 Training Loss: 0.24979323720932006 Test Loss: 0.27131070963541665\n",
      "Epoch: 2360 Training Loss: 0.2497576756477356 Test Loss: 0.2713538818359375\n",
      "Epoch: 2361 Training Loss: 0.24973176225026447 Test Loss: 0.27125274658203125\n",
      "Epoch: 2362 Training Loss: 0.2496894474029541 Test Loss: 0.2712232666015625\n",
      "Epoch: 2363 Training Loss: 0.24966623385747275 Test Loss: 0.27128287760416664\n",
      "Epoch: 2364 Training Loss: 0.24963408501942952 Test Loss: 0.2711973063151042\n",
      "Epoch: 2365 Training Loss: 0.24960448296864826 Test Loss: 0.27118595377604165\n",
      "Epoch: 2366 Training Loss: 0.24957071018218993 Test Loss: 0.27119698079427085\n",
      "Epoch: 2367 Training Loss: 0.249537615776062 Test Loss: 0.2710887858072917\n",
      "Epoch: 2368 Training Loss: 0.2495083303451538 Test Loss: 0.271021240234375\n",
      "Epoch: 2369 Training Loss: 0.24947259934743246 Test Loss: 0.27094248453776043\n",
      "Epoch: 2370 Training Loss: 0.24944271596272785 Test Loss: 0.27095646158854164\n",
      "Epoch: 2371 Training Loss: 0.24941784445444742 Test Loss: 0.27098697916666664\n",
      "Epoch: 2372 Training Loss: 0.2493849630355835 Test Loss: 0.2710038248697917\n",
      "Epoch: 2373 Training Loss: 0.24935401074091593 Test Loss: 0.2709091796875\n",
      "Epoch: 2374 Training Loss: 0.2493260399500529 Test Loss: 0.27099090576171875\n",
      "Epoch: 2375 Training Loss: 0.24930122979482014 Test Loss: 0.2709744669596354\n",
      "Epoch: 2376 Training Loss: 0.24926757589975992 Test Loss: 0.27095621744791665\n",
      "Epoch: 2377 Training Loss: 0.24924536482493082 Test Loss: 0.27093560791015625\n",
      "Epoch: 2378 Training Loss: 0.24921214946111042 Test Loss: 0.27090779622395833\n",
      "Epoch: 2379 Training Loss: 0.24917840878168743 Test Loss: 0.2707939656575521\n",
      "Epoch: 2380 Training Loss: 0.24915351883570352 Test Loss: 0.27083721923828125\n",
      "Epoch: 2381 Training Loss: 0.2491334382692973 Test Loss: 0.2708375244140625\n",
      "Epoch: 2382 Training Loss: 0.24909701983133953 Test Loss: 0.2707427978515625\n",
      "Epoch: 2383 Training Loss: 0.2490721435546875 Test Loss: 0.27076021321614585\n",
      "Epoch: 2384 Training Loss: 0.24905338255564372 Test Loss: 0.27076951090494794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2385 Training Loss: 0.2490203226407369 Test Loss: 0.2707857666015625\n",
      "Epoch: 2386 Training Loss: 0.24899211835861207 Test Loss: 0.2707308349609375\n",
      "Epoch: 2387 Training Loss: 0.2489764641125997 Test Loss: 0.2707672119140625\n",
      "Epoch: 2388 Training Loss: 0.24894678576787313 Test Loss: 0.2706727498372396\n",
      "Epoch: 2389 Training Loss: 0.24891745233535767 Test Loss: 0.27070296223958334\n",
      "Epoch: 2390 Training Loss: 0.24888292169570922 Test Loss: 0.2706980183919271\n",
      "Epoch: 2391 Training Loss: 0.24886874135335285 Test Loss: 0.2706117350260417\n",
      "Epoch: 2392 Training Loss: 0.24884916766484577 Test Loss: 0.2706159464518229\n",
      "Epoch: 2393 Training Loss: 0.24880715004603068 Test Loss: 0.2706539103190104\n",
      "Epoch: 2394 Training Loss: 0.2487821626663208 Test Loss: 0.2706528930664063\n",
      "Epoch: 2395 Training Loss: 0.2487547337214152 Test Loss: 0.270451416015625\n",
      "Epoch: 2396 Training Loss: 0.2487342726389567 Test Loss: 0.27053204345703125\n",
      "Epoch: 2397 Training Loss: 0.24870365365346273 Test Loss: 0.27049446614583333\n",
      "Epoch: 2398 Training Loss: 0.24867438697814942 Test Loss: 0.2704981689453125\n",
      "Epoch: 2399 Training Loss: 0.2486515113512675 Test Loss: 0.2704373779296875\n",
      "Epoch: 2400 Training Loss: 0.24862887970606487 Test Loss: 0.27045574951171875\n",
      "Epoch: 2401 Training Loss: 0.24860423151652017 Test Loss: 0.27055045572916664\n",
      "Epoch: 2402 Training Loss: 0.24857934776941934 Test Loss: 0.2704098307291667\n",
      "Epoch: 2403 Training Loss: 0.24855959701538086 Test Loss: 0.2704974365234375\n",
      "Epoch: 2404 Training Loss: 0.2485365727742513 Test Loss: 0.2704092203776042\n",
      "Epoch: 2405 Training Loss: 0.24850378290812175 Test Loss: 0.27040932210286456\n",
      "Epoch: 2406 Training Loss: 0.2484717189470927 Test Loss: 0.2704677734375\n",
      "Epoch: 2407 Training Loss: 0.24845381704966227 Test Loss: 0.27039583333333334\n",
      "Epoch: 2408 Training Loss: 0.24843510230382285 Test Loss: 0.27037190755208335\n",
      "Epoch: 2409 Training Loss: 0.24840826797485352 Test Loss: 0.2703424072265625\n",
      "Epoch: 2410 Training Loss: 0.24838102261225384 Test Loss: 0.2702245279947917\n",
      "Epoch: 2411 Training Loss: 0.24836344623565673 Test Loss: 0.27025583902994793\n",
      "Epoch: 2412 Training Loss: 0.24833129692077638 Test Loss: 0.27028619384765623\n",
      "Epoch: 2413 Training Loss: 0.2483159686724345 Test Loss: 0.27018902587890625\n",
      "Epoch: 2414 Training Loss: 0.24829839468002318 Test Loss: 0.27023614501953125\n",
      "Epoch: 2415 Training Loss: 0.24826390981674196 Test Loss: 0.27022904459635416\n",
      "Epoch: 2416 Training Loss: 0.24825012874603272 Test Loss: 0.2702090250651042\n",
      "Epoch: 2417 Training Loss: 0.24821855147679647 Test Loss: 0.27012648518880206\n",
      "Epoch: 2418 Training Loss: 0.2481971084276835 Test Loss: 0.2701955159505208\n",
      "Epoch: 2419 Training Loss: 0.2481691740353902 Test Loss: 0.2702023722330729\n",
      "Epoch: 2420 Training Loss: 0.2481469513575236 Test Loss: 0.2701245727539062\n",
      "Epoch: 2421 Training Loss: 0.24811960776646932 Test Loss: 0.2701692097981771\n",
      "Epoch: 2422 Training Loss: 0.2480901428858439 Test Loss: 0.26996921793619794\n",
      "Epoch: 2423 Training Loss: 0.24807029167811076 Test Loss: 0.2699386393229167\n",
      "Epoch: 2424 Training Loss: 0.24803405872980755 Test Loss: 0.27005208333333336\n",
      "Epoch: 2425 Training Loss: 0.24801411326726278 Test Loss: 0.2699463500976563\n",
      "Epoch: 2426 Training Loss: 0.24798709599177043 Test Loss: 0.2699649251302083\n",
      "Epoch: 2427 Training Loss: 0.24796573893229168 Test Loss: 0.27002693684895834\n",
      "Epoch: 2428 Training Loss: 0.2479430940945943 Test Loss: 0.2699799601236979\n",
      "Epoch: 2429 Training Loss: 0.24791416470209757 Test Loss: 0.2699031778971354\n",
      "Epoch: 2430 Training Loss: 0.2478844936688741 Test Loss: 0.26991438802083334\n",
      "Epoch: 2431 Training Loss: 0.24785957527160646 Test Loss: 0.2698797607421875\n",
      "Epoch: 2432 Training Loss: 0.24783143854141235 Test Loss: 0.2698638916015625\n",
      "Epoch: 2433 Training Loss: 0.24781394100189208 Test Loss: 0.26993275960286456\n",
      "Epoch: 2434 Training Loss: 0.24779093917210898 Test Loss: 0.2697687784830729\n",
      "Epoch: 2435 Training Loss: 0.24776402393976849 Test Loss: 0.26971732584635416\n",
      "Epoch: 2436 Training Loss: 0.24774092944463094 Test Loss: 0.2697561442057292\n",
      "Epoch: 2437 Training Loss: 0.2477136494318644 Test Loss: 0.2697676391601562\n",
      "Epoch: 2438 Training Loss: 0.2476879890759786 Test Loss: 0.2696400146484375\n",
      "Epoch: 2439 Training Loss: 0.24767407258351645 Test Loss: 0.2696127726236979\n",
      "Epoch: 2440 Training Loss: 0.24764609956741332 Test Loss: 0.26958064778645835\n",
      "Epoch: 2441 Training Loss: 0.247631529490153 Test Loss: 0.2695809326171875\n",
      "Epoch: 2442 Training Loss: 0.2475998910268148 Test Loss: 0.2695469767252604\n",
      "Epoch: 2443 Training Loss: 0.24757293796539306 Test Loss: 0.26963242594401043\n",
      "Epoch: 2444 Training Loss: 0.24754797331492107 Test Loss: 0.26962939453125\n",
      "Epoch: 2445 Training Loss: 0.24752618010838826 Test Loss: 0.26940303548177086\n",
      "Epoch: 2446 Training Loss: 0.24750264501571656 Test Loss: 0.26952252197265625\n",
      "Epoch: 2447 Training Loss: 0.24748832114537556 Test Loss: 0.26949845377604165\n",
      "Epoch: 2448 Training Loss: 0.24745640929539997 Test Loss: 0.26946929931640623\n",
      "Epoch: 2449 Training Loss: 0.24742697397867838 Test Loss: 0.2694809773763021\n",
      "Epoch: 2450 Training Loss: 0.24739717102050782 Test Loss: 0.269369384765625\n",
      "Epoch: 2451 Training Loss: 0.24738541396458943 Test Loss: 0.26942500813802084\n",
      "Epoch: 2452 Training Loss: 0.24735845486323038 Test Loss: 0.2694521280924479\n",
      "Epoch: 2453 Training Loss: 0.24731718762715657 Test Loss: 0.26936000569661456\n",
      "Epoch: 2454 Training Loss: 0.24731284872690837 Test Loss: 0.2693332926432292\n",
      "Epoch: 2455 Training Loss: 0.24727849006652833 Test Loss: 0.2693126220703125\n",
      "Epoch: 2456 Training Loss: 0.24725219599405923 Test Loss: 0.2692574462890625\n",
      "Epoch: 2457 Training Loss: 0.2472299526532491 Test Loss: 0.2692166748046875\n",
      "Epoch: 2458 Training Loss: 0.2472094408671061 Test Loss: 0.26929585774739584\n",
      "Epoch: 2459 Training Loss: 0.24717809549967448 Test Loss: 0.2692765096028646\n",
      "Epoch: 2460 Training Loss: 0.24716122500101725 Test Loss: 0.2692718098958333\n",
      "Epoch: 2461 Training Loss: 0.2471329944928487 Test Loss: 0.2692866617838542\n",
      "Epoch: 2462 Training Loss: 0.24711604913075766 Test Loss: 0.2692362467447917\n",
      "Epoch: 2463 Training Loss: 0.2470936745007833 Test Loss: 0.26913932291666665\n",
      "Epoch: 2464 Training Loss: 0.24706526358922323 Test Loss: 0.26921622721354166\n",
      "Epoch: 2465 Training Loss: 0.24705537414550782 Test Loss: 0.26915254720052084\n",
      "Epoch: 2466 Training Loss: 0.2470190896987915 Test Loss: 0.26910013834635416\n",
      "Epoch: 2467 Training Loss: 0.24700604899724324 Test Loss: 0.2690540974934896\n",
      "Epoch: 2468 Training Loss: 0.24697079118092854 Test Loss: 0.26901322428385416\n",
      "Epoch: 2469 Training Loss: 0.24694680960973103 Test Loss: 0.26907051595052084\n",
      "Epoch: 2470 Training Loss: 0.2469344916343689 Test Loss: 0.26907470703125\n",
      "Epoch: 2471 Training Loss: 0.24691052865982055 Test Loss: 0.2689945475260417\n",
      "Epoch: 2472 Training Loss: 0.2468830067316691 Test Loss: 0.2689903157552083\n",
      "Epoch: 2473 Training Loss: 0.24686432488759358 Test Loss: 0.2690567626953125\n",
      "Epoch: 2474 Training Loss: 0.2468380200068156 Test Loss: 0.2689949747721354\n",
      "Epoch: 2475 Training Loss: 0.24682374652226766 Test Loss: 0.2690040079752604\n",
      "Epoch: 2476 Training Loss: 0.2468006108601888 Test Loss: 0.268919677734375\n",
      "Epoch: 2477 Training Loss: 0.24677488040924073 Test Loss: 0.2689875284830729\n",
      "Epoch: 2478 Training Loss: 0.2467450377146403 Test Loss: 0.26897674560546875\n",
      "Epoch: 2479 Training Loss: 0.24672603845596314 Test Loss: 0.26887880452473956\n",
      "Epoch: 2480 Training Loss: 0.24670457665125528 Test Loss: 0.2688385823567708\n",
      "Epoch: 2481 Training Loss: 0.24667787075042724 Test Loss: 0.268901611328125\n",
      "Epoch: 2482 Training Loss: 0.24664346027374268 Test Loss: 0.2688503824869792\n",
      "Epoch: 2483 Training Loss: 0.24663336833318075 Test Loss: 0.268782958984375\n",
      "Epoch: 2484 Training Loss: 0.24660860538482665 Test Loss: 0.26874100748697916\n",
      "Epoch: 2485 Training Loss: 0.24657691796620687 Test Loss: 0.26863887532552083\n",
      "Epoch: 2486 Training Loss: 0.24655972766876222 Test Loss: 0.2686995442708333\n",
      "Epoch: 2487 Training Loss: 0.24654033009211224 Test Loss: 0.2687413533528646\n",
      "Epoch: 2488 Training Loss: 0.2465031894048055 Test Loss: 0.2686707356770833\n",
      "Epoch: 2489 Training Loss: 0.24648652346928915 Test Loss: 0.26863441975911456\n",
      "Epoch: 2490 Training Loss: 0.24645800431569417 Test Loss: 0.2686533610026042\n",
      "Epoch: 2491 Training Loss: 0.24644508043924968 Test Loss: 0.2686846923828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2492 Training Loss: 0.24641648308436076 Test Loss: 0.2686849568684896\n",
      "Epoch: 2493 Training Loss: 0.24638381910324098 Test Loss: 0.26859912109375\n",
      "Epoch: 2494 Training Loss: 0.24637042713165283 Test Loss: 0.26860432942708334\n",
      "Epoch: 2495 Training Loss: 0.24633625841140747 Test Loss: 0.2685693155924479\n",
      "Epoch: 2496 Training Loss: 0.24631517044703166 Test Loss: 0.26853690592447915\n",
      "Epoch: 2497 Training Loss: 0.24630273167292277 Test Loss: 0.26855741373697917\n",
      "Epoch: 2498 Training Loss: 0.24628016328811644 Test Loss: 0.2685312093098958\n",
      "Epoch: 2499 Training Loss: 0.24624917523066203 Test Loss: 0.26854703776041666\n",
      "Epoch: 2500 Training Loss: 0.24622146288553873 Test Loss: 0.26851692708333336\n",
      "Epoch: 2501 Training Loss: 0.24620068327585856 Test Loss: 0.2684325968424479\n",
      "Epoch: 2502 Training Loss: 0.2461741935412089 Test Loss: 0.26834031168619793\n",
      "Epoch: 2503 Training Loss: 0.24616580057144166 Test Loss: 0.268431640625\n",
      "Epoch: 2504 Training Loss: 0.24612660932540895 Test Loss: 0.26832657877604166\n",
      "Epoch: 2505 Training Loss: 0.2461040727297465 Test Loss: 0.2683827107747396\n",
      "Epoch: 2506 Training Loss: 0.2460861611366272 Test Loss: 0.268365234375\n",
      "Epoch: 2507 Training Loss: 0.24606155459086101 Test Loss: 0.26825516764322915\n",
      "Epoch: 2508 Training Loss: 0.24604151519139608 Test Loss: 0.26829278564453124\n",
      "Epoch: 2509 Training Loss: 0.2460050007502238 Test Loss: 0.26836568196614585\n",
      "Epoch: 2510 Training Loss: 0.2459971211751302 Test Loss: 0.26826666259765625\n",
      "Epoch: 2511 Training Loss: 0.2459591072400411 Test Loss: 0.26831856282552086\n",
      "Epoch: 2512 Training Loss: 0.24594500478108725 Test Loss: 0.26825004069010416\n",
      "Epoch: 2513 Training Loss: 0.24591383123397828 Test Loss: 0.26822989908854167\n",
      "Epoch: 2514 Training Loss: 0.24589416297276814 Test Loss: 0.2682230021158854\n",
      "Epoch: 2515 Training Loss: 0.24587139066060384 Test Loss: 0.26813592529296876\n",
      "Epoch: 2516 Training Loss: 0.24583190234502156 Test Loss: 0.26818017578125\n",
      "Epoch: 2517 Training Loss: 0.2458219534556071 Test Loss: 0.26810306803385414\n",
      "Epoch: 2518 Training Loss: 0.24580123456319172 Test Loss: 0.2681710001627604\n",
      "Epoch: 2519 Training Loss: 0.24577126423517862 Test Loss: 0.2681114705403646\n",
      "Epoch: 2520 Training Loss: 0.2457541675567627 Test Loss: 0.26806032307942707\n",
      "Epoch: 2521 Training Loss: 0.24572511196136473 Test Loss: 0.26808351643880207\n",
      "Epoch: 2522 Training Loss: 0.24571677621205648 Test Loss: 0.26803023274739585\n",
      "Epoch: 2523 Training Loss: 0.24567601283391316 Test Loss: 0.26802752685546877\n",
      "Epoch: 2524 Training Loss: 0.24565238682428997 Test Loss: 0.2679351806640625\n",
      "Epoch: 2525 Training Loss: 0.24563373788197834 Test Loss: 0.2680204264322917\n",
      "Epoch: 2526 Training Loss: 0.24560654576619467 Test Loss: 0.2679581095377604\n",
      "Epoch: 2527 Training Loss: 0.24558229223887126 Test Loss: 0.2679329833984375\n",
      "Epoch: 2528 Training Loss: 0.24556362454096475 Test Loss: 0.26788602701822917\n",
      "Epoch: 2529 Training Loss: 0.2455369068781535 Test Loss: 0.2679297688802083\n",
      "Epoch: 2530 Training Loss: 0.24551059452692667 Test Loss: 0.26779404703776044\n",
      "Epoch: 2531 Training Loss: 0.24548672930399576 Test Loss: 0.2678843790690104\n",
      "Epoch: 2532 Training Loss: 0.24546219603220623 Test Loss: 0.2678556315104167\n",
      "Epoch: 2533 Training Loss: 0.2454492548306783 Test Loss: 0.267865234375\n",
      "Epoch: 2534 Training Loss: 0.24541371472676596 Test Loss: 0.26794219970703126\n",
      "Epoch: 2535 Training Loss: 0.24539384508132936 Test Loss: 0.26781217447916666\n",
      "Epoch: 2536 Training Loss: 0.24537657086054485 Test Loss: 0.26776619466145835\n",
      "Epoch: 2537 Training Loss: 0.24534821637471516 Test Loss: 0.267697021484375\n",
      "Epoch: 2538 Training Loss: 0.24532168976465862 Test Loss: 0.26771468098958334\n",
      "Epoch: 2539 Training Loss: 0.24530190388361614 Test Loss: 0.26778682454427083\n",
      "Epoch: 2540 Training Loss: 0.24527789402008057 Test Loss: 0.2676995442708333\n",
      "Epoch: 2541 Training Loss: 0.24525878111521404 Test Loss: 0.2677579142252604\n",
      "Epoch: 2542 Training Loss: 0.24522939252853393 Test Loss: 0.26767909749348956\n",
      "Epoch: 2543 Training Loss: 0.24521408494313557 Test Loss: 0.2676205851236979\n",
      "Epoch: 2544 Training Loss: 0.24518105189005535 Test Loss: 0.26766552734375\n",
      "Epoch: 2545 Training Loss: 0.24515917762120565 Test Loss: 0.26763421630859374\n",
      "Epoch: 2546 Training Loss: 0.24513986476262412 Test Loss: 0.2675666097005208\n",
      "Epoch: 2547 Training Loss: 0.24511776336034138 Test Loss: 0.26756903076171873\n",
      "Epoch: 2548 Training Loss: 0.24508456548055013 Test Loss: 0.26760428873697917\n",
      "Epoch: 2549 Training Loss: 0.24505731741587322 Test Loss: 0.2675429280598958\n",
      "Epoch: 2550 Training Loss: 0.2450349596341451 Test Loss: 0.26756070963541667\n",
      "Epoch: 2551 Training Loss: 0.24500846036275228 Test Loss: 0.26751558430989586\n",
      "Epoch: 2552 Training Loss: 0.24498619683583578 Test Loss: 0.2675293986002604\n",
      "Epoch: 2553 Training Loss: 0.24496884679794312 Test Loss: 0.2674759521484375\n",
      "Epoch: 2554 Training Loss: 0.24495026954015095 Test Loss: 0.26761185709635416\n",
      "Epoch: 2555 Training Loss: 0.24491471338272094 Test Loss: 0.2675008544921875\n",
      "Epoch: 2556 Training Loss: 0.24489399894078573 Test Loss: 0.26738997395833336\n",
      "Epoch: 2557 Training Loss: 0.24486802609761557 Test Loss: 0.2674261474609375\n",
      "Epoch: 2558 Training Loss: 0.24484883562723794 Test Loss: 0.26740690104166664\n",
      "Epoch: 2559 Training Loss: 0.2448327790896098 Test Loss: 0.2673350830078125\n",
      "Epoch: 2560 Training Loss: 0.2448042689959208 Test Loss: 0.2674100748697917\n",
      "Epoch: 2561 Training Loss: 0.24477866554260255 Test Loss: 0.2673355712890625\n",
      "Epoch: 2562 Training Loss: 0.24475339492162068 Test Loss: 0.26734757486979166\n",
      "Epoch: 2563 Training Loss: 0.2447238082885742 Test Loss: 0.26734891764322916\n",
      "Epoch: 2564 Training Loss: 0.24470931180318198 Test Loss: 0.2673523152669271\n",
      "Epoch: 2565 Training Loss: 0.24469737497965494 Test Loss: 0.2673636067708333\n",
      "Epoch: 2566 Training Loss: 0.2446661337216695 Test Loss: 0.2673001302083333\n",
      "Epoch: 2567 Training Loss: 0.2446443149248759 Test Loss: 0.2672006429036458\n",
      "Epoch: 2568 Training Loss: 0.24462277698516846 Test Loss: 0.2672198486328125\n",
      "Epoch: 2569 Training Loss: 0.24460300397872925 Test Loss: 0.2672674153645833\n",
      "Epoch: 2570 Training Loss: 0.24456932417551677 Test Loss: 0.2672954915364583\n",
      "Epoch: 2571 Training Loss: 0.24455133787790934 Test Loss: 0.2671270141601563\n",
      "Epoch: 2572 Training Loss: 0.24452325089772542 Test Loss: 0.26728057861328125\n",
      "Epoch: 2573 Training Loss: 0.24450169595082602 Test Loss: 0.26725160725911457\n",
      "Epoch: 2574 Training Loss: 0.2444795298576355 Test Loss: 0.26715321858723956\n",
      "Epoch: 2575 Training Loss: 0.24444747225443522 Test Loss: 0.2672499593098958\n",
      "Epoch: 2576 Training Loss: 0.24443190081914265 Test Loss: 0.2670016072591146\n",
      "Epoch: 2577 Training Loss: 0.24440930716196696 Test Loss: 0.2670572713216146\n",
      "Epoch: 2578 Training Loss: 0.244387748559316 Test Loss: 0.26712540690104164\n",
      "Epoch: 2579 Training Loss: 0.24436193052927652 Test Loss: 0.2669768269856771\n",
      "Epoch: 2580 Training Loss: 0.24434689966837564 Test Loss: 0.2670422973632812\n",
      "Epoch: 2581 Training Loss: 0.24432899792989096 Test Loss: 0.26701407877604166\n",
      "Epoch: 2582 Training Loss: 0.24430054044723512 Test Loss: 0.2670175170898437\n",
      "Epoch: 2583 Training Loss: 0.2442835825284322 Test Loss: 0.26698553466796876\n",
      "Epoch: 2584 Training Loss: 0.2442577166557312 Test Loss: 0.26695041910807293\n",
      "Epoch: 2585 Training Loss: 0.24422678168614706 Test Loss: 0.26695066324869793\n",
      "Epoch: 2586 Training Loss: 0.24421408621470134 Test Loss: 0.26695994059244793\n",
      "Epoch: 2587 Training Loss: 0.24418473513921102 Test Loss: 0.2669806315104167\n",
      "Epoch: 2588 Training Loss: 0.24416005277633668 Test Loss: 0.2669930419921875\n",
      "Epoch: 2589 Training Loss: 0.24414327065149943 Test Loss: 0.2669078572591146\n",
      "Epoch: 2590 Training Loss: 0.24411562363306682 Test Loss: 0.26694970703125\n",
      "Epoch: 2591 Training Loss: 0.24409837420781452 Test Loss: 0.2669380900065104\n",
      "Epoch: 2592 Training Loss: 0.2440794637997945 Test Loss: 0.26687721761067706\n",
      "Epoch: 2593 Training Loss: 0.24406595420837401 Test Loss: 0.266910400390625\n",
      "Epoch: 2594 Training Loss: 0.24403708505630492 Test Loss: 0.2668668212890625\n",
      "Epoch: 2595 Training Loss: 0.24402430327733357 Test Loss: 0.26693017578125\n",
      "Epoch: 2596 Training Loss: 0.24400435256958009 Test Loss: 0.2668465983072917\n",
      "Epoch: 2597 Training Loss: 0.24397741651535035 Test Loss: 0.26677132161458333\n",
      "Epoch: 2598 Training Loss: 0.24394489669799804 Test Loss: 0.266781005859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2599 Training Loss: 0.24392950534820557 Test Loss: 0.266739501953125\n",
      "Epoch: 2600 Training Loss: 0.2439125657081604 Test Loss: 0.2667315673828125\n",
      "Epoch: 2601 Training Loss: 0.24388107999165853 Test Loss: 0.2666774088541667\n",
      "Epoch: 2602 Training Loss: 0.24385754998524983 Test Loss: 0.266734130859375\n",
      "Epoch: 2603 Training Loss: 0.2438379208246867 Test Loss: 0.2667648111979167\n",
      "Epoch: 2604 Training Loss: 0.24381179745992024 Test Loss: 0.2667395222981771\n",
      "Epoch: 2605 Training Loss: 0.24379373009999594 Test Loss: 0.2667176920572917\n",
      "Epoch: 2606 Training Loss: 0.243761146068573 Test Loss: 0.2666601969401042\n",
      "Epoch: 2607 Training Loss: 0.24373413769404093 Test Loss: 0.26669287109375\n",
      "Epoch: 2608 Training Loss: 0.24371899557113647 Test Loss: 0.26662516276041665\n",
      "Epoch: 2609 Training Loss: 0.24369175577163696 Test Loss: 0.2667541910807292\n",
      "Epoch: 2610 Training Loss: 0.2436777982711792 Test Loss: 0.2666083984375\n",
      "Epoch: 2611 Training Loss: 0.24364861234029134 Test Loss: 0.2666104125976563\n",
      "Epoch: 2612 Training Loss: 0.24363015174865724 Test Loss: 0.26654508463541665\n",
      "Epoch: 2613 Training Loss: 0.24359799512227376 Test Loss: 0.26649660237630207\n",
      "Epoch: 2614 Training Loss: 0.24358184957504272 Test Loss: 0.26662245686848957\n",
      "Epoch: 2615 Training Loss: 0.2435597227414449 Test Loss: 0.26662381998697915\n",
      "Epoch: 2616 Training Loss: 0.24353558270136516 Test Loss: 0.2664401448567708\n",
      "Epoch: 2617 Training Loss: 0.2435080173810323 Test Loss: 0.2664647013346354\n",
      "Epoch: 2618 Training Loss: 0.2434755695660909 Test Loss: 0.266515380859375\n",
      "Epoch: 2619 Training Loss: 0.24345008977254232 Test Loss: 0.26644537353515624\n",
      "Epoch: 2620 Training Loss: 0.24343601767222087 Test Loss: 0.2664241739908854\n",
      "Epoch: 2621 Training Loss: 0.2434046343167623 Test Loss: 0.2664007161458333\n",
      "Epoch: 2622 Training Loss: 0.2433821268081665 Test Loss: 0.26641817220052083\n",
      "Epoch: 2623 Training Loss: 0.24335525433222452 Test Loss: 0.26643058268229164\n",
      "Epoch: 2624 Training Loss: 0.24333707427978515 Test Loss: 0.2663916829427083\n",
      "Epoch: 2625 Training Loss: 0.24331274127960206 Test Loss: 0.2663016357421875\n",
      "Epoch: 2626 Training Loss: 0.24327878713607787 Test Loss: 0.26632991536458334\n",
      "Epoch: 2627 Training Loss: 0.24325254535675048 Test Loss: 0.2662954915364583\n",
      "Epoch: 2628 Training Loss: 0.24322890535990396 Test Loss: 0.26623297119140626\n",
      "Epoch: 2629 Training Loss: 0.24321724446614584 Test Loss: 0.26636346435546876\n",
      "Epoch: 2630 Training Loss: 0.24319681024551393 Test Loss: 0.2662470296223958\n",
      "Epoch: 2631 Training Loss: 0.24316459846496583 Test Loss: 0.2662463582356771\n",
      "Epoch: 2632 Training Loss: 0.24314699776967366 Test Loss: 0.2662475382486979\n",
      "Epoch: 2633 Training Loss: 0.24311339950561522 Test Loss: 0.2661853841145833\n",
      "Epoch: 2634 Training Loss: 0.24309303092956544 Test Loss: 0.26621885172526044\n",
      "Epoch: 2635 Training Loss: 0.24306947708129883 Test Loss: 0.26618697102864586\n",
      "Epoch: 2636 Training Loss: 0.24304060204823813 Test Loss: 0.2661802978515625\n",
      "Epoch: 2637 Training Loss: 0.24301433595021565 Test Loss: 0.26611962890625\n",
      "Epoch: 2638 Training Loss: 0.24299031035105387 Test Loss: 0.26619655354817706\n",
      "Epoch: 2639 Training Loss: 0.24296978410085043 Test Loss: 0.2660254720052083\n",
      "Epoch: 2640 Training Loss: 0.24294491243362426 Test Loss: 0.2660862630208333\n",
      "Epoch: 2641 Training Loss: 0.2429235340754191 Test Loss: 0.26612247721354165\n",
      "Epoch: 2642 Training Loss: 0.2428990429242452 Test Loss: 0.26609720865885417\n",
      "Epoch: 2643 Training Loss: 0.24287793604532878 Test Loss: 0.266141357421875\n",
      "Epoch: 2644 Training Loss: 0.24284788592656453 Test Loss: 0.2659974365234375\n",
      "Epoch: 2645 Training Loss: 0.24282012923558552 Test Loss: 0.26600569661458334\n",
      "Epoch: 2646 Training Loss: 0.2428057770729065 Test Loss: 0.2660616455078125\n",
      "Epoch: 2647 Training Loss: 0.24277394278844197 Test Loss: 0.2659727986653646\n",
      "Epoch: 2648 Training Loss: 0.24275202528635662 Test Loss: 0.2659698486328125\n",
      "Epoch: 2649 Training Loss: 0.24272071679433188 Test Loss: 0.26586971028645834\n",
      "Epoch: 2650 Training Loss: 0.24269429937998455 Test Loss: 0.26606734212239586\n",
      "Epoch: 2651 Training Loss: 0.24267225122451783 Test Loss: 0.26585310872395834\n",
      "Epoch: 2652 Training Loss: 0.24264362637201944 Test Loss: 0.26583186848958335\n",
      "Epoch: 2653 Training Loss: 0.24262546666463217 Test Loss: 0.2657855224609375\n",
      "Epoch: 2654 Training Loss: 0.2426092004776001 Test Loss: 0.2659829915364583\n",
      "Epoch: 2655 Training Loss: 0.24257458798090617 Test Loss: 0.26585272216796874\n",
      "Epoch: 2656 Training Loss: 0.2425480734507243 Test Loss: 0.26584006754557293\n",
      "Epoch: 2657 Training Loss: 0.24252596012751262 Test Loss: 0.2658384195963542\n",
      "Epoch: 2658 Training Loss: 0.2425020990371704 Test Loss: 0.2658851318359375\n",
      "Epoch: 2659 Training Loss: 0.24248094558715821 Test Loss: 0.26593353271484377\n",
      "Epoch: 2660 Training Loss: 0.2424508171081543 Test Loss: 0.26577638753255206\n",
      "Epoch: 2661 Training Loss: 0.24242597262064616 Test Loss: 0.26581048583984374\n",
      "Epoch: 2662 Training Loss: 0.24239607095718385 Test Loss: 0.26574452718098956\n",
      "Epoch: 2663 Training Loss: 0.242367293993632 Test Loss: 0.265733154296875\n",
      "Epoch: 2664 Training Loss: 0.24235536209742228 Test Loss: 0.2657381998697917\n",
      "Epoch: 2665 Training Loss: 0.24231439797083537 Test Loss: 0.26588543701171874\n",
      "Epoch: 2666 Training Loss: 0.24230068349838257 Test Loss: 0.2657068277994792\n",
      "Epoch: 2667 Training Loss: 0.2422762869199117 Test Loss: 0.26564388020833335\n",
      "Epoch: 2668 Training Loss: 0.24225112692515055 Test Loss: 0.26572066243489584\n",
      "Epoch: 2669 Training Loss: 0.24223390245437623 Test Loss: 0.2656794230143229\n",
      "Epoch: 2670 Training Loss: 0.24221221399307252 Test Loss: 0.26557733154296875\n",
      "Epoch: 2671 Training Loss: 0.24219209575653075 Test Loss: 0.2655943603515625\n",
      "Epoch: 2672 Training Loss: 0.24216074752807618 Test Loss: 0.26572664388020834\n",
      "Epoch: 2673 Training Loss: 0.24214318879445393 Test Loss: 0.26553664143880207\n",
      "Epoch: 2674 Training Loss: 0.2421129118601481 Test Loss: 0.26559855143229166\n",
      "Epoch: 2675 Training Loss: 0.24209326203664144 Test Loss: 0.2656221923828125\n",
      "Epoch: 2676 Training Loss: 0.24206527169545491 Test Loss: 0.26546903483072914\n",
      "Epoch: 2677 Training Loss: 0.2420404829978943 Test Loss: 0.2654553833007812\n",
      "Epoch: 2678 Training Loss: 0.2420138177871704 Test Loss: 0.26552384440104165\n",
      "Epoch: 2679 Training Loss: 0.24199632422129314 Test Loss: 0.26540445963541665\n",
      "Epoch: 2680 Training Loss: 0.24195983378092448 Test Loss: 0.2654715169270833\n",
      "Epoch: 2681 Training Loss: 0.2419398226737976 Test Loss: 0.26543601481119794\n",
      "Epoch: 2682 Training Loss: 0.2419251790046692 Test Loss: 0.26533740234375\n",
      "Epoch: 2683 Training Loss: 0.24189581473668417 Test Loss: 0.26544637044270836\n",
      "Epoch: 2684 Training Loss: 0.24186602926254272 Test Loss: 0.2654669392903646\n",
      "Epoch: 2685 Training Loss: 0.24184523407618205 Test Loss: 0.2653368123372396\n",
      "Epoch: 2686 Training Loss: 0.241807141939799 Test Loss: 0.26536761474609377\n",
      "Epoch: 2687 Training Loss: 0.24179273223876954 Test Loss: 0.26539597574869794\n",
      "Epoch: 2688 Training Loss: 0.2417665408452352 Test Loss: 0.26534163411458334\n",
      "Epoch: 2689 Training Loss: 0.24173787625630697 Test Loss: 0.26533601888020836\n",
      "Epoch: 2690 Training Loss: 0.24172203493118286 Test Loss: 0.26530267333984375\n",
      "Epoch: 2691 Training Loss: 0.24169324032465617 Test Loss: 0.26525535074869794\n",
      "Epoch: 2692 Training Loss: 0.24166529226303102 Test Loss: 0.26527783203125\n",
      "Epoch: 2693 Training Loss: 0.24163841056823732 Test Loss: 0.26540437825520835\n",
      "Epoch: 2694 Training Loss: 0.24160633897781372 Test Loss: 0.26535465494791666\n",
      "Epoch: 2695 Training Loss: 0.2415871793429057 Test Loss: 0.2652093505859375\n",
      "Epoch: 2696 Training Loss: 0.24157269303003948 Test Loss: 0.26524849446614585\n",
      "Epoch: 2697 Training Loss: 0.24154392798741658 Test Loss: 0.26524300130208334\n",
      "Epoch: 2698 Training Loss: 0.2415209568341573 Test Loss: 0.26519651285807294\n",
      "Epoch: 2699 Training Loss: 0.24149361975987751 Test Loss: 0.26527193196614585\n",
      "Epoch: 2700 Training Loss: 0.24147465006510416 Test Loss: 0.265346923828125\n",
      "Epoch: 2701 Training Loss: 0.24144762372970582 Test Loss: 0.265165771484375\n",
      "Epoch: 2702 Training Loss: 0.24141913493474323 Test Loss: 0.265191162109375\n",
      "Epoch: 2703 Training Loss: 0.24139135408401488 Test Loss: 0.26519561767578126\n",
      "Epoch: 2704 Training Loss: 0.24137121311823528 Test Loss: 0.2651497802734375\n",
      "Epoch: 2705 Training Loss: 0.24135375293095906 Test Loss: 0.2651873779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2706 Training Loss: 0.24132790581385294 Test Loss: 0.2650917765299479\n",
      "Epoch: 2707 Training Loss: 0.24129922517140706 Test Loss: 0.2651417846679687\n",
      "Epoch: 2708 Training Loss: 0.2412878435452779 Test Loss: 0.2650567626953125\n",
      "Epoch: 2709 Training Loss: 0.24125138060251872 Test Loss: 0.2650547078450521\n",
      "Epoch: 2710 Training Loss: 0.24122475926081338 Test Loss: 0.2649613037109375\n",
      "Epoch: 2711 Training Loss: 0.24120813051859538 Test Loss: 0.265081298828125\n",
      "Epoch: 2712 Training Loss: 0.24118332560857136 Test Loss: 0.2650646158854167\n",
      "Epoch: 2713 Training Loss: 0.24115429846445718 Test Loss: 0.26501582845052085\n",
      "Epoch: 2714 Training Loss: 0.24113892141977947 Test Loss: 0.2649434407552083\n",
      "Epoch: 2715 Training Loss: 0.2411133550008138 Test Loss: 0.2649923502604167\n",
      "Epoch: 2716 Training Loss: 0.24108408641815185 Test Loss: 0.2649834798177083\n",
      "Epoch: 2717 Training Loss: 0.24105826377868653 Test Loss: 0.2649932047526042\n",
      "Epoch: 2718 Training Loss: 0.2410452208518982 Test Loss: 0.2649521484375\n",
      "Epoch: 2719 Training Loss: 0.2410213843981425 Test Loss: 0.2649920654296875\n",
      "Epoch: 2720 Training Loss: 0.24099395895004272 Test Loss: 0.26502054850260415\n",
      "Epoch: 2721 Training Loss: 0.24096942408879599 Test Loss: 0.26496085611979164\n",
      "Epoch: 2722 Training Loss: 0.2409458869298299 Test Loss: 0.26503116861979165\n",
      "Epoch: 2723 Training Loss: 0.24092613554000855 Test Loss: 0.2648815104166667\n",
      "Epoch: 2724 Training Loss: 0.24089800993601482 Test Loss: 0.2648592529296875\n",
      "Epoch: 2725 Training Loss: 0.24088247108459473 Test Loss: 0.26486704508463543\n",
      "Epoch: 2726 Training Loss: 0.2408562569618225 Test Loss: 0.26487042236328123\n",
      "Epoch: 2727 Training Loss: 0.24084262323379516 Test Loss: 0.26480021158854167\n",
      "Epoch: 2728 Training Loss: 0.24081679407755535 Test Loss: 0.2648732706705729\n",
      "Epoch: 2729 Training Loss: 0.2407958238919576 Test Loss: 0.2647564697265625\n",
      "Epoch: 2730 Training Loss: 0.240782550017039 Test Loss: 0.26474296061197916\n",
      "Epoch: 2731 Training Loss: 0.240749232451121 Test Loss: 0.26474114990234376\n",
      "Epoch: 2732 Training Loss: 0.24072990481058756 Test Loss: 0.26463702392578126\n",
      "Epoch: 2733 Training Loss: 0.2407149869600932 Test Loss: 0.2647915445963542\n",
      "Epoch: 2734 Training Loss: 0.24068435033162436 Test Loss: 0.2647466227213542\n",
      "Epoch: 2735 Training Loss: 0.240665224870046 Test Loss: 0.26465938313802084\n",
      "Epoch: 2736 Training Loss: 0.24064721806844075 Test Loss: 0.2645242919921875\n",
      "Epoch: 2737 Training Loss: 0.24062370761235555 Test Loss: 0.26464518229166667\n",
      "Epoch: 2738 Training Loss: 0.24060092624028523 Test Loss: 0.26460807291666666\n",
      "Epoch: 2739 Training Loss: 0.24058287413914997 Test Loss: 0.26450880940755206\n",
      "Epoch: 2740 Training Loss: 0.24056778303782145 Test Loss: 0.2645224609375\n",
      "Epoch: 2741 Training Loss: 0.24053602981567382 Test Loss: 0.2646939697265625\n",
      "Epoch: 2742 Training Loss: 0.24052079184850056 Test Loss: 0.2644798990885417\n",
      "Epoch: 2743 Training Loss: 0.24049710448582967 Test Loss: 0.2645423583984375\n",
      "Epoch: 2744 Training Loss: 0.2404737033843994 Test Loss: 0.2644494222005208\n",
      "Epoch: 2745 Training Loss: 0.24045707337061564 Test Loss: 0.2644071655273437\n",
      "Epoch: 2746 Training Loss: 0.24043240324656168 Test Loss: 0.26457668050130206\n",
      "Epoch: 2747 Training Loss: 0.24042058642705283 Test Loss: 0.2644064737955729\n",
      "Epoch: 2748 Training Loss: 0.24039808591206868 Test Loss: 0.264513671875\n",
      "Epoch: 2749 Training Loss: 0.24037065537770588 Test Loss: 0.2643251342773438\n",
      "Epoch: 2750 Training Loss: 0.24034857670466106 Test Loss: 0.2644872233072917\n",
      "Epoch: 2751 Training Loss: 0.24034643443425496 Test Loss: 0.26445992024739584\n",
      "Epoch: 2752 Training Loss: 0.2403149938583374 Test Loss: 0.2643554077148437\n",
      "Epoch: 2753 Training Loss: 0.24031485319137574 Test Loss: 0.2644489339192708\n",
      "Epoch: 2754 Training Loss: 0.24028100109100342 Test Loss: 0.2644779052734375\n",
      "Epoch: 2755 Training Loss: 0.24026400073369344 Test Loss: 0.26443609619140623\n",
      "Epoch: 2756 Training Loss: 0.2402477339108785 Test Loss: 0.26436767578125\n",
      "Epoch: 2757 Training Loss: 0.24023009808858237 Test Loss: 0.2642802530924479\n",
      "Epoch: 2758 Training Loss: 0.24019949706395466 Test Loss: 0.2643298136393229\n",
      "Epoch: 2759 Training Loss: 0.24017448091506957 Test Loss: 0.2643540242513021\n",
      "Epoch: 2760 Training Loss: 0.2401597975095113 Test Loss: 0.2642847086588542\n",
      "Epoch: 2761 Training Loss: 0.24014254379272462 Test Loss: 0.2642081705729167\n",
      "Epoch: 2762 Training Loss: 0.24010613091786703 Test Loss: 0.2642371826171875\n",
      "Epoch: 2763 Training Loss: 0.2400955605506897 Test Loss: 0.2642388916015625\n",
      "Epoch: 2764 Training Loss: 0.24007038990656535 Test Loss: 0.2642656656901042\n",
      "Epoch: 2765 Training Loss: 0.24005033270517986 Test Loss: 0.264138916015625\n",
      "Epoch: 2766 Training Loss: 0.24002502059936523 Test Loss: 0.26428739420572916\n",
      "Epoch: 2767 Training Loss: 0.24000815359751385 Test Loss: 0.26423079427083335\n",
      "Epoch: 2768 Training Loss: 0.23998745997746784 Test Loss: 0.2641550699869792\n",
      "Epoch: 2769 Training Loss: 0.23996955347061158 Test Loss: 0.2641175333658854\n",
      "Epoch: 2770 Training Loss: 0.23995227320988974 Test Loss: 0.2641323038736979\n",
      "Epoch: 2771 Training Loss: 0.23994023100535075 Test Loss: 0.26409598795572914\n",
      "Epoch: 2772 Training Loss: 0.23991544818878174 Test Loss: 0.2640849609375\n",
      "Epoch: 2773 Training Loss: 0.2398924872080485 Test Loss: 0.2640405680338542\n",
      "Epoch: 2774 Training Loss: 0.23986049286524455 Test Loss: 0.2641785278320313\n",
      "Epoch: 2775 Training Loss: 0.2398511912027995 Test Loss: 0.26396146647135416\n",
      "Epoch: 2776 Training Loss: 0.23981795676549275 Test Loss: 0.2640641072591146\n",
      "Epoch: 2777 Training Loss: 0.23981115277608236 Test Loss: 0.2640409138997396\n",
      "Epoch: 2778 Training Loss: 0.23978940025965373 Test Loss: 0.26400396728515624\n",
      "Epoch: 2779 Training Loss: 0.23976641623179118 Test Loss: 0.26409322102864585\n",
      "Epoch: 2780 Training Loss: 0.23974172767003377 Test Loss: 0.26404856363932294\n",
      "Epoch: 2781 Training Loss: 0.23972289752960205 Test Loss: 0.2639496053059896\n",
      "Epoch: 2782 Training Loss: 0.23970083077748616 Test Loss: 0.2640228271484375\n",
      "Epoch: 2783 Training Loss: 0.23967159875233968 Test Loss: 0.26393330891927086\n",
      "Epoch: 2784 Training Loss: 0.2396558672587077 Test Loss: 0.2639945475260417\n",
      "Epoch: 2785 Training Loss: 0.23962844292322794 Test Loss: 0.2640062662760417\n",
      "Epoch: 2786 Training Loss: 0.2396171383857727 Test Loss: 0.2638949788411458\n",
      "Epoch: 2787 Training Loss: 0.23958795881271364 Test Loss: 0.2640699462890625\n",
      "Epoch: 2788 Training Loss: 0.23957262341181437 Test Loss: 0.26383544921875\n",
      "Epoch: 2789 Training Loss: 0.2395513931910197 Test Loss: 0.26389003499348956\n",
      "Epoch: 2790 Training Loss: 0.239523033618927 Test Loss: 0.2639005533854167\n",
      "Epoch: 2791 Training Loss: 0.23950235509872436 Test Loss: 0.2638441162109375\n",
      "Epoch: 2792 Training Loss: 0.23948620796203612 Test Loss: 0.2638941650390625\n",
      "Epoch: 2793 Training Loss: 0.23947018925348917 Test Loss: 0.2638339436848958\n",
      "Epoch: 2794 Training Loss: 0.23943894147872924 Test Loss: 0.2638090006510417\n",
      "Epoch: 2795 Training Loss: 0.23942482582728067 Test Loss: 0.26367976888020833\n",
      "Epoch: 2796 Training Loss: 0.2394001874923706 Test Loss: 0.26379093424479166\n",
      "Epoch: 2797 Training Loss: 0.23938570753733318 Test Loss: 0.2638343505859375\n",
      "Epoch: 2798 Training Loss: 0.23936845334370932 Test Loss: 0.26380293782552083\n",
      "Epoch: 2799 Training Loss: 0.23934418185551962 Test Loss: 0.2637499593098958\n",
      "Epoch: 2800 Training Loss: 0.239323317527771 Test Loss: 0.26376485188802085\n",
      "Epoch: 2801 Training Loss: 0.23930493227640787 Test Loss: 0.263710205078125\n",
      "Epoch: 2802 Training Loss: 0.2392883513768514 Test Loss: 0.26369258626302083\n",
      "Epoch: 2803 Training Loss: 0.23926482073465982 Test Loss: 0.26376424153645833\n",
      "Epoch: 2804 Training Loss: 0.2392446044286092 Test Loss: 0.26374409993489584\n",
      "Epoch: 2805 Training Loss: 0.2392169564565023 Test Loss: 0.26362579345703124\n",
      "Epoch: 2806 Training Loss: 0.2392046694755554 Test Loss: 0.26363767496744794\n",
      "Epoch: 2807 Training Loss: 0.23918258428573608 Test Loss: 0.2635864461263021\n",
      "Epoch: 2808 Training Loss: 0.23916296354929606 Test Loss: 0.2636689453125\n",
      "Epoch: 2809 Training Loss: 0.23913076400756836 Test Loss: 0.26361029052734375\n",
      "Epoch: 2810 Training Loss: 0.23911562951405843 Test Loss: 0.26357954915364584\n",
      "Epoch: 2811 Training Loss: 0.23909567244847615 Test Loss: 0.2635780029296875\n",
      "Epoch: 2812 Training Loss: 0.23906333986918132 Test Loss: 0.2635179850260417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2813 Training Loss: 0.23905164384841918 Test Loss: 0.2635257161458333\n",
      "Epoch: 2814 Training Loss: 0.23902312882741292 Test Loss: 0.2635694580078125\n",
      "Epoch: 2815 Training Loss: 0.23901089175542195 Test Loss: 0.26344553629557294\n",
      "Epoch: 2816 Training Loss: 0.23898553085327148 Test Loss: 0.26350665283203123\n",
      "Epoch: 2817 Training Loss: 0.23897088511784872 Test Loss: 0.26337748209635414\n",
      "Epoch: 2818 Training Loss: 0.23894130802154542 Test Loss: 0.2634160359700521\n",
      "Epoch: 2819 Training Loss: 0.2389138175646464 Test Loss: 0.2635637410481771\n",
      "Epoch: 2820 Training Loss: 0.23890676959355672 Test Loss: 0.26345135498046873\n",
      "Epoch: 2821 Training Loss: 0.23888628800710043 Test Loss: 0.26339129638671877\n",
      "Epoch: 2822 Training Loss: 0.23885297123591104 Test Loss: 0.26337766520182293\n",
      "Epoch: 2823 Training Loss: 0.2388349962234497 Test Loss: 0.26338651529947915\n",
      "Epoch: 2824 Training Loss: 0.23882287613550823 Test Loss: 0.263423583984375\n",
      "Epoch: 2825 Training Loss: 0.23879902442296347 Test Loss: 0.26329193115234373\n",
      "Epoch: 2826 Training Loss: 0.23877537870407103 Test Loss: 0.2633799031575521\n",
      "Epoch: 2827 Training Loss: 0.23875465599695841 Test Loss: 0.2633331298828125\n",
      "Epoch: 2828 Training Loss: 0.23872849289576212 Test Loss: 0.2632284138997396\n",
      "Epoch: 2829 Training Loss: 0.2387105410893758 Test Loss: 0.2633068033854167\n",
      "Epoch: 2830 Training Loss: 0.23868299770355225 Test Loss: 0.2632651774088542\n",
      "Epoch: 2831 Training Loss: 0.2386639323234558 Test Loss: 0.26316251627604165\n",
      "Epoch: 2832 Training Loss: 0.2386453496615092 Test Loss: 0.26326468912760415\n",
      "Epoch: 2833 Training Loss: 0.23862458006540935 Test Loss: 0.26338822428385417\n",
      "Epoch: 2834 Training Loss: 0.23859839089711507 Test Loss: 0.26322861735026043\n",
      "Epoch: 2835 Training Loss: 0.23859039576848348 Test Loss: 0.2631499837239583\n",
      "Epoch: 2836 Training Loss: 0.23856474129358926 Test Loss: 0.2632665812174479\n",
      "Epoch: 2837 Training Loss: 0.23854675070444742 Test Loss: 0.26330202229817706\n",
      "Epoch: 2838 Training Loss: 0.2385272914568583 Test Loss: 0.26330845133463543\n",
      "Epoch: 2839 Training Loss: 0.23850779962539673 Test Loss: 0.26317889404296874\n",
      "Epoch: 2840 Training Loss: 0.238494221051534 Test Loss: 0.2632646484375\n",
      "Epoch: 2841 Training Loss: 0.23846625995635987 Test Loss: 0.26320186360677084\n",
      "Epoch: 2842 Training Loss: 0.2384551345507304 Test Loss: 0.26317439778645835\n",
      "Epoch: 2843 Training Loss: 0.2384341713587443 Test Loss: 0.26310603841145835\n",
      "Epoch: 2844 Training Loss: 0.23839958890279134 Test Loss: 0.26315047200520836\n",
      "Epoch: 2845 Training Loss: 0.2383787407875061 Test Loss: 0.26315283203125\n",
      "Epoch: 2846 Training Loss: 0.23837248786290485 Test Loss: 0.2630500284830729\n",
      "Epoch: 2847 Training Loss: 0.23834576368331908 Test Loss: 0.26308658854166667\n",
      "Epoch: 2848 Training Loss: 0.23832990058263143 Test Loss: 0.2630740763346354\n",
      "Epoch: 2849 Training Loss: 0.2383149487177531 Test Loss: 0.26309749348958333\n",
      "Epoch: 2850 Training Loss: 0.23828846375147503 Test Loss: 0.2631234130859375\n",
      "Epoch: 2851 Training Loss: 0.23826984786987304 Test Loss: 0.2630345865885417\n",
      "Epoch: 2852 Training Loss: 0.23825090328852336 Test Loss: 0.263002197265625\n",
      "Epoch: 2853 Training Loss: 0.23822688722610474 Test Loss: 0.26295731608072914\n",
      "Epoch: 2854 Training Loss: 0.23820463387171428 Test Loss: 0.26311043294270836\n",
      "Epoch: 2855 Training Loss: 0.23818210538228352 Test Loss: 0.26303363037109373\n",
      "Epoch: 2856 Training Loss: 0.2381638929049174 Test Loss: 0.26296881103515624\n",
      "Epoch: 2857 Training Loss: 0.23813376061121622 Test Loss: 0.26296940104166666\n",
      "Epoch: 2858 Training Loss: 0.23812512715657552 Test Loss: 0.26300240071614583\n",
      "Epoch: 2859 Training Loss: 0.23809633000691732 Test Loss: 0.2628065592447917\n",
      "Epoch: 2860 Training Loss: 0.23808292325337727 Test Loss: 0.2628989664713542\n",
      "Epoch: 2861 Training Loss: 0.23806810585657756 Test Loss: 0.262854248046875\n",
      "Epoch: 2862 Training Loss: 0.23804199934005738 Test Loss: 0.2628660074869792\n",
      "Epoch: 2863 Training Loss: 0.23801980829238892 Test Loss: 0.2628408406575521\n",
      "Epoch: 2864 Training Loss: 0.23799372879664102 Test Loss: 0.2629310709635417\n",
      "Epoch: 2865 Training Loss: 0.23797487036387124 Test Loss: 0.26282212320963544\n",
      "Epoch: 2866 Training Loss: 0.23794744618733724 Test Loss: 0.262745849609375\n",
      "Epoch: 2867 Training Loss: 0.23793096590042115 Test Loss: 0.2629091389973958\n",
      "Epoch: 2868 Training Loss: 0.23791126696268716 Test Loss: 0.26280159505208334\n",
      "Epoch: 2869 Training Loss: 0.2378922135035197 Test Loss: 0.2628204345703125\n",
      "Epoch: 2870 Training Loss: 0.23787372938791912 Test Loss: 0.2628555908203125\n",
      "Epoch: 2871 Training Loss: 0.23784717162450156 Test Loss: 0.26284645589192707\n",
      "Epoch: 2872 Training Loss: 0.23783098602294922 Test Loss: 0.2627798055013021\n",
      "Epoch: 2873 Training Loss: 0.2378122016588847 Test Loss: 0.2629700113932292\n",
      "Epoch: 2874 Training Loss: 0.23779627354939778 Test Loss: 0.26279429117838543\n",
      "Epoch: 2875 Training Loss: 0.23776289542516071 Test Loss: 0.2627888590494792\n",
      "Epoch: 2876 Training Loss: 0.23775669034322103 Test Loss: 0.2627366943359375\n",
      "Epoch: 2877 Training Loss: 0.2377362569173177 Test Loss: 0.2628072102864583\n",
      "Epoch: 2878 Training Loss: 0.2377124915122986 Test Loss: 0.2626942545572917\n",
      "Epoch: 2879 Training Loss: 0.23769146903355917 Test Loss: 0.26282478841145834\n",
      "Epoch: 2880 Training Loss: 0.2376730326016744 Test Loss: 0.26288273111979166\n",
      "Epoch: 2881 Training Loss: 0.23765882953008016 Test Loss: 0.2627552083333333\n",
      "Epoch: 2882 Training Loss: 0.2376369620958964 Test Loss: 0.2626796061197917\n",
      "Epoch: 2883 Training Loss: 0.23761661799748737 Test Loss: 0.26274802652994794\n",
      "Epoch: 2884 Training Loss: 0.2375964037577311 Test Loss: 0.2625889485677083\n",
      "Epoch: 2885 Training Loss: 0.23757004340489704 Test Loss: 0.2626974894205729\n",
      "Epoch: 2886 Training Loss: 0.23756188456217447 Test Loss: 0.26281134033203124\n",
      "Epoch: 2887 Training Loss: 0.2375293296178182 Test Loss: 0.262691162109375\n",
      "Epoch: 2888 Training Loss: 0.2375180204709371 Test Loss: 0.2626395670572917\n",
      "Epoch: 2889 Training Loss: 0.23749139706293743 Test Loss: 0.2625763346354167\n",
      "Epoch: 2890 Training Loss: 0.23747269026438395 Test Loss: 0.26270670572916666\n",
      "Epoch: 2891 Training Loss: 0.2374491662979126 Test Loss: 0.26259330240885415\n",
      "Epoch: 2892 Training Loss: 0.23743458493550618 Test Loss: 0.2625673014322917\n",
      "Epoch: 2893 Training Loss: 0.23741609938939412 Test Loss: 0.26272922770182294\n",
      "Epoch: 2894 Training Loss: 0.23738693046569825 Test Loss: 0.26269032796223957\n",
      "Epoch: 2895 Training Loss: 0.23737186829249063 Test Loss: 0.26255326334635415\n",
      "Epoch: 2896 Training Loss: 0.23734665409723918 Test Loss: 0.26252512613932294\n",
      "Epoch: 2897 Training Loss: 0.237330015818278 Test Loss: 0.26249466959635415\n",
      "Epoch: 2898 Training Loss: 0.23729747835795084 Test Loss: 0.2625474039713542\n",
      "Epoch: 2899 Training Loss: 0.2372814178466797 Test Loss: 0.26259041341145833\n",
      "Epoch: 2900 Training Loss: 0.2372741864522298 Test Loss: 0.2624835001627604\n",
      "Epoch: 2901 Training Loss: 0.23724693648020426 Test Loss: 0.2625537312825521\n",
      "Epoch: 2902 Training Loss: 0.23722393115361531 Test Loss: 0.2625407511393229\n",
      "Epoch: 2903 Training Loss: 0.23720358101526895 Test Loss: 0.26242633056640624\n",
      "Epoch: 2904 Training Loss: 0.23717965698242188 Test Loss: 0.262256103515625\n",
      "Epoch: 2905 Training Loss: 0.237170778910319 Test Loss: 0.26238555908203126\n",
      "Epoch: 2906 Training Loss: 0.23714693975448609 Test Loss: 0.2625174560546875\n",
      "Epoch: 2907 Training Loss: 0.23712243207295736 Test Loss: 0.26248384602864583\n",
      "Epoch: 2908 Training Loss: 0.2371023203531901 Test Loss: 0.26246305338541664\n",
      "Epoch: 2909 Training Loss: 0.23707840347290038 Test Loss: 0.26243489583333335\n",
      "Epoch: 2910 Training Loss: 0.23706526279449464 Test Loss: 0.26247125244140623\n",
      "Epoch: 2911 Training Loss: 0.2370336194038391 Test Loss: 0.2624776611328125\n",
      "Epoch: 2912 Training Loss: 0.23702075862884522 Test Loss: 0.26235378011067706\n",
      "Epoch: 2913 Training Loss: 0.23700988690058392 Test Loss: 0.26233915201822916\n",
      "Epoch: 2914 Training Loss: 0.23697041749954223 Test Loss: 0.26242765299479165\n",
      "Epoch: 2915 Training Loss: 0.23696960560480754 Test Loss: 0.2624275309244792\n",
      "Epoch: 2916 Training Loss: 0.23693764336903889 Test Loss: 0.26241328938802083\n",
      "Epoch: 2917 Training Loss: 0.23691730785369874 Test Loss: 0.26236767578125\n",
      "Epoch: 2918 Training Loss: 0.23689970890680948 Test Loss: 0.26236861165364583\n",
      "Epoch: 2919 Training Loss: 0.23688707033793133 Test Loss: 0.26227740478515627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2920 Training Loss: 0.23686249224344888 Test Loss: 0.26239583333333333\n",
      "Epoch: 2921 Training Loss: 0.23685425901412963 Test Loss: 0.26232234700520835\n",
      "Epoch: 2922 Training Loss: 0.23681639591852824 Test Loss: 0.262311767578125\n",
      "Epoch: 2923 Training Loss: 0.23680878289540608 Test Loss: 0.26237235514322915\n",
      "Epoch: 2924 Training Loss: 0.2367865424156189 Test Loss: 0.26224007161458335\n",
      "Epoch: 2925 Training Loss: 0.23675939671198526 Test Loss: 0.26239892578125\n",
      "Epoch: 2926 Training Loss: 0.2367400590578715 Test Loss: 0.26231876627604167\n",
      "Epoch: 2927 Training Loss: 0.23672527233759563 Test Loss: 0.2623560384114583\n",
      "Epoch: 2928 Training Loss: 0.2367081832885742 Test Loss: 0.2622826131184896\n",
      "Epoch: 2929 Training Loss: 0.2366807723045349 Test Loss: 0.26224613444010414\n",
      "Epoch: 2930 Training Loss: 0.23666169834136963 Test Loss: 0.262304931640625\n",
      "Epoch: 2931 Training Loss: 0.2366419684092204 Test Loss: 0.26224454752604165\n",
      "Epoch: 2932 Training Loss: 0.23662126604715983 Test Loss: 0.2622516682942708\n",
      "Epoch: 2933 Training Loss: 0.2365924336115519 Test Loss: 0.262275146484375\n",
      "Epoch: 2934 Training Loss: 0.23657519245147704 Test Loss: 0.26213480631510416\n",
      "Epoch: 2935 Training Loss: 0.23656308094660441 Test Loss: 0.2621566569010417\n",
      "Epoch: 2936 Training Loss: 0.23653312317530314 Test Loss: 0.26219989013671874\n",
      "Epoch: 2937 Training Loss: 0.2365086669921875 Test Loss: 0.2621570027669271\n",
      "Epoch: 2938 Training Loss: 0.23650005133946736 Test Loss: 0.2621196899414063\n",
      "Epoch: 2939 Training Loss: 0.2364822374979655 Test Loss: 0.262133544921875\n",
      "Epoch: 2940 Training Loss: 0.23646261930465698 Test Loss: 0.2620468546549479\n",
      "Epoch: 2941 Training Loss: 0.23642626174290976 Test Loss: 0.2620986531575521\n",
      "Epoch: 2942 Training Loss: 0.23640889199574788 Test Loss: 0.26215771484375\n",
      "Epoch: 2943 Training Loss: 0.23639743121465048 Test Loss: 0.2620431315104167\n",
      "Epoch: 2944 Training Loss: 0.236366996606191 Test Loss: 0.2621014404296875\n",
      "Epoch: 2945 Training Loss: 0.2363519916534424 Test Loss: 0.26205696614583335\n",
      "Epoch: 2946 Training Loss: 0.23633614826202393 Test Loss: 0.2620283406575521\n",
      "Epoch: 2947 Training Loss: 0.23631578477223714 Test Loss: 0.26199574788411456\n",
      "Epoch: 2948 Training Loss: 0.2362907716433207 Test Loss: 0.2619996134440104\n",
      "Epoch: 2949 Training Loss: 0.23626397244135539 Test Loss: 0.2621041463216146\n",
      "Epoch: 2950 Training Loss: 0.2362496821085612 Test Loss: 0.26203558349609374\n",
      "Epoch: 2951 Training Loss: 0.2362223906517029 Test Loss: 0.2618788655598958\n",
      "Epoch: 2952 Training Loss: 0.23620002094904582 Test Loss: 0.2620189208984375\n",
      "Epoch: 2953 Training Loss: 0.23619373687108358 Test Loss: 0.26189711507161456\n",
      "Epoch: 2954 Training Loss: 0.23617102924982708 Test Loss: 0.2618912353515625\n",
      "Epoch: 2955 Training Loss: 0.23614961020151773 Test Loss: 0.26190950520833334\n",
      "Epoch: 2956 Training Loss: 0.2361305791536967 Test Loss: 0.26178672281901044\n",
      "Epoch: 2957 Training Loss: 0.23610030444463095 Test Loss: 0.26192757161458335\n",
      "Epoch: 2958 Training Loss: 0.2360820196469625 Test Loss: 0.26198626708984374\n",
      "Epoch: 2959 Training Loss: 0.23605751371383668 Test Loss: 0.26196809895833334\n",
      "Epoch: 2960 Training Loss: 0.23604469458262126 Test Loss: 0.26188228352864584\n",
      "Epoch: 2961 Training Loss: 0.23601786851882933 Test Loss: 0.26181268310546874\n",
      "Epoch: 2962 Training Loss: 0.23599331633249918 Test Loss: 0.2617766927083333\n",
      "Epoch: 2963 Training Loss: 0.23597562535603842 Test Loss: 0.26200504557291665\n",
      "Epoch: 2964 Training Loss: 0.23595176235834758 Test Loss: 0.2618519490559896\n",
      "Epoch: 2965 Training Loss: 0.23593172152837116 Test Loss: 0.26182012939453125\n",
      "Epoch: 2966 Training Loss: 0.23592188119888305 Test Loss: 0.26186090087890623\n",
      "Epoch: 2967 Training Loss: 0.23588985363642376 Test Loss: 0.2618190714518229\n",
      "Epoch: 2968 Training Loss: 0.23586742083231607 Test Loss: 0.2617955322265625\n",
      "Epoch: 2969 Training Loss: 0.2358508243560791 Test Loss: 0.2616913452148438\n",
      "Epoch: 2970 Training Loss: 0.23582762908935548 Test Loss: 0.26180802408854165\n",
      "Epoch: 2971 Training Loss: 0.23580522966384887 Test Loss: 0.2618080647786458\n",
      "Epoch: 2972 Training Loss: 0.2357821151415507 Test Loss: 0.26171864827473956\n",
      "Epoch: 2973 Training Loss: 0.23575709676742554 Test Loss: 0.26173299153645835\n",
      "Epoch: 2974 Training Loss: 0.23574010388056438 Test Loss: 0.2617121785481771\n",
      "Epoch: 2975 Training Loss: 0.23570976336797078 Test Loss: 0.26164872233072917\n",
      "Epoch: 2976 Training Loss: 0.2356691706975301 Test Loss: 0.2614883015950521\n",
      "Epoch: 2977 Training Loss: 0.2356284475326538 Test Loss: 0.261710693359375\n",
      "Epoch: 2978 Training Loss: 0.2355961620012919 Test Loss: 0.26162797037760416\n",
      "Epoch: 2979 Training Loss: 0.2355686871210734 Test Loss: 0.26146897379557293\n",
      "Epoch: 2980 Training Loss: 0.23553078174591063 Test Loss: 0.26160160319010417\n",
      "Epoch: 2981 Training Loss: 0.2355113385518392 Test Loss: 0.26159195963541665\n",
      "Epoch: 2982 Training Loss: 0.2354907906850179 Test Loss: 0.26166778564453125\n",
      "Epoch: 2983 Training Loss: 0.2354637549718221 Test Loss: 0.2615654703776042\n",
      "Epoch: 2984 Training Loss: 0.23545680999755858 Test Loss: 0.2614219563802083\n",
      "Epoch: 2985 Training Loss: 0.23542629464467366 Test Loss: 0.26161279296875\n",
      "Epoch: 2986 Training Loss: 0.23540861177444458 Test Loss: 0.26139090983072916\n",
      "Epoch: 2987 Training Loss: 0.2353759822845459 Test Loss: 0.26146077473958335\n",
      "Epoch: 2988 Training Loss: 0.23536388540267944 Test Loss: 0.2615835978190104\n",
      "Epoch: 2989 Training Loss: 0.23534606154759724 Test Loss: 0.2614564208984375\n",
      "Epoch: 2990 Training Loss: 0.23532500648498536 Test Loss: 0.2614559326171875\n",
      "Epoch: 2991 Training Loss: 0.23530981842676799 Test Loss: 0.2615721638997396\n",
      "Epoch: 2992 Training Loss: 0.2352843017578125 Test Loss: 0.2614420369466146\n",
      "Epoch: 2993 Training Loss: 0.2352678254445394 Test Loss: 0.26145918782552086\n",
      "Epoch: 2994 Training Loss: 0.2352471130688985 Test Loss: 0.26138425699869794\n",
      "Epoch: 2995 Training Loss: 0.23521663618087768 Test Loss: 0.2614482828776042\n",
      "Epoch: 2996 Training Loss: 0.23520285415649414 Test Loss: 0.26143804931640624\n",
      "Epoch: 2997 Training Loss: 0.23518489297231038 Test Loss: 0.26133846028645835\n",
      "Epoch: 2998 Training Loss: 0.23516380310058593 Test Loss: 0.2614066975911458\n",
      "Epoch: 2999 Training Loss: 0.2351452938715617 Test Loss: 0.26143509928385417\n",
      "Epoch: 3000 Training Loss: 0.23512919855117798 Test Loss: 0.2613335164388021\n",
      "Epoch: 3001 Training Loss: 0.23510506884257 Test Loss: 0.26128778076171877\n",
      "Epoch: 3002 Training Loss: 0.23508464193344117 Test Loss: 0.26138319905598956\n",
      "Epoch: 3003 Training Loss: 0.23507821480433147 Test Loss: 0.26138846842447916\n",
      "Epoch: 3004 Training Loss: 0.235051376024882 Test Loss: 0.2615191650390625\n",
      "Epoch: 3005 Training Loss: 0.23503980684280396 Test Loss: 0.2613001708984375\n",
      "Epoch: 3006 Training Loss: 0.23501590220133464 Test Loss: 0.2612530110677083\n",
      "Epoch: 3007 Training Loss: 0.23499913358688354 Test Loss: 0.26129534912109376\n",
      "Epoch: 3008 Training Loss: 0.23498947111765545 Test Loss: 0.2612554524739583\n",
      "Epoch: 3009 Training Loss: 0.23497277975082398 Test Loss: 0.26136942545572917\n",
      "Epoch: 3010 Training Loss: 0.23495179001490274 Test Loss: 0.2613611653645833\n",
      "Epoch: 3011 Training Loss: 0.23493439626693727 Test Loss: 0.2612616984049479\n",
      "Epoch: 3012 Training Loss: 0.2349263048171997 Test Loss: 0.2612081298828125\n",
      "Epoch: 3013 Training Loss: 0.2349083709716797 Test Loss: 0.261163330078125\n",
      "Epoch: 3014 Training Loss: 0.2348777494430542 Test Loss: 0.26133235677083333\n",
      "Epoch: 3015 Training Loss: 0.2348732854525248 Test Loss: 0.2612537841796875\n",
      "Epoch: 3016 Training Loss: 0.23485695632298786 Test Loss: 0.26123170979817706\n",
      "Epoch: 3017 Training Loss: 0.23483515437444052 Test Loss: 0.26115763346354165\n",
      "Epoch: 3018 Training Loss: 0.23480724016825358 Test Loss: 0.2611689453125\n",
      "Epoch: 3019 Training Loss: 0.2347944253285726 Test Loss: 0.26134529622395836\n",
      "Epoch: 3020 Training Loss: 0.23477527999877928 Test Loss: 0.26123030598958336\n",
      "Epoch: 3021 Training Loss: 0.23475264962514242 Test Loss: 0.26117513020833333\n",
      "Epoch: 3022 Training Loss: 0.2347381836573283 Test Loss: 0.2611369222005208\n",
      "Epoch: 3023 Training Loss: 0.23471621719996136 Test Loss: 0.26111466471354167\n",
      "Epoch: 3024 Training Loss: 0.2347061613400777 Test Loss: 0.26120448811848956\n",
      "Epoch: 3025 Training Loss: 0.23468603579203287 Test Loss: 0.26117154947916665\n",
      "Epoch: 3026 Training Loss: 0.23466423829396565 Test Loss: 0.2612070109049479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3027 Training Loss: 0.23464474391937257 Test Loss: 0.26116141764322914\n",
      "Epoch: 3028 Training Loss: 0.2346310954093933 Test Loss: 0.2610290323893229\n",
      "Epoch: 3029 Training Loss: 0.23460705200831095 Test Loss: 0.26116288248697916\n",
      "Epoch: 3030 Training Loss: 0.2345947659810384 Test Loss: 0.2611307576497396\n",
      "Epoch: 3031 Training Loss: 0.23458921114603679 Test Loss: 0.2610973714192708\n",
      "Epoch: 3032 Training Loss: 0.2345590707461039 Test Loss: 0.2610467732747396\n",
      "Epoch: 3033 Training Loss: 0.2345477509498596 Test Loss: 0.2610944620768229\n",
      "Epoch: 3034 Training Loss: 0.23452761888504028 Test Loss: 0.2610754191080729\n",
      "Epoch: 3035 Training Loss: 0.2345077993075053 Test Loss: 0.26110213216145833\n",
      "Epoch: 3036 Training Loss: 0.23448987118403117 Test Loss: 0.2609576416015625\n",
      "Epoch: 3037 Training Loss: 0.23446292893091839 Test Loss: 0.2610524291992187\n",
      "Epoch: 3038 Training Loss: 0.2344629961649577 Test Loss: 0.26096944173177083\n",
      "Epoch: 3039 Training Loss: 0.2344389966328939 Test Loss: 0.2609900716145833\n",
      "Epoch: 3040 Training Loss: 0.2344214817682902 Test Loss: 0.2610601399739583\n",
      "Epoch: 3041 Training Loss: 0.234397554397583 Test Loss: 0.2609618326822917\n",
      "Epoch: 3042 Training Loss: 0.23438695923487346 Test Loss: 0.26099589029947917\n",
      "Epoch: 3043 Training Loss: 0.23436731894810994 Test Loss: 0.26098876953125\n",
      "Epoch: 3044 Training Loss: 0.23436263672510782 Test Loss: 0.2611128336588542\n",
      "Epoch: 3045 Training Loss: 0.23433922656377157 Test Loss: 0.2609409586588542\n",
      "Epoch: 3046 Training Loss: 0.23431498193740843 Test Loss: 0.2610350138346354\n",
      "Epoch: 3047 Training Loss: 0.2343136297861735 Test Loss: 0.260919921875\n",
      "Epoch: 3048 Training Loss: 0.23429377206166585 Test Loss: 0.2609118245442708\n",
      "Epoch: 3049 Training Loss: 0.23427865680058796 Test Loss: 0.26102398681640626\n",
      "Epoch: 3050 Training Loss: 0.23425263770421345 Test Loss: 0.26094049072265624\n",
      "Epoch: 3051 Training Loss: 0.23423776896794637 Test Loss: 0.26093888346354166\n",
      "Epoch: 3052 Training Loss: 0.23422296301523846 Test Loss: 0.2609078369140625\n",
      "Epoch: 3053 Training Loss: 0.2342107793490092 Test Loss: 0.2608985392252604\n",
      "Epoch: 3054 Training Loss: 0.2341934479077657 Test Loss: 0.2609210611979167\n",
      "Epoch: 3055 Training Loss: 0.23417656644185383 Test Loss: 0.2608215535481771\n",
      "Epoch: 3056 Training Loss: 0.2341592321395874 Test Loss: 0.2608597615559896\n",
      "Epoch: 3057 Training Loss: 0.23414708073933918 Test Loss: 0.26084765625\n",
      "Epoch: 3058 Training Loss: 0.2341388274828593 Test Loss: 0.26078782145182294\n",
      "Epoch: 3059 Training Loss: 0.23411846446990967 Test Loss: 0.2608870442708333\n",
      "Epoch: 3060 Training Loss: 0.2341029502550761 Test Loss: 0.2607773640950521\n",
      "Epoch: 3061 Training Loss: 0.2340879740715027 Test Loss: 0.26082480875651043\n",
      "Epoch: 3062 Training Loss: 0.2340616332689921 Test Loss: 0.2608450724283854\n",
      "Epoch: 3063 Training Loss: 0.23405634784698487 Test Loss: 0.26075093587239584\n",
      "Epoch: 3064 Training Loss: 0.2340428703625997 Test Loss: 0.26082918294270835\n",
      "Epoch: 3065 Training Loss: 0.2340180950164795 Test Loss: 0.2606665852864583\n",
      "Epoch: 3066 Training Loss: 0.23400360679626464 Test Loss: 0.2608612263997396\n",
      "Epoch: 3067 Training Loss: 0.23398565022150675 Test Loss: 0.2607158203125\n",
      "Epoch: 3068 Training Loss: 0.23396662028630574 Test Loss: 0.2606634114583333\n",
      "Epoch: 3069 Training Loss: 0.2339544432957967 Test Loss: 0.26081620279947915\n",
      "Epoch: 3070 Training Loss: 0.23393413750330608 Test Loss: 0.26067203776041664\n",
      "Epoch: 3071 Training Loss: 0.23392534494400025 Test Loss: 0.2608079833984375\n",
      "Epoch: 3072 Training Loss: 0.2339072930018107 Test Loss: 0.26084977213541666\n",
      "Epoch: 3073 Training Loss: 0.2338899450302124 Test Loss: 0.26053597005208334\n",
      "Epoch: 3074 Training Loss: 0.233876349290212 Test Loss: 0.260537353515625\n",
      "Epoch: 3075 Training Loss: 0.23384663168589273 Test Loss: 0.2607176310221354\n",
      "Epoch: 3076 Training Loss: 0.23383793385823567 Test Loss: 0.26050628662109376\n",
      "Epoch: 3077 Training Loss: 0.23383414236704508 Test Loss: 0.2607003580729167\n",
      "Epoch: 3078 Training Loss: 0.23381041447321574 Test Loss: 0.2605853678385417\n",
      "Epoch: 3079 Training Loss: 0.2337978472709656 Test Loss: 0.2605984700520833\n",
      "Epoch: 3080 Training Loss: 0.23377807505925496 Test Loss: 0.2605519612630208\n",
      "Epoch: 3081 Training Loss: 0.23375719451904298 Test Loss: 0.26053462727864585\n",
      "Epoch: 3082 Training Loss: 0.23376073916753135 Test Loss: 0.2605910847981771\n",
      "Epoch: 3083 Training Loss: 0.23373536427815755 Test Loss: 0.2605675252278646\n",
      "Epoch: 3084 Training Loss: 0.23371679576237997 Test Loss: 0.26038509114583336\n",
      "Epoch: 3085 Training Loss: 0.23370339790980021 Test Loss: 0.2604624633789063\n",
      "Epoch: 3086 Training Loss: 0.23368297576904296 Test Loss: 0.2603777669270833\n",
      "Epoch: 3087 Training Loss: 0.23366866461435953 Test Loss: 0.26049485270182293\n",
      "Epoch: 3088 Training Loss: 0.23366380008061727 Test Loss: 0.26047298177083333\n",
      "Epoch: 3089 Training Loss: 0.2336448974609375 Test Loss: 0.26036191813151044\n",
      "Epoch: 3090 Training Loss: 0.2336340365409851 Test Loss: 0.26042024739583336\n",
      "Epoch: 3091 Training Loss: 0.2336023284594218 Test Loss: 0.2604869791666667\n",
      "Epoch: 3092 Training Loss: 0.23359200684229534 Test Loss: 0.2603178914388021\n",
      "Epoch: 3093 Training Loss: 0.233583354473114 Test Loss: 0.26042390950520833\n",
      "Epoch: 3094 Training Loss: 0.23356642150878906 Test Loss: 0.26036971028645833\n",
      "Epoch: 3095 Training Loss: 0.2335472478866577 Test Loss: 0.2603599853515625\n",
      "Epoch: 3096 Training Loss: 0.23353284168243407 Test Loss: 0.2602525634765625\n",
      "Epoch: 3097 Training Loss: 0.233528111298879 Test Loss: 0.2603626505533854\n",
      "Epoch: 3098 Training Loss: 0.23350749333699544 Test Loss: 0.2601817626953125\n",
      "Epoch: 3099 Training Loss: 0.23349263461430866 Test Loss: 0.2602344767252604\n",
      "Epoch: 3100 Training Loss: 0.23346524127324422 Test Loss: 0.26022123209635417\n",
      "Epoch: 3101 Training Loss: 0.2334558211962382 Test Loss: 0.26030021158854166\n",
      "Epoch: 3102 Training Loss: 0.2334423411687215 Test Loss: 0.26024609375\n",
      "Epoch: 3103 Training Loss: 0.23343172184626262 Test Loss: 0.2602317911783854\n",
      "Epoch: 3104 Training Loss: 0.2334067635536194 Test Loss: 0.2602128092447917\n",
      "Epoch: 3105 Training Loss: 0.23338877232869465 Test Loss: 0.260310302734375\n",
      "Epoch: 3106 Training Loss: 0.23338867568969726 Test Loss: 0.2601495768229167\n",
      "Epoch: 3107 Training Loss: 0.23336837339401245 Test Loss: 0.2601126302083333\n",
      "Epoch: 3108 Training Loss: 0.2333554131189982 Test Loss: 0.26013863118489583\n",
      "Epoch: 3109 Training Loss: 0.23333230098088584 Test Loss: 0.26025516764322915\n",
      "Epoch: 3110 Training Loss: 0.23332144117355347 Test Loss: 0.26015254720052083\n",
      "Epoch: 3111 Training Loss: 0.23330951929092408 Test Loss: 0.2600836181640625\n",
      "Epoch: 3112 Training Loss: 0.23328298807144165 Test Loss: 0.2601031290690104\n",
      "Epoch: 3113 Training Loss: 0.233286186059316 Test Loss: 0.260089599609375\n",
      "Epoch: 3114 Training Loss: 0.233266100247701 Test Loss: 0.26006608072916665\n",
      "Epoch: 3115 Training Loss: 0.23325039227803548 Test Loss: 0.26009395345052083\n",
      "Epoch: 3116 Training Loss: 0.23323091554641723 Test Loss: 0.26007533772786456\n",
      "Epoch: 3117 Training Loss: 0.23321144421895346 Test Loss: 0.2601269734700521\n",
      "Epoch: 3118 Training Loss: 0.23320088958740234 Test Loss: 0.2599365437825521\n",
      "Epoch: 3119 Training Loss: 0.23317454242706298 Test Loss: 0.26008260091145835\n",
      "Epoch: 3120 Training Loss: 0.23317113892237346 Test Loss: 0.25999442545572915\n",
      "Epoch: 3121 Training Loss: 0.2331634842554728 Test Loss: 0.25993070475260416\n",
      "Epoch: 3122 Training Loss: 0.23314262549082437 Test Loss: 0.2600321044921875\n",
      "Epoch: 3123 Training Loss: 0.23312046496073405 Test Loss: 0.25992891438802085\n",
      "Epoch: 3124 Training Loss: 0.2331030035018921 Test Loss: 0.25994266764322915\n",
      "Epoch: 3125 Training Loss: 0.2330963775316874 Test Loss: 0.2599297281901042\n",
      "Epoch: 3126 Training Loss: 0.23307696644465128 Test Loss: 0.25998604329427083\n",
      "Epoch: 3127 Training Loss: 0.2330591802597046 Test Loss: 0.25993672688802083\n",
      "Epoch: 3128 Training Loss: 0.23304200649261475 Test Loss: 0.25985201009114584\n",
      "Epoch: 3129 Training Loss: 0.23302716922760008 Test Loss: 0.259796875\n",
      "Epoch: 3130 Training Loss: 0.23302179145812987 Test Loss: 0.2597787068684896\n",
      "Epoch: 3131 Training Loss: 0.23300918181737265 Test Loss: 0.25985986328125\n",
      "Epoch: 3132 Training Loss: 0.23299053001403808 Test Loss: 0.2597488810221354\n",
      "Epoch: 3133 Training Loss: 0.2329709184964498 Test Loss: 0.2598068033854167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3134 Training Loss: 0.2329629947344462 Test Loss: 0.259949462890625\n",
      "Epoch: 3135 Training Loss: 0.23294821627934773 Test Loss: 0.25981679280598957\n",
      "Epoch: 3136 Training Loss: 0.23292825682957968 Test Loss: 0.25985992431640625\n",
      "Epoch: 3137 Training Loss: 0.23290907033284505 Test Loss: 0.2597451171875\n",
      "Epoch: 3138 Training Loss: 0.2328914229075114 Test Loss: 0.2597459513346354\n",
      "Epoch: 3139 Training Loss: 0.23288242880503338 Test Loss: 0.25978875732421874\n",
      "Epoch: 3140 Training Loss: 0.2328599050839742 Test Loss: 0.2598282063802083\n",
      "Epoch: 3141 Training Loss: 0.23285056877136232 Test Loss: 0.2597043660481771\n",
      "Epoch: 3142 Training Loss: 0.23282840649286907 Test Loss: 0.259732421875\n",
      "Epoch: 3143 Training Loss: 0.23282349427541096 Test Loss: 0.25974983723958334\n",
      "Epoch: 3144 Training Loss: 0.23280532375971477 Test Loss: 0.2596692301432292\n",
      "Epoch: 3145 Training Loss: 0.2327860442797343 Test Loss: 0.2597826131184896\n",
      "Epoch: 3146 Training Loss: 0.23277618010838827 Test Loss: 0.2596400146484375\n",
      "Epoch: 3147 Training Loss: 0.2327603562672933 Test Loss: 0.2597191365559896\n",
      "Epoch: 3148 Training Loss: 0.23274364789326984 Test Loss: 0.2597802327473958\n",
      "Epoch: 3149 Training Loss: 0.2327381731669108 Test Loss: 0.25975309244791667\n",
      "Epoch: 3150 Training Loss: 0.23271506134668987 Test Loss: 0.25974249267578126\n",
      "Epoch: 3151 Training Loss: 0.23270489009221396 Test Loss: 0.2597305094401042\n",
      "Epoch: 3152 Training Loss: 0.23268876616160075 Test Loss: 0.2597545166015625\n",
      "Epoch: 3153 Training Loss: 0.23266834131876626 Test Loss: 0.25963321940104167\n",
      "Epoch: 3154 Training Loss: 0.23265565236409505 Test Loss: 0.25971968587239586\n",
      "Epoch: 3155 Training Loss: 0.23264215803146363 Test Loss: 0.2597255859375\n",
      "Epoch: 3156 Training Loss: 0.23262858311335247 Test Loss: 0.2597208251953125\n",
      "Epoch: 3157 Training Loss: 0.23260690593719482 Test Loss: 0.259693359375\n",
      "Epoch: 3158 Training Loss: 0.23260100173950196 Test Loss: 0.2595949300130208\n",
      "Epoch: 3159 Training Loss: 0.2325797748565674 Test Loss: 0.2596561686197917\n",
      "Epoch: 3160 Training Loss: 0.23255465205510456 Test Loss: 0.25962129720052085\n",
      "Epoch: 3161 Training Loss: 0.23254238367080687 Test Loss: 0.25966310628255207\n",
      "Epoch: 3162 Training Loss: 0.23252419217427572 Test Loss: 0.25968192545572916\n",
      "Epoch: 3163 Training Loss: 0.2325094420115153 Test Loss: 0.25964105224609374\n",
      "Epoch: 3164 Training Loss: 0.2324931076367696 Test Loss: 0.2595760498046875\n",
      "Epoch: 3165 Training Loss: 0.23248038975397745 Test Loss: 0.25959639485677083\n",
      "Epoch: 3166 Training Loss: 0.23245619678497315 Test Loss: 0.2596382039388021\n",
      "Epoch: 3167 Training Loss: 0.23244406747817994 Test Loss: 0.25947074381510415\n",
      "Epoch: 3168 Training Loss: 0.23243408727645873 Test Loss: 0.2595015869140625\n",
      "Epoch: 3169 Training Loss: 0.23241069650650023 Test Loss: 0.2594947509765625\n",
      "Epoch: 3170 Training Loss: 0.23240606037775677 Test Loss: 0.2594609578450521\n",
      "Epoch: 3171 Training Loss: 0.23237490606307984 Test Loss: 0.25952091471354166\n",
      "Epoch: 3172 Training Loss: 0.23236610349019368 Test Loss: 0.2595174560546875\n",
      "Epoch: 3173 Training Loss: 0.23235002740224203 Test Loss: 0.2595208740234375\n",
      "Epoch: 3174 Training Loss: 0.23233575773239135 Test Loss: 0.25946498616536456\n",
      "Epoch: 3175 Training Loss: 0.23232062578201293 Test Loss: 0.25930517578125\n",
      "Epoch: 3176 Training Loss: 0.23230626805623372 Test Loss: 0.25943607584635414\n",
      "Epoch: 3177 Training Loss: 0.2322803339958191 Test Loss: 0.2595338134765625\n",
      "Epoch: 3178 Training Loss: 0.23227431599299114 Test Loss: 0.2594071858723958\n",
      "Epoch: 3179 Training Loss: 0.23225966771443685 Test Loss: 0.25951300048828124\n",
      "Epoch: 3180 Training Loss: 0.23223657019933064 Test Loss: 0.2594696044921875\n",
      "Epoch: 3181 Training Loss: 0.23221562623977662 Test Loss: 0.25946720377604165\n",
      "Epoch: 3182 Training Loss: 0.23221647230784098 Test Loss: 0.2594088134765625\n",
      "Epoch: 3183 Training Loss: 0.2321902453104655 Test Loss: 0.2594237060546875\n",
      "Epoch: 3184 Training Loss: 0.23217859093348187 Test Loss: 0.2593876546223958\n",
      "Epoch: 3185 Training Loss: 0.232157373269399 Test Loss: 0.2594250691731771\n",
      "Epoch: 3186 Training Loss: 0.23215012407302857 Test Loss: 0.25930118815104164\n",
      "Epoch: 3187 Training Loss: 0.23212887907028198 Test Loss: 0.2594595947265625\n",
      "Epoch: 3188 Training Loss: 0.23212055571873982 Test Loss: 0.2593890177408854\n",
      "Epoch: 3189 Training Loss: 0.23209942738215128 Test Loss: 0.25941544596354166\n",
      "Epoch: 3190 Training Loss: 0.2320894015630086 Test Loss: 0.259416259765625\n",
      "Epoch: 3191 Training Loss: 0.23207485548655193 Test Loss: 0.25935152180989585\n",
      "Epoch: 3192 Training Loss: 0.2320520413716634 Test Loss: 0.2593934733072917\n",
      "Epoch: 3193 Training Loss: 0.2320594439506531 Test Loss: 0.25941827392578126\n",
      "Epoch: 3194 Training Loss: 0.23202327664693195 Test Loss: 0.2594956461588542\n",
      "Epoch: 3195 Training Loss: 0.23201640367507934 Test Loss: 0.25936588541666666\n",
      "Epoch: 3196 Training Loss: 0.2320087235768636 Test Loss: 0.2595725504557292\n",
      "Epoch: 3197 Training Loss: 0.23198489586512247 Test Loss: 0.259398193359375\n",
      "Epoch: 3198 Training Loss: 0.23198299757639568 Test Loss: 0.2594036458333333\n",
      "Epoch: 3199 Training Loss: 0.23196435197194418 Test Loss: 0.2595580240885417\n",
      "Epoch: 3200 Training Loss: 0.23194991715749105 Test Loss: 0.259492919921875\n",
      "Epoch: 3201 Training Loss: 0.23193823846181233 Test Loss: 0.2595406494140625\n",
      "Epoch: 3202 Training Loss: 0.23191880973180135 Test Loss: 0.25944816080729166\n",
      "Epoch: 3203 Training Loss: 0.23190158240000408 Test Loss: 0.25952718098958333\n",
      "Epoch: 3204 Training Loss: 0.2318942723274231 Test Loss: 0.2595286865234375\n",
      "Epoch: 3205 Training Loss: 0.23188673909505209 Test Loss: 0.2595147705078125\n",
      "Epoch: 3206 Training Loss: 0.23186656093597413 Test Loss: 0.25944576009114584\n",
      "Epoch: 3207 Training Loss: 0.2318493798573812 Test Loss: 0.25953987630208336\n",
      "Epoch: 3208 Training Loss: 0.23184277884165447 Test Loss: 0.2594627685546875\n",
      "Epoch: 3209 Training Loss: 0.2318334043820699 Test Loss: 0.25958638509114584\n",
      "Epoch: 3210 Training Loss: 0.23183117961883545 Test Loss: 0.2596479899088542\n",
      "Epoch: 3211 Training Loss: 0.23180287679036457 Test Loss: 0.25947652180989583\n",
      "Epoch: 3212 Training Loss: 0.2317924222946167 Test Loss: 0.25958060709635417\n",
      "Epoch: 3213 Training Loss: 0.23177091932296753 Test Loss: 0.25957200113932294\n",
      "Epoch: 3214 Training Loss: 0.23176478576660156 Test Loss: 0.25954984537760417\n",
      "Epoch: 3215 Training Loss: 0.23175758441289265 Test Loss: 0.259569091796875\n",
      "Epoch: 3216 Training Loss: 0.23174140469233195 Test Loss: 0.25973368326822915\n",
      "Epoch: 3217 Training Loss: 0.23173382027943928 Test Loss: 0.25951788330078124\n",
      "Epoch: 3218 Training Loss: 0.2317312150001526 Test Loss: 0.25973388671875\n",
      "Epoch: 3219 Training Loss: 0.2317182977994283 Test Loss: 0.2596304931640625\n",
      "Epoch: 3220 Training Loss: 0.23170601749420167 Test Loss: 0.25966206868489583\n",
      "Epoch: 3221 Training Loss: 0.2316968134244283 Test Loss: 0.2597230224609375\n",
      "Epoch: 3222 Training Loss: 0.23168724711736044 Test Loss: 0.25969380696614586\n",
      "Epoch: 3223 Training Loss: 0.2316811769803365 Test Loss: 0.2595579630533854\n",
      "Epoch: 3224 Training Loss: 0.23168371375401814 Test Loss: 0.259665771484375\n",
      "Epoch: 3225 Training Loss: 0.2316763219833374 Test Loss: 0.2597130126953125\n",
      "Epoch: 3226 Training Loss: 0.23164629872639975 Test Loss: 0.2597331136067708\n",
      "Epoch: 3227 Training Loss: 0.23164882802963258 Test Loss: 0.25976265462239584\n",
      "Epoch: 3228 Training Loss: 0.23163762537638347 Test Loss: 0.2599562581380208\n",
      "Epoch: 3229 Training Loss: 0.23162574259440105 Test Loss: 0.25986545817057294\n",
      "Epoch: 3230 Training Loss: 0.2316368932723999 Test Loss: 0.2597631632486979\n",
      "Epoch: 3231 Training Loss: 0.23162083832422892 Test Loss: 0.2598780517578125\n",
      "Epoch: 3232 Training Loss: 0.2316004678408305 Test Loss: 0.2600622355143229\n",
      "Epoch: 3233 Training Loss: 0.23159744453430176 Test Loss: 0.259963134765625\n",
      "Epoch: 3234 Training Loss: 0.23159365193049114 Test Loss: 0.2601057535807292\n",
      "Epoch: 3235 Training Loss: 0.23158863560358683 Test Loss: 0.2601698608398437\n",
      "Epoch: 3236 Training Loss: 0.23158780797322592 Test Loss: 0.26012027994791664\n",
      "Epoch: 3237 Training Loss: 0.2315829602877299 Test Loss: 0.26019901529947914\n",
      "Epoch: 3238 Training Loss: 0.23157481686274212 Test Loss: 0.2601233927408854\n",
      "Epoch: 3239 Training Loss: 0.2315696055094401 Test Loss: 0.2602848714192708\n",
      "Epoch: 3240 Training Loss: 0.23155730867385865 Test Loss: 0.2604693400065104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3241 Training Loss: 0.23156669187545775 Test Loss: 0.26049503580729166\n",
      "Epoch: 3242 Training Loss: 0.23155259307225545 Test Loss: 0.2603072102864583\n",
      "Epoch: 3243 Training Loss: 0.23155618556340535 Test Loss: 0.26065091959635417\n",
      "Epoch: 3244 Training Loss: 0.23154985348383586 Test Loss: 0.26071728515625\n",
      "Epoch: 3245 Training Loss: 0.23155357170104982 Test Loss: 0.26074735514322916\n",
      "Epoch: 3246 Training Loss: 0.23155973323186238 Test Loss: 0.26062068684895834\n",
      "Epoch: 3247 Training Loss: 0.2315537044207255 Test Loss: 0.2609427897135417\n",
      "Epoch: 3248 Training Loss: 0.23154477580388386 Test Loss: 0.26095790608723957\n",
      "Epoch: 3249 Training Loss: 0.2315478604634603 Test Loss: 0.26093070475260416\n",
      "Epoch: 3250 Training Loss: 0.23154535245895386 Test Loss: 0.26099468994140623\n",
      "Epoch: 3251 Training Loss: 0.23154538138707478 Test Loss: 0.26093292236328125\n",
      "Epoch: 3252 Training Loss: 0.23154312292734783 Test Loss: 0.2610409952799479\n",
      "Epoch: 3253 Training Loss: 0.23154938141504924 Test Loss: 0.261305908203125\n",
      "Epoch: 3254 Training Loss: 0.23155228487650553 Test Loss: 0.2613814900716146\n",
      "Epoch: 3255 Training Loss: 0.2315603108406067 Test Loss: 0.26152587890625\n",
      "Epoch: 3256 Training Loss: 0.2315742123921712 Test Loss: 0.2616090087890625\n",
      "Epoch: 3257 Training Loss: 0.2315883623758952 Test Loss: 0.2615613199869792\n",
      "Epoch: 3258 Training Loss: 0.2316035539309184 Test Loss: 0.26142974853515627\n",
      "Epoch: 3259 Training Loss: 0.23162661250432331 Test Loss: 0.2616862386067708\n",
      "Epoch: 3260 Training Loss: 0.2316527945200602 Test Loss: 0.2614119873046875\n",
      "Epoch: 3261 Training Loss: 0.23168666950861613 Test Loss: 0.2617187703450521\n",
      "Epoch: 3262 Training Loss: 0.23174342346191407 Test Loss: 0.2616444498697917\n",
      "Epoch: 3263 Training Loss: 0.23181644121805828 Test Loss: 0.26171492513020833\n",
      "Epoch: 3264 Training Loss: 0.2318943174680074 Test Loss: 0.2617081705729167\n",
      "Epoch: 3265 Training Loss: 0.2319930953979492 Test Loss: 0.2616508382161458\n",
      "Epoch: 3266 Training Loss: 0.2321165828704834 Test Loss: 0.261395263671875\n",
      "Epoch: 3267 Training Loss: 0.23226091496149698 Test Loss: 0.26119673665364584\n",
      "Epoch: 3268 Training Loss: 0.23238744433720906 Test Loss: 0.26056146240234374\n",
      "Epoch: 3269 Training Loss: 0.232501727104187 Test Loss: 0.25973783365885417\n",
      "Epoch: 3270 Training Loss: 0.23252684084574382 Test Loss: 0.25898046875\n",
      "Epoch: 3271 Training Loss: 0.23244361925125123 Test Loss: 0.25828804524739585\n",
      "Epoch: 3272 Training Loss: 0.23229704666137696 Test Loss: 0.25801778157552085\n",
      "Epoch: 3273 Training Loss: 0.2321368025143941 Test Loss: 0.25776017252604166\n",
      "Epoch: 3274 Training Loss: 0.2320083573659261 Test Loss: 0.25760430908203125\n",
      "Epoch: 3275 Training Loss: 0.23188038857777912 Test Loss: 0.25779345703125\n",
      "Epoch: 3276 Training Loss: 0.2317803807258606 Test Loss: 0.25774759928385416\n",
      "Epoch: 3277 Training Loss: 0.23167114639282227 Test Loss: 0.25772172037760416\n",
      "Epoch: 3278 Training Loss: 0.23160079749425252 Test Loss: 0.257838623046875\n",
      "Epoch: 3279 Training Loss: 0.2315248384475708 Test Loss: 0.2577432454427083\n",
      "Epoch: 3280 Training Loss: 0.23146347204844156 Test Loss: 0.2577844441731771\n",
      "Epoch: 3281 Training Loss: 0.2313848222096761 Test Loss: 0.2580022583007813\n",
      "Epoch: 3282 Training Loss: 0.23132897663116456 Test Loss: 0.2580223795572917\n",
      "Epoch: 3283 Training Loss: 0.23127370405197142 Test Loss: 0.25781903076171875\n",
      "Epoch: 3284 Training Loss: 0.23121290254592897 Test Loss: 0.25811112467447916\n",
      "Epoch: 3285 Training Loss: 0.23115327469507854 Test Loss: 0.25802095540364584\n",
      "Epoch: 3286 Training Loss: 0.23110604015986125 Test Loss: 0.2579975992838542\n",
      "Epoch: 3287 Training Loss: 0.23105696964263917 Test Loss: 0.2580220947265625\n",
      "Epoch: 3288 Training Loss: 0.23100566307703654 Test Loss: 0.2582705281575521\n",
      "Epoch: 3289 Training Loss: 0.2309574327468872 Test Loss: 0.25826837158203125\n",
      "Epoch: 3290 Training Loss: 0.23091783905029298 Test Loss: 0.2582357177734375\n",
      "Epoch: 3291 Training Loss: 0.2308724447886149 Test Loss: 0.25831575520833333\n",
      "Epoch: 3292 Training Loss: 0.23083228619893392 Test Loss: 0.2583744303385417\n",
      "Epoch: 3293 Training Loss: 0.23078667227427166 Test Loss: 0.25847200520833336\n",
      "Epoch: 3294 Training Loss: 0.23073825740814208 Test Loss: 0.25840509033203124\n",
      "Epoch: 3295 Training Loss: 0.2307021371523539 Test Loss: 0.25841831461588544\n",
      "Epoch: 3296 Training Loss: 0.2306538438796997 Test Loss: 0.25847222900390626\n",
      "Epoch: 3297 Training Loss: 0.2306130658785502 Test Loss: 0.25841668701171877\n",
      "Epoch: 3298 Training Loss: 0.23056673399607341 Test Loss: 0.25842138671875\n",
      "Epoch: 3299 Training Loss: 0.23053542772928873 Test Loss: 0.2584569091796875\n",
      "Epoch: 3300 Training Loss: 0.2304986098607381 Test Loss: 0.25846207682291666\n",
      "Epoch: 3301 Training Loss: 0.23046048688888549 Test Loss: 0.25833341471354165\n",
      "Epoch: 3302 Training Loss: 0.23042209974924724 Test Loss: 0.25837253824869794\n",
      "Epoch: 3303 Training Loss: 0.23039392026265462 Test Loss: 0.25828544108072915\n",
      "Epoch: 3304 Training Loss: 0.2303483517964681 Test Loss: 0.258368896484375\n",
      "Epoch: 3305 Training Loss: 0.23032967249552408 Test Loss: 0.2581990559895833\n",
      "Epoch: 3306 Training Loss: 0.23029462019602456 Test Loss: 0.25813741048177086\n",
      "Epoch: 3307 Training Loss: 0.2302681427001953 Test Loss: 0.2582291259765625\n",
      "Epoch: 3308 Training Loss: 0.23024759181340534 Test Loss: 0.25816115315755206\n",
      "Epoch: 3309 Training Loss: 0.2302173539797465 Test Loss: 0.2580001220703125\n",
      "Epoch: 3310 Training Loss: 0.23019069798787434 Test Loss: 0.2579854329427083\n",
      "Epoch: 3311 Training Loss: 0.2301676114400228 Test Loss: 0.2580721435546875\n",
      "Epoch: 3312 Training Loss: 0.2301388716697693 Test Loss: 0.2580289306640625\n",
      "Epoch: 3313 Training Loss: 0.23012729501724244 Test Loss: 0.2579965006510417\n",
      "Epoch: 3314 Training Loss: 0.23010216442743936 Test Loss: 0.25796044921875\n",
      "Epoch: 3315 Training Loss: 0.23007802693049112 Test Loss: 0.257857421875\n",
      "Epoch: 3316 Training Loss: 0.2300574312210083 Test Loss: 0.2578076578776042\n",
      "Epoch: 3317 Training Loss: 0.23003083356221518 Test Loss: 0.2579161580403646\n",
      "Epoch: 3318 Training Loss: 0.23002411460876465 Test Loss: 0.25784537760416665\n",
      "Epoch: 3319 Training Loss: 0.2299983860651652 Test Loss: 0.2577234293619792\n",
      "Epoch: 3320 Training Loss: 0.22997871796290079 Test Loss: 0.25760272216796876\n",
      "Epoch: 3321 Training Loss: 0.22996107498804727 Test Loss: 0.25761295572916665\n",
      "Epoch: 3322 Training Loss: 0.2299455041885376 Test Loss: 0.2575587158203125\n",
      "Epoch: 3323 Training Loss: 0.22991510518391928 Test Loss: 0.25747808837890623\n",
      "Epoch: 3324 Training Loss: 0.22991141033172607 Test Loss: 0.25757686360677085\n",
      "Epoch: 3325 Training Loss: 0.22989207490285238 Test Loss: 0.25756266276041667\n",
      "Epoch: 3326 Training Loss: 0.2298723799387614 Test Loss: 0.25758040364583334\n",
      "Epoch: 3327 Training Loss: 0.22985258277257284 Test Loss: 0.25748398844401044\n",
      "Epoch: 3328 Training Loss: 0.2298330176671346 Test Loss: 0.25748836263020836\n",
      "Epoch: 3329 Training Loss: 0.2298232305844625 Test Loss: 0.25748116048177083\n",
      "Epoch: 3330 Training Loss: 0.22980075661341348 Test Loss: 0.2574670817057292\n",
      "Epoch: 3331 Training Loss: 0.22978712383906047 Test Loss: 0.257369873046875\n",
      "Epoch: 3332 Training Loss: 0.22976620292663574 Test Loss: 0.25743448893229165\n",
      "Epoch: 3333 Training Loss: 0.22975649229685466 Test Loss: 0.2574891560872396\n",
      "Epoch: 3334 Training Loss: 0.22974295616149903 Test Loss: 0.25739664713541666\n",
      "Epoch: 3335 Training Loss: 0.22971854988733928 Test Loss: 0.2574423624674479\n",
      "Epoch: 3336 Training Loss: 0.22971017662684123 Test Loss: 0.257276611328125\n",
      "Epoch: 3337 Training Loss: 0.2296985855102539 Test Loss: 0.25742783610026043\n",
      "Epoch: 3338 Training Loss: 0.22968529876073202 Test Loss: 0.2573447875976563\n",
      "Epoch: 3339 Training Loss: 0.2296685004234314 Test Loss: 0.2573217366536458\n",
      "Epoch: 3340 Training Loss: 0.22964999310175577 Test Loss: 0.2572705078125\n",
      "Epoch: 3341 Training Loss: 0.22964685646692912 Test Loss: 0.2573689371744792\n",
      "Epoch: 3342 Training Loss: 0.22962790616353354 Test Loss: 0.2572596028645833\n",
      "Epoch: 3343 Training Loss: 0.22960921589533487 Test Loss: 0.2572261962890625\n",
      "Epoch: 3344 Training Loss: 0.22959237670898439 Test Loss: 0.25738623046875\n",
      "Epoch: 3345 Training Loss: 0.22957393391927083 Test Loss: 0.257256103515625\n",
      "Epoch: 3346 Training Loss: 0.22956680313746133 Test Loss: 0.25733207194010416\n",
      "Epoch: 3347 Training Loss: 0.22954800287882487 Test Loss: 0.2572349039713542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3348 Training Loss: 0.22953890609741212 Test Loss: 0.2573982747395833\n",
      "Epoch: 3349 Training Loss: 0.22952515443166097 Test Loss: 0.2573131917317708\n",
      "Epoch: 3350 Training Loss: 0.22951032908757527 Test Loss: 0.2573328450520833\n",
      "Epoch: 3351 Training Loss: 0.2295057463645935 Test Loss: 0.2573020833333333\n",
      "Epoch: 3352 Training Loss: 0.22948241249720255 Test Loss: 0.2571793212890625\n",
      "Epoch: 3353 Training Loss: 0.22946793492635092 Test Loss: 0.2572564900716146\n",
      "Epoch: 3354 Training Loss: 0.2294580545425415 Test Loss: 0.257133544921875\n",
      "Epoch: 3355 Training Loss: 0.22944278717041017 Test Loss: 0.257262451171875\n",
      "Epoch: 3356 Training Loss: 0.229439528465271 Test Loss: 0.2571298624674479\n",
      "Epoch: 3357 Training Loss: 0.2294153553644816 Test Loss: 0.25723258463541665\n",
      "Epoch: 3358 Training Loss: 0.22939652490615844 Test Loss: 0.25711625162760415\n",
      "Epoch: 3359 Training Loss: 0.22939344024658204 Test Loss: 0.2571780802408854\n",
      "Epoch: 3360 Training Loss: 0.22937535794576008 Test Loss: 0.25719453938802084\n",
      "Epoch: 3361 Training Loss: 0.22936040337880453 Test Loss: 0.25714145914713543\n",
      "Epoch: 3362 Training Loss: 0.22934716828664145 Test Loss: 0.2570935465494792\n",
      "Epoch: 3363 Training Loss: 0.22933817148208618 Test Loss: 0.25716304524739586\n",
      "Epoch: 3364 Training Loss: 0.22931664800643922 Test Loss: 0.2571422526041667\n",
      "Epoch: 3365 Training Loss: 0.22930545139312744 Test Loss: 0.2572283935546875\n",
      "Epoch: 3366 Training Loss: 0.22929736741383872 Test Loss: 0.2571511433919271\n",
      "Epoch: 3367 Training Loss: 0.22928706789016723 Test Loss: 0.2570985107421875\n",
      "Epoch: 3368 Training Loss: 0.22926878531773884 Test Loss: 0.25715028889973957\n",
      "Epoch: 3369 Training Loss: 0.22924695094426473 Test Loss: 0.25704296875\n",
      "Epoch: 3370 Training Loss: 0.22924457359313966 Test Loss: 0.25715403238932294\n",
      "Epoch: 3371 Training Loss: 0.22922017939885458 Test Loss: 0.2570982666015625\n",
      "Epoch: 3372 Training Loss: 0.22921259673436484 Test Loss: 0.2570559285481771\n",
      "Epoch: 3373 Training Loss: 0.22919387944539388 Test Loss: 0.25715071614583335\n",
      "Epoch: 3374 Training Loss: 0.22918886677424113 Test Loss: 0.2571381632486979\n",
      "Epoch: 3375 Training Loss: 0.2291720550855001 Test Loss: 0.2570822550455729\n",
      "Epoch: 3376 Training Loss: 0.22915853452682494 Test Loss: 0.2570577189127604\n",
      "Epoch: 3377 Training Loss: 0.22914138809839885 Test Loss: 0.2570911458333333\n",
      "Epoch: 3378 Training Loss: 0.22913090880711873 Test Loss: 0.2571091105143229\n",
      "Epoch: 3379 Training Loss: 0.22912268702189129 Test Loss: 0.25709320068359376\n",
      "Epoch: 3380 Training Loss: 0.2291030076344808 Test Loss: 0.25702496337890623\n",
      "Epoch: 3381 Training Loss: 0.2290838073094686 Test Loss: 0.2569281412760417\n",
      "Epoch: 3382 Training Loss: 0.22907625182469685 Test Loss: 0.25702888997395834\n",
      "Epoch: 3383 Training Loss: 0.22907268492380778 Test Loss: 0.25698726399739585\n",
      "Epoch: 3384 Training Loss: 0.22904387664794923 Test Loss: 0.25713240559895834\n",
      "Epoch: 3385 Training Loss: 0.22904563156763713 Test Loss: 0.2570906982421875\n",
      "Epoch: 3386 Training Loss: 0.2290342911084493 Test Loss: 0.25708902994791666\n",
      "Epoch: 3387 Training Loss: 0.22900425926844278 Test Loss: 0.2570851847330729\n",
      "Epoch: 3388 Training Loss: 0.22900747315088907 Test Loss: 0.25708548990885416\n",
      "Epoch: 3389 Training Loss: 0.22898981428146362 Test Loss: 0.257065673828125\n",
      "Epoch: 3390 Training Loss: 0.22897598584493 Test Loss: 0.2569880574544271\n",
      "Epoch: 3391 Training Loss: 0.22897693538665773 Test Loss: 0.257007568359375\n",
      "Epoch: 3392 Training Loss: 0.228955917040507 Test Loss: 0.25705635579427083\n",
      "Epoch: 3393 Training Loss: 0.22894757239023844 Test Loss: 0.25695682779947915\n",
      "Epoch: 3394 Training Loss: 0.22893346309661866 Test Loss: 0.2569347941080729\n",
      "Epoch: 3395 Training Loss: 0.22892449220021566 Test Loss: 0.256998779296875\n",
      "Epoch: 3396 Training Loss: 0.22890573167800904 Test Loss: 0.25697705078125\n",
      "Epoch: 3397 Training Loss: 0.22888097842534383 Test Loss: 0.25699727376302084\n",
      "Epoch: 3398 Training Loss: 0.22887363243103026 Test Loss: 0.25698990885416667\n",
      "Epoch: 3399 Training Loss: 0.22886675834655762 Test Loss: 0.25689754231770834\n",
      "Epoch: 3400 Training Loss: 0.22885580078760784 Test Loss: 0.25699898274739585\n",
      "Epoch: 3401 Training Loss: 0.22884647369384767 Test Loss: 0.2569566853841146\n",
      "Epoch: 3402 Training Loss: 0.22882424132029217 Test Loss: 0.25689485677083335\n",
      "Epoch: 3403 Training Loss: 0.22881083997090657 Test Loss: 0.25692716471354166\n",
      "Epoch: 3404 Training Loss: 0.22880470530192057 Test Loss: 0.2569540201822917\n",
      "Epoch: 3405 Training Loss: 0.228794908841451 Test Loss: 0.2569576416015625\n",
      "Epoch: 3406 Training Loss: 0.22878392505645753 Test Loss: 0.2569303792317708\n",
      "Epoch: 3407 Training Loss: 0.22876411358515422 Test Loss: 0.25688045247395835\n",
      "Epoch: 3408 Training Loss: 0.22875545612970988 Test Loss: 0.25703533935546874\n",
      "Epoch: 3409 Training Loss: 0.22874529679616293 Test Loss: 0.25690496826171877\n",
      "Epoch: 3410 Training Loss: 0.22873907486597697 Test Loss: 0.2567882080078125\n",
      "Epoch: 3411 Training Loss: 0.22871742423375446 Test Loss: 0.2568679402669271\n",
      "Epoch: 3412 Training Loss: 0.22871195220947266 Test Loss: 0.2567376708984375\n",
      "Epoch: 3413 Training Loss: 0.22870381593704223 Test Loss: 0.2569049072265625\n",
      "Epoch: 3414 Training Loss: 0.22868993759155273 Test Loss: 0.25692608642578124\n",
      "Epoch: 3415 Training Loss: 0.22868366718292235 Test Loss: 0.2567100626627604\n",
      "Epoch: 3416 Training Loss: 0.22866717942555745 Test Loss: 0.256878662109375\n",
      "Epoch: 3417 Training Loss: 0.2286604439417521 Test Loss: 0.25673492431640627\n",
      "Epoch: 3418 Training Loss: 0.22864466937383016 Test Loss: 0.2567731119791667\n",
      "Epoch: 3419 Training Loss: 0.22863241624832153 Test Loss: 0.256804443359375\n",
      "Epoch: 3420 Training Loss: 0.22862356090545655 Test Loss: 0.256850341796875\n",
      "Epoch: 3421 Training Loss: 0.22861594025293985 Test Loss: 0.25678181966145835\n",
      "Epoch: 3422 Training Loss: 0.22860375022888182 Test Loss: 0.2568122762044271\n",
      "Epoch: 3423 Training Loss: 0.22859015782674152 Test Loss: 0.2567533976236979\n",
      "Epoch: 3424 Training Loss: 0.2285689034461975 Test Loss: 0.2568107096354167\n",
      "Epoch: 3425 Training Loss: 0.22857331864039104 Test Loss: 0.2568595784505208\n",
      "Epoch: 3426 Training Loss: 0.22856161880493164 Test Loss: 0.2568572998046875\n",
      "Epoch: 3427 Training Loss: 0.22855370791753132 Test Loss: 0.25673527018229164\n",
      "Epoch: 3428 Training Loss: 0.2285331311225891 Test Loss: 0.2568343912760417\n",
      "Epoch: 3429 Training Loss: 0.22852822907765707 Test Loss: 0.256818115234375\n",
      "Epoch: 3430 Training Loss: 0.22852156925201417 Test Loss: 0.2567554931640625\n",
      "Epoch: 3431 Training Loss: 0.22850165478388468 Test Loss: 0.25679109700520836\n",
      "Epoch: 3432 Training Loss: 0.22849163961410524 Test Loss: 0.256742431640625\n",
      "Epoch: 3433 Training Loss: 0.22849418401718138 Test Loss: 0.2567132161458333\n",
      "Epoch: 3434 Training Loss: 0.22847563298543294 Test Loss: 0.256786865234375\n",
      "Epoch: 3435 Training Loss: 0.22846281353632608 Test Loss: 0.2567231241861979\n",
      "Epoch: 3436 Training Loss: 0.2284485109647115 Test Loss: 0.25666890462239583\n",
      "Epoch: 3437 Training Loss: 0.22843891032536826 Test Loss: 0.2568050130208333\n",
      "Epoch: 3438 Training Loss: 0.22842590570449828 Test Loss: 0.2567419026692708\n",
      "Epoch: 3439 Training Loss: 0.2284242254892985 Test Loss: 0.25674556477864585\n",
      "Epoch: 3440 Training Loss: 0.22839580901463827 Test Loss: 0.2567138468424479\n",
      "Epoch: 3441 Training Loss: 0.2283892347017924 Test Loss: 0.2567960205078125\n",
      "Epoch: 3442 Training Loss: 0.22837877162297568 Test Loss: 0.25674916585286456\n",
      "Epoch: 3443 Training Loss: 0.22836185709635418 Test Loss: 0.2567711588541667\n",
      "Epoch: 3444 Training Loss: 0.22835066397984824 Test Loss: 0.2566336263020833\n",
      "Epoch: 3445 Training Loss: 0.22834652837117514 Test Loss: 0.2567275594075521\n",
      "Epoch: 3446 Training Loss: 0.22833100938796996 Test Loss: 0.25655511474609377\n",
      "Epoch: 3447 Training Loss: 0.22831844663619996 Test Loss: 0.2567719523111979\n",
      "Epoch: 3448 Training Loss: 0.2283005069096883 Test Loss: 0.2566739501953125\n",
      "Epoch: 3449 Training Loss: 0.22828763341903688 Test Loss: 0.25666135660807293\n",
      "Epoch: 3450 Training Loss: 0.22828348112106323 Test Loss: 0.2567103678385417\n",
      "Epoch: 3451 Training Loss: 0.22828120660781862 Test Loss: 0.256771240234375\n",
      "Epoch: 3452 Training Loss: 0.22825972684224446 Test Loss: 0.25665486653645836\n",
      "Epoch: 3453 Training Loss: 0.22824942890803018 Test Loss: 0.256747802734375\n",
      "Epoch: 3454 Training Loss: 0.2282337589263916 Test Loss: 0.2567452799479167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3455 Training Loss: 0.2282232255935669 Test Loss: 0.25675445556640625\n",
      "Epoch: 3456 Training Loss: 0.22821180613835654 Test Loss: 0.25668499755859375\n",
      "Epoch: 3457 Training Loss: 0.22819656562805177 Test Loss: 0.2566346435546875\n",
      "Epoch: 3458 Training Loss: 0.22818650436401366 Test Loss: 0.25670050048828125\n",
      "Epoch: 3459 Training Loss: 0.22817837874094646 Test Loss: 0.256733642578125\n",
      "Epoch: 3460 Training Loss: 0.2281644868850708 Test Loss: 0.2567346598307292\n",
      "Epoch: 3461 Training Loss: 0.228142582098643 Test Loss: 0.25661995442708335\n",
      "Epoch: 3462 Training Loss: 0.22814192501703898 Test Loss: 0.25666910807291665\n",
      "Epoch: 3463 Training Loss: 0.2281229478518168 Test Loss: 0.2567183024088542\n",
      "Epoch: 3464 Training Loss: 0.22811620728174845 Test Loss: 0.2567709147135417\n",
      "Epoch: 3465 Training Loss: 0.2281007448832194 Test Loss: 0.2567591552734375\n",
      "Epoch: 3466 Training Loss: 0.22809177843729656 Test Loss: 0.25659796142578123\n",
      "Epoch: 3467 Training Loss: 0.22807505798339844 Test Loss: 0.2566330769856771\n",
      "Epoch: 3468 Training Loss: 0.2280732380549113 Test Loss: 0.2566196492513021\n",
      "Epoch: 3469 Training Loss: 0.2280589254697164 Test Loss: 0.2566410725911458\n",
      "Epoch: 3470 Training Loss: 0.22805175256729127 Test Loss: 0.25657930501302084\n",
      "Epoch: 3471 Training Loss: 0.22803143548965454 Test Loss: 0.2566583658854167\n",
      "Epoch: 3472 Training Loss: 0.22802779595057168 Test Loss: 0.2565918375651042\n",
      "Epoch: 3473 Training Loss: 0.2280155603090922 Test Loss: 0.2567503255208333\n",
      "Epoch: 3474 Training Loss: 0.22801019128163655 Test Loss: 0.2566329345703125\n",
      "Epoch: 3475 Training Loss: 0.22800152397155762 Test Loss: 0.2566787516276042\n",
      "Epoch: 3476 Training Loss: 0.22798274103800456 Test Loss: 0.2566756591796875\n",
      "Epoch: 3477 Training Loss: 0.22798261801401773 Test Loss: 0.256629638671875\n",
      "Epoch: 3478 Training Loss: 0.2279708547592163 Test Loss: 0.2566630859375\n",
      "Epoch: 3479 Training Loss: 0.22796126556396484 Test Loss: 0.2565318196614583\n",
      "Epoch: 3480 Training Loss: 0.22794471184412637 Test Loss: 0.25668408203125\n",
      "Epoch: 3481 Training Loss: 0.22793600702285766 Test Loss: 0.2566016845703125\n",
      "Epoch: 3482 Training Loss: 0.22792572832107544 Test Loss: 0.2565252278645833\n",
      "Epoch: 3483 Training Loss: 0.22791210858027142 Test Loss: 0.2566826171875\n",
      "Epoch: 3484 Training Loss: 0.22789020427068074 Test Loss: 0.2566663411458333\n",
      "Epoch: 3485 Training Loss: 0.22788471269607544 Test Loss: 0.2564898681640625\n",
      "Epoch: 3486 Training Loss: 0.22787224515279134 Test Loss: 0.2565778401692708\n",
      "Epoch: 3487 Training Loss: 0.22787250741322834 Test Loss: 0.256504150390625\n",
      "Epoch: 3488 Training Loss: 0.227838813940684 Test Loss: 0.25653169759114586\n",
      "Epoch: 3489 Training Loss: 0.2278450123469035 Test Loss: 0.25651997884114586\n",
      "Epoch: 3490 Training Loss: 0.22783136145273844 Test Loss: 0.2565195515950521\n",
      "Epoch: 3491 Training Loss: 0.2278126843770345 Test Loss: 0.25654569498697916\n",
      "Epoch: 3492 Training Loss: 0.2278047072092692 Test Loss: 0.25653507486979166\n",
      "Epoch: 3493 Training Loss: 0.227794220606486 Test Loss: 0.2564927775065104\n",
      "Epoch: 3494 Training Loss: 0.2277859306335449 Test Loss: 0.2566276245117188\n",
      "Epoch: 3495 Training Loss: 0.22778462807337443 Test Loss: 0.256592529296875\n",
      "Epoch: 3496 Training Loss: 0.2277693756421407 Test Loss: 0.25656247965494794\n",
      "Epoch: 3497 Training Loss: 0.22774311939875286 Test Loss: 0.2565213623046875\n",
      "Epoch: 3498 Training Loss: 0.22774243720372517 Test Loss: 0.2564668375651042\n",
      "Epoch: 3499 Training Loss: 0.22773027070363364 Test Loss: 0.2564273681640625\n",
      "Epoch: 3500 Training Loss: 0.22772389872868856 Test Loss: 0.2564453125\n",
      "Epoch: 3501 Training Loss: 0.22770486545562743 Test Loss: 0.25656669108072916\n",
      "Epoch: 3502 Training Loss: 0.2276978270212809 Test Loss: 0.25650590006510415\n",
      "Epoch: 3503 Training Loss: 0.2276863087018331 Test Loss: 0.2565179443359375\n",
      "Epoch: 3504 Training Loss: 0.22766787910461425 Test Loss: 0.2564562174479167\n",
      "Epoch: 3505 Training Loss: 0.22767039601008096 Test Loss: 0.25645011393229167\n",
      "Epoch: 3506 Training Loss: 0.2276505953470866 Test Loss: 0.2564349772135417\n",
      "Epoch: 3507 Training Loss: 0.227644685904185 Test Loss: 0.2564366455078125\n",
      "Epoch: 3508 Training Loss: 0.22763362169265747 Test Loss: 0.256554443359375\n",
      "Epoch: 3509 Training Loss: 0.2276215353012085 Test Loss: 0.2563916015625\n",
      "Epoch: 3510 Training Loss: 0.22760772641499838 Test Loss: 0.256337158203125\n",
      "Epoch: 3511 Training Loss: 0.22759551652272542 Test Loss: 0.25643294270833333\n",
      "Epoch: 3512 Training Loss: 0.22759702348709107 Test Loss: 0.2564706217447917\n",
      "Epoch: 3513 Training Loss: 0.22757811403274536 Test Loss: 0.2564331868489583\n",
      "Epoch: 3514 Training Loss: 0.22756858905156455 Test Loss: 0.25629315185546875\n",
      "Epoch: 3515 Training Loss: 0.22754170799255372 Test Loss: 0.2565692138671875\n",
      "Epoch: 3516 Training Loss: 0.22753441603978475 Test Loss: 0.2564538370768229\n",
      "Epoch: 3517 Training Loss: 0.2275237940152486 Test Loss: 0.25640201822916664\n",
      "Epoch: 3518 Training Loss: 0.22751345682144164 Test Loss: 0.2563297526041667\n",
      "Epoch: 3519 Training Loss: 0.22749905872344972 Test Loss: 0.25638818359375\n",
      "Epoch: 3520 Training Loss: 0.22749836285909017 Test Loss: 0.25641920979817706\n",
      "Epoch: 3521 Training Loss: 0.22747593434651692 Test Loss: 0.25627264404296873\n",
      "Epoch: 3522 Training Loss: 0.22746978712081908 Test Loss: 0.256358154296875\n",
      "Epoch: 3523 Training Loss: 0.22746276442209878 Test Loss: 0.25641721598307293\n",
      "Epoch: 3524 Training Loss: 0.22743889633814493 Test Loss: 0.25635459391276044\n",
      "Epoch: 3525 Training Loss: 0.22743630441029866 Test Loss: 0.2563660278320313\n",
      "Epoch: 3526 Training Loss: 0.22741970205307008 Test Loss: 0.25617134602864583\n",
      "Epoch: 3527 Training Loss: 0.22741039641698202 Test Loss: 0.25634395345052086\n",
      "Epoch: 3528 Training Loss: 0.22739966456095378 Test Loss: 0.2562804361979167\n",
      "Epoch: 3529 Training Loss: 0.22738387219111125 Test Loss: 0.2562180989583333\n",
      "Epoch: 3530 Training Loss: 0.22738149277369182 Test Loss: 0.25631148274739585\n",
      "Epoch: 3531 Training Loss: 0.22736929273605347 Test Loss: 0.25631390380859376\n",
      "Epoch: 3532 Training Loss: 0.22736387856801352 Test Loss: 0.2563152058919271\n",
      "Epoch: 3533 Training Loss: 0.22734058507283528 Test Loss: 0.25627852376302085\n",
      "Epoch: 3534 Training Loss: 0.22734210618336995 Test Loss: 0.25627473958333336\n",
      "Epoch: 3535 Training Loss: 0.22732171519597372 Test Loss: 0.2564281412760417\n",
      "Epoch: 3536 Training Loss: 0.22730801598230999 Test Loss: 0.256298095703125\n",
      "Epoch: 3537 Training Loss: 0.22729591560363768 Test Loss: 0.2561458740234375\n",
      "Epoch: 3538 Training Loss: 0.22728886524836223 Test Loss: 0.2563350423177083\n",
      "Epoch: 3539 Training Loss: 0.2272755970954895 Test Loss: 0.2564008992513021\n",
      "Epoch: 3540 Training Loss: 0.22727095731099448 Test Loss: 0.2561962890625\n",
      "Epoch: 3541 Training Loss: 0.2272394978205363 Test Loss: 0.2562463785807292\n",
      "Epoch: 3542 Training Loss: 0.22723691765467327 Test Loss: 0.25625848388671874\n",
      "Epoch: 3543 Training Loss: 0.227240025361379 Test Loss: 0.25625667317708334\n",
      "Epoch: 3544 Training Loss: 0.22722821283340455 Test Loss: 0.25624873860677083\n",
      "Epoch: 3545 Training Loss: 0.22720928064982096 Test Loss: 0.25632718912760416\n",
      "Epoch: 3546 Training Loss: 0.22720296319325764 Test Loss: 0.2561724853515625\n",
      "Epoch: 3547 Training Loss: 0.22719534715016684 Test Loss: 0.2562581583658854\n",
      "Epoch: 3548 Training Loss: 0.22717828067143758 Test Loss: 0.25620513916015625\n",
      "Epoch: 3549 Training Loss: 0.22718478933970135 Test Loss: 0.25617720540364586\n",
      "Epoch: 3550 Training Loss: 0.22716482003529867 Test Loss: 0.2562138671875\n",
      "Epoch: 3551 Training Loss: 0.22715630928675334 Test Loss: 0.2563087361653646\n",
      "Epoch: 3552 Training Loss: 0.22713029464085896 Test Loss: 0.2561549072265625\n",
      "Epoch: 3553 Training Loss: 0.2271274224917094 Test Loss: 0.25616111246744794\n",
      "Epoch: 3554 Training Loss: 0.22711856349309287 Test Loss: 0.2561912638346354\n",
      "Epoch: 3555 Training Loss: 0.22710596005121866 Test Loss: 0.2561362711588542\n",
      "Epoch: 3556 Training Loss: 0.22708718919754028 Test Loss: 0.2562366739908854\n",
      "Epoch: 3557 Training Loss: 0.22708205302556356 Test Loss: 0.25613165283203126\n",
      "Epoch: 3558 Training Loss: 0.2270739911397298 Test Loss: 0.2561156819661458\n",
      "Epoch: 3559 Training Loss: 0.22705629539489747 Test Loss: 0.25624513753255207\n",
      "Epoch: 3560 Training Loss: 0.22703560161590577 Test Loss: 0.2561522420247396\n",
      "Epoch: 3561 Training Loss: 0.22704414335886638 Test Loss: 0.25607552083333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3562 Training Loss: 0.22703195778528848 Test Loss: 0.2561251220703125\n",
      "Epoch: 3563 Training Loss: 0.2270113844871521 Test Loss: 0.2561598714192708\n",
      "Epoch: 3564 Training Loss: 0.22700276947021483 Test Loss: 0.256202392578125\n",
      "Epoch: 3565 Training Loss: 0.22699340629577636 Test Loss: 0.2561865234375\n",
      "Epoch: 3566 Training Loss: 0.22698707485198974 Test Loss: 0.25617220052083334\n",
      "Epoch: 3567 Training Loss: 0.22696935796737672 Test Loss: 0.2561300455729167\n",
      "Epoch: 3568 Training Loss: 0.22696801662445068 Test Loss: 0.25609476725260416\n",
      "Epoch: 3569 Training Loss: 0.22694821723302205 Test Loss: 0.2561357625325521\n",
      "Epoch: 3570 Training Loss: 0.2269397899309794 Test Loss: 0.25611761474609374\n",
      "Epoch: 3571 Training Loss: 0.22692357953389486 Test Loss: 0.2560232340494792\n",
      "Epoch: 3572 Training Loss: 0.22690902773539226 Test Loss: 0.25614261881510414\n",
      "Epoch: 3573 Training Loss: 0.22691225163141887 Test Loss: 0.25608984375\n",
      "Epoch: 3574 Training Loss: 0.2268961420059204 Test Loss: 0.25608658854166666\n",
      "Epoch: 3575 Training Loss: 0.22689833784103394 Test Loss: 0.25605924479166664\n",
      "Epoch: 3576 Training Loss: 0.22688168795903524 Test Loss: 0.25613326009114584\n",
      "Epoch: 3577 Training Loss: 0.22686132510503132 Test Loss: 0.2560388997395833\n",
      "Epoch: 3578 Training Loss: 0.226846151192983 Test Loss: 0.2561048380533854\n",
      "Epoch: 3579 Training Loss: 0.22685498889287312 Test Loss: 0.25602496337890623\n",
      "Epoch: 3580 Training Loss: 0.22683894618352254 Test Loss: 0.2561545817057292\n",
      "Epoch: 3581 Training Loss: 0.22682681624094644 Test Loss: 0.25604451497395836\n",
      "Epoch: 3582 Training Loss: 0.22681063588460287 Test Loss: 0.2560767415364583\n",
      "Epoch: 3583 Training Loss: 0.22680917421976726 Test Loss: 0.2560960286458333\n",
      "Epoch: 3584 Training Loss: 0.22678884156545004 Test Loss: 0.2559971720377604\n",
      "Epoch: 3585 Training Loss: 0.22678135792414347 Test Loss: 0.2560482584635417\n",
      "Epoch: 3586 Training Loss: 0.22677248446146647 Test Loss: 0.25597776285807294\n",
      "Epoch: 3587 Training Loss: 0.22676684204737346 Test Loss: 0.25605716959635416\n",
      "Epoch: 3588 Training Loss: 0.2267408234278361 Test Loss: 0.2560724894205729\n",
      "Epoch: 3589 Training Loss: 0.2267421154975891 Test Loss: 0.2560473836263021\n",
      "Epoch: 3590 Training Loss: 0.22672333399454753 Test Loss: 0.2560706787109375\n",
      "Epoch: 3591 Training Loss: 0.22671538750330608 Test Loss: 0.2561095377604167\n",
      "Epoch: 3592 Training Loss: 0.22670500723520914 Test Loss: 0.25604150390625\n",
      "Epoch: 3593 Training Loss: 0.2266911096572876 Test Loss: 0.25594974772135415\n",
      "Epoch: 3594 Training Loss: 0.22668694877624512 Test Loss: 0.25600590006510415\n",
      "Epoch: 3595 Training Loss: 0.22666748507817586 Test Loss: 0.2559036051432292\n",
      "Epoch: 3596 Training Loss: 0.22666899903615315 Test Loss: 0.2559224853515625\n",
      "Epoch: 3597 Training Loss: 0.22664683135350544 Test Loss: 0.2560582275390625\n",
      "Epoch: 3598 Training Loss: 0.2266406772931417 Test Loss: 0.255871337890625\n",
      "Epoch: 3599 Training Loss: 0.22661856190363566 Test Loss: 0.2558812255859375\n",
      "Epoch: 3600 Training Loss: 0.22661715888977052 Test Loss: 0.2559765625\n",
      "Epoch: 3601 Training Loss: 0.226598863919576 Test Loss: 0.2559339599609375\n",
      "Epoch: 3602 Training Loss: 0.2265950665473938 Test Loss: 0.2559513753255208\n",
      "Epoch: 3603 Training Loss: 0.22658459695180258 Test Loss: 0.2559904988606771\n",
      "Epoch: 3604 Training Loss: 0.22657345978418986 Test Loss: 0.2559190673828125\n",
      "Epoch: 3605 Training Loss: 0.2265615161259969 Test Loss: 0.2558618570963542\n",
      "Epoch: 3606 Training Loss: 0.22655422751108806 Test Loss: 0.2558472290039063\n",
      "Epoch: 3607 Training Loss: 0.2265394066174825 Test Loss: 0.2559695231119792\n",
      "Epoch: 3608 Training Loss: 0.22652769470214842 Test Loss: 0.2559383544921875\n",
      "Epoch: 3609 Training Loss: 0.22651599947611492 Test Loss: 0.25583451334635415\n",
      "Epoch: 3610 Training Loss: 0.22649700339635212 Test Loss: 0.2558608601888021\n",
      "Epoch: 3611 Training Loss: 0.22649269326527913 Test Loss: 0.255786865234375\n",
      "Epoch: 3612 Training Loss: 0.2264747420946757 Test Loss: 0.25587117513020835\n",
      "Epoch: 3613 Training Loss: 0.22647446982065836 Test Loss: 0.255906982421875\n",
      "Epoch: 3614 Training Loss: 0.22645939095815024 Test Loss: 0.25585795084635415\n",
      "Epoch: 3615 Training Loss: 0.22643344942728677 Test Loss: 0.2559056193033854\n",
      "Epoch: 3616 Training Loss: 0.22642868026097615 Test Loss: 0.255907470703125\n",
      "Epoch: 3617 Training Loss: 0.22642099952697753 Test Loss: 0.2557873128255208\n",
      "Epoch: 3618 Training Loss: 0.22640215889612833 Test Loss: 0.2558565673828125\n",
      "Epoch: 3619 Training Loss: 0.22640420277913412 Test Loss: 0.25580930582682293\n",
      "Epoch: 3620 Training Loss: 0.22637800900141397 Test Loss: 0.2559010009765625\n",
      "Epoch: 3621 Training Loss: 0.2263805742263794 Test Loss: 0.2557832845052083\n",
      "Epoch: 3622 Training Loss: 0.2263639251391093 Test Loss: 0.255775634765625\n",
      "Epoch: 3623 Training Loss: 0.22634438625971476 Test Loss: 0.2557952880859375\n",
      "Epoch: 3624 Training Loss: 0.22633949851989746 Test Loss: 0.2558231404622396\n",
      "Epoch: 3625 Training Loss: 0.22632488107681276 Test Loss: 0.25578934733072917\n",
      "Epoch: 3626 Training Loss: 0.22631842724482218 Test Loss: 0.25582259114583333\n",
      "Epoch: 3627 Training Loss: 0.2263063398996989 Test Loss: 0.2557320353190104\n",
      "Epoch: 3628 Training Loss: 0.22629187774658202 Test Loss: 0.25574385579427084\n",
      "Epoch: 3629 Training Loss: 0.22628593985239664 Test Loss: 0.2557751668294271\n",
      "Epoch: 3630 Training Loss: 0.22627187983194988 Test Loss: 0.25576873779296877\n",
      "Epoch: 3631 Training Loss: 0.22625680907567342 Test Loss: 0.25571002197265624\n",
      "Epoch: 3632 Training Loss: 0.22624289004007975 Test Loss: 0.25574159749348957\n",
      "Epoch: 3633 Training Loss: 0.2262304417292277 Test Loss: 0.255703125\n",
      "Epoch: 3634 Training Loss: 0.22622673575083416 Test Loss: 0.2557633260091146\n",
      "Epoch: 3635 Training Loss: 0.2262165805498759 Test Loss: 0.25567010498046877\n",
      "Epoch: 3636 Training Loss: 0.2262036608060201 Test Loss: 0.2555962320963542\n",
      "Epoch: 3637 Training Loss: 0.22619527292251587 Test Loss: 0.25579243977864585\n",
      "Epoch: 3638 Training Loss: 0.22617879899342855 Test Loss: 0.25561490885416666\n",
      "Epoch: 3639 Training Loss: 0.2261745432217916 Test Loss: 0.2556107177734375\n",
      "Epoch: 3640 Training Loss: 0.22615869903564453 Test Loss: 0.2557222493489583\n",
      "Epoch: 3641 Training Loss: 0.22615068101882935 Test Loss: 0.25571262613932294\n",
      "Epoch: 3642 Training Loss: 0.22613498179117839 Test Loss: 0.2558061116536458\n",
      "Epoch: 3643 Training Loss: 0.22612172094980876 Test Loss: 0.2557068277994792\n",
      "Epoch: 3644 Training Loss: 0.22611652167638144 Test Loss: 0.25569000244140627\n",
      "Epoch: 3645 Training Loss: 0.22609607855478922 Test Loss: 0.25571504720052085\n",
      "Epoch: 3646 Training Loss: 0.22608909638722738 Test Loss: 0.25562373860677085\n",
      "Epoch: 3647 Training Loss: 0.22608485333124798 Test Loss: 0.2556141153971354\n",
      "Epoch: 3648 Training Loss: 0.22606667868296304 Test Loss: 0.2556449178059896\n",
      "Epoch: 3649 Training Loss: 0.2260597496032715 Test Loss: 0.2556773681640625\n",
      "Epoch: 3650 Training Loss: 0.22604265371958415 Test Loss: 0.2556976521809896\n",
      "Epoch: 3651 Training Loss: 0.22603626585006714 Test Loss: 0.25565132649739586\n",
      "Epoch: 3652 Training Loss: 0.22602301740646363 Test Loss: 0.2557923583984375\n",
      "Epoch: 3653 Training Loss: 0.22600639645258586 Test Loss: 0.25567354329427083\n",
      "Epoch: 3654 Training Loss: 0.2259905532201131 Test Loss: 0.25571834309895836\n",
      "Epoch: 3655 Training Loss: 0.22599833075205486 Test Loss: 0.25565228271484375\n",
      "Epoch: 3656 Training Loss: 0.2259785164197286 Test Loss: 0.2556092936197917\n",
      "Epoch: 3657 Training Loss: 0.22597016223271688 Test Loss: 0.255683837890625\n",
      "Epoch: 3658 Training Loss: 0.22595433616638183 Test Loss: 0.2557427978515625\n",
      "Epoch: 3659 Training Loss: 0.22595054308573406 Test Loss: 0.25567645263671873\n",
      "Epoch: 3660 Training Loss: 0.2259347562789917 Test Loss: 0.2556545817057292\n",
      "Epoch: 3661 Training Loss: 0.2259293196996053 Test Loss: 0.25563633219401044\n",
      "Epoch: 3662 Training Loss: 0.2259167634646098 Test Loss: 0.2557184651692708\n",
      "Epoch: 3663 Training Loss: 0.22590788110097249 Test Loss: 0.25570735677083334\n",
      "Epoch: 3664 Training Loss: 0.2258965172767639 Test Loss: 0.25556203206380207\n",
      "Epoch: 3665 Training Loss: 0.22589408206939698 Test Loss: 0.2556378173828125\n",
      "Epoch: 3666 Training Loss: 0.2258814123471578 Test Loss: 0.25564208984375\n",
      "Epoch: 3667 Training Loss: 0.22585876433054605 Test Loss: 0.255685546875\n",
      "Epoch: 3668 Training Loss: 0.22584706528981527 Test Loss: 0.2556141764322917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3669 Training Loss: 0.22584763224919638 Test Loss: 0.2556270751953125\n",
      "Epoch: 3670 Training Loss: 0.2258364159266154 Test Loss: 0.255636962890625\n",
      "Epoch: 3671 Training Loss: 0.2258276570638021 Test Loss: 0.25561319986979164\n",
      "Epoch: 3672 Training Loss: 0.22581911325454712 Test Loss: 0.25562685139973956\n",
      "Epoch: 3673 Training Loss: 0.2258012367884318 Test Loss: 0.2556817626953125\n",
      "Epoch: 3674 Training Loss: 0.2257917267481486 Test Loss: 0.25564546712239583\n",
      "Epoch: 3675 Training Loss: 0.22577918752034506 Test Loss: 0.25556842041015626\n",
      "Epoch: 3676 Training Loss: 0.22576616255442303 Test Loss: 0.25569891357421876\n",
      "Epoch: 3677 Training Loss: 0.22576710446675619 Test Loss: 0.2555967000325521\n",
      "Epoch: 3678 Training Loss: 0.2257493119239807 Test Loss: 0.25556005859375\n",
      "Epoch: 3679 Training Loss: 0.22574383370081583 Test Loss: 0.25566422526041666\n",
      "Epoch: 3680 Training Loss: 0.225730406443278 Test Loss: 0.2556571044921875\n",
      "Epoch: 3681 Training Loss: 0.22570946486790974 Test Loss: 0.2556059773763021\n",
      "Epoch: 3682 Training Loss: 0.22570635906855266 Test Loss: 0.2556068522135417\n",
      "Epoch: 3683 Training Loss: 0.22569132773081463 Test Loss: 0.2555053304036458\n",
      "Epoch: 3684 Training Loss: 0.22567864497502646 Test Loss: 0.2556837565104167\n",
      "Epoch: 3685 Training Loss: 0.22566142797470093 Test Loss: 0.2554885660807292\n",
      "Epoch: 3686 Training Loss: 0.22565647459030153 Test Loss: 0.2555699055989583\n",
      "Epoch: 3687 Training Loss: 0.22564080174763998 Test Loss: 0.2555712076822917\n",
      "Epoch: 3688 Training Loss: 0.22563258028030395 Test Loss: 0.25557967122395836\n",
      "Epoch: 3689 Training Loss: 0.22561974732081094 Test Loss: 0.25551611328125\n",
      "Epoch: 3690 Training Loss: 0.22562102588017782 Test Loss: 0.2555928955078125\n",
      "Epoch: 3691 Training Loss: 0.22560549704233807 Test Loss: 0.25549214680989585\n",
      "Epoch: 3692 Training Loss: 0.2255926113128662 Test Loss: 0.25566068522135416\n",
      "Epoch: 3693 Training Loss: 0.22557478666305542 Test Loss: 0.2555577189127604\n",
      "Epoch: 3694 Training Loss: 0.22556992769241332 Test Loss: 0.255632568359375\n",
      "Epoch: 3695 Training Loss: 0.22555913162231445 Test Loss: 0.25560825602213544\n",
      "Epoch: 3696 Training Loss: 0.22555164607365927 Test Loss: 0.25564656575520833\n",
      "Epoch: 3697 Training Loss: 0.22554410727818808 Test Loss: 0.2556194254557292\n",
      "Epoch: 3698 Training Loss: 0.2255360918045044 Test Loss: 0.25566776529947915\n",
      "Epoch: 3699 Training Loss: 0.2255111969312032 Test Loss: 0.25560965983072914\n",
      "Epoch: 3700 Training Loss: 0.22550491364796957 Test Loss: 0.2555559895833333\n",
      "Epoch: 3701 Training Loss: 0.22549679772059122 Test Loss: 0.2555909016927083\n",
      "Epoch: 3702 Training Loss: 0.22548740657170613 Test Loss: 0.25551873779296874\n",
      "Epoch: 3703 Training Loss: 0.22546138668060303 Test Loss: 0.25562556966145833\n",
      "Epoch: 3704 Training Loss: 0.22546070178349814 Test Loss: 0.25564404296875\n",
      "Epoch: 3705 Training Loss: 0.22543995602925618 Test Loss: 0.2556951904296875\n",
      "Epoch: 3706 Training Loss: 0.2254304575920105 Test Loss: 0.2555672200520833\n",
      "Epoch: 3707 Training Loss: 0.2254221494992574 Test Loss: 0.2556524454752604\n",
      "Epoch: 3708 Training Loss: 0.2254255305926005 Test Loss: 0.25560013834635414\n",
      "Epoch: 3709 Training Loss: 0.22541296497980753 Test Loss: 0.25557769775390626\n",
      "Epoch: 3710 Training Loss: 0.22540419483184815 Test Loss: 0.255575927734375\n",
      "Epoch: 3711 Training Loss: 0.22537438583374023 Test Loss: 0.2555357666015625\n",
      "Epoch: 3712 Training Loss: 0.22537851095199585 Test Loss: 0.25557242838541666\n",
      "Epoch: 3713 Training Loss: 0.2253707920710246 Test Loss: 0.25552734375\n",
      "Epoch: 3714 Training Loss: 0.22535323413213093 Test Loss: 0.25545835367838543\n",
      "Epoch: 3715 Training Loss: 0.2253407497406006 Test Loss: 0.2556619669596354\n",
      "Epoch: 3716 Training Loss: 0.22533039474487304 Test Loss: 0.2554864501953125\n",
      "Epoch: 3717 Training Loss: 0.22531286525726318 Test Loss: 0.2556133015950521\n",
      "Epoch: 3718 Training Loss: 0.2253042934735616 Test Loss: 0.2556123453776042\n",
      "Epoch: 3719 Training Loss: 0.22529952081044516 Test Loss: 0.2554383748372396\n",
      "Epoch: 3720 Training Loss: 0.22528685394922893 Test Loss: 0.25547100830078123\n",
      "Epoch: 3721 Training Loss: 0.2252779509226481 Test Loss: 0.2555169881184896\n",
      "Epoch: 3722 Training Loss: 0.22526256497701008 Test Loss: 0.2554600016276042\n",
      "Epoch: 3723 Training Loss: 0.2252519227663676 Test Loss: 0.25560139973958335\n",
      "Epoch: 3724 Training Loss: 0.22522669665018719 Test Loss: 0.2555867716471354\n",
      "Epoch: 3725 Training Loss: 0.22522858270009358 Test Loss: 0.25553285725911457\n",
      "Epoch: 3726 Training Loss: 0.22521680116653442 Test Loss: 0.2555860799153646\n",
      "Epoch: 3727 Training Loss: 0.22520142555236816 Test Loss: 0.25544466145833333\n",
      "Epoch: 3728 Training Loss: 0.22519071276982625 Test Loss: 0.25565268961588544\n",
      "Epoch: 3729 Training Loss: 0.22518716541926065 Test Loss: 0.25546073404947917\n",
      "Epoch: 3730 Training Loss: 0.2251767477989197 Test Loss: 0.2554858805338542\n",
      "Epoch: 3731 Training Loss: 0.22517321411768595 Test Loss: 0.2554671630859375\n",
      "Epoch: 3732 Training Loss: 0.22515396610895794 Test Loss: 0.25541208902994794\n",
      "Epoch: 3733 Training Loss: 0.22514294687906902 Test Loss: 0.2554690958658854\n",
      "Epoch: 3734 Training Loss: 0.22512913052241007 Test Loss: 0.25536627197265627\n",
      "Epoch: 3735 Training Loss: 0.225116827805837 Test Loss: 0.25552840169270835\n",
      "Epoch: 3736 Training Loss: 0.22510252539316813 Test Loss: 0.255452880859375\n",
      "Epoch: 3737 Training Loss: 0.22510864893595378 Test Loss: 0.25536216227213543\n",
      "Epoch: 3738 Training Loss: 0.22508148527145386 Test Loss: 0.2555496012369792\n",
      "Epoch: 3739 Training Loss: 0.22507933950424194 Test Loss: 0.25545902506510415\n",
      "Epoch: 3740 Training Loss: 0.22506427590052286 Test Loss: 0.2554115397135417\n",
      "Epoch: 3741 Training Loss: 0.22506869586308798 Test Loss: 0.2555089111328125\n",
      "Epoch: 3742 Training Loss: 0.2250493852297465 Test Loss: 0.25535239664713544\n",
      "Epoch: 3743 Training Loss: 0.22503733984629312 Test Loss: 0.25547941080729164\n",
      "Epoch: 3744 Training Loss: 0.22502676423390705 Test Loss: 0.2552574462890625\n",
      "Epoch: 3745 Training Loss: 0.22501341072718303 Test Loss: 0.2554644368489583\n",
      "Epoch: 3746 Training Loss: 0.22501002089182537 Test Loss: 0.25540625\n",
      "Epoch: 3747 Training Loss: 0.22499031702677408 Test Loss: 0.25540281168619794\n",
      "Epoch: 3748 Training Loss: 0.22497861909866332 Test Loss: 0.25532845052083336\n",
      "Epoch: 3749 Training Loss: 0.22497235743204752 Test Loss: 0.2553850504557292\n",
      "Epoch: 3750 Training Loss: 0.22496290493011475 Test Loss: 0.25543229166666664\n",
      "Epoch: 3751 Training Loss: 0.22495816294352214 Test Loss: 0.2555453287760417\n",
      "Epoch: 3752 Training Loss: 0.2249311490058899 Test Loss: 0.25546101888020833\n",
      "Epoch: 3753 Training Loss: 0.22492618115743002 Test Loss: 0.2554507039388021\n",
      "Epoch: 3754 Training Loss: 0.22492038742701212 Test Loss: 0.2554922688802083\n",
      "Epoch: 3755 Training Loss: 0.22490565395355225 Test Loss: 0.2554113362630208\n",
      "Epoch: 3756 Training Loss: 0.22489517641067505 Test Loss: 0.25546626790364585\n",
      "Epoch: 3757 Training Loss: 0.22489333661397298 Test Loss: 0.25536029052734377\n",
      "Epoch: 3758 Training Loss: 0.2248711724281311 Test Loss: 0.2554268595377604\n",
      "Epoch: 3759 Training Loss: 0.2248615452448527 Test Loss: 0.25546099853515625\n",
      "Epoch: 3760 Training Loss: 0.22485506852467854 Test Loss: 0.2554252522786458\n",
      "Epoch: 3761 Training Loss: 0.22483103402455648 Test Loss: 0.25544637044270835\n",
      "Epoch: 3762 Training Loss: 0.22482375001907348 Test Loss: 0.2553978474934896\n",
      "Epoch: 3763 Training Loss: 0.22480929056803386 Test Loss: 0.2554098103841146\n",
      "Epoch: 3764 Training Loss: 0.22480047496159872 Test Loss: 0.2554055989583333\n",
      "Epoch: 3765 Training Loss: 0.22479757134119668 Test Loss: 0.2554462483723958\n",
      "Epoch: 3766 Training Loss: 0.22477756516138714 Test Loss: 0.255434326171875\n",
      "Epoch: 3767 Training Loss: 0.22476966349283853 Test Loss: 0.2554635009765625\n",
      "Epoch: 3768 Training Loss: 0.22475848547617594 Test Loss: 0.2554033203125\n",
      "Epoch: 3769 Training Loss: 0.2247445429166158 Test Loss: 0.2552333170572917\n",
      "Epoch: 3770 Training Loss: 0.22473591136932372 Test Loss: 0.2553314208984375\n",
      "Epoch: 3771 Training Loss: 0.22472266753514608 Test Loss: 0.25548044840494794\n",
      "Epoch: 3772 Training Loss: 0.2247077571551005 Test Loss: 0.25531575520833333\n",
      "Epoch: 3773 Training Loss: 0.22469748624165853 Test Loss: 0.2554451497395833\n",
      "Epoch: 3774 Training Loss: 0.22468396139144897 Test Loss: 0.25533186848958334\n",
      "Epoch: 3775 Training Loss: 0.22468824052810668 Test Loss: 0.2553644612630208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3776 Training Loss: 0.22466835975646973 Test Loss: 0.2553093465169271\n",
      "Epoch: 3777 Training Loss: 0.2246462140083313 Test Loss: 0.2554700927734375\n",
      "Epoch: 3778 Training Loss: 0.22464492162068686 Test Loss: 0.255307861328125\n",
      "Epoch: 3779 Training Loss: 0.22461827611923219 Test Loss: 0.25536081949869793\n",
      "Epoch: 3780 Training Loss: 0.2246183457374573 Test Loss: 0.2553469645182292\n",
      "Epoch: 3781 Training Loss: 0.22460527467727662 Test Loss: 0.25532914225260417\n",
      "Epoch: 3782 Training Loss: 0.22459690173467 Test Loss: 0.2551581827799479\n",
      "Epoch: 3783 Training Loss: 0.22457874170939127 Test Loss: 0.2552873128255208\n",
      "Epoch: 3784 Training Loss: 0.22457790676752726 Test Loss: 0.25523341878255207\n",
      "Epoch: 3785 Training Loss: 0.22456053463617962 Test Loss: 0.2551951090494792\n",
      "Epoch: 3786 Training Loss: 0.22455288712183635 Test Loss: 0.255338134765625\n",
      "Epoch: 3787 Training Loss: 0.22453025070826213 Test Loss: 0.2551580607096354\n",
      "Epoch: 3788 Training Loss: 0.22453981892267863 Test Loss: 0.2552632242838542\n",
      "Epoch: 3789 Training Loss: 0.22451229254404703 Test Loss: 0.25525634765625\n",
      "Epoch: 3790 Training Loss: 0.22450468015670777 Test Loss: 0.25507816569010416\n",
      "Epoch: 3791 Training Loss: 0.22449084043502807 Test Loss: 0.25539333089192706\n",
      "Epoch: 3792 Training Loss: 0.22448726002375285 Test Loss: 0.2552890828450521\n",
      "Epoch: 3793 Training Loss: 0.2244713691075643 Test Loss: 0.25529443359375\n",
      "Epoch: 3794 Training Loss: 0.2244651716550191 Test Loss: 0.2551811116536458\n",
      "Epoch: 3795 Training Loss: 0.2244482626914978 Test Loss: 0.2551810302734375\n",
      "Epoch: 3796 Training Loss: 0.22444415473937987 Test Loss: 0.25521512858072914\n",
      "Epoch: 3797 Training Loss: 0.2244348340034485 Test Loss: 0.25526908365885415\n",
      "Epoch: 3798 Training Loss: 0.22442025391260784 Test Loss: 0.2552369384765625\n",
      "Epoch: 3799 Training Loss: 0.22441410462061565 Test Loss: 0.25525311279296875\n",
      "Epoch: 3800 Training Loss: 0.22440125274658204 Test Loss: 0.25513785807291667\n",
      "Epoch: 3801 Training Loss: 0.2243863383928935 Test Loss: 0.255146240234375\n",
      "Epoch: 3802 Training Loss: 0.224376283009847 Test Loss: 0.255234130859375\n",
      "Epoch: 3803 Training Loss: 0.22437793525060018 Test Loss: 0.2552514444986979\n",
      "Epoch: 3804 Training Loss: 0.2243616730372111 Test Loss: 0.2551559244791667\n",
      "Epoch: 3805 Training Loss: 0.22434465551376342 Test Loss: 0.2552014973958333\n",
      "Epoch: 3806 Training Loss: 0.22434539127349853 Test Loss: 0.25534259033203127\n",
      "Epoch: 3807 Training Loss: 0.22432910998662312 Test Loss: 0.2551715901692708\n",
      "Epoch: 3808 Training Loss: 0.22431676657994587 Test Loss: 0.25518656412760415\n",
      "Epoch: 3809 Training Loss: 0.22430493927001954 Test Loss: 0.25523828125\n",
      "Epoch: 3810 Training Loss: 0.2243048652013143 Test Loss: 0.25516200764973956\n",
      "Epoch: 3811 Training Loss: 0.22429365571339926 Test Loss: 0.25517215983072916\n",
      "Epoch: 3812 Training Loss: 0.22428081210454304 Test Loss: 0.25527058919270834\n",
      "Epoch: 3813 Training Loss: 0.22426485919952394 Test Loss: 0.25515730794270836\n",
      "Epoch: 3814 Training Loss: 0.224264879544576 Test Loss: 0.25511444091796875\n",
      "Epoch: 3815 Training Loss: 0.22425531832377116 Test Loss: 0.25510581461588544\n",
      "Epoch: 3816 Training Loss: 0.22424045944213866 Test Loss: 0.25516485595703126\n",
      "Epoch: 3817 Training Loss: 0.22423715511957804 Test Loss: 0.2551263020833333\n",
      "Epoch: 3818 Training Loss: 0.2242215725580851 Test Loss: 0.2551355997721354\n",
      "Epoch: 3819 Training Loss: 0.22421839666366578 Test Loss: 0.25516337076822915\n",
      "Epoch: 3820 Training Loss: 0.22420909531911215 Test Loss: 0.25519140625\n",
      "Epoch: 3821 Training Loss: 0.2242073140144348 Test Loss: 0.2551173095703125\n",
      "Epoch: 3822 Training Loss: 0.2241847465833028 Test Loss: 0.2551877848307292\n",
      "Epoch: 3823 Training Loss: 0.22417802747090657 Test Loss: 0.25502384440104164\n",
      "Epoch: 3824 Training Loss: 0.22416653140385945 Test Loss: 0.2550698649088542\n",
      "Epoch: 3825 Training Loss: 0.22416600449879964 Test Loss: 0.2550159098307292\n",
      "Epoch: 3826 Training Loss: 0.22415092611312867 Test Loss: 0.25507661946614585\n",
      "Epoch: 3827 Training Loss: 0.2241439453760783 Test Loss: 0.2551623128255208\n",
      "Epoch: 3828 Training Loss: 0.2241285212834676 Test Loss: 0.25505194091796873\n",
      "Epoch: 3829 Training Loss: 0.22412966696421305 Test Loss: 0.2551358235677083\n",
      "Epoch: 3830 Training Loss: 0.22411737887064617 Test Loss: 0.25516217041015626\n",
      "Epoch: 3831 Training Loss: 0.2241060824394226 Test Loss: 0.25505051676432294\n",
      "Epoch: 3832 Training Loss: 0.22409669812520344 Test Loss: 0.25510664876302086\n",
      "Epoch: 3833 Training Loss: 0.22409022919336954 Test Loss: 0.255065673828125\n",
      "Epoch: 3834 Training Loss: 0.22407873360315958 Test Loss: 0.2551210123697917\n",
      "Epoch: 3835 Training Loss: 0.22407140843073528 Test Loss: 0.25494677734375\n",
      "Epoch: 3836 Training Loss: 0.22406167793273926 Test Loss: 0.2550345052083333\n",
      "Epoch: 3837 Training Loss: 0.2240429523785909 Test Loss: 0.25504219563802083\n",
      "Epoch: 3838 Training Loss: 0.22403707615534466 Test Loss: 0.25508101399739586\n",
      "Epoch: 3839 Training Loss: 0.22402333386739096 Test Loss: 0.25493577067057294\n",
      "Epoch: 3840 Training Loss: 0.22401188611984252 Test Loss: 0.2550102335611979\n",
      "Epoch: 3841 Training Loss: 0.2240041340192159 Test Loss: 0.25500590006510415\n",
      "Epoch: 3842 Training Loss: 0.2239993896484375 Test Loss: 0.2548984375\n",
      "Epoch: 3843 Training Loss: 0.2239881118138631 Test Loss: 0.2551157430013021\n",
      "Epoch: 3844 Training Loss: 0.2239760306676229 Test Loss: 0.25493953450520834\n",
      "Epoch: 3845 Training Loss: 0.22396834707260133 Test Loss: 0.25502311197916666\n",
      "Epoch: 3846 Training Loss: 0.2239521196683248 Test Loss: 0.2549883219401042\n",
      "Epoch: 3847 Training Loss: 0.22395254151026409 Test Loss: 0.2549124755859375\n",
      "Epoch: 3848 Training Loss: 0.223940891901652 Test Loss: 0.2550804850260417\n",
      "Epoch: 3849 Training Loss: 0.22392908636728923 Test Loss: 0.2549278971354167\n",
      "Epoch: 3850 Training Loss: 0.22392156394322713 Test Loss: 0.25498201497395834\n",
      "Epoch: 3851 Training Loss: 0.22390176916122437 Test Loss: 0.2550030517578125\n",
      "Epoch: 3852 Training Loss: 0.22388950649897257 Test Loss: 0.2550391845703125\n",
      "Epoch: 3853 Training Loss: 0.22388886992136636 Test Loss: 0.25487060546875\n",
      "Epoch: 3854 Training Loss: 0.22387829653422037 Test Loss: 0.2548865966796875\n",
      "Epoch: 3855 Training Loss: 0.2238634419441223 Test Loss: 0.2549935913085937\n",
      "Epoch: 3856 Training Loss: 0.22386796156565347 Test Loss: 0.25495149739583334\n",
      "Epoch: 3857 Training Loss: 0.22384511280059816 Test Loss: 0.254873046875\n",
      "Epoch: 3858 Training Loss: 0.2238348798751831 Test Loss: 0.25487428792317707\n",
      "Epoch: 3859 Training Loss: 0.2238397068977356 Test Loss: 0.25477372233072915\n",
      "Epoch: 3860 Training Loss: 0.22381511688232422 Test Loss: 0.254860595703125\n",
      "Epoch: 3861 Training Loss: 0.2238039223353068 Test Loss: 0.254935546875\n",
      "Epoch: 3862 Training Loss: 0.2238039666811625 Test Loss: 0.25489510091145834\n",
      "Epoch: 3863 Training Loss: 0.22379284874598185 Test Loss: 0.25507086181640626\n",
      "Epoch: 3864 Training Loss: 0.22378268925348918 Test Loss: 0.2549838460286458\n",
      "Epoch: 3865 Training Loss: 0.22378864653905234 Test Loss: 0.25491630045572916\n",
      "Epoch: 3866 Training Loss: 0.223747185866038 Test Loss: 0.25493184407552083\n",
      "Epoch: 3867 Training Loss: 0.2237465723355611 Test Loss: 0.2548939208984375\n",
      "Epoch: 3868 Training Loss: 0.22374730348587035 Test Loss: 0.2549222208658854\n",
      "Epoch: 3869 Training Loss: 0.22374150387446085 Test Loss: 0.2548978474934896\n",
      "Epoch: 3870 Training Loss: 0.22372866217295329 Test Loss: 0.2550232340494792\n",
      "Epoch: 3871 Training Loss: 0.22371980619430543 Test Loss: 0.25484503173828127\n",
      "Epoch: 3872 Training Loss: 0.22370723454157512 Test Loss: 0.2547795003255208\n",
      "Epoch: 3873 Training Loss: 0.22369334491093953 Test Loss: 0.25488873291015623\n",
      "Epoch: 3874 Training Loss: 0.22369559892018637 Test Loss: 0.2549102376302083\n",
      "Epoch: 3875 Training Loss: 0.22368701950709025 Test Loss: 0.25494287109375\n",
      "Epoch: 3876 Training Loss: 0.22367203172047934 Test Loss: 0.2549513549804687\n",
      "Epoch: 3877 Training Loss: 0.22366934521993 Test Loss: 0.2549150797526042\n",
      "Epoch: 3878 Training Loss: 0.22366268761952718 Test Loss: 0.2548118286132813\n",
      "Epoch: 3879 Training Loss: 0.22364631446202596 Test Loss: 0.25489493815104164\n",
      "Epoch: 3880 Training Loss: 0.2236445172627767 Test Loss: 0.2549102579752604\n",
      "Epoch: 3881 Training Loss: 0.2236336094538371 Test Loss: 0.2548787638346354\n",
      "Epoch: 3882 Training Loss: 0.22361810207366944 Test Loss: 0.2549150797526042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3883 Training Loss: 0.2236156703631083 Test Loss: 0.25483603922526044\n",
      "Epoch: 3884 Training Loss: 0.2236041145324707 Test Loss: 0.2548692626953125\n",
      "Epoch: 3885 Training Loss: 0.22360214726130168 Test Loss: 0.2548024698893229\n",
      "Epoch: 3886 Training Loss: 0.22358383782704672 Test Loss: 0.2548611653645833\n",
      "Epoch: 3887 Training Loss: 0.2235739321708679 Test Loss: 0.25485445149739583\n",
      "Epoch: 3888 Training Loss: 0.22356349643071494 Test Loss: 0.2547822062174479\n",
      "Epoch: 3889 Training Loss: 0.22355434147516887 Test Loss: 0.254727294921875\n",
      "Epoch: 3890 Training Loss: 0.22354455057779948 Test Loss: 0.2547749430338542\n",
      "Epoch: 3891 Training Loss: 0.22353973547617595 Test Loss: 0.25485599772135414\n",
      "Epoch: 3892 Training Loss: 0.22352734835942586 Test Loss: 0.2548660888671875\n",
      "Epoch: 3893 Training Loss: 0.2235250358581543 Test Loss: 0.25492781575520834\n",
      "Epoch: 3894 Training Loss: 0.22350633112589519 Test Loss: 0.25486372884114583\n",
      "Epoch: 3895 Training Loss: 0.22350230344136557 Test Loss: 0.2548834838867188\n",
      "Epoch: 3896 Training Loss: 0.22349387423197428 Test Loss: 0.25493770345052086\n",
      "Epoch: 3897 Training Loss: 0.22348443937301635 Test Loss: 0.2547188720703125\n",
      "Epoch: 3898 Training Loss: 0.2234699290593465 Test Loss: 0.25481351725260415\n",
      "Epoch: 3899 Training Loss: 0.223465478738149 Test Loss: 0.2549383138020833\n",
      "Epoch: 3900 Training Loss: 0.2234548004468282 Test Loss: 0.25485591634114585\n",
      "Epoch: 3901 Training Loss: 0.22344656070073446 Test Loss: 0.25478361002604166\n",
      "Epoch: 3902 Training Loss: 0.22344240347544353 Test Loss: 0.254802978515625\n",
      "Epoch: 3903 Training Loss: 0.2234309228261312 Test Loss: 0.25487211100260415\n",
      "Epoch: 3904 Training Loss: 0.22342385021845498 Test Loss: 0.25477766927083334\n",
      "Epoch: 3905 Training Loss: 0.22340940014521282 Test Loss: 0.25481038411458334\n",
      "Epoch: 3906 Training Loss: 0.22339971764882405 Test Loss: 0.2547826334635417\n",
      "Epoch: 3907 Training Loss: 0.22338842646280924 Test Loss: 0.2548387247721354\n",
      "Epoch: 3908 Training Loss: 0.22339246209462485 Test Loss: 0.25482515462239586\n",
      "Epoch: 3909 Training Loss: 0.2233714968363444 Test Loss: 0.2548177490234375\n",
      "Epoch: 3910 Training Loss: 0.22336963272094726 Test Loss: 0.2548927408854167\n",
      "Epoch: 3911 Training Loss: 0.22336553001403808 Test Loss: 0.25478450520833335\n",
      "Epoch: 3912 Training Loss: 0.22336002333958943 Test Loss: 0.2546640828450521\n",
      "Epoch: 3913 Training Loss: 0.223349325021108 Test Loss: 0.25481817626953124\n",
      "Epoch: 3914 Training Loss: 0.22334397633870443 Test Loss: 0.2547198689778646\n",
      "Epoch: 3915 Training Loss: 0.22333322858810425 Test Loss: 0.25471195475260416\n",
      "Epoch: 3916 Training Loss: 0.2233204607963562 Test Loss: 0.254782470703125\n",
      "Epoch: 3917 Training Loss: 0.223319842338562 Test Loss: 0.2546428426106771\n",
      "Epoch: 3918 Training Loss: 0.22330616807937623 Test Loss: 0.2545614827473958\n",
      "Epoch: 3919 Training Loss: 0.22330023511250813 Test Loss: 0.25473881022135414\n",
      "Epoch: 3920 Training Loss: 0.22330553166071573 Test Loss: 0.25465604654947915\n",
      "Epoch: 3921 Training Loss: 0.22329325977961223 Test Loss: 0.254740478515625\n",
      "Epoch: 3922 Training Loss: 0.22328899765014648 Test Loss: 0.25477543131510416\n",
      "Epoch: 3923 Training Loss: 0.22327204513549806 Test Loss: 0.2547715657552083\n",
      "Epoch: 3924 Training Loss: 0.2232658829689026 Test Loss: 0.25475118001302083\n",
      "Epoch: 3925 Training Loss: 0.22326825650533041 Test Loss: 0.25467179361979164\n",
      "Epoch: 3926 Training Loss: 0.22324619913101196 Test Loss: 0.25483426920572916\n",
      "Epoch: 3927 Training Loss: 0.22324310111999512 Test Loss: 0.2547560628255208\n",
      "Epoch: 3928 Training Loss: 0.22323978535334268 Test Loss: 0.25479689534505207\n",
      "Epoch: 3929 Training Loss: 0.22322519795099893 Test Loss: 0.2548746337890625\n",
      "Epoch: 3930 Training Loss: 0.2232197289466858 Test Loss: 0.2548011678059896\n",
      "Epoch: 3931 Training Loss: 0.22320609649022422 Test Loss: 0.25467390950520835\n",
      "Epoch: 3932 Training Loss: 0.22319618558883667 Test Loss: 0.25469401041666667\n",
      "Epoch: 3933 Training Loss: 0.22319400596618652 Test Loss: 0.2547138264973958\n",
      "Epoch: 3934 Training Loss: 0.22319804382324218 Test Loss: 0.2546831258138021\n",
      "Epoch: 3935 Training Loss: 0.22317982546488443 Test Loss: 0.2546944783528646\n",
      "Epoch: 3936 Training Loss: 0.22316928482055665 Test Loss: 0.2547679443359375\n",
      "Epoch: 3937 Training Loss: 0.22315933513641356 Test Loss: 0.2547600708007812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-29534cd9a340>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msum_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msum_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_of_epochs=15000\n",
    "batch_size=int(x_variable.shape[0]/100)\n",
    "# Train the network #\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    \n",
    "    my_net.eval()\n",
    "    test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "    my_net.train()\n",
    "    print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../network_datasets/connectivity_NN/'+str(nb_of_points)+'_NN_qualities.pkl','wb') as f:\n",
    "    pickle.dump(my_net,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size [1, 30]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bc7a692ce5c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minput_contour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpredicted_quality_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpredicted_quality_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_quality_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-422a63ba5d23>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mACTIVATION\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mpre_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_bn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# input batch normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mlayer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m         return F.batch_norm(\n\u001b[0;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size [1, 30]"
     ]
    }
   ],
   "source": [
    "######################################## TIMING#######################################################################\n",
    "\n",
    "# Timing procedure \n",
    "%timeit\n",
    "procrustes_contour=apply_procrustes(generate_contour(15))\n",
    "procrustes_contour=procrustes_contour.reshape(2*procrustes_contour.shape[0]).reshape(1,-1)\n",
    "\n",
    "input_contour=Variable(torch.from_numpy(procrustes_contour).type(torch.FloatTensor)).cuda()\n",
    "predicted_quality_matrix=my_net(input_contour)\n",
    "\n",
    "predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(15,15)\n",
    "procrustes_contour=procrustes_contour.reshape(15,2)\n",
    "ordered_matrix=order_quality_matrix(predicted_quality_matrix,procrustes_contour)\n",
    "triangulate(procrustes_contour,ordered_matrix)\n",
    "fig=plt.figure()\n",
    "shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "triangulated=triangle.triangulate(shape,'pq0')\n",
    "plot.plot(plt.axes(), **triangulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid triangulation 14 0 4\n",
      "initial set edges: {(0, 1), (1, 2), (7, 8), (6, 7), (10, 11), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4)}\n",
      "Edge: (14, 0) targeting: 13\n",
      "(14, 0, 13)\n",
      "Vertex locked: 14\n",
      "edges inserted: (0, 13)\n",
      "set of interior edges updated: {(0, 13)}\n",
      "set of edges updated: {(0, 1), (1, 2), (7, 8), (6, 7), (10, 11), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (0, 13), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4)}\n",
      "element inserted: (14, 0, 13)\n",
      "Edge: (13, 14) targeting: 0\n",
      "(13, 14, 0)\n",
      "Element (14, 0, 13) already in set\n",
      "Edge: (12, 13) targeting: 11\n",
      "(12, 13, 11)\n",
      "Vertex locked: 12\n",
      "edges inserted: (13, 11)\n",
      "set of interior edges updated: {(0, 13), (13, 11)}\n",
      "set of edges updated: {(0, 1), (1, 2), (7, 8), (6, 7), (10, 11), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (0, 13), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4), (13, 11)}\n",
      "element inserted: (12, 13, 11)\n",
      "Edge: (11, 12) targeting: 13\n",
      "(11, 12, 13)\n",
      "Element (12, 13, 11) already in set\n",
      "Edge: (10, 11) targeting: 13\n",
      "(10, 11, 13)\n",
      "Vertex locked: 11\n",
      "edges inserted: (10, 13)\n",
      "set of interior edges updated: {(10, 13), (0, 13), (13, 11)}\n",
      "set of edges updated: {(0, 1), (1, 2), (7, 8), (6, 7), (10, 11), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (0, 13), (10, 13), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4), (13, 11)}\n",
      "element inserted: (10, 11, 13)\n",
      "Edge: (9, 10) targeting: 13\n",
      "(9, 10, 13)\n",
      "Vertex locked: 10\n",
      "edges inserted: (9, 13)\n",
      "set of interior edges updated: {(9, 13), (10, 13), (0, 13), (13, 11)}\n",
      "set of edges updated: {(0, 1), (1, 2), (7, 8), (9, 13), (6, 7), (10, 11), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (0, 13), (10, 13), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4), (13, 11)}\n",
      "element inserted: (9, 10, 13)\n",
      "Edge: (8, 9) targeting: 7\n",
      "(8, 9, 7)\n",
      "Vertex locked: 8\n",
      "edges inserted: (9, 7)\n",
      "set of interior edges updated: {(9, 13), (13, 11), (10, 13), (0, 13), (9, 7)}\n",
      "set of edges updated: {(0, 1), (1, 2), (7, 8), (9, 13), (6, 7), (10, 11), (9, 7), (4, 5), (5, 6), (12, 13), (11, 12), (13, 14), (0, 13), (10, 13), (8, 9), (9, 10), (2, 3), (14, 0), (3, 4), (13, 11)}\n",
      "element inserted: (8, 9, 7)\n",
      "Edge: (7, 8) targeting: 9\n",
      "(7, 8, 9)\n",
      "Element (8, 9, 7) already in set\n",
      "Edge: (6, 7) targeting: 13\n",
      "(6, 7, 13)\n",
      "edges inserted: (6, 13)\n",
      "set of interior edges updated: {(9, 13), (13, 11), (10, 13), (0, 13), (6, 13), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 4), (9, 7), (13, 11), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (0, 13), (6, 13), (7, 8)}\n",
      "edges inserted: (7, 13)\n",
      "set of interior edges updated: {(9, 13), (13, 11), (10, 13), (7, 13), (0, 13), (6, 13), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 4), (9, 7), (13, 11), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (6, 13), (7, 8)}\n",
      "element inserted: (6, 7, 13)\n",
      "Edge: (5, 6) targeting: 4\n",
      "(5, 6, 4)\n",
      "Vertex locked: 5\n",
      "edges inserted: (6, 4)\n",
      "set of interior edges updated: {(9, 13), (6, 4), (13, 11), (10, 13), (7, 13), (0, 13), (6, 13), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 4), (9, 7), (13, 11), (6, 4), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (6, 13), (7, 8)}\n",
      "element inserted: (5, 6, 4)\n",
      "Edge: (4, 5) targeting: 6\n",
      "(4, 5, 6)\n",
      "Element (5, 6, 4) already in set\n",
      "Edge: (3, 4) targeting: 6\n",
      "(3, 4, 6)\n",
      "Vertex locked: 4\n",
      "edges inserted: (3, 6)\n",
      "set of interior edges updated: {(9, 13), (6, 4), (13, 11), (10, 13), (7, 13), (0, 13), (6, 13), (3, 6), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 6), (3, 4), (9, 7), (13, 11), (6, 4), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (6, 13), (7, 8)}\n",
      "element inserted: (3, 4, 6)\n",
      "Edge: (2, 3) targeting: 6\n",
      "(2, 3, 6)\n",
      "Vertex locked: 3\n",
      "edges inserted: (2, 6)\n",
      "set of interior edges updated: {(9, 13), (6, 4), (2, 6), (13, 11), (10, 13), (7, 13), (0, 13), (6, 13), (3, 6), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 6), (3, 4), (9, 7), (13, 11), (6, 4), (2, 6), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (6, 13), (7, 8)}\n",
      "element inserted: (2, 3, 6)\n",
      "Edge: (1, 2) targeting: 0\n",
      "(1, 2, 0)\n",
      "Vertex locked: 1\n",
      "edges inserted: (2, 0)\n",
      "set of interior edges updated: {(9, 13), (6, 4), (2, 6), (13, 11), (10, 13), (7, 13), (0, 13), (2, 0), (6, 13), (3, 6), (9, 7)}\n",
      "set of edges updated: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 6), (3, 4), (9, 7), (13, 11), (6, 4), (2, 6), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (2, 0), (6, 13), (7, 8)}\n",
      "element inserted: (1, 2, 0)\n",
      "Edge: (0, 1) targeting: 2\n",
      "(0, 1, 2)\n",
      "Element (1, 2, 0) already in set\n",
      "Final edges: {(10, 11), (5, 6), (8, 9), (14, 0), (1, 2), (6, 7), (12, 13), (3, 6), (3, 4), (9, 7), (13, 11), (6, 4), (2, 6), (4, 5), (10, 13), (9, 10), (2, 3), (11, 12), (0, 1), (9, 13), (13, 14), (7, 13), (0, 13), (2, 0), (6, 13), (7, 8)}\n",
      "Elements created: {(3, 4, 6), (6, 7, 13), (2, 3, 6), (5, 6, 4), (9, 10, 13), (14, 0, 13), (10, 11, 13), (1, 2, 0), (8, 9, 7), (12, 13, 11)}\n",
      "Set of locked vertices: {1, 3, 4, 5, 8, 10, 11, 12, 14}\n",
      "Set of open vertices: {0, 2, 6, 7, 9, 13}\n",
      "[[0, 2, 6, 13]]\n",
      "[[0.         0.         0.2707999  0.75335258]\n",
      " [0.2707999  0.         0.         0.75335258]\n",
      " [0.2707999  0.75335258 0.         0.        ]\n",
      " [0.         0.75335258 0.2707999  0.        ]] {(3, 0): [((0.7533525764492992, 1), (0.2707998954965426, 2), (0.0, 3), (0.0, 0))], (2, 3): [((0.7533525764492992, 1), (0.2707998954965426, 0), (0.0, 3), (0.0, 2))], (1, 2): [((0.7533525764492992, 3), (0.27079989549654254, 0), (0.0, 2), (0.0, 1))], (0, 1): [((0.7533525764492992, 3), (0.2707998954965426, 2), (0.0, 1), (0.0, 0))]}\n",
      "initial set edges: {(0, 1), (3, 0), (2, 3), (1, 2)}\n",
      "Edge: (3, 0) targeting: 1\n",
      "(3, 0, 1)\n",
      "Vertex locked: 0\n",
      "edges inserted: (3, 1)\n",
      "set of interior edges updated: {(3, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "element inserted: (3, 0, 1)\n",
      "Edge: (2, 3) targeting: 1\n",
      "(2, 3, 1)\n",
      "Vertex locked: 2\n",
      "Vertex locked: 3\n",
      "Vertex locked: 1\n",
      "element inserted: (2, 3, 1)\n",
      "Edge: (1, 2) targeting: 3\n",
      "(1, 2, 3)\n",
      "Element (2, 3, 1) already in set\n",
      "Edge: (0, 1) targeting: 3\n",
      "(0, 1, 3)\n",
      "Element (3, 0, 1) already in set\n",
      "Final edges: {(0, 1), (1, 2), (3, 0), (3, 1), (2, 3)}\n",
      "Elements created: {(3, 0, 1), (2, 3, 1)}\n",
      "Set of locked vertices: {0, 1, 2, 3}\n",
      "Set of open vertices: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize result\n",
    "original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "triangulate(procrustes_contour,ordered_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "triangulated=triangle.triangulate(shape,'pq0')\n",
    "plot.plot(plt.axes(), **triangulated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for applying procrustes --- 0.0009884834289550781 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:93: RuntimeWarning: invalid value encountered in true_divide\n",
      "D:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:94: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2f39c1d7673c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;31m# Project to space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtime2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcontour_transformed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprojection_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed time for projection --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprojection_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'components_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Timing procedure seperately\n",
    "\n",
    "start_time=time.time()\n",
    "# Perform procrustes\n",
    "time1=time.time()\n",
    "procrustes_contour=apply_procrustes(contour)\n",
    "procrustes_time=time.time() - time1\n",
    "print(\"Elapsed time for applying procrustes --- %s seconds ---\" % (procrustes_time))\n",
    "\n",
    "\n",
    "procrustes_contour=procrustes_contour.reshape(2*contour.shape[0]).reshape(1,-1)\n",
    "\n",
    "\n",
    "\n",
    "# Project to space \n",
    "time2=time.time()\n",
    "contour_transformed=pca.transform(procrustes_contour)\n",
    "projection_time=time.time() - time2\n",
    "print(\"Elapsed time for projection --- %s seconds ---\" % (projection_time))\n",
    "\n",
    "\n",
    "# Fire to network\n",
    "time3=time.time()\n",
    "input_contour=Variable(torch.from_numpy(contour_transformed).type(torch.FloatTensor)).cuda()\n",
    "predicted_quality_matrix=network9(input_contour)\n",
    "network_time=time.time() - time3\n",
    "print(\"Elapsed time for getting quality matrix from neural net --- %s seconds ---\" % (network_time))\n",
    "\n",
    "predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(9,9)\n",
    "procrustes_contour=procrustes_contour.reshape(9,2)\n",
    "\n",
    "\n",
    "# Calculating quality matrix\n",
    "time4=time.time()\n",
    "original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "calculation_quality_matrix_time=time.time() - time4\n",
    "print(\"Elapsed time for calculating original quality matrix  --- %s seconds ---\" % (calculation_quality_matrix_time))\n",
    "\n",
    "\n",
    "# Ordering quality matrix\n",
    "time5=time.time()\n",
    "ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "ordering_matrix_time=time.time() - time5\n",
    "print(\"Elapsed time for ordering the quality matrix  --- %s seconds ---\" % (ordering_matrix_time))\n",
    "\n",
    "\n",
    "# Triangulation\n",
    "time6=time.time()\n",
    "triangulate(procrustes_contour,ordered_matrix)\n",
    "triangulation_time=time.time() - time6\n",
    "print(\"Elapsed time to triangulate according to matrix   --- %s seconds ---\" % (triangulation_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total time  of triangulation with calculation of matrix :   --- %s seconds ---\" % ( (procrustes_time+calculation_quality_matrix_time\n",
    "                                                                                      +ordering_matrix_time+triangulation_time)))\n",
    "\n",
    "print(\"Total time  of triangulation using neural network :   --- %s seconds ---\" % ( (procrustes_time+projection_time\n",
    "                                                                                      +network_time+triangulation_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must have at least three input vertices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-fbe4ed1d88c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime7\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_contour_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriangle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriangulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pq0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed time to triangulate using triangle module   --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\triangle\\__init__.py\u001b[0m in \u001b[0;36mtriangulate\u001b[0;34m(tri, opts)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'vertices'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vertices'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input must have at least three input vertices.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTriangulateIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must have at least three input vertices."
     ]
    }
   ],
   "source": [
    "#  Time using the triangle module for meshing \n",
    "time7=time.time()\n",
    "shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "t = triangle.triangulate(shape, 'pq0')\n",
    "plot.plot(plt.axes(), **t)\n",
    "print(\"Elapsed time to triangulate using triangle module   --- %s seconds ---\" % (time.time() - time7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for applying procrustes --- 0.00032223501011685585 seconds ---\n",
      "Elapsed time for projection --- 0.0001631666941648291 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006677795472569414 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.13363634011739123 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003150628472212702 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0010122994899575133 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.13528593746468687 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0021654807414961397 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.345530449285434e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00037115940313015017 seconds ---\n",
      "Elapsed time for projection --- 0.0001813532489904901 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0007092756295605795 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.0481077914528214 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0002730544679252489 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.00127254652488773 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.05002455184876453 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0025343348065689497 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.4032637949512715e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00030865913095112774 seconds ---\n",
      "Elapsed time for projection --- 0.00012038986869811197 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006652180609307834 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08299190851994354 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.00028048277908965247 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0013066142960269644 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08488766472601128 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0024008813566069875 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.3824798929690584e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00028458115684770746 seconds ---\n",
      "Elapsed time for projection --- 0.00017136345104518114 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006183428563417692 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.0765518190014518 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003160874421155313 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0010327913828405144 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.07818527898325556 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0021070788470751722 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 4.9365225207202455e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00027843358930113027 seconds ---\n",
      "Elapsed time for projection --- 0.000162142099270568 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006670111010862456 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08030644600648884 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.00037013480823588907 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0012927822690471658 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08224779667307303 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0024003690587051096 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 4.7894829460074104e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.0002966201441267913 seconds ---\n",
      "Elapsed time for projection --- 0.00017136345104518114 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.000616549816186307 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08400446415862461 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003268456853220414 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0013458050402732624 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08597373502834671 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.002430338451631542 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-4263ba5acd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtimes4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtime_elapsed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_time_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtimes4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtimes5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-e3d194d7e1a9>\u001b[0m in \u001b[0;36mrun_time_test\u001b[0;34m(start, finish)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtime7\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriangle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriangulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pq0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mremeshing_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime7\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mremeshing_time\u001b[0m\u001b[1;33m/=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\triangle\\__init__.py\u001b[0m in \u001b[0;36mtriangulate\u001b[0;34m(tri, opts)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn1\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtri\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times4=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(4,4)\n",
    "    times4[j]=time_elapsed\n",
    "times5=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(5,5)\n",
    "    times5[j]=time_elapsed\n",
    "    \n",
    "times6=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(6,6)\n",
    "    times6[j]=time_elapsed\n",
    "    \n",
    "times7=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(7,7)\n",
    "    times7[j]=time_elapsed\n",
    "    \n",
    "times8=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(8,8)\n",
    "    times8[j]=time_elapsed\n",
    "    \n",
    "times9=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(9,9)\n",
    "    times9[j]=time_elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_elapsed_mean=np.array([times4.mean(0),times5.mean(0),times6.mean(0),times7.mean(0),times8.mean(0),times9.mean(0)])\n",
    "times_elapsed_std=np.array([times4.std(0),times5.std(0),times6.std(0),times7.std(0),times8.std(0),times9.std(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1c1f549f780>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f5259b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Timing procrustes\n",
    "\n",
    "procrustes_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,0],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,0]-times_elapsed_std[:,0],times_elapsed_mean[:,0]+times_elapsed_std[:,0],alpha=.1)\n",
    "plt.title(' Procrustes')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x22332076c88>"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+/8BOYdelKZmucsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22333242748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Timing projection\n",
    "\n",
    "projection_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,1],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,1]-times_elapsed_std[:,1],times_elapsed_mean[:,1]+times_elapsed_std[:,1],alpha=.1)\n",
    "\n",
    "plt.title(' Projection')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x22332097208>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22333155400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Timing network output\n",
    "\n",
    "network_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,2],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,2]-times_elapsed_std[:,2],times_elapsed_mean[:,2]+times_elapsed_std[:,2],alpha=.1)\n",
    "\n",
    "plt.title(' Network output')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x223331cb8d0>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYKFgjDGmg4WCMcaYDv8fMScnfpOVh/0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223320764e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Timing connection table\n",
    "\n",
    "calculation_matrix_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,3],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,3]-times_elapsed_std[:,3],times_elapsed_mean[:,3]+times_elapsed_std[:,3],alpha=.1)\n",
    "\n",
    "plt.title(' calculation of matrix')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2232e27ed68>"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVlpOCmZmVlpOCmZmVlpOCmZmVlp/A9zekYBn7nQswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2232e1f50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Timing time to mesh with reference\n",
    "\n",
    "triangulation_graph_with_library=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,8],marker='o',label='Using library')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,8]-times_elapsed_std[:,8],times_elapsed_mean[:,8]+times_elapsed_std[:,8],alpha=.1)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(' Triangulations')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_of_contours=100\n",
    "contours=[generate_contour(i) for i in range(4,nb_of_contours) ]\n",
    "processing_ctimes=np.empty([int(nb_of_contours-4)])\n",
    "processing_timer=np.empty([int(nb_of_contours-4)])\n",
    "processing_time=np.empty([int(nb_of_contours-4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=ctimer.start()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=ctimer.stop()\n",
    "    remeshing_time=(ctimer.diff(time2,time1))*10**(-9)\n",
    "    \n",
    "    processing_ctimes[index]=remeshing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "\n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=timer()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=timer()\n",
    "    remeshing_time=time2-time1\n",
    "    \n",
    "    processing_timer[index]=remeshing_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "\n",
    "    \n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=time.perf_counter()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=time.perf_counter()\n",
    "    remeshing_time=time2-time1\n",
    "    \n",
    "    processing_time[index]=remeshing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.06760359,  0.05959082,  0.05308127,  0.05258083,  0.05258036,\n",
       "         0.04657149,  0.05558467,  0.05308199,  0.05758834,  0.05458331,\n",
       "         0.06159472,  0.05308104,  0.05308199,  0.0565865 ,  0.04506946,\n",
       "         0.06059241,  0.05308175,  0.05108547,  0.06309676,  0.06059265,\n",
       "         0.06309676,  0.063097  ,  0.06359696,  0.06710291,  0.07211065,\n",
       "         0.06810474,  0.08412838,  0.07261181,  0.0751152 ,  0.08112407,\n",
       "         0.07862043,  0.08863616,  0.1011548 ,  0.10365868,  0.07561564,\n",
       "         0.07811999,  0.07862091,  0.07361293,  0.07711792,  0.07962227,\n",
       "         0.076617  ,  0.07661748,  0.07661724,  0.0560863 ,  0.05458379,\n",
       "         0.08563089,  0.08563137,  0.08763433,  0.08162498,  0.07862115,\n",
       "         0.0826261 ,  0.08663273,  0.09564686,  0.08412933,  0.0906384 ,\n",
       "         0.0951457 ,  0.10365939,  0.09814978,  0.11016965,  0.10466003,\n",
       "         0.1076653 ,  0.11066961,  0.11016917,  0.12018418,  0.11116982,\n",
       "         0.10516119,  0.11317372,  0.11317348,  0.13070011,  0.12268806,\n",
       "         0.12018442,  0.13771081,  0.1266942 ,  0.13170171,  0.13270402,\n",
       "         0.13570738,  0.13570809,  0.14572358,  0.14872813,  0.14572334,\n",
       "         0.1477263 ,  0.14772654,  0.14922833,  0.1532352 ,  0.15123177,\n",
       "         0.14271879,  0.13821149,  0.15173316,  0.15173268,  0.13821149,\n",
       "         0.14371991,  0.1592443 ,  0.15673995,  0.16174793,  0.1657548 ,\n",
       "         0.1807766 ]),\n",
       " array([ 0.07337993,  0.0711271 ,  0.0560069 ,  0.0531447 ,  0.05541341,\n",
       "         0.0536196 ,  0.05326278,  0.04559728,  0.05776895,  0.05497949,\n",
       "         0.04778684,  0.05893494,  0.06098976,  0.05976537,  0.05792008,\n",
       "         0.05365264,  0.05507939,  0.05509578,  0.06265396,  0.06037706,\n",
       "         0.05214085,  0.06301052,  0.06427385,  0.06282456,  0.05809375,\n",
       "         0.0606227 ,  0.06911531,  0.06448081,  0.06504588,  0.06537452,\n",
       "         0.06130764,  0.06633405,  0.07603875,  0.06728718,  0.07121009,\n",
       "         0.07282178,  0.07420652,  0.0842204 ,  0.07689788,  0.08232823,\n",
       "         0.07738328,  0.07925239,  0.08792764,  0.08585309,  0.09093021,\n",
       "         0.08192095,  0.08860822,  0.09088667,  0.08970531,  0.09470072,\n",
       "         0.09944331,  0.1035358 ,  0.09803552,  0.09820637,  0.09040562,\n",
       "         0.10554938,  0.09366793,  0.09388847,  0.09877989,  0.10523355,\n",
       "         0.10140567,  0.10796154,  0.10666004,  0.11220515,  0.11306274,\n",
       "         0.09497044,  0.09183442,  0.11717239,  0.10019383,  0.11863935,\n",
       "         0.11420516,  0.11802946,  0.10938342,  0.12136246,  0.13039504,\n",
       "         0.12025129,  0.12055585,  0.13675213,  0.14345144,  0.1347575 ,\n",
       "         0.1480009 ,  0.14563511,  0.15072376,  0.14487384,  0.14721299,\n",
       "         0.15058903,  0.14279724,  0.14881187,  0.14136306,  0.14817303,\n",
       "         0.15477117,  0.16227709,  0.16143948,  0.15963876,  0.16323227,\n",
       "         0.16234984]),\n",
       " array([  5.60000000e-08,   8.57247520e-02,   8.93975040e-02,\n",
       "          8.93967840e-02,   8.93970720e-02,   9.42136800e-02,\n",
       "          8.94237280e-02,   8.94240880e-02,   8.94239440e-02,\n",
       "          8.94243760e-02,   8.94248080e-02,   8.94271120e-02,\n",
       "          8.94266800e-02,   8.94238720e-02,   8.94255280e-02,\n",
       "          8.94267520e-02,   8.94269680e-02,   8.94247360e-02,\n",
       "          8.94248800e-02,   8.94265360e-02,   8.94272560e-02,\n",
       "          8.94235840e-02,   8.94251680e-02,   8.94236560e-02,\n",
       "          8.94243040e-02,   8.93562560e-02,   8.93553920e-02,\n",
       "          8.93565440e-02,   8.93542400e-02,   8.93564000e-02,\n",
       "          8.93569040e-02,   8.93549600e-02,   8.93552480e-02,\n",
       "          8.93566880e-02,   8.93559680e-02,   8.93568320e-02,\n",
       "          8.93569760e-02,   8.93548880e-02,   8.93567600e-02,\n",
       "          8.93550320e-02,   8.93574080e-02,   8.93556800e-02,\n",
       "          8.93554640e-02,   8.93571920e-02,   8.93540240e-02,\n",
       "          8.93544560e-02,   8.93536640e-02,   8.93556080e-02,\n",
       "          8.93555360e-02,   8.93538080e-02,   8.93558960e-02,\n",
       "          8.93545280e-02,   8.93551040e-02,   8.93546000e-02,\n",
       "          8.93576240e-02,   8.93557520e-02,   8.93558240e-02,\n",
       "          8.93573360e-02,   8.93570480e-02,   8.93575520e-02,\n",
       "          8.93543120e-02,   8.93564720e-02,   8.93574800e-02,\n",
       "          8.93560400e-02,   8.93537360e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv3POJcz/AeCRoKpvZf1TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f562fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(processing_time,marker='o',label='time')\n",
    "#plt.plot(processing_ctimes,marker='x')\n",
    "plt.plot(processing_timer,marker='o',label='time_clock')\n",
    "plt.legend()\n",
    "processing_time,processing_timer,processing_ctimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remeshing time : 0.0022776704\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'triangulated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-42742607e414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"remeshing time : %.10f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtriangulated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'triangulated' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16673614208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "\n",
    "time1=timer()\n",
    "Delaunay(contour)\n",
    "\n",
    "time2=timer()\n",
    "\n",
    "print(\"remeshing time : %.10f\"%((time2-time1)))\n",
    "plot.plot(plt.axes(), **triangulated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "from Delaunay_2d import Delaunay2d\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "\n",
    "qualities=[]\n",
    "for j in range(4,30):\n",
    "    print(j)\n",
    "    contour=get_reference_polygon(j)\n",
    "    quality,_=quality_matrix(contour)\n",
    "    qualities.append(quality)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 point polygon\n",
      "Elapsed time using Delaunay: 0.000068\n",
      "Elapsed time using my method: 0.000584\n",
      "5 point polygon\n",
      "Elapsed time using Delaunay: 0.000096\n",
      "Elapsed time using my method: 0.000880\n",
      "6 point polygon\n",
      "Elapsed time using Delaunay: 0.000164\n",
      "Elapsed time using my method: 0.001135\n",
      "7 point polygon\n",
      "Elapsed time using Delaunay: 0.000222\n",
      "Elapsed time using my method: 0.001502\n",
      "8 point polygon\n",
      "Elapsed time using Delaunay: 0.000295\n",
      "Elapsed time using my method: 0.002021\n",
      "9 point polygon\n",
      "Elapsed time using Delaunay: 0.000393\n",
      "Elapsed time using my method: 0.002831\n",
      "10 point polygon\n",
      "Elapsed time using Delaunay: 0.000508\n",
      "Elapsed time using my method: 0.003020\n",
      "11 point polygon\n",
      "Elapsed time using Delaunay: 0.000637\n",
      "Elapsed time using my method: 0.004245\n",
      "12 point polygon\n",
      "Elapsed time using Delaunay: 0.000751\n",
      "Elapsed time using my method: 0.004978\n",
      "13 point polygon\n",
      "Elapsed time using Delaunay: 0.000887\n",
      "Elapsed time using my method: 0.007775\n",
      "14 point polygon\n",
      "Elapsed time using Delaunay: 0.001031\n",
      "Elapsed time using my method: 0.006556\n",
      "15 point polygon\n",
      "Elapsed time using Delaunay: 0.001224\n",
      "Elapsed time using my method: 0.006633\n",
      "16 point polygon\n",
      "Elapsed time using Delaunay: 0.001413\n",
      "Elapsed time using my method: 0.011675\n",
      "17 point polygon\n",
      "Elapsed time using Delaunay: 0.001579\n",
      "Elapsed time using my method: 0.014115\n",
      "18 point polygon\n",
      "Elapsed time using Delaunay: 0.001827\n",
      "Elapsed time using my method: 0.023685\n",
      "19 point polygon\n",
      "Elapsed time using Delaunay: 0.001964\n",
      "Elapsed time using my method: 0.013547\n",
      "20 point polygon\n",
      "Elapsed time using Delaunay: 0.002231\n",
      "Elapsed time using my method: 0.015944\n",
      "21 point polygon\n",
      "Elapsed time using Delaunay: 0.002446\n",
      "Elapsed time using my method: 0.032731\n",
      "22 point polygon\n",
      "Elapsed time using Delaunay: 0.002705\n",
      "Elapsed time using my method: 0.019930\n",
      "23 point polygon\n",
      "Elapsed time using Delaunay: 0.002963\n",
      "Elapsed time using my method: 0.024289\n",
      "24 point polygon\n",
      "Elapsed time using Delaunay: 0.003266\n",
      "Elapsed time using my method: 0.042307\n",
      "25 point polygon\n",
      "Elapsed time using Delaunay: 0.003525\n",
      "Elapsed time using my method: 0.028974\n",
      "26 point polygon\n",
      "Elapsed time using Delaunay: 0.003791\n",
      "Elapsed time using my method: 0.036158\n",
      "27 point polygon\n",
      "Elapsed time using Delaunay: 0.004307\n",
      "Elapsed time using my method: 0.044887\n",
      "28 point polygon\n",
      "Elapsed time using Delaunay: 0.004461\n",
      "Elapsed time using my method: 0.045261\n",
      "29 point polygon\n",
      "Elapsed time using Delaunay: 0.004882\n",
      "Elapsed time using my method: 0.117225\n"
     ]
    }
   ],
   "source": [
    "time_delaunay=[]\n",
    "time_triangulation=[]\n",
    "\n",
    "\n",
    "for index,j in enumerate(range(4,30)):\n",
    "    nb_of_repetitions=100\n",
    "    print(\"{} point polygon\".format(j))\n",
    "    contour = [np.array([i[0],i[1]]) for i in get_reference_polygon(j)]\n",
    "    start=timer()\n",
    "    for i in range(nb_of_repetitions):\n",
    "        delaunay = Delaunay2d(contour)\n",
    "    stop=timer()\n",
    "    delaunay_remeshing_time=(stop-start)/nb_of_repetitions\n",
    "    print(\"Elapsed time using Delaunay: %f\"%(delaunay_remeshing_time))\n",
    "    time_delaunay.append(delaunay_remeshing_time)\n",
    "    \n",
    "    \n",
    "    contour=get_reference_polygon(j)\n",
    "    quality=qualities[index]\n",
    "    ordered_matrix=order_quality_matrix(quality,contour)\n",
    "    start=timer()\n",
    "    for i in range(nb_of_repetitions):\n",
    "        pure_triangulate(contour,ordered_matrix,recursive=False)\n",
    "    stop=timer()\n",
    "    construction_from_matrix_time=(stop-start)/nb_of_repetitions\n",
    "    print(\"Elapsed time using my method: %f\"%(construction_from_matrix_time))\n",
    "    time_triangulation.append(construction_from_matrix_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1667510b358>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x166750576d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_delaunay[:],label='Delaunay') \n",
    "#plt.plot(time_scipy_triangulation[:],label='Scipy  Delaunay')\n",
    "plt.plot(time_triangulation[:],label='Construction from matrix')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
