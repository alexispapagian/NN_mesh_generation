{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point_coordinates_regression import *\n",
    "from Deformation_gradient import *\n",
    "from math import acos,sqrt,pi\n",
    "\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to include a distance quality to a point by smapling points around a specific point\n",
    "\n",
    "polygons_initial=load_dataset('12_polygons.pkl')\n",
    "point_coordinates_initial=load_dataset('12_point_coordinates_del.pkl')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points_del.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_points=12\n",
    "polygons_reshaped,point_coordinates=reshape_data(polygons_initial,point_coordinates_initial,number_of_insertion_points,nb_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour=np.delete(polygons_reshaped[0],24).reshape(12,2)\n",
    "optimal_point=point_coordinates[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bc6b726550>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_contour(contour)\n",
    "plt.scatter(optimal_point[0],optimal_point[1],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_points=[]\n",
    "points_qualities=[]\n",
    "training_inputs=[]\n",
    "training_outputs=[]\n",
    "for i in np.linspace(0,1,10): #number of radius steps\n",
    "    points_quality=1-i\n",
    "    radius=i\n",
    "    for j in range(1,20): # number of point sampling on circle\n",
    "        angle = 1/2*j+i+pi\n",
    "        random_point = np.array([radius * np.cos(angle)+optimal_point[0],radius * np.sin(angle)+optimal_point[1]])\n",
    "        random_points.append(random_point)\n",
    "        points_qualities.append(points_quality)\n",
    "        training_inputs.append(np.hstack([contour.reshape(24),points_quality]))\n",
    "        training_outputs.append(random_point)\n",
    "random_points=np.array( random_points)\n",
    "training_inputs=np.array(training_inputs)\n",
    "training_outputs=np.array(training_outputs)\n",
    "\n",
    "points_qualities=points_qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bc9d05a8d0>"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(random_points[:,0],random_points[:,1],marker='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(training_inputs).type(torch.FloatTensor)\n",
    "y_tensor=torch.from_numpy(training_outputs.reshape(len(training_outputs),1,2)).type(torch.FloatTensor)\n",
    "x_variable=Variable(x_tensor)\n",
    "y_variable=Variable(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net=Net(x_variable.size()[1],y_variable.size()[2],nb_of_hidden_layers=1, nb_of_hidden_nodes=15,batch_normalization=True)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0.3)\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.5299932379471628\n",
      "Epoch: 10 Training Loss: 0.5245864868164063\n",
      "Epoch: 20 Training Loss: 0.5193093550832648\n",
      "Epoch: 30 Training Loss: 0.5141550164473684\n",
      "Epoch: 40 Training Loss: 0.5091319033974095\n",
      "Epoch: 50 Training Loss: 0.5042471233167146\n",
      "Epoch: 60 Training Loss: 0.4995009572882401\n",
      "Epoch: 70 Training Loss: 0.49487617894222863\n",
      "Epoch: 80 Training Loss: 0.4903793736508018\n",
      "Epoch: 90 Training Loss: 0.48599917763157896\n",
      "Epoch: 100 Training Loss: 0.4817303306178043\n",
      "Epoch: 110 Training Loss: 0.47756580553556743\n",
      "Epoch: 120 Training Loss: 0.4734987359297903\n",
      "Epoch: 130 Training Loss: 0.4695269132915296\n",
      "Epoch: 140 Training Loss: 0.46564700478001647\n",
      "Epoch: 150 Training Loss: 0.46185632002981086\n",
      "Epoch: 160 Training Loss: 0.4581630506013569\n",
      "Epoch: 170 Training Loss: 0.4545551099275288\n",
      "Epoch: 180 Training Loss: 0.4510395250822368\n",
      "Epoch: 190 Training Loss: 0.44760509290193257\n",
      "Epoch: 200 Training Loss: 0.4442664297003495\n",
      "Epoch: 210 Training Loss: 0.441008156224301\n",
      "Epoch: 220 Training Loss: 0.4378301118549548\n",
      "Epoch: 230 Training Loss: 0.4347311722604852\n",
      "Epoch: 240 Training Loss: 0.43170840614720396\n",
      "Epoch: 250 Training Loss: 0.42876434326171875\n",
      "Epoch: 260 Training Loss: 0.42588954724763567\n",
      "Epoch: 270 Training Loss: 0.42308799342105263\n",
      "Epoch: 280 Training Loss: 0.42035426089638156\n",
      "Epoch: 290 Training Loss: 0.4176901566354852\n",
      "Epoch: 300 Training Loss: 0.41509102269222864\n",
      "Epoch: 310 Training Loss: 0.4125579030890214\n",
      "Epoch: 320 Training Loss: 0.41008931210166527\n",
      "Epoch: 330 Training Loss: 0.4076842057077508\n",
      "Epoch: 340 Training Loss: 0.40534085725483143\n",
      "Epoch: 350 Training Loss: 0.40305557250976565\n",
      "Epoch: 360 Training Loss: 0.40083288895456415\n",
      "Epoch: 370 Training Loss: 0.39867461355108963\n",
      "Epoch: 380 Training Loss: 0.39657893933747945\n",
      "Epoch: 390 Training Loss: 0.3945799978155839\n",
      "Epoch: 400 Training Loss: 0.3926646985505757\n",
      "Epoch: 410 Training Loss: 0.39082007157175164\n",
      "Epoch: 420 Training Loss: 0.38904748213918583\n",
      "Epoch: 430 Training Loss: 0.38734660901521384\n",
      "Epoch: 440 Training Loss: 0.3857200221011513\n",
      "Epoch: 450 Training Loss: 0.38419526752672695\n",
      "Epoch: 460 Training Loss: 0.38279563502261515\n",
      "Epoch: 470 Training Loss: 0.38151763112921466\n",
      "Epoch: 480 Training Loss: 0.38033563714278373\n",
      "Epoch: 490 Training Loss: 0.3792104620682566\n",
      "Epoch: 500 Training Loss: 0.3781279312936883\n",
      "Epoch: 510 Training Loss: 0.3770876834267064\n",
      "Epoch: 520 Training Loss: 0.3760882728978207\n",
      "Epoch: 530 Training Loss: 0.3751299406352796\n",
      "Epoch: 540 Training Loss: 0.374210598594264\n",
      "Epoch: 550 Training Loss: 0.37333040739360607\n",
      "Epoch: 560 Training Loss: 0.3724861546566612\n",
      "Epoch: 570 Training Loss: 0.371678322239926\n",
      "Epoch: 580 Training Loss: 0.3709052638003701\n",
      "Epoch: 590 Training Loss: 0.37016549361379525\n",
      "Epoch: 600 Training Loss: 0.36945945338199015\n",
      "Epoch: 610 Training Loss: 0.36878625970137746\n",
      "Epoch: 620 Training Loss: 0.3681447480854235\n",
      "Epoch: 630 Training Loss: 0.3675309432180304\n",
      "Epoch: 640 Training Loss: 0.36694580881219163\n",
      "Epoch: 650 Training Loss: 0.36638749775133633\n",
      "Epoch: 660 Training Loss: 0.3658524764211554\n",
      "Epoch: 670 Training Loss: 0.3653429533305921\n",
      "Epoch: 680 Training Loss: 0.36485700105365954\n",
      "Epoch: 690 Training Loss: 0.36439200953433387\n",
      "Epoch: 700 Training Loss: 0.3639448467053865\n",
      "Epoch: 710 Training Loss: 0.3635146291632401\n",
      "Epoch: 720 Training Loss: 0.3630989476254112\n",
      "Epoch: 730 Training Loss: 0.36270009091025907\n",
      "Epoch: 740 Training Loss: 0.3623180991724918\n",
      "Epoch: 750 Training Loss: 0.36195321334035774\n",
      "Epoch: 760 Training Loss: 0.36160342567845394\n",
      "Epoch: 770 Training Loss: 0.3612686558773643\n",
      "Epoch: 780 Training Loss: 0.36094806068821955\n",
      "Epoch: 790 Training Loss: 0.36064055593390215\n",
      "Epoch: 800 Training Loss: 0.36034569991262333\n",
      "Epoch: 810 Training Loss: 0.3600626092208059\n",
      "Epoch: 820 Training Loss: 0.359790721692537\n",
      "Epoch: 830 Training Loss: 0.35952927438836346\n",
      "Epoch: 840 Training Loss: 0.35927718313116774\n",
      "Epoch: 850 Training Loss: 0.35903452823036597\n",
      "Epoch: 860 Training Loss: 0.35880022550884044\n",
      "Epoch: 870 Training Loss: 0.35857335140830593\n",
      "Epoch: 880 Training Loss: 0.358353705155222\n",
      "Epoch: 890 Training Loss: 0.35814072458367596\n",
      "Epoch: 900 Training Loss: 0.35793276335063734\n",
      "Epoch: 910 Training Loss: 0.3577309859426398\n",
      "Epoch: 920 Training Loss: 0.35753414756373353\n",
      "Epoch: 930 Training Loss: 0.35734188682154605\n",
      "Epoch: 940 Training Loss: 0.35715311953895973\n",
      "Epoch: 950 Training Loss: 0.3569680464895148\n",
      "Epoch: 960 Training Loss: 0.3567861858167146\n",
      "Epoch: 970 Training Loss: 0.3566233183208265\n",
      "Epoch: 980 Training Loss: 0.35646655434056335\n",
      "Epoch: 990 Training Loss: 0.35631352474814965\n",
      "Epoch: 1000 Training Loss: 0.3561649121736225\n",
      "Epoch: 1010 Training Loss: 0.3560198332134046\n",
      "Epoch: 1020 Training Loss: 0.35587764539216693\n",
      "Epoch: 1030 Training Loss: 0.355738669947574\n",
      "Epoch: 1040 Training Loss: 0.35560294703433387\n",
      "Epoch: 1050 Training Loss: 0.35546987433182564\n",
      "Epoch: 1060 Training Loss: 0.35533896998355263\n",
      "Epoch: 1070 Training Loss: 0.3552107158460115\n",
      "Epoch: 1080 Training Loss: 0.35508454975328946\n",
      "Epoch: 1090 Training Loss: 0.3549603110865543\n",
      "Epoch: 1100 Training Loss: 0.3548358716462788\n",
      "Epoch: 1110 Training Loss: 0.35470970555355674\n",
      "Epoch: 1120 Training Loss: 0.35458418193616364\n",
      "Epoch: 1130 Training Loss: 0.35446006373355265\n",
      "Epoch: 1140 Training Loss: 0.354330082943565\n",
      "Epoch: 1150 Training Loss: 0.35419885735762746\n",
      "Epoch: 1160 Training Loss: 0.35406775223581416\n",
      "Epoch: 1170 Training Loss: 0.3539380525287829\n",
      "Epoch: 1180 Training Loss: 0.3538089952970806\n",
      "Epoch: 1190 Training Loss: 0.3536808214689556\n",
      "Epoch: 1200 Training Loss: 0.35355349088969984\n",
      "Epoch: 1210 Training Loss: 0.3534269232498972\n",
      "Epoch: 1220 Training Loss: 0.3533007170024671\n",
      "Epoch: 1230 Training Loss: 0.35317575555098685\n",
      "Epoch: 1240 Training Loss: 0.3530511554918791\n",
      "Epoch: 1250 Training Loss: 0.3529274388363487\n",
      "Epoch: 1260 Training Loss: 0.35280484651264393\n",
      "Epoch: 1270 Training Loss: 0.3526826155813117\n",
      "Epoch: 1280 Training Loss: 0.3525609066611842\n",
      "Epoch: 1290 Training Loss: 0.35243947882401316\n",
      "Epoch: 1300 Training Loss: 0.35231869346217104\n",
      "Epoch: 1310 Training Loss: 0.35219794825503703\n",
      "Epoch: 1320 Training Loss: 0.3520774439761513\n",
      "Epoch: 1330 Training Loss: 0.35195734124434624\n",
      "Epoch: 1340 Training Loss: 0.35183759990491364\n",
      "Epoch: 1350 Training Loss: 0.35171781841077304\n",
      "Epoch: 1360 Training Loss: 0.3515981975354646\n",
      "Epoch: 1370 Training Loss: 0.3514789380525288\n",
      "Epoch: 1380 Training Loss: 0.35135967856959294\n",
      "Epoch: 1390 Training Loss: 0.3512409009431538\n",
      "Epoch: 1400 Training Loss: 0.35112192254317437\n",
      "Epoch: 1410 Training Loss: 0.3510031449167352\n",
      "Epoch: 1420 Training Loss: 0.3508845279091283\n",
      "Epoch: 1430 Training Loss: 0.35076595105622943\n",
      "Epoch: 1440 Training Loss: 0.35064749466745476\n",
      "Epoch: 1450 Training Loss: 0.35052899812397204\n",
      "Epoch: 1460 Training Loss: 0.3504104212710732\n",
      "Epoch: 1470 Training Loss: 0.3502920050370066\n",
      "Epoch: 1480 Training Loss: 0.3501736691123561\n",
      "Epoch: 1490 Training Loss: 0.35005529303299754\n",
      "Epoch: 1500 Training Loss: 0.34993703741776316\n",
      "Epoch: 1510 Training Loss: 0.3498185810289885\n",
      "Epoch: 1520 Training Loss: 0.34970028525904606\n",
      "Epoch: 1530 Training Loss: 0.34958138716848275\n",
      "Epoch: 1540 Training Loss: 0.3494577508223684\n",
      "Epoch: 1550 Training Loss: 0.3493330704538446\n",
      "Epoch: 1560 Training Loss: 0.3492076271458676\n",
      "Epoch: 1570 Training Loss: 0.34908226414730675\n",
      "Epoch: 1580 Training Loss: 0.34895625867341695\n",
      "Epoch: 1590 Training Loss: 0.3488303736636513\n",
      "Epoch: 1600 Training Loss: 0.34870432803505347\n",
      "Epoch: 1610 Training Loss: 0.34857768008583473\n",
      "Epoch: 1620 Training Loss: 0.34845083136307564\n",
      "Epoch: 1630 Training Loss: 0.34832394248560855\n",
      "Epoch: 1640 Training Loss: 0.3481963308233964\n",
      "Epoch: 1650 Training Loss: 0.34806839792351973\n",
      "Epoch: 1660 Training Loss: 0.3479399430124383\n",
      "Epoch: 1670 Training Loss: 0.34781088578073605\n",
      "Epoch: 1680 Training Loss: 0.3476812663831209\n",
      "Epoch: 1690 Training Loss: 0.34755096435546873\n",
      "Epoch: 1700 Training Loss: 0.3474210638748972\n",
      "Epoch: 1710 Training Loss: 0.34729582134046055\n",
      "Epoch: 1720 Training Loss: 0.34717090004368834\n",
      "Epoch: 1730 Training Loss: 0.34704613936574835\n",
      "Epoch: 1740 Training Loss: 0.3469207763671875\n",
      "Epoch: 1750 Training Loss: 0.3467928434673109\n",
      "Epoch: 1760 Training Loss: 0.34665788349352383\n",
      "Epoch: 1770 Training Loss: 0.34652043392783716\n",
      "Epoch: 1780 Training Loss: 0.3463814183285362\n",
      "Epoch: 1790 Training Loss: 0.34626083374023436\n",
      "Epoch: 1800 Training Loss: 0.3461460314298931\n",
      "Epoch: 1810 Training Loss: 0.34602946231239723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1820 Training Loss: 0.34591068468595804\n",
      "Epoch: 1830 Training Loss: 0.34579174644068666\n",
      "Epoch: 1840 Training Loss: 0.3456723263389186\n",
      "Epoch: 1850 Training Loss: 0.34555298654656663\n",
      "Epoch: 1860 Training Loss: 0.3454332853618421\n",
      "Epoch: 1870 Training Loss: 0.34531374479594984\n",
      "Epoch: 1880 Training Loss: 0.34519380268297695\n",
      "Epoch: 1890 Training Loss: 0.34507374010587993\n",
      "Epoch: 1900 Training Loss: 0.34495355706465874\n",
      "Epoch: 1910 Training Loss: 0.3448332937140214\n",
      "Epoch: 1920 Training Loss: 0.3447115044844778\n",
      "Epoch: 1930 Training Loss: 0.34458762721011515\n",
      "Epoch: 1940 Training Loss: 0.34446326807925576\n",
      "Epoch: 1950 Training Loss: 0.34433862786543995\n",
      "Epoch: 1960 Training Loss: 0.3442134656404194\n",
      "Epoch: 1970 Training Loss: 0.3440883435701069\n",
      "Epoch: 1980 Training Loss: 0.34396209716796877\n",
      "Epoch: 1990 Training Loss: 0.34383275885331005\n",
      "Epoch: 2000 Training Loss: 0.3437024568256579\n",
      "Epoch: 2010 Training Loss: 0.34357115093030427\n",
      "Epoch: 2020 Training Loss: 0.34343928286903785\n",
      "Epoch: 2030 Training Loss: 0.3433065314041941\n",
      "Epoch: 2040 Training Loss: 0.34317329808285363\n",
      "Epoch: 2050 Training Loss: 0.3430390608938117\n",
      "Epoch: 2060 Training Loss: 0.34290430169356495\n",
      "Epoch: 2070 Training Loss: 0.342768699244449\n",
      "Epoch: 2080 Training Loss: 0.3426325346294202\n",
      "Epoch: 2090 Training Loss: 0.34248231586657074\n",
      "Epoch: 2100 Training Loss: 0.3423183240388569\n",
      "Epoch: 2110 Training Loss: 0.3421531275699013\n",
      "Epoch: 2120 Training Loss: 0.3419892963610197\n",
      "Epoch: 2130 Training Loss: 0.34182723195929277\n",
      "Epoch: 2140 Training Loss: 0.3416674162212171\n",
      "Epoch: 2150 Training Loss: 0.3415017780504729\n",
      "Epoch: 2160 Training Loss: 0.34134003488641035\n",
      "Epoch: 2170 Training Loss: 0.3411844353926809\n",
      "Epoch: 2180 Training Loss: 0.3410355818899054\n",
      "Epoch: 2190 Training Loss: 0.34091949462890625\n",
      "Epoch: 2200 Training Loss: 0.34080778423108554\n",
      "Epoch: 2210 Training Loss: 0.34070233796772204\n",
      "Epoch: 2220 Training Loss: 0.34060134887695315\n",
      "Epoch: 2230 Training Loss: 0.3405032509251645\n",
      "Epoch: 2240 Training Loss: 0.340405956067537\n",
      "Epoch: 2250 Training Loss: 0.3403050472861842\n",
      "Epoch: 2260 Training Loss: 0.3402035361842105\n",
      "Epoch: 2270 Training Loss: 0.34010242662931744\n",
      "Epoch: 2280 Training Loss: 0.33999846609015216\n",
      "Epoch: 2290 Training Loss: 0.33987671701531663\n",
      "Epoch: 2300 Training Loss: 0.3397574173776727\n",
      "Epoch: 2310 Training Loss: 0.33963735480057566\n",
      "Epoch: 2320 Training Loss: 0.3395086589612459\n",
      "Epoch: 2330 Training Loss: 0.3393735785233347\n",
      "Epoch: 2340 Training Loss: 0.33924821552477386\n",
      "Epoch: 2350 Training Loss: 0.3391347383197985\n",
      "Epoch: 2360 Training Loss: 0.33902828818873354\n",
      "Epoch: 2370 Training Loss: 0.3389253716719778\n",
      "Epoch: 2380 Training Loss: 0.3388277555766859\n",
      "Epoch: 2390 Training Loss: 0.3387356808311061\n",
      "Epoch: 2400 Training Loss: 0.3386448107267681\n",
      "Epoch: 2410 Training Loss: 0.3385550247995477\n",
      "Epoch: 2420 Training Loss: 0.3384788914730674\n",
      "Epoch: 2430 Training Loss: 0.33840580990439967\n",
      "Epoch: 2440 Training Loss: 0.33833144338507404\n",
      "Epoch: 2450 Training Loss: 0.3382581208881579\n",
      "Epoch: 2460 Training Loss: 0.33818596287777547\n",
      "Epoch: 2470 Training Loss: 0.33811512997275905\n",
      "Epoch: 2480 Training Loss: 0.3380457827919408\n",
      "Epoch: 2490 Training Loss: 0.33797792133532073\n",
      "Epoch: 2500 Training Loss: 0.33791226838764393\n",
      "Epoch: 2510 Training Loss: 0.33785091199372946\n",
      "Epoch: 2520 Training Loss: 0.337791603489926\n",
      "Epoch: 2530 Training Loss: 0.3377337004009046\n",
      "Epoch: 2540 Training Loss: 0.3376770822625411\n",
      "Epoch: 2550 Training Loss: 0.33762712980571546\n",
      "Epoch: 2560 Training Loss: 0.3375809518914474\n",
      "Epoch: 2570 Training Loss: 0.33753577784488076\n",
      "Epoch: 2580 Training Loss: 0.3374910053453947\n",
      "Epoch: 2590 Training Loss: 0.3374468351665296\n",
      "Epoch: 2600 Training Loss: 0.33740286576120476\n",
      "Epoch: 2610 Training Loss: 0.3373592577482525\n",
      "Epoch: 2620 Training Loss: 0.3373158906635485\n",
      "Epoch: 2630 Training Loss: 0.337272804661801\n",
      "Epoch: 2640 Training Loss: 0.33723020051655017\n",
      "Epoch: 2650 Training Loss: 0.3371879577636719\n",
      "Epoch: 2660 Training Loss: 0.33714623702199836\n",
      "Epoch: 2670 Training Loss: 0.337105319374486\n",
      "Epoch: 2680 Training Loss: 0.3370653654399671\n",
      "Epoch: 2690 Training Loss: 0.3370262547543174\n",
      "Epoch: 2700 Training Loss: 0.336987987317537\n",
      "Epoch: 2710 Training Loss: 0.33695068359375\n",
      "Epoch: 2720 Training Loss: 0.3369144238923725\n",
      "Epoch: 2730 Training Loss: 0.33687916805869655\n",
      "Epoch: 2740 Training Loss: 0.336844875938014\n",
      "Epoch: 2750 Training Loss: 0.33681168807180306\n",
      "Epoch: 2760 Training Loss: 0.3367795241506476\n",
      "Epoch: 2770 Training Loss: 0.3367484444066098\n",
      "Epoch: 2780 Training Loss: 0.3367184086849815\n",
      "Epoch: 2790 Training Loss: 0.3366895173725329\n",
      "Epoch: 2800 Training Loss: 0.33666169015984787\n",
      "Epoch: 2810 Training Loss: 0.33663476642809415\n",
      "Epoch: 2820 Training Loss: 0.33660904733758223\n",
      "Epoch: 2830 Training Loss: 0.33658427188270973\n",
      "Epoch: 2840 Training Loss: 0.33656048021818463\n",
      "Epoch: 2850 Training Loss: 0.33653765226665294\n",
      "Epoch: 2860 Training Loss: 0.33651580810546877\n",
      "Epoch: 2870 Training Loss: 0.3364948875025699\n",
      "Epoch: 2880 Training Loss: 0.33647485030324836\n",
      "Epoch: 2890 Training Loss: 0.3364556965075041\n",
      "Epoch: 2900 Training Loss: 0.3364373658832751\n",
      "Epoch: 2910 Training Loss: 0.3364198383532072\n",
      "Epoch: 2920 Training Loss: 0.3364031339946546\n",
      "Epoch: 2930 Training Loss: 0.3363872126529091\n",
      "Epoch: 2940 Training Loss: 0.3363720140959087\n",
      "Epoch: 2950 Training Loss: 0.33635749816894533\n",
      "Epoch: 2960 Training Loss: 0.3363436447946649\n",
      "Epoch: 2970 Training Loss: 0.3363304740504215\n",
      "Epoch: 2980 Training Loss: 0.3363179056267989\n",
      "Epoch: 2990 Training Loss: 0.3363059194464433\n",
      "Epoch: 3000 Training Loss: 0.33629453558670847\n",
      "Epoch: 3010 Training Loss: 0.33628367373817847\n",
      "Epoch: 3020 Training Loss: 0.3362733138234992\n",
      "Epoch: 3030 Training Loss: 0.33626345584267064\n",
      "Epoch: 3040 Training Loss: 0.3362540997956928\n",
      "Epoch: 3050 Training Loss: 0.33624514529579563\n",
      "Epoch: 3060 Training Loss: 0.3362366525750411\n",
      "Epoch: 3070 Training Loss: 0.3362285814787212\n",
      "Epoch: 3080 Training Loss: 0.33622087177477383\n",
      "Epoch: 3090 Training Loss: 0.336213523463199\n",
      "Epoch: 3100 Training Loss: 0.33620655662135074\n",
      "Epoch: 3110 Training Loss: 0.3361998909398129\n",
      "Epoch: 3120 Training Loss: 0.3361935264185855\n",
      "Epoch: 3130 Training Loss: 0.33618750321237667\n",
      "Epoch: 3140 Training Loss: 0.3361817410117702\n",
      "Epoch: 3150 Training Loss: 0.336176199662058\n",
      "Epoch: 3160 Training Loss: 0.33617093939530224\n",
      "Epoch: 3170 Training Loss: 0.33616594013414886\n",
      "Epoch: 3180 Training Loss: 0.33616112156918176\n",
      "Epoch: 3190 Training Loss: 0.33615658408717103\n",
      "Epoch: 3200 Training Loss: 0.3361521871466386\n",
      "Epoch: 3210 Training Loss: 0.33614799097964637\n",
      "Epoch: 3220 Training Loss: 0.33614397550884045\n",
      "Epoch: 3230 Training Loss: 0.33614016081157483\n",
      "Epoch: 3240 Training Loss: 0.33613648665578744\n",
      "Epoch: 3250 Training Loss: 0.3361329731188322\n",
      "Epoch: 3260 Training Loss: 0.3361295198139391\n",
      "Epoch: 3270 Training Loss: 0.33612498233192845\n",
      "Epoch: 3280 Training Loss: 0.3361206657008121\n",
      "Epoch: 3290 Training Loss: 0.33611636914704973\n",
      "Epoch: 3300 Training Loss: 0.33611229344418175\n",
      "Epoch: 3310 Training Loss: 0.33610845866956207\n",
      "Epoch: 3320 Training Loss: 0.3361048648231908\n",
      "Epoch: 3330 Training Loss: 0.3361014115182977\n",
      "Epoch: 3340 Training Loss: 0.3360981589869449\n",
      "Epoch: 3350 Training Loss: 0.3360950670744243\n",
      "Epoch: 3360 Training Loss: 0.33609213578073605\n",
      "Epoch: 3370 Training Loss: 0.33608938518323395\n",
      "Epoch: 3380 Training Loss: 0.3360867550498561\n",
      "Epoch: 3390 Training Loss: 0.33608420522589433\n",
      "Epoch: 3400 Training Loss: 0.3360818160207648\n",
      "Epoch: 3410 Training Loss: 0.33607954727975947\n",
      "Epoch: 3420 Training Loss: 0.33607735884817025\n",
      "Epoch: 3430 Training Loss: 0.3360752306486431\n",
      "Epoch: 3440 Training Loss: 0.3360732630679482\n",
      "Epoch: 3450 Training Loss: 0.3360713757966694\n",
      "Epoch: 3460 Training Loss: 0.3360695286800987\n",
      "Epoch: 3470 Training Loss: 0.33606774179559007\n",
      "Epoch: 3480 Training Loss: 0.3360660753752056\n",
      "Epoch: 3490 Training Loss: 0.33606442903217515\n",
      "Epoch: 3500 Training Loss: 0.33606266222502057\n",
      "Epoch: 3510 Training Loss: 0.33606041356136923\n",
      "Epoch: 3520 Training Loss: 0.3360581247430099\n",
      "Epoch: 3530 Training Loss: 0.3360558961567126\n",
      "Epoch: 3540 Training Loss: 0.33605378803453945\n",
      "Epoch: 3550 Training Loss: 0.3360517802991365\n",
      "Epoch: 3560 Training Loss: 0.33604983279579564\n",
      "Epoch: 3570 Training Loss: 0.33604800575657895\n",
      "Epoch: 3580 Training Loss: 0.3360462188720703\n",
      "Epoch: 3590 Training Loss: 0.33604453237433185\n",
      "Epoch: 3600 Training Loss: 0.33604290610865545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3610 Training Loss: 0.3360413400750411\n",
      "Epoch: 3620 Training Loss: 0.33603985435084294\n",
      "Epoch: 3630 Training Loss: 0.33603840878135277\n",
      "Epoch: 3640 Training Loss: 0.3360370033665707\n",
      "Epoch: 3650 Training Loss: 0.3360356381064967\n",
      "Epoch: 3660 Training Loss: 0.33603431300113074\n",
      "Epoch: 3670 Training Loss: 0.33603302805047286\n",
      "Epoch: 3680 Training Loss: 0.3360318234092311\n",
      "Epoch: 3690 Training Loss: 0.3360306187679893\n",
      "Epoch: 3700 Training Loss: 0.3360294141267475\n",
      "Epoch: 3710 Training Loss: 0.3360282897949219\n",
      "Epoch: 3720 Training Loss: 0.33602718554045025\n",
      "Epoch: 3730 Training Loss: 0.33602604113127055\n",
      "Epoch: 3740 Training Loss: 0.3360250372635691\n",
      "Epoch: 3750 Training Loss: 0.33602401331851356\n",
      "Epoch: 3760 Training Loss: 0.33602296929610403\n",
      "Epoch: 3770 Training Loss: 0.33602196542840257\n",
      "Epoch: 3780 Training Loss: 0.33602102179276316\n",
      "Epoch: 3790 Training Loss: 0.3360200380024157\n",
      "Epoch: 3800 Training Loss: 0.33601913452148435\n",
      "Epoch: 3810 Training Loss: 0.33601817080849095\n",
      "Epoch: 3820 Training Loss: 0.3360173476369757\n",
      "Epoch: 3830 Training Loss: 0.3360164642333984\n",
      "Epoch: 3840 Training Loss: 0.33601558082982114\n",
      "Epoch: 3850 Training Loss: 0.33601471750359785\n",
      "Epoch: 3860 Training Loss: 0.3360138943320827\n",
      "Epoch: 3870 Training Loss: 0.33601307116056744\n",
      "Epoch: 3880 Training Loss: 0.3360122881437603\n",
      "Epoch: 3890 Training Loss: 0.3360114850495991\n",
      "Epoch: 3900 Training Loss: 0.33601070203279193\n",
      "Epoch: 3910 Training Loss: 0.3360099390933388\n",
      "Epoch: 3920 Training Loss: 0.33600921630859376\n",
      "Epoch: 3930 Training Loss: 0.33600847344649465\n",
      "Epoch: 3940 Training Loss: 0.33600773058439554\n",
      "Epoch: 3950 Training Loss: 0.3360070077996505\n",
      "Epoch: 3960 Training Loss: 0.3360063050922595\n",
      "Epoch: 3970 Training Loss: 0.3360056425395765\n",
      "Epoch: 3980 Training Loss: 0.3360049599095395\n",
      "Epoch: 3990 Training Loss: 0.33600425720214844\n",
      "Epoch: 4000 Training Loss: 0.33600359464946544\n",
      "Epoch: 4010 Training Loss: 0.3360029521741365\n",
      "Epoch: 4020 Training Loss: 0.33600230969880757\n",
      "Epoch: 4030 Training Loss: 0.33600168730083263\n",
      "Epoch: 4040 Training Loss: 0.33600106490285775\n",
      "Epoch: 4050 Training Loss: 0.3360004425048828\n",
      "Epoch: 4060 Training Loss: 0.33599984018426193\n",
      "Epoch: 4070 Training Loss: 0.33599925794099506\n",
      "Epoch: 4080 Training Loss: 0.3359986556203742\n",
      "Epoch: 4090 Training Loss: 0.3359980532997533\n",
      "Epoch: 4100 Training Loss: 0.3359974911338405\n",
      "Epoch: 4110 Training Loss: 0.3359969691226357\n",
      "Epoch: 4120 Training Loss: 0.33599638687936884\n",
      "Epoch: 4130 Training Loss: 0.3359958648681641\n",
      "Epoch: 4140 Training Loss: 0.33599530270225125\n",
      "Epoch: 4150 Training Loss: 0.33599474053633843\n",
      "Epoch: 4160 Training Loss: 0.33599423860248767\n",
      "Epoch: 4170 Training Loss: 0.3359937366686369\n",
      "Epoch: 4180 Training Loss: 0.33599319458007815\n",
      "Epoch: 4190 Training Loss: 0.3359927127235814\n",
      "Epoch: 4200 Training Loss: 0.3359922107897307\n",
      "Epoch: 4210 Training Loss: 0.3359917088558799\n",
      "Epoch: 4220 Training Loss: 0.3359912671540913\n",
      "Epoch: 4230 Training Loss: 0.3359907451428865\n",
      "Epoch: 4240 Training Loss: 0.33599030344109787\n",
      "Epoch: 4250 Training Loss: 0.3359898617393092\n",
      "Epoch: 4260 Training Loss: 0.33598935980545847\n",
      "Epoch: 4270 Training Loss: 0.3359889381810238\n",
      "Epoch: 4280 Training Loss: 0.33598841616981906\n",
      "Epoch: 4290 Training Loss: 0.33598765323036595\n",
      "Epoch: 4300 Training Loss: 0.3359867296720806\n",
      "Epoch: 4310 Training Loss: 0.33598580611379525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-628-fe525e0e15dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# backpropagation, compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0] )\n",
    "nb_of_epochs=23000\n",
    "my_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bc866b7630>"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in np.linspace(0,0.5,20):\n",
    "    quality_demand=i\n",
    "    quality_label=str(quality_demand)\n",
    "    x_test=np.hstack([contour.reshape(24),quality_demand])\n",
    "    x_test_variable=Variable(torch.from_numpy(x_test).type(torch.FloatTensor))\n",
    "    x_test_variable=x_test_variable.expand(100,25)\n",
    "\n",
    "    my_net.eval()\n",
    "    predicted_point=my_net(x_test_variable.cuda())\n",
    "    predicted_point=predicted_point[0].data.cpu().numpy()\n",
    "    plt.scatter(predicted_point[0],predicted_point[1],label=quality_label)\n",
    "plot_contour(contour)\n",
    "plt.scatter(optimal_point[0],optimal_point[1],marker='x')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(1, 32, batch_first=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "Dependencies:\n",
    "torch: 0.4\n",
    "matplotlib\n",
    "numpy\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "TIME_STEP = 10     # rnn time step\n",
    "INPUT_SIZE = 1      # rnn input size\n",
    "LR = 0.02           # learning rate\n",
    "\n",
    "# show data\n",
    "steps = np.linspace(0, np.pi*2, 100, dtype=np.float32)\n",
    "x_np = np.sin(steps)    # float32 for converting torch FloatTensor\n",
    "y_np = np.cos(steps)\n",
    "plt.plot(steps, y_np, 'r-', label='target (cos)')\n",
    "plt.plot(steps, x_np, 'b-', label='input (sin)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=32,     # rnn hidden unit\n",
    "            num_layers=1,       # number of rnn layer\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "\n",
    "        outs = []    # save all predictions\n",
    "        for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        # instead, for simplicity, you can replace above codes by follows\n",
    "        # r_out = r_out.view(-1, 32)\n",
    "        # outs = self.out(r_out)\n",
    "        # return outs, h_state\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "h_state = None      # for initial hidden state\n",
    "\n",
    "plt.figure(1, figsize=(12, 5))\n",
    "plt.ion()           # continuously plot\n",
    "\n",
    "for step in range(100):\n",
    "    start, end = step * np.pi, (step+1)*np.pi   # time range\n",
    "    # use sin predicts cos\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)\n",
    "    x_np = np.sin(steps)    # float32 for converting torch FloatTensor\n",
    "    y_np = np.cos(steps)\n",
    "\n",
    "    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))    # shape (batch, time_step, input_size)\n",
    "    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))\n",
    "\n",
    "    prediction, h_state = rnn(x, h_state)   # rnn output\n",
    "    # !! next step is important !!\n",
    "    h_state = Variable(h_state.data)        # repack the hidden state, break the connection from last iteration\n",
    "\n",
    "    loss = loss_func(prediction, y)         # calculate loss\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step()                        # apply gradients\n",
    "\n",
    "    # plotting\n",
    "    plt.plot(steps, y_np.flatten(), 'r-')\n",
    "    plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n",
    "    plt.draw(); plt.pause(0.05)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-658-4e7fac089080>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-658-4e7fac089080>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cell=nn.RNN(batch_first=True,l)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "cell=nn.RNN(batch_first=True,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
